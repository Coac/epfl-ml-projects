{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from helpers import *\n",
    "from costs import *\n",
    "from gradient_descent import *\n",
    "from stochastic_gradient_descent import *\n",
    "\n",
    "random.seed(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250000, 30)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y, x, ids = load_csv_data(data_path=\"datas/train.csv\", sub_sample=False)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4262084.377439185"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = np.random.rand(x.shape[1])\n",
    "compute_loss(y, x, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30,)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient = compute_gradient(y, x, w)\n",
    "gradient.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/99): loss=9314585.010996735\t\t0.339944\n",
      "Gradient Descent(1/99): loss=4894723.352659363\t\t0.626876\n",
      "Gradient Descent(2/99): loss=2590852.2618884495\t\t0.342664\n",
      "Gradient Descent(3/99): loss=1387373.5026232044\t\t0.626976\n",
      "Gradient Descent(4/99): loss=756649.92168339\t\t0.342668\n",
      "Gradient Descent(5/99): loss=424432.64680752525\t\t0.62608\n",
      "Gradient Descent(6/99): loss=248084.73937329336\t\t0.342668\n",
      "Gradient Descent(7/99): loss=153353.304449154\t\t0.619288\n",
      "Gradient Descent(8/99): loss=101534.3357602267\t\t0.342668\n",
      "Gradient Descent(9/99): loss=72416.29336609752\t\t0.606804\n",
      "Gradient Descent(10/99): loss=55416.65611938198\t\t0.347908\n",
      "Gradient Descent(11/99): loss=44973.5278868575\t\t0.591972\n",
      "Gradient Descent(12/99): loss=38147.791193514466\t\t0.395108\n",
      "Gradient Descent(13/99): loss=33374.15027100143\t\t0.570916\n",
      "Gradient Descent(14/99): loss=29809.63999582467\t\t0.44698\n",
      "Gradient Descent(15/99): loss=26993.095931255302\t\t0.548068\n",
      "Gradient Descent(16/99): loss=24666.655425712812\t\t0.47704\n",
      "Gradient Descent(17/99): loss=22681.822679129687\t\t0.528208\n",
      "Gradient Descent(18/99): loss=20949.765880897106\t\t0.488108\n",
      "Gradient Descent(19/99): loss=19414.82162123389\t\t0.514072\n",
      "Gradient Descent(20/99): loss=18040.22711160706\t\t0.490028\n",
      "Gradient Descent(21/99): loss=16800.332557784146\t\t0.50328\n",
      "Gradient Descent(22/99): loss=15676.274697551968\t\t0.489676\n",
      "Gradient Descent(23/99): loss=14653.518442244098\t\t0.49584\n",
      "Gradient Descent(24/99): loss=13720.420722097748\t\t0.487932\n",
      "Gradient Descent(25/99): loss=12867.363586280877\t\t0.490372\n",
      "Gradient Descent(26/99): loss=12086.211316740813\t\t0.485496\n",
      "Gradient Descent(27/99): loss=11369.956857333416\t\t0.486152\n",
      "Gradient Descent(28/99): loss=10712.482226720256\t\t0.482748\n",
      "Gradient Descent(29/99): loss=10108.389847785578\t\t0.482616\n",
      "Gradient Descent(30/99): loss=9552.879529484373\t\t0.480304\n",
      "Gradient Descent(31/99): loss=9041.655847802826\t\t0.479848\n",
      "Gradient Descent(32/99): loss=8570.856430267184\t\t0.477984\n",
      "Gradient Descent(33/99): loss=8136.9950470436515\t\t0.477256\n",
      "Gradient Descent(34/99): loss=7736.91547542778\t\t0.475908\n",
      "Gradient Descent(35/99): loss=7367.75339511111\t\t0.4749\n",
      "Gradient Descent(36/99): loss=7026.904402408917\t\t0.473724\n",
      "Gradient Descent(37/99): loss=6711.996781162927\t\t0.473116\n",
      "Gradient Descent(38/99): loss=6420.86804050287\t\t0.47204\n",
      "Gradient Descent(39/99): loss=6151.544487540559\t\t0.471188\n",
      "Gradient Descent(40/99): loss=5902.223284915725\t\t0.470576\n",
      "Gradient Descent(41/99): loss=5671.256573357881\t\t0.469964\n",
      "Gradient Descent(42/99): loss=5457.1373339950405\t\t0.469364\n",
      "Gradient Descent(43/99): loss=5258.486734649002\t\t0.468736\n",
      "Gradient Descent(44/99): loss=5074.042756037754\t\t0.468236\n",
      "Gradient Descent(45/99): loss=4902.649932665619\t\t0.467832\n",
      "Gradient Descent(46/99): loss=4743.250072735346\t\t0.467448\n",
      "Gradient Descent(47/99): loss=4594.87384415858\t\t0.467196\n",
      "Gradient Descent(48/99): loss=4456.633131458482\t\t0.466952\n",
      "Gradient Descent(49/99): loss=4327.714082339661\t\t0.466364\n",
      "Gradient Descent(50/99): loss=4207.370773881264\t\t0.465992\n",
      "Gradient Descent(51/99): loss=4094.9194373720725\t\t0.465876\n",
      "Gradient Descent(52/99): loss=3989.733188252232\t\t0.465684\n",
      "Gradient Descent(53/99): loss=3891.237213824629\t\t0.465576\n",
      "Gradient Descent(54/99): loss=3798.90437662383\t\t0.465536\n",
      "Gradient Descent(55/99): loss=3712.2511957863976\t\t0.465432\n",
      "Gradient Descent(56/99): loss=3630.8341726072003\t\t0.465532\n",
      "Gradient Descent(57/99): loss=3554.2464298084\t\t0.465568\n",
      "Gradient Descent(58/99): loss=3482.1146369802314\t\t0.465696\n",
      "Gradient Descent(59/99): loss=3414.0961972442065\t\t0.465632\n",
      "Gradient Descent(60/99): loss=3349.876672493726\t\t0.465568\n",
      "Gradient Descent(61/99): loss=3289.167426626421\t\t0.465668\n",
      "Gradient Descent(62/99): loss=3231.7034680311763\t\t0.465616\n",
      "Gradient Descent(63/99): loss=3177.2414742577653\t\t0.465644\n",
      "Gradient Descent(64/99): loss=3125.5579833012557\t\t0.4658\n",
      "Gradient Descent(65/99): loss=3076.447737295457\t\t0.465932\n",
      "Gradient Descent(66/99): loss=3029.72216564542\t\t0.466044\n",
      "Gradient Descent(67/99): loss=2985.207995752051\t\t0.465988\n",
      "Gradient Descent(68/99): loss=2942.7459805036447\t\t0.46604\n",
      "Gradient Descent(69/99): loss=2902.1897326397725\t\t0.466208\n",
      "Gradient Descent(70/99): loss=2863.404656941359\t\t0.466228\n",
      "Gradient Descent(71/99): loss=2826.2669719745727\t\t0.466304\n",
      "Gradient Descent(72/99): loss=2790.662813822546\t\t0.466368\n",
      "Gradient Descent(73/99): loss=2756.4874148838094\t\t0.466456\n",
      "Gradient Descent(74/99): loss=2723.6443514055222\t\t0.46654\n",
      "Gradient Descent(75/99): loss=2692.0448539578156\t\t0.466648\n",
      "Gradient Descent(76/99): loss=2661.6071755476464\t\t0.4667\n",
      "Gradient Descent(77/99): loss=2632.2560125202685\t\t0.466852\n",
      "Gradient Descent(78/99): loss=2603.9219738077136\t\t0.467024\n",
      "Gradient Descent(79/99): loss=2576.5410944597184\t\t0.467216\n",
      "Gradient Descent(80/99): loss=2550.054389736522\t\t0.46744\n",
      "Gradient Descent(81/99): loss=2524.407446357539\t\t0.467384\n",
      "Gradient Descent(82/99): loss=2499.5500477877704\t\t0.467512\n",
      "Gradient Descent(83/99): loss=2475.4358307071134\t\t0.467824\n",
      "Gradient Descent(84/99): loss=2452.0219700486577\t\t0.46792\n",
      "Gradient Descent(85/99): loss=2429.268890212528\t\t0.4681\n",
      "Gradient Descent(86/99): loss=2407.140000263568\t\t0.468308\n",
      "Gradient Descent(87/99): loss=2385.6014511058024\t\t0.468552\n",
      "Gradient Descent(88/99): loss=2364.6219127955815\t\t0.46878\n",
      "Gradient Descent(89/99): loss=2344.1723703099774\t\t0.468956\n",
      "Gradient Descent(90/99): loss=2324.225936228591\t\t0.4692\n",
      "Gradient Descent(91/99): loss=2304.757678916485\t\t0.4694\n",
      "Gradient Descent(92/99): loss=2285.744464914645\t\t0.469484\n",
      "Gradient Descent(93/99): loss=2267.164814352926\t\t0.469548\n",
      "Gradient Descent(94/99): loss=2248.9987682999204\t\t0.469624\n",
      "Gradient Descent(95/99): loss=2231.2277670552103\t\t0.469864\n",
      "Gradient Descent(96/99): loss=2213.83453847284\t\t0.47004\n",
      "Gradient Descent(97/99): loss=2196.802995481176\t\t0.47014\n",
      "Gradient Descent(98/99): loss=2180.118142034204\t\t0.470328\n",
      "Gradient Descent(99/99): loss=2163.7659867934035\t\t0.470496\n"
     ]
    }
   ],
   "source": [
    "w_init = np.random.rand(x.shape[1])\n",
    "w, loss = least_squares_GD(y, x, w_init, max_iters=100, gamma=0.0000003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w, loss = least_squares_SGD(y, x, w, 10, max_iters=100, gamma=0.0000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.470496"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_accuracy(x, y, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test, x_test, ids_test = load_csv_data(data_path=\"datas/test.csv\", sub_sample=False)\n",
    "pred_y = predict_labels(w, x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done !\n"
     ]
    }
   ],
   "source": [
    "create_csv_submission(ids_test, pred_y, \"datas/submission.csv\")\n",
    "print('Done !')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
