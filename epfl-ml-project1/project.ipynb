{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from helpers import *\n",
    "from costs import *\n",
    "from gradient_descent import *\n",
    "from stochastic_gradient_descent import *\n",
    "\n",
    "random.seed(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "y, x, ids = load_csv_data(data_path=\"datas/train.csv\", sub_sample=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set to 0 all column containing -999\n",
    "def remove_NaN(x):\n",
    "    columns_with_NaN = set(\"\")\n",
    "    for row in x:\n",
    "        for i,feature in enumerate(row):\n",
    "            if feature == -999:\n",
    "                columns_with_NaN.add(i)\n",
    "\n",
    "    for col in columns_with_NaN:\n",
    "        x[:, col] = 0\n",
    "        \n",
    "    print(\"Cleaned \" + str(len(columns_with_NaN)) + \" columns\")\n",
    "        \n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.46141372,  0.06833197,  0.40768027, ...,  1.5668    ,\n",
       "         1.55858439,  0.4125105 ],\n",
       "       [ 0.51670419,  0.55250482,  0.54013641, ..., -0.63936657,\n",
       "        -0.63936694, -0.27381996],\n",
       "       [-2.33785898,  3.19515553,  1.09655998, ..., -0.63936657,\n",
       "        -0.63936694, -0.29396985],\n",
       "       ..., \n",
       "       [ 0.38016991,  0.31931645, -0.13086367, ..., -0.63936657,\n",
       "        -0.63936694, -0.31701723],\n",
       "       [ 0.35431502, -0.84532397, -0.30297338, ..., -0.63936657,\n",
       "        -0.63936694, -0.74543941],\n",
       "       [-2.33785898,  0.66533608, -0.25352276, ..., -0.63936657,\n",
       "        -0.63936694, -0.74543941]])"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def normalize(x):\n",
    "    return (x - x.mean(axis=0)) / (x.std(axis=0) + 0.0000000001)\n",
    "\n",
    "\n",
    "def preprocess_data(x):\n",
    "    return normalize(remove_NaN(x))\n",
    "x = preprocess_data(x)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200000, 30)\n",
      "(200000,)\n",
      "(50000, 30)\n",
      "(50000,)\n"
     ]
    }
   ],
   "source": [
    "def separate_set(x, y):\n",
    "    x_and_y = np.concatenate((y.reshape((y.shape[0], 1)), x), axis=1)\n",
    "    np.random.shuffle(x_and_y)\n",
    "    \n",
    "    count = x_and_y.shape[0]\n",
    "    last_train_index = int(count * 0.8)\n",
    "    \n",
    "    train_set = x_and_y[0:last_train_index, :]\n",
    "    test_set = x_and_y[last_train_index:, :]\n",
    "    \n",
    "    train_y = train_set[:, 0]\n",
    "    test_y = test_set[:, 0]\n",
    "\n",
    "    train_x = train_set[:, 1:]\n",
    "    test_x = test_set[:, 1:]\n",
    "\n",
    "    return train_x, train_y, test_x, test_y\n",
    "\n",
    "train_x, train_y, test_x, test_y = separate_set(x, y)\n",
    "\n",
    "print(train_x.shape)\n",
    "print(train_y.shape)\n",
    "print(test_x.shape)\n",
    "print(test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/99): loss=4.6864009908428566\t\t0.467105\n",
      "Gradient Descent(1/99): loss=2.3369461357483683\t\t0.622535\n",
      "Gradient Descent(2/99): loss=1.3538534692832582\t\t0.52797\n",
      "Gradient Descent(3/99): loss=0.8906455452879156\t\t0.63975\n",
      "Gradient Descent(4/99): loss=0.6622724796872997\t\t0.603625\n",
      "Gradient Descent(5/99): loss=0.5466720801523842\t\t0.65679\n",
      "Gradient Descent(6/99): loss=0.4867159094753799\t\t0.66169\n",
      "Gradient Descent(7/99): loss=0.4547311526035819\t\t0.67663\n",
      "Gradient Descent(8/99): loss=0.43706234546071376\t\t0.69138\n",
      "Gradient Descent(9/99): loss=0.4268733744001167\t\t0.69199\n",
      "Gradient Descent(10/99): loss=0.42069288013178047\t\t0.70096\n",
      "Gradient Descent(11/99): loss=0.4167293779626533\t\t0.69936\n",
      "Gradient Descent(12/99): loss=0.41403996175656876\t\t0.70345\n",
      "Gradient Descent(13/99): loss=0.412116046228937\t\t0.702995\n",
      "Gradient Descent(14/99): loss=0.4106748804593888\t\t0.70499\n",
      "Gradient Descent(15/99): loss=0.4095534706743221\t\t0.70475\n",
      "Gradient Descent(16/99): loss=0.40865390475385144\t\t0.70578\n",
      "Gradient Descent(17/99): loss=0.40791471531672546\t\t0.705945\n",
      "Gradient Descent(18/99): loss=0.4072955762162674\t\t0.70663\n",
      "Gradient Descent(19/99): loss=0.4067689175910257\t\t0.706665\n",
      "Gradient Descent(20/99): loss=0.40631519099438823\t\t0.70699\n",
      "Gradient Descent(21/99): loss=0.4059201007634566\t\t0.706915\n",
      "Gradient Descent(22/99): loss=0.40557292209820106\t\t0.70699\n",
      "Gradient Descent(23/99): loss=0.40526543838279694\t\t0.706815\n",
      "Gradient Descent(24/99): loss=0.40499124383735774\t\t0.7071\n",
      "Gradient Descent(25/99): loss=0.4047452698880564\t\t0.707285\n",
      "Gradient Descent(26/99): loss=0.4045234537876675\t\t0.707445\n",
      "Gradient Descent(27/99): loss=0.4043225009681414\t\t0.707515\n",
      "Gradient Descent(28/99): loss=0.4041397111568378\t\t0.70756\n",
      "Gradient Descent(29/99): loss=0.403972849065084\t\t0.70771\n",
      "Gradient Descent(30/99): loss=0.4038200469384272\t\t0.707705\n",
      "Gradient Descent(31/99): loss=0.40367973029650867\t\t0.707795\n",
      "Gradient Descent(32/99): loss=0.4035505607959951\t\t0.70783\n",
      "Gradient Descent(33/99): loss=0.4034313918853003\t\t0.707785\n",
      "Gradient Descent(34/99): loss=0.4033212341080428\t\t0.70778\n",
      "Gradient Descent(35/99): loss=0.4032192277447869\t\t0.70787\n",
      "Gradient Descent(36/99): loss=0.4031246210770328\t\t0.707915\n",
      "Gradient Descent(37/99): loss=0.4030367529881214\t\t0.70803\n",
      "Gradient Descent(38/99): loss=0.40295503893143686\t\t0.708015\n",
      "Gradient Descent(39/99): loss=0.4028789595298775\t\t0.708015\n",
      "Gradient Descent(40/99): loss=0.4028080512446769\t\t0.70803\n",
      "Gradient Descent(41/99): loss=0.40274189868223875\t\t0.70802\n",
      "Gradient Descent(42/99): loss=0.4026801282061133\t\t0.708095\n",
      "Gradient Descent(43/99): loss=0.4026224025958484\t\t0.708145\n",
      "Gradient Descent(44/99): loss=0.4025684165512277\t\t0.708155\n",
      "Gradient Descent(45/99): loss=0.40251789288382206\t\t0.708115\n",
      "Gradient Descent(46/99): loss=0.40247057927110974\t\t0.70812\n",
      "Gradient Descent(47/99): loss=0.4024262454741353\t\t0.708095\n",
      "Gradient Descent(48/99): loss=0.4023846809396044\t\t0.708105\n",
      "Gradient Descent(49/99): loss=0.4023456927228345\t\t0.708045\n",
      "Gradient Descent(50/99): loss=0.4023091036801273\t\t0.707975\n",
      "Gradient Descent(51/99): loss=0.4022747508886963\t\t0.707975\n",
      "Gradient Descent(52/99): loss=0.402242484259853\t\t0.70796\n",
      "Gradient Descent(53/99): loss=0.40221216531717835\t\t0.707925\n",
      "Gradient Descent(54/99): loss=0.4021836661162349\t\t0.70793\n",
      "Gradient Descent(55/99): loss=0.4021568682862635\t\t0.707915\n",
      "Gradient Descent(56/99): loss=0.4021316621774583\t\t0.70787\n",
      "Gradient Descent(57/99): loss=0.40210794609998973\t\t0.707855\n",
      "Gradient Descent(58/99): loss=0.402085625643045\t\t0.70787\n",
      "Gradient Descent(59/99): loss=0.4020646130639082\t\t0.70792\n",
      "Gradient Descent(60/99): loss=0.4020448267385357\t\t0.70786\n",
      "Gradient Descent(61/99): loss=0.4020261906662937\t\t0.707885\n",
      "Gradient Descent(62/99): loss=0.4020086340225362\t\t0.70782\n",
      "Gradient Descent(63/99): loss=0.40199209075355125\t\t0.70783\n",
      "Gradient Descent(64/99): loss=0.4019764992091243\t\t0.707815\n",
      "Gradient Descent(65/99): loss=0.4019618018085872\t\t0.70777\n",
      "Gradient Descent(66/99): loss=0.401947944736738\t\t0.70777\n",
      "Gradient Descent(67/99): loss=0.40193487766647007\t\t0.70774\n",
      "Gradient Descent(68/99): loss=0.4019225535053352\t\t0.70769\n",
      "Gradient Descent(69/99): loss=0.40191092816359575\t\t0.70766\n",
      "Gradient Descent(70/99): loss=0.4018999603416134\t\t0.70764\n",
      "Gradient Descent(71/99): loss=0.401889611334667\t\t0.7076\n",
      "Gradient Descent(72/99): loss=0.4018798448535143\t\t0.70759\n",
      "Gradient Descent(73/99): loss=0.40187062685920094\t\t0.70761\n",
      "Gradient Descent(74/99): loss=0.40186192541077914\t\t0.707625\n",
      "Gradient Descent(75/99): loss=0.40185371052475705\t\t0.70759\n",
      "Gradient Descent(76/99): loss=0.40184595404521345\t\t0.707585\n",
      "Gradient Descent(77/99): loss=0.4018386295236311\t\t0.707575\n",
      "Gradient Descent(78/99): loss=0.4018317121076015\t\t0.707515\n",
      "Gradient Descent(79/99): loss=0.40182517843763627\t\t0.70747\n",
      "Gradient Descent(80/99): loss=0.40181900655140096\t\t0.707445\n",
      "Gradient Descent(81/99): loss=0.40181317579475306\t\t0.70743\n",
      "Gradient Descent(82/99): loss=0.40180766673902735\t\t0.707395\n",
      "Gradient Descent(83/99): loss=0.4018024611040642\t\t0.70741\n",
      "Gradient Descent(84/99): loss=0.40179754168652476\t\t0.707425\n",
      "Gradient Descent(85/99): loss=0.40179289229308085\t\t0.707365\n",
      "Gradient Descent(86/99): loss=0.4017884976780998\t\t0.70735\n",
      "Gradient Descent(87/99): loss=0.40178434348548747\t\t0.70733\n",
      "Gradient Descent(88/99): loss=0.4017804161943725\t\t0.707325\n",
      "Gradient Descent(89/99): loss=0.40177670306835195\t\t0.707315\n",
      "Gradient Descent(90/99): loss=0.40177319210803486\t\t0.707295\n",
      "Gradient Descent(91/99): loss=0.4017698720066465\t\t0.70729\n",
      "Gradient Descent(92/99): loss=0.40176673210847746\t\t0.707275\n",
      "Gradient Descent(93/99): loss=0.4017637623699722\t\t0.707285\n",
      "Gradient Descent(94/99): loss=0.4017609533232787\t\t0.70727\n",
      "Gradient Descent(95/99): loss=0.4017582960420846\t\t0.707245\n",
      "Gradient Descent(96/99): loss=0.4017557821095872\t\t0.707265\n",
      "Gradient Descent(97/99): loss=0.40175340358845074\t\t0.70727\n",
      "Gradient Descent(98/99): loss=0.4017511529926197\t\t0.70726\n",
      "Gradient Descent(99/99): loss=0.4017490232608623\t\t0.70727\n"
     ]
    }
   ],
   "source": [
    "w_init = np.random.rand(x.shape[1])\n",
    "w, loss = least_squares_GD(train_y, train_x, w_init, max_iters=100, gamma=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD (0/99): loss=9.491306913659226\t\t0.57564\n",
      "SGD (1/99): loss=8.834314810848172\t\t0.571265\n",
      "SGD (2/99): loss=7.816103257852662\t\t0.57043\n",
      "SGD (3/99): loss=7.823711805249579\t\t0.57096\n",
      "SGD (4/99): loss=7.218297244495725\t\t0.570445\n",
      "SGD (5/99): loss=7.305700470608432\t\t0.574485\n",
      "SGD (6/99): loss=7.264329729422425\t\t0.5738\n",
      "SGD (7/99): loss=7.006150239762559\t\t0.57053\n",
      "SGD (8/99): loss=6.960262243015965\t\t0.57068\n",
      "SGD (9/99): loss=5.802231588486342\t\t0.55542\n",
      "SGD (10/99): loss=5.824283840074395\t\t0.555815\n",
      "SGD (11/99): loss=2.1752018422353223\t\t0.51689\n",
      "SGD (12/99): loss=2.172737107948579\t\t0.51491\n",
      "SGD (13/99): loss=2.1742030083267343\t\t0.515055\n",
      "SGD (14/99): loss=2.2248484937244957\t\t0.501445\n",
      "SGD (15/99): loss=2.226512379714472\t\t0.501965\n",
      "SGD (16/99): loss=2.2126747616654177\t\t0.50305\n",
      "SGD (17/99): loss=2.14389957215831\t\t0.50632\n",
      "SGD (18/99): loss=2.0235792541988276\t\t0.509495\n",
      "SGD (19/99): loss=2.006647473992341\t\t0.513785\n",
      "SGD (20/99): loss=2.0176038854358604\t\t0.511625\n",
      "SGD (21/99): loss=2.012635899832488\t\t0.512605\n",
      "SGD (22/99): loss=1.998425376428446\t\t0.513975\n",
      "SGD (23/99): loss=1.8310729004396733\t\t0.50621\n",
      "SGD (24/99): loss=1.8265765821434747\t\t0.505855\n",
      "SGD (25/99): loss=1.74779028472411\t\t0.504865\n",
      "SGD (26/99): loss=1.75505112540094\t\t0.506165\n",
      "SGD (27/99): loss=1.7541179006421348\t\t0.504535\n",
      "SGD (28/99): loss=1.7084498275554687\t\t0.508335\n",
      "SGD (29/99): loss=1.7086038314834773\t\t0.50734\n",
      "SGD (30/99): loss=1.6363621647065851\t\t0.50802\n",
      "SGD (31/99): loss=1.6282819381987068\t\t0.502495\n",
      "SGD (32/99): loss=1.551238849497806\t\t0.5121\n",
      "SGD (33/99): loss=1.5531508793018676\t\t0.51162\n",
      "SGD (34/99): loss=1.5548230516735144\t\t0.51208\n",
      "SGD (35/99): loss=1.5540831053248383\t\t0.51215\n",
      "SGD (36/99): loss=1.5237759840392064\t\t0.520235\n",
      "SGD (37/99): loss=1.5204919984441416\t\t0.52159\n",
      "SGD (38/99): loss=1.5037437603108212\t\t0.526875\n",
      "SGD (39/99): loss=1.4423360442164068\t\t0.52925\n",
      "SGD (40/99): loss=1.4426642069390363\t\t0.5292\n",
      "SGD (41/99): loss=1.4334814312722743\t\t0.527685\n",
      "SGD (42/99): loss=1.4329153321594446\t\t0.532055\n",
      "SGD (43/99): loss=1.4281409691800473\t\t0.53553\n",
      "SGD (44/99): loss=1.4304949541104934\t\t0.53758\n",
      "SGD (45/99): loss=1.3110038561187374\t\t0.545035\n",
      "SGD (46/99): loss=1.3244844312117148\t\t0.548855\n",
      "SGD (47/99): loss=1.3445177451198704\t\t0.517115\n",
      "SGD (48/99): loss=1.3119433729801138\t\t0.52839\n",
      "SGD (49/99): loss=1.3182194175107123\t\t0.52752\n",
      "SGD (50/99): loss=1.3056232414972329\t\t0.529185\n",
      "SGD (51/99): loss=1.3056444394321212\t\t0.52916\n",
      "SGD (52/99): loss=1.3211238518824975\t\t0.52562\n",
      "SGD (53/99): loss=1.3117254704040933\t\t0.528515\n",
      "SGD (54/99): loss=1.30566507594532\t\t0.528025\n",
      "SGD (55/99): loss=1.2994137019892704\t\t0.532885\n",
      "SGD (56/99): loss=1.2454492517229079\t\t0.53851\n",
      "SGD (57/99): loss=1.2452519815731258\t\t0.538445\n",
      "SGD (58/99): loss=1.244704138389013\t\t0.53832\n",
      "SGD (59/99): loss=1.2359636241395089\t\t0.537085\n",
      "SGD (60/99): loss=1.2388026741471188\t\t0.53711\n",
      "SGD (61/99): loss=1.2614385160751387\t\t0.53453\n",
      "SGD (62/99): loss=1.2673376408307444\t\t0.53746\n",
      "SGD (63/99): loss=1.2484295808698234\t\t0.53484\n",
      "SGD (64/99): loss=1.2488495276100928\t\t0.53734\n",
      "SGD (65/99): loss=1.2449155386869\t\t0.536445\n",
      "SGD (66/99): loss=1.2293901187292737\t\t0.53444\n",
      "SGD (67/99): loss=1.228415653246827\t\t0.539155\n",
      "SGD (68/99): loss=1.1970492188831625\t\t0.54789\n",
      "SGD (69/99): loss=1.19432935451409\t\t0.551575\n",
      "SGD (70/99): loss=1.1977136442942502\t\t0.551295\n",
      "SGD (71/99): loss=1.1951277127152893\t\t0.550305\n",
      "SGD (72/99): loss=1.0925920229769535\t\t0.547015\n",
      "SGD (73/99): loss=1.0877648190055322\t\t0.55367\n",
      "SGD (74/99): loss=1.095416808531404\t\t0.55525\n",
      "SGD (75/99): loss=1.0477792745288097\t\t0.55527\n",
      "SGD (76/99): loss=1.056639929351283\t\t0.555975\n",
      "SGD (77/99): loss=1.047137669611469\t\t0.553795\n",
      "SGD (78/99): loss=1.0634324101946866\t\t0.554125\n",
      "SGD (79/99): loss=0.9820383130287893\t\t0.54247\n",
      "SGD (80/99): loss=0.925164888289004\t\t0.555935\n",
      "SGD (81/99): loss=0.9393063265352564\t\t0.5563\n",
      "SGD (82/99): loss=0.948735226091569\t\t0.553955\n",
      "SGD (83/99): loss=0.9188513252598416\t\t0.550165\n",
      "SGD (84/99): loss=0.7404798742515274\t\t0.55812\n",
      "SGD (85/99): loss=0.7403639299696547\t\t0.55943\n",
      "SGD (86/99): loss=0.7393426275476275\t\t0.558245\n",
      "SGD (87/99): loss=0.7356010664553926\t\t0.56664\n",
      "SGD (88/99): loss=0.7282742433227611\t\t0.566005\n",
      "SGD (89/99): loss=0.7235484438302573\t\t0.56505\n",
      "SGD (90/99): loss=0.7201379731662889\t\t0.55904\n",
      "SGD (91/99): loss=0.7187193705855648\t\t0.560845\n",
      "SGD (92/99): loss=0.7115314037084115\t\t0.55991\n",
      "SGD (93/99): loss=0.6917373069091092\t\t0.561025\n",
      "SGD (94/99): loss=0.6950564359051395\t\t0.556425\n",
      "SGD (95/99): loss=0.6914502809909117\t\t0.56335\n",
      "SGD (96/99): loss=0.6878124282701098\t\t0.56347\n",
      "SGD (97/99): loss=0.6882005916585182\t\t0.570255\n",
      "SGD (98/99): loss=0.6748745334407464\t\t0.579605\n",
      "SGD (99/99): loss=0.6651721271493043\t\t0.574385\n"
     ]
    }
   ],
   "source": [
    "w, loss = least_squares_SGD(train_y, train_x, w_init, 100, gamma=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.641"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_accuracy(test_x, test_y, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test, x_test, ids_test = load_csv_data(data_path=\"datas/test.csv\", sub_sample=False)\n",
    "x_test = preprocess_data(x_test)\n",
    "pred_y = predict_labels(w, x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done !\n"
     ]
    }
   ],
   "source": [
    "create_csv_submission(ids_test, pred_y, \"datas/submission.csv\")\n",
    "print('Done !')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# https://stackoverflow.com/a/7941594/4810319\n",
    "def main():\n",
    "    np.random.seed(1977)\n",
    "    numvars, numdata = 5, 100\n",
    "    data = 10 * np.random.random((numvars, numdata))\n",
    "    data = x[0:300, 0:7].T\n",
    "    print(x[0:200, 7])\n",
    "    fig = scatterplot_matrix(data, ['DER_mass_MMC', 'DER_mass_transverse_met_lep', 'DER_mass_vis', 'DER_pt_h', 'DER_deltaeta_jet_jet', 'DER_mass_jet_jet', 'DER_prodeta_jet_jet'],\n",
    "            linestyle='none', marker='o', color='black', mfc='none')\n",
    "    fig.suptitle('Simple Scatterplot Matrix')\n",
    "    plt.show()\n",
    "\n",
    "def scatterplot_matrix(data, names, **kwargs):\n",
    "    \"\"\"Plots a scatterplot matrix of subplots.  Each row of \"data\" is plotted\n",
    "    against other rows, resulting in a nrows by nrows grid of subplots with the\n",
    "    diagonal subplots labeled with \"names\".  Additional keyword arguments are\n",
    "    passed on to matplotlib's \"plot\" command. Returns the matplotlib figure\n",
    "    object containg the subplot grid.\"\"\"\n",
    "    numvars, numdata = data.shape\n",
    "    fig, axes = plt.subplots(nrows=numvars, ncols=numvars, figsize=(8,8))\n",
    "    fig.subplots_adjust(hspace=0.05, wspace=0.05)\n",
    "\n",
    "    for ax in axes.flat:\n",
    "        # Hide all ticks and labels\n",
    "        ax.xaxis.set_visible(False)\n",
    "        ax.yaxis.set_visible(False)\n",
    "\n",
    "        # Set up ticks only on one side for the \"edge\" subplots...\n",
    "        if ax.is_first_col():\n",
    "            ax.yaxis.set_ticks_position('left')\n",
    "        if ax.is_last_col():\n",
    "            ax.yaxis.set_ticks_position('right')\n",
    "        if ax.is_first_row():\n",
    "            ax.xaxis.set_ticks_position('top')\n",
    "        if ax.is_last_row():\n",
    "            ax.xaxis.set_ticks_position('bottom')\n",
    "\n",
    "    # Plot the data.\n",
    "    for i, j in zip(*np.triu_indices_from(axes, k=1)):\n",
    "        for x, y in [(i,j), (j,i)]:\n",
    "            axes[x,y].plot(data[x], data[y], **kwargs)\n",
    "\n",
    "    # Label the diagonal subplots...\n",
    "    for i, label in enumerate(names):\n",
    "        axes[i,i].annotate(label, (0.5, 0.5), xycoords='axes fraction',\n",
    "                ha='center', va='center')\n",
    "\n",
    "    # Turn on the proper x or y axes ticks.\n",
    "    for i, j in zip(range(numvars), itertools.cycle((-1, 0))):\n",
    "        axes[j,i].xaxis.set_visible(True)\n",
    "        axes[i,j].yaxis.set_visible(True)\n",
    "\n",
    "    return fig\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank: 30\n",
      "Loss: 0.38955531725\n",
      "Accuracy: 0.71784\n",
      "Accuracy: 0.72126\n"
     ]
    }
   ],
   "source": [
    "def least_squares(y, tx):\n",
    "    gram = tx.T.dot(tx)\n",
    "    print(\"Rank: \" + str(np.linalg.matrix_rank(gram)))\n",
    "    w = np.linalg.inv(gram).dot(tx.T).dot(y)\n",
    "    return w, compute_loss(y, tx, w)\n",
    "    \n",
    "w, loss = least_squares(train_y, train_x)\n",
    "print(\"Loss: \" + str(loss))\n",
    "print(\"Accuracy: \" + str(get_accuracy(train_x, train_y, w)))\n",
    "print(\"Accuracy: \" + str(get_accuracy(test_x, test_y, w)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
