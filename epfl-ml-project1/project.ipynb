{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from helpers import *\n",
    "from costs import *\n",
    "from gradient_descent import *\n",
    "from stochastic_gradient_descent import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 30)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y, x, ids = load_csv_data(data_path=\"datas/train.csv\", sub_sample=True)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8255096.3165090261"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = np.random.rand(x.shape[1])\n",
    "compute_loss(y, x, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30,)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient = compute_gradient(y, x, w)\n",
    "gradient.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/99): loss=9703990.846997293, w0=0.5664059540581982, w1=0.6438594505631134\n",
      "Gradient Descent(1/99): loss=1853397.6601247566, w0=0.5359649540159034, w1=0.6523952364047307\n",
      "Gradient Descent(2/99): loss=397234.4905549066, w0=0.5171954392379912, w1=0.6560093168917573\n",
      "Gradient Descent(3/99): loss=124489.61959344831, w0=0.5035936885081641, w1=0.6575054319131519\n",
      "Gradient Descent(4/99): loss=70983.31510441193, w0=0.49235717139689983, w1=0.6580896178172918\n",
      "Gradient Descent(5/99): loss=58295.49410143504, w0=0.48227560246548073, w1=0.6582806708698796\n",
      "Gradient Descent(6/99): loss=53391.25099974265, w0=0.47282378606554365, w1=0.6583017756305299\n",
      "Gradient Descent(7/99): loss=50137.31821712238, w0=0.46377140188898786, w1=0.6582489607061832\n",
      "Gradient Descent(8/99): loss=47379.80623923063, w0=0.45501514874655785, w1=0.6581635586435703\n",
      "Gradient Descent(9/99): loss=44887.91608607682, w0=0.4465065343173581, w1=0.6580633739937246\n",
      "Gradient Descent(10/99): loss=42603.26620869061, w0=0.43822080661885127, w1=0.6579560900361886\n",
      "Gradient Descent(11/99): loss=40500.84683569375, w0=0.4301435826252769, w1=0.6578450356077402\n",
      "Gradient Descent(12/99): loss=38563.153586139444, w0=0.4222650895272615, w1=0.6577316656915397\n",
      "Gradient Descent(13/99): loss=36775.27509760093, w0=0.41457768084772845, w1=0.6576166284558737\n",
      "Gradient Descent(14/99): loss=35123.86969718615, w0=0.4070747615351804, w1=0.6575002242643387\n",
      "Gradient Descent(15/99): loss=33596.87256414973, w0=0.3997503194261522, w1=0.6573826031461035\n",
      "Gradient Descent(16/99): loss=32183.347936242153, w0=0.3925987178265204, w1=0.657263849765538\n",
      "Gradient Descent(17/99): loss=30873.377092719587, w0=0.385614600690806, w1=0.6571440199992971\n",
      "Gradient Descent(18/99): loss=29657.961041473558, w0=0.37879284650085143, w1=0.6570231566964855\n",
      "Gradient Descent(19/99): loss=28528.933237630983, w0=0.37212854334547946, w1=0.6569012964833901\n",
      "Gradient Descent(20/99): loss=27478.880769431795, w0=0.3656169733606291, w1=0.6567784727149598\n",
      "Gradient Descent(21/99): loss=26501.073089338744, w0=0.35925360142517676, w1=0.6566547167677452\n",
      "Gradient Descent(22/99): loss=25589.39754773963, w0=0.3530340659055346, w1=0.6565300586183901\n",
      "Gradient Descent(23/99): loss=24738.301074585153, w0=0.346954170489162, w1=0.6564045271138143\n",
      "Gradient Descent(24/99): loss=23942.737420123158, w0=0.34100987668405014, w1=0.6562781501078355\n",
      "Gradient Descent(25/99): loss=23198.119422861226, w0=0.3351972967926839, w1=0.6561509545394388\n",
      "Gradient Descent(26/99): loss=22500.275823931464, w0=0.3295126872690084, w1=0.6560229664850873\n",
      "Gradient Descent(27/99): loss=21845.412193096643, w0=0.32395244241039534, w1=0.6558942111990483\n",
      "Gradient Descent(28/99): loss=21230.075573272385, w0=0.31851308835572495, w1=0.6557647131477851\n",
      "Gradient Descent(29/99): loss=20651.12248808391, w0=0.31319127736932745, w1=0.655634496041051\n",
      "Gradient Descent(30/99): loss=20105.689991012816, w0=0.3079837823946199, w1=0.6555035828608583\n",
      "Gradient Descent(31/99): loss=19591.169465465253, w0=0.3028874918634001, w1=0.6553719958888564\n",
      "Gradient Descent(32/99): loss=19105.182912920805, w0=0.29789940474801874, w1=0.6552397567323814\n",
      "Gradient Descent(33/99): loss=18645.561491482975, w0=0.29301662584452176, w1=0.6551068863493218\n",
      "Gradient Descent(34/99): loss=18210.326089903916, w0=0.288236361275543, w1=0.6549734050718881\n",
      "Gradient Descent(35/99): loss=17797.66974272819, w0=0.2835559142023201, w1=0.6548393326293541\n",
      "Gradient Descent(36/99): loss=17405.941710801624, w0=0.2789726807357457, w1=0.6547046881698232\n",
      "Gradient Descent(37/99): loss=17033.6330682113, w0=0.2744841460368648, w1=0.6545694902810674\n",
      "Gradient Descent(38/99): loss=16679.36365193125, w0=0.27008788059770306, w1=0.6544337570104842\n",
      "Gradient Descent(39/99): loss=16341.870244200432, w0=0.265781536693755, w1=0.6542975058842121\n",
      "Gradient Descent(40/99): loss=16019.9958700944, w0=0.26156284499988486, w1=0.6541607539254432\n",
      "Gradient Descent(41/99): loss=15712.680103995977, w0=0.2574296113617959, w1=0.654023517671971\n",
      "Gradient Descent(42/99): loss=15418.950288837303, w0=0.2533797137156052, w1=0.6538858131930086\n",
      "Gradient Descent(43/99): loss=15137.913581179006, w0=0.24941109914842605, w1=0.6537476561053095\n",
      "Gradient Descent(44/99): loss=14868.749743505323, w0=0.24552178109320372, w1=0.6536090615886239\n",
      "Gradient Descent(45/99): loss=14610.704612630916, w0=0.24170983665138154, w1=0.6534700444005214\n",
      "Gradient Descent(46/99): loss=14363.084179912383, w0=0.2379734040372852, w1=0.6533306188906076\n",
      "Gradient Descent(47/99): loss=14125.249225103831, w0=0.23431068013841116, w1=0.6531907990141635\n",
      "Gradient Descent(48/99): loss=13896.610451253882, w0=0.23071991818608822, w1=0.6530505983452327\n",
      "Gradient Descent(49/99): loss=13676.624073067213, w0=0.22719942553124953, w1=0.6529100300891821\n",
      "Gradient Descent(50/99): loss=13464.787815698503, w0=0.22374756152030875, w1=0.6527691070947602\n",
      "Gradient Descent(51/99): loss=13260.637285056366, w0=0.22036273546637727, w1=0.6526278418656748\n",
      "Gradient Descent(52/99): loss=13063.74267441115, w0=0.21704340471129033, w1=0.6524862465717118\n",
      "Gradient Descent(53/99): loss=12873.705775461101, w0=0.2137880727741305, w1=0.6523443330594167\n",
      "Gradient Descent(54/99): loss=12690.157265050439, w0=0.21059528758214613, w1=0.6522021128623567\n",
      "Gradient Descent(55/99): loss=12512.754241481223, w0=0.20746363978016105, w1=0.6520595972109835\n",
      "Gradient Descent(56/99): loss=12341.177986845923, w0=0.20439176111476168, w1=0.6519167970421136\n",
      "Gradient Descent(57/99): loss=12175.13193405539, w0=0.20137832288972746, w1=0.6517737230080439\n",
      "Gradient Descent(58/99): loss=12014.339819269255, w0=0.19842203448934173, w1=0.6516303854853179\n",
      "Gradient Descent(59/99): loss=11858.544002273975, w0=0.19552164196638325, w1=0.6514867945831583\n",
      "Gradient Descent(60/99): loss=11707.503939015973, w0=0.19267592669175337, w1=0.6513429601515803\n",
      "Gradient Descent(61/99): loss=11560.994792000867, w0=0.18988370406284122, w1=0.6511988917892004\n",
      "Gradient Descent(62/99): loss=11418.806165629188, w0=0.18714382226786933, w1=0.6510545988507528\n",
      "Gradient Descent(63/99): loss=11280.740954768804, w0=0.18445516110359575, w1=0.6509100904543269\n",
      "Gradient Descent(64/99): loss=11146.614295976387, w0=0.1818166308438751, w1=0.6507653754883381\n",
      "Gradient Descent(65/99): loss=11016.252611786027, w0=0.1792271711567016, w1=0.6506204626182425\n",
      "Gradient Descent(66/99): loss=10889.492739392968, w0=0.17668575006747245, w1=0.6504753602930073\n",
      "Gradient Descent(67/99): loss=10766.18113588325, w0=0.17419136296631818, w1=0.6503300767513477\n",
      "Gradient Descent(68/99): loss=10646.173152904315, w0=0.17174303165745095, w1=0.6501846200277384\n",
      "Gradient Descent(69/99): loss=10529.332374344865, w0=0.16933980344858013, w1=0.650038997958212\n",
      "Gradient Descent(70/99): loss=10415.530011201206, w0=0.16698075027853906, w1=0.649893218185951\n",
      "Gradient Descent(71/99): loss=10304.644348358182, w0=0.1646649678813551, w1=0.6497472881666835\n",
      "Gradient Descent(72/99): loss=10196.560238511069, w0=0.1623915749850811, w1=0.6496012151738895\n",
      "Gradient Descent(73/99): loss=10091.168638905601, w0=0.16015971254378683, w1=0.6494550063038276\n",
      "Gradient Descent(74/99): loss=9988.366186981068, w0=0.15796854300118532, w1=0.6493086684803877\n",
      "Gradient Descent(75/99): loss=9888.054811370379, w0=0.15581724958444337, w1=0.6491622084597785\n",
      "Gradient Descent(76/99): loss=9790.141375044808, w0=0.15370503562679383, w1=0.6490156328350545\n",
      "Gradient Descent(77/99): loss=9694.537347693027, w0=0.151631123917634, w1=0.648868948040492\n",
      "Gradient Descent(78/99): loss=9601.15850469744, w0=0.1495947560788576, w1=0.6487221603558175\n",
      "Gradient Descent(79/99): loss=9509.924650317935, w0=0.14759519196622747, w1=0.6485752759102962\n",
      "Gradient Descent(80/99): loss=9420.759362917033, w0=0.14563170909465287, w1=0.6484283006866854\n",
      "Gradient Descent(81/99): loss=9333.589760262821, w0=0.14370360208629016, w1=0.6482812405250591\n",
      "Gradient Descent(82/99): loss=9248.346283129293, w0=0.14181018214043634, w1=0.6481341011265075\n",
      "Gradient Descent(83/99): loss=9164.96249557965, w0=0.13995077652423488, w1=0.6479868880567183\n",
      "Gradient Descent(84/99): loss=9083.374900468083, w0=0.13812472808325907, w1=0.647839606749443\n",
      "Gradient Descent(85/99): loss=9003.522768831599, w0=0.1363313947710835, w1=0.6476922625098525\n",
      "Gradient Descent(86/99): loss=8925.347981966323, w0=0.13457014919699545, w1=0.6475448605177885\n",
      "Gradient Descent(87/99): loss=8848.79488509414, w0=0.13284037819103933, w1=0.6473974058309122\n",
      "Gradient Descent(88/99): loss=8773.810151626345, w0=0.1311414823856245, w1=0.6472499033877556\n",
      "Gradient Descent(89/99): loss=8700.342657122317, w0=0.12947287581296393, w1=0.647102358010679\n",
      "Gradient Descent(90/99): loss=8628.343362123782, w0=0.12783398551764555, w1=0.646954774408738\n",
      "Gradient Descent(91/99): loss=8557.765203120274, w0=0.126224251183671, w1=0.6468071571804644\n",
      "Gradient Descent(92/99): loss=8488.562990969125, w0=0.12464312477532807, w1=0.6466595108165621\n",
      "Gradient Descent(93/99): loss=8420.69331615477, w0=0.1230900701912927, w1=0.6465118397025238\n",
      "Gradient Descent(94/99): loss=8354.114460327843, w0=0.12156456293138503, w1=0.6463641481211695\n",
      "Gradient Descent(95/99): loss=8288.786313614992, w0=0.12006608977543096, w1=0.6462164402551114\n",
      "Gradient Descent(96/99): loss=8224.670297235969, w0=0.11859414847370628, w1=0.6460687201891455\n",
      "Gradient Descent(97/99): loss=8161.7292910061515, w0=0.11714824744846512, w1=0.6459209919125756\n",
      "Gradient Descent(98/99): loss=8099.927565340013, w0=0.11572790550607749, w1=0.645773259321469\n",
      "Gradient Descent(99/99): loss=8039.230717405269, w0=0.1143326515593231, w1=0.6456255262208495\n"
     ]
    }
   ],
   "source": [
    "w = np.random.rand(x.shape[1])\n",
    "w, loss = least_squares(y, x, w, max_iters=100, gamma=0.0000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(1/99): loss=45258.78507090701, w0=0.11293539437844557, w1=0.6455004821126032\n",
      "Gradient Descent(1/99): loss=7190.511424623448, w0=0.11118639937176304, w1=0.6451110397971282\n",
      "Gradient Descent(1/99): loss=3837.0191198747643, w0=0.1094944235868044, w1=0.6449565710772031\n",
      "Gradient Descent(1/99): loss=4293.6945415792725, w0=0.106969596885694, w1=0.6450228795100331\n",
      "Gradient Descent(1/99): loss=7400.96843130032, w0=0.10719027662879528, w1=0.6443888756886011\n",
      "Gradient Descent(1/99): loss=4043.2134814057663, w0=0.10579230988482818, w1=0.6442773453210575\n",
      "Gradient Descent(1/99): loss=10266.902082366654, w0=0.10563103099283752, w1=0.6434370481621378\n",
      "Gradient Descent(1/99): loss=6071.894996608618, w0=0.10341692039120715, w1=0.6433767366971894\n",
      "Gradient Descent(1/99): loss=2852.8892142817126, w0=0.10229211745153118, w1=0.6435456436871679\n",
      "Gradient Descent(1/99): loss=5531.618183653429, w0=0.09651865728760854, w1=0.6439977107493577\n",
      "Gradient Descent(1/99): loss=2035.9673440934062, w0=0.0951295705910206, w1=0.6438822518316559\n",
      "Gradient Descent(1/99): loss=4135.7762654945445, w0=0.09391919906637473, w1=0.6435898818389646\n",
      "Gradient Descent(1/99): loss=3415.4005140461377, w0=0.09318830392034946, w1=0.6432726690778665\n",
      "Gradient Descent(1/99): loss=9769.85465666068, w0=0.0921768091765385, w1=0.6430312187665471\n",
      "Gradient Descent(1/99): loss=9667.130930170973, w0=0.08998371691378311, w1=0.6427377548717389\n",
      "Gradient Descent(1/99): loss=5661.710309030892, w0=0.08791630438464525, w1=0.6426421981352308\n",
      "Gradient Descent(1/99): loss=6869.188653957379, w0=0.08692662593442123, w1=0.642213545545533\n",
      "Gradient Descent(1/99): loss=2442.9090191963996, w0=0.08574309726799209, w1=0.6422005540974719\n",
      "Gradient Descent(1/99): loss=3633.262914265691, w0=0.08412939764581294, w1=0.6420280231454885\n",
      "Gradient Descent(1/99): loss=1369.72490218248, w0=0.0832311216076256, w1=0.6420173149155217\n",
      "Gradient Descent(1/99): loss=5829.5902069875465, w0=0.08144878706390436, w1=0.641980427499378\n",
      "Gradient Descent(1/99): loss=891.6500027540797, w0=0.08137504802800098, w1=0.6419725421632684\n",
      "Gradient Descent(1/99): loss=3445.068582286831, w0=0.08084868254057448, w1=0.6418954485394424\n",
      "Gradient Descent(1/99): loss=10107.636038939407, w0=0.08076290729400579, w1=0.6413038188353405\n",
      "Gradient Descent(1/99): loss=6005.779391773582, w0=0.08383316575154333, w1=0.6411286093652367\n",
      "Gradient Descent(1/99): loss=1498.976530329038, w0=0.08314208811497074, w1=0.6409855032616821\n",
      "Gradient Descent(1/99): loss=7263.2446706593855, w0=0.08249338474503418, w1=0.6405441329605671\n",
      "Gradient Descent(1/99): loss=2427.1680905720764, w0=0.0809481054654439, w1=0.6405740985821108\n",
      "Gradient Descent(1/99): loss=1296.0597762898517, w0=0.07976279055024184, w1=0.6406343472234423\n",
      "Gradient Descent(1/99): loss=10336.262567209425, w0=0.07966977580980153, w1=0.6406600481816396\n",
      "Gradient Descent(1/99): loss=6066.566641419284, w0=0.07900547400215663, w1=0.6405779022599499\n",
      "Gradient Descent(1/99): loss=5509.893613458158, w0=0.0788540489684398, w1=0.6404433439540161\n",
      "Gradient Descent(1/99): loss=3906.446061848769, w0=0.07763149844114667, w1=0.6401416285453264\n",
      "Gradient Descent(1/99): loss=8521.848972366977, w0=0.07668913170627836, w1=0.6402690832151758\n",
      "Gradient Descent(1/99): loss=3439.741526716005, w0=0.07611194628629941, w1=0.6401457878650847\n",
      "Gradient Descent(1/99): loss=2143.3536325806117, w0=0.07440458106719705, w1=0.6402899173282276\n",
      "Gradient Descent(1/99): loss=4416.738572489677, w0=0.07371183691628577, w1=0.6402080557288204\n",
      "Gradient Descent(1/99): loss=5000.082183891484, w0=0.07328279937333854, w1=0.639986582164313\n",
      "Gradient Descent(1/99): loss=2549.9830194515325, w0=0.07342015858569016, w1=0.6398742633296021\n",
      "Gradient Descent(1/99): loss=7440.072326475816, w0=0.07113372113176308, w1=0.6397586400919332\n",
      "Gradient Descent(1/99): loss=2928.416051035427, w0=0.07308696650230623, w1=0.6394171759474967\n",
      "Gradient Descent(1/99): loss=3127.0229122268997, w0=0.0726760311223722, w1=0.6392252218173401\n",
      "Gradient Descent(1/99): loss=1876.8065030123844, w0=0.07233772415102904, w1=0.6391622896937633\n",
      "Gradient Descent(1/99): loss=1829.821364014864, w0=0.07056056436986097, w1=0.6391581941450234\n",
      "Gradient Descent(1/99): loss=16236.361496955207, w0=0.07224661259857756, w1=0.6392272232514356\n",
      "Gradient Descent(1/99): loss=2908.6398333805028, w0=0.07109645748579867, w1=0.6392913702926398\n",
      "Gradient Descent(1/99): loss=1231.6419117691971, w0=0.0698638663969836, w1=0.6392563155184308\n",
      "Gradient Descent(1/99): loss=12952.767828739992, w0=0.07047939720340342, w1=0.6392441797078764\n",
      "Gradient Descent(1/99): loss=2526.907633770801, w0=0.06966902327740179, w1=0.639051463590568\n",
      "Gradient Descent(1/99): loss=4337.621329615512, w0=0.06935933859361144, w1=0.6389210852844658\n",
      "Gradient Descent(1/99): loss=5771.694870279447, w0=0.06880775858385382, w1=0.6385982265925921\n",
      "Gradient Descent(1/99): loss=6876.1377220437635, w0=0.06993551976177116, w1=0.6383910651749332\n",
      "Gradient Descent(1/99): loss=11591.29567794853, w0=0.07031065697417112, w1=0.6383868106171264\n",
      "Gradient Descent(1/99): loss=4307.35675992106, w0=0.06891683360046591, w1=0.63820485808681\n",
      "Gradient Descent(1/99): loss=2017.3927077507856, w0=0.06870432106783561, w1=0.6381261948150896\n",
      "Gradient Descent(1/99): loss=11240.849291335773, w0=0.06841908502954144, w1=0.6376797260667232\n",
      "Gradient Descent(1/99): loss=781.2880295700655, w0=0.06757468952048296, w1=0.6376399570853458\n",
      "Gradient Descent(1/99): loss=1737.533449786498, w0=0.06747846710977667, w1=0.6375598828600945\n",
      "Gradient Descent(1/99): loss=5268.388336504533, w0=0.06702590578766081, w1=0.6372188483208097\n",
      "Gradient Descent(1/99): loss=1017.8877507889098, w0=0.06656187135091332, w1=0.6372275545571887\n",
      "Gradient Descent(1/99): loss=612.9488432860164, w0=0.06669903261124215, w1=0.6371977424146159\n",
      "Gradient Descent(1/99): loss=3563.701049328438, w0=0.0657263876083631, w1=0.6368894477574089\n",
      "Gradient Descent(1/99): loss=3211.7638667792853, w0=0.06710050093872524, w1=0.636541215129852\n",
      "Gradient Descent(1/99): loss=5873.812183313188, w0=0.06828553011987121, w1=0.6362799785622135\n",
      "Gradient Descent(1/99): loss=1860.035480363646, w0=0.06757184370272246, w1=0.6362626820067319\n",
      "Gradient Descent(1/99): loss=7007.640928819123, w0=0.06680171607712782, w1=0.6358948231182998\n",
      "Gradient Descent(1/99): loss=7762.685244887892, w0=0.06485437309756673, w1=0.6353575795912809\n",
      "Gradient Descent(1/99): loss=4701.7821987238485, w0=0.06450845720357092, w1=0.6351694676704484\n",
      "Gradient Descent(1/99): loss=6362.637720106475, w0=0.06364919952628546, w1=0.6354746056691543\n",
      "Gradient Descent(1/99): loss=3921.489372541206, w0=0.06255315602712319, w1=0.635424529963127\n",
      "Gradient Descent(1/99): loss=1837.0171279921624, w0=0.06185006619939926, w1=0.6353746127609717\n",
      "Gradient Descent(1/99): loss=2220.6885197145853, w0=0.06347229700808292, w1=0.6351551655379536\n",
      "Gradient Descent(1/99): loss=1671.203329980326, w0=0.06262955808430896, w1=0.6350934979584769\n",
      "Gradient Descent(1/99): loss=4308.428235533267, w0=0.06182473232729303, w1=0.6347733339147618\n",
      "Gradient Descent(1/99): loss=633.1417118268668, w0=0.06225166055061832, w1=0.6347025450641398\n",
      "Gradient Descent(1/99): loss=4727.8199613388615, w0=0.06122673121464489, w1=0.6344048406801593\n",
      "Gradient Descent(1/99): loss=3305.5692431018115, w0=0.05899527440210973, w1=0.634356171941494\n",
      "Gradient Descent(1/99): loss=2303.092960777203, w0=0.0583276202779315, w1=0.6342060407632806\n",
      "Gradient Descent(1/99): loss=3706.6530522855996, w0=0.05728013095073652, w1=0.6341046029969992\n",
      "Gradient Descent(1/99): loss=8413.707925491586, w0=0.05913968822211861, w1=0.634016433127271\n",
      "Gradient Descent(1/99): loss=535.0450401371594, w0=0.05818812188355412, w1=0.6340104227541327\n",
      "Gradient Descent(1/99): loss=1647.2854265964427, w0=0.05670856173142749, w1=0.633961019755301\n",
      "Gradient Descent(1/99): loss=6038.023997950995, w0=0.05627565869965405, w1=0.6339066937814185\n",
      "Gradient Descent(1/99): loss=3314.3552913088556, w0=0.056061398488804086, w1=0.633767336905889\n",
      "Gradient Descent(1/99): loss=20281.128961385955, w0=0.05598606983152449, w1=0.6337738745578262\n",
      "Gradient Descent(1/99): loss=16034.67322195688, w0=0.05576564704508168, w1=0.6338628135575991\n",
      "Gradient Descent(1/99): loss=1952.4533015072566, w0=0.055036528754394055, w1=0.6336857513835409\n",
      "Gradient Descent(1/99): loss=482.3343279213334, w0=0.05578907726937585, w1=0.6336577610856601\n",
      "Gradient Descent(1/99): loss=8332.394777621243, w0=0.05489446444999607, w1=0.6334197827539637\n",
      "Gradient Descent(1/99): loss=3956.258278738073, w0=0.05420779863121236, w1=0.6332311209206902\n",
      "Gradient Descent(1/99): loss=6722.721048022947, w0=0.052909098726865694, w1=0.632849320056104\n",
      "Gradient Descent(1/99): loss=6160.608552558961, w0=0.05206357186673165, w1=0.6326804429093976\n",
      "Gradient Descent(1/99): loss=1605.5224502498056, w0=0.05095664646938844, w1=0.6325722557902131\n",
      "Gradient Descent(1/99): loss=4053.8321522546694, w0=0.050103911198031266, w1=0.6323277190958135\n",
      "Gradient Descent(1/99): loss=1947.2173044688805, w0=0.05115666604330348, w1=0.6322475555864043\n",
      "Gradient Descent(1/99): loss=15215.229729332495, w0=0.04809738829198891, w1=0.6310250120049418\n",
      "Gradient Descent(1/99): loss=10478.489302905149, w0=0.04709547765442307, w1=0.6308599272478432\n",
      "Gradient Descent(1/99): loss=14924.439140090888, w0=0.04751736013765502, w1=0.6310010585131137\n",
      "Gradient Descent(1/99): loss=580.0450630779039, w0=0.047422866164365515, w1=0.6309666899356543\n",
      "Gradient Descent(1/99): loss=1781.233176727935, w0=0.047204106128672645, w1=0.6307577299442193\n"
     ]
    }
   ],
   "source": [
    "w, loss = least_squares_SGD(y, x, w, 10, max_iters=100, gamma=0.0000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
