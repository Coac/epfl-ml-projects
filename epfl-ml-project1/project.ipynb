{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "from datetime import datetime\n",
    "from helpers import *\n",
    "from implementation import *\n",
    "from features_engineering import *\n",
    "from cross_validation import *\n",
    "from pre_processing import *\n",
    "from group_by import *\n",
    "from knn import *\n",
    "from feature_selection import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y, x, ids = load_csv_data(data_path=\"datas/train.csv\", sub_sample=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission_y, submission_x, submission_ids = load_csv_data(data_path=\"datas/test.csv\", sub_sample=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create sub dataset \n",
    "- Group by numjet column (categorical data : (0, 1, 2, 3))\n",
    "- Group by the NaN columns\n",
    "\n",
    "We obtain at the end 8 datasets, one for each numjet and for each of these, 2 according to the NaN columns removed.  \n",
    "We cleaned also the columns which as low variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 (227458, 30) (227458,) (227458,)\n",
      "1 (175338, 30) (175338,) (175338,)\n",
      "2 (114648, 30) (114648,) (114648,)\n",
      "3 (50794, 30) (50794,) (50794,)\n",
      "num_jet: 0\n",
      "(0, 4, 5, 6, 12, 23, 24, 25, 26, 27, 28) (59263, 19) (59263, 1) (59263, 1)\n",
      "(4, 5, 6, 12, 23, 24, 25, 26, 27, 28) (168195, 20) (168195, 1) (168195, 1)\n",
      "\tRemove col : \n",
      "\t (0, 4, 5, 6, 12, 23, 24, 25, 26, 27, 28) 17 [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "\t (0, 4, 5, 6, 12, 23, 24, 25, 26, 27, 28) 18 [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "\t (4, 5, 6, 12, 23, 24, 25, 26, 27, 28) 18 [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "\t (4, 5, 6, 12, 23, 24, 25, 26, 27, 28) 19 [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "num_jet: 1\n",
      "(4, 5, 6, 12, 26, 27, 28) (158095, 23) (158095, 1) (158095, 1)\n",
      "(0, 4, 5, 6, 12, 26, 27, 28) (17243, 22) (17243, 1) (17243, 1)\n",
      "\tRemove col : \n",
      "\t (4, 5, 6, 12, 26, 27, 28) 18 [ 1.  1.  1. ...,  1.  1.  1.]\n",
      "\t (0, 4, 5, 6, 12, 26, 27, 28) 17 [ 1.  1.  1. ...,  1.  1.  1.]\n",
      "num_jet: 2\n",
      "() (107905, 30) (107905, 1) (107905, 1)\n",
      "(0,) (6743, 29) (6743, 1) (6743, 1)\n",
      "\tRemove col : \n",
      "\t () 22 [ 2.  2.  2. ...,  2.  2.  2.]\n",
      "\t (0,) 21 [ 2.  2.  2. ...,  2.  2.  2.]\n",
      "num_jet: 3\n",
      "() (47555, 30) (47555, 1) (47555, 1)\n",
      "(0,) (3239, 29) (3239, 1) (3239, 1)\n",
      "\tRemove col : \n",
      "\t () 22 [ 3.  3.  3. ...,  3.  3.  3.]\n",
      "\t (0,) 21 [ 3.  3.  3. ...,  3.  3.  3.]\n"
     ]
    }
   ],
   "source": [
    "sub_jet_num_x_dict, sub_jet_num_y_dict, sub_jet_num_ids_dict = group_by_jetnum_NaN(submission_x, submission_y, submission_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 (99913, 30) (99913,) (99913,)\n",
      "1 (77544, 30) (77544,) (77544,)\n",
      "2 (50379, 30) (50379,) (50379,)\n",
      "3 (22164, 30) (22164,) (22164,)\n",
      "num_jet: 0\n",
      "(4, 5, 6, 12, 23, 24, 25, 26, 27, 28) (73790, 20) (73790, 1) (73790, 1)\n",
      "(0, 4, 5, 6, 12, 23, 24, 25, 26, 27, 28) (26123, 19) (26123, 1) (26123, 1)\n",
      "\tRemove col : \n",
      "\t (4, 5, 6, 12, 23, 24, 25, 26, 27, 28) 18 [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "\t (4, 5, 6, 12, 23, 24, 25, 26, 27, 28) 19 [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "\t (0, 4, 5, 6, 12, 23, 24, 25, 26, 27, 28) 17 [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "\t (0, 4, 5, 6, 12, 23, 24, 25, 26, 27, 28) 18 [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "num_jet: 1\n",
      "(4, 5, 6, 12, 26, 27, 28) (69982, 23) (69982, 1) (69982, 1)\n",
      "(0, 4, 5, 6, 12, 26, 27, 28) (7562, 22) (7562, 1) (7562, 1)\n",
      "\tRemove col : \n",
      "\t (4, 5, 6, 12, 26, 27, 28) 18 [ 1.  1.  1. ...,  1.  1.  1.]\n",
      "\t (0, 4, 5, 6, 12, 26, 27, 28) 17 [ 1.  1.  1. ...,  1.  1.  1.]\n",
      "num_jet: 2\n",
      "() (47427, 30) (47427, 1) (47427, 1)\n",
      "(0,) (2952, 29) (2952, 1) (2952, 1)\n",
      "\tRemove col : \n",
      "\t () 22 [ 2.  2.  2. ...,  2.  2.  2.]\n",
      "\t (0,) 21 [ 2.  2.  2. ...,  2.  2.  2.]\n",
      "num_jet: 3\n",
      "() (20687, 30) (20687, 1) (20687, 1)\n",
      "(0,) (1477, 29) (1477, 1) (1477, 1)\n",
      "\tRemove col : \n",
      "\t () 22 [ 3.  3.  3. ...,  3.  3.  3.]\n",
      "\t (0,) 21 [ 3.  3.  3. ...,  3.  3.  3.]\n"
     ]
    }
   ],
   "source": [
    "jet_num_x_dict, jet_num_y_dict, jet_num_ids_dict = group_by_jetnum_NaN(x, y, ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Build the best model for each of the sub dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_false(x, y, w, predict_threshold):\n",
    "    \"\"\"Get the ratio of negative predictions over wrong predictions\"\"\"\n",
    "    \n",
    "    # Get the predicted values\n",
    "    pred_y = predict_labels(w, x, predict_threshold)\n",
    "    # Initialize at 0\n",
    "    false_count = 0\n",
    "    count_negatif = 0\n",
    "    \n",
    "    # If prediction is wrong, add 1, if prediction is wrong and negative, add 1\n",
    "    for index, yi in enumerate(y):\n",
    "        pred_yi = pred_y[index]\n",
    "        if pred_yi != yi:\n",
    "            false_count += 1\n",
    "            if pred_yi == -1:\n",
    "                count_negatif += 1\n",
    "                \n",
    "    # Calculate which percentage of wrong predictions are due to negative value\n",
    "    return count_negatif / false_count\n",
    "\n",
    "\n",
    "\n",
    "def get_data_numjet(jet_num_x_dict, jet_num_y_dict, jet_num_ids_dict, numjet, index):\n",
    "    # Get the column number of the features that wil be removed\n",
    "    removed_col_key = list(jet_num_x_dict[numjet])[index]\n",
    "    # Get the samples of the category numjet of PRI_num_jet and removed data\n",
    "    x = jet_num_x_dict[numjet][removed_col_key]\n",
    "    y = jet_num_y_dict[numjet][removed_col_key]\n",
    "    ids = jet_num_ids_dict[numjet][removed_col_key]\n",
    "    return x, y, ids\n",
    "\n",
    "def build_features(x, numjet, index):\n",
    "    \"\"\"\n",
    "    Calculate different features depending on the data (category of PRI_num_jet and nan or not)\n",
    "    Which features are used has been done with trial and error to improve the loss\n",
    "    1. Normalize data\n",
    "    2. Build combinations\n",
    "    \"\"\"\n",
    "    if numjet == 0 and index == 0:\n",
    "        polynomial_x = normalize(x)\n",
    "        polynomial_x = build_polynomial(polynomial_x, 3)\n",
    "        polynomial_x = build_combinations_lvl(polynomial_x, 2, 8)\n",
    "        polynomial_x = build_combinations_lvl(polynomial_x, 3, 8)\n",
    "        polynomial_x = build_combinations_lvl(polynomial_x, 4, 8)\n",
    "        polynomial_x = build_combinations_lvl(polynomial_x, 5, 8)\n",
    "        polynomial_x = build_combinations_lvl(polynomial_x, 6, 8)\n",
    "        polynomial_x = build_combinations_lvl(polynomial_x, 7, 8)\n",
    "    elif numjet == 0 and index == 1:\n",
    "        x_numjet0_index1 = normalize(x)\n",
    "        polynomial_x = x_numjet0_index1\n",
    "        polynomial_x = np.concatenate((polynomial_x, np.tanh(x_numjet0_index1)), axis=1)\n",
    "        polynomial_x = np.concatenate((polynomial_x, np.sqrt(np.abs(x_numjet0_index1))), axis=1)\n",
    "        polynomial_x = np.concatenate((polynomial_x, np.power(x_numjet0_index1, 2)), axis=1)\n",
    "        polynomial_x = np.concatenate((polynomial_x, np.power(np.tanh(x_numjet0_index1), 2)), axis=1)\n",
    "        polynomial_x = np.concatenate((polynomial_x, np.power(np.log(np.abs(x_numjet0_index1)), 2)), axis=1)\n",
    "    elif numjet == 1 and index == 1:\n",
    "        polynomial_x = normalize(x)\n",
    "        polynomial_x = build_polynomial(polynomial_x, 3)\n",
    "        polynomial_x = build_combinations_lvl(polynomial_x, 2, 10)\n",
    "        polynomial_x = build_combinations_lvl(polynomial_x, 3, 10)\n",
    "        polynomial_x = build_combinations_lvl(polynomial_x, 4, 10)\n",
    "        polynomial_x = build_combinations_lvl(polynomial_x, 5, 10)\n",
    "    elif numjet == 2 and index == 0:\n",
    "        polynomial_x = normalize(x)\n",
    "        polynomial_x = build_polynomial(polynomial_x, 3)\n",
    "        polynomial_x = build_combinations_lvl(polynomial_x, 2, 10)\n",
    "        polynomial_x = build_combinations_lvl(polynomial_x, 3, 10)\n",
    "        polynomial_x = build_combinations_lvl(polynomial_x, 4, 10)\n",
    "    elif numjet == 2 and index == 1:\n",
    "        polynomial_x = normalize(x)\n",
    "        polynomial_x = build_polynomial(polynomial_x, 2)\n",
    "        polynomial_x = build_combinations_lvl(polynomial_x, 2, 10)\n",
    "        polynomial_x = build_combinations_lvl(polynomial_x, 3, 10)\n",
    "        polynomial_x = build_combinations_lvl(polynomial_x, 4, 10)\n",
    "        polynomial_x = build_combinations_lvl(polynomial_x, 5, 10)\n",
    "    elif numjet == 3 and index == 0:\n",
    "        polynomial_x = normalize(x)\n",
    "        polynomial_x = build_polynomial(polynomial_x, 5)\n",
    "        polynomial_x = build_combinations_lvl(polynomial_x, 2, 8)\n",
    "        polynomial_x = build_combinations_lvl(polynomial_x, 3, 8)\n",
    "        polynomial_x = build_combinations_lvl(polynomial_x, 4, 8)\n",
    "        polynomial_x = build_combinations_lvl(polynomial_x, 5, 8)\n",
    "        polynomial_x = build_combinations_lvl(polynomial_x, 6, 8)\n",
    "    elif numjet == 3 and index == 1:\n",
    "        x_train, y_train, _ = get_data_numjet(jet_num_x_dict, jet_num_y_dict, jet_num_ids_dict, numjet, index)\n",
    "        polynomial_x = normalize(reorder_mi(x, x_train, y_train))\n",
    "        polynomial_x = build_polynomial(polynomial_x, 3)\n",
    "        polynomial_x = build_combinations_lvl(polynomial_x, 2, 8)\n",
    "        polynomial_x = build_combinations_lvl(polynomial_x, 3, 8)\n",
    "        polynomial_x = build_combinations_lvl(polynomial_x, 4, 8)\n",
    "        polynomial_x = build_combinations_lvl(polynomial_x, 5, 8)\n",
    "        polynomial_x = build_combinations_lvl(polynomial_x, 6, 8)\n",
    "        polynomial_x = build_combinations_lvl(polynomial_x, 7, 8)\n",
    "    else:\n",
    "        polynomial_x = normalize(x)\n",
    "        polynomial_x = build_polynomial(polynomial_x, 3)\n",
    "        polynomial_x = build_combinations_lvl(polynomial_x, 2, 10)\n",
    "        polynomial_x = build_combinations_lvl(polynomial_x, 3, 10)\n",
    "        polynomial_x = build_combinations_lvl(polynomial_x, 4, 10)\n",
    "\n",
    "    return polynomial_x\n",
    "\n",
    "def build_best_model(x_, y_, numjet, index):\n",
    "    \"\"\"\n",
    "    Build the best model with the best parameters\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize k_fold and prediction threshold and build features\n",
    "    k = 5\n",
    "    predict_threshold = 0\n",
    "    polynomial_x = build_features(x_, numjet, index)\n",
    "\n",
    "    # Use the best lambda for best result\n",
    "    if numjet == 0 and index == 0:\n",
    "        lambda_ = 7.27895384398e-05\n",
    "    elif numjet == 0 and index == 1:\n",
    "        lambda_ = 1e-08\n",
    "    elif numjet == 1 and index == 1:\n",
    "        lambda_ = 0.137382379588\n",
    "    elif numjet == 2 and index == 0:\n",
    "        lambda_ = 2.39502661999e-07\n",
    "    elif numjet == 2 and index == 1:\n",
    "        lambda_ = 0.0417531893656\n",
    "        predict_threshold = -0.0323232323232\n",
    "    elif numjet == 3 and index == 0:\n",
    "        lambda_ = 7.27895384398e-05\n",
    "    elif numjet == 3 and index == 1:\n",
    "        lambda_ = 1.0\n",
    "    else:\n",
    "        lambda_ = 0.000001\n",
    "\n",
    "\n",
    "    #Gest the accuracy of test and train using k_fold_corss_validation\n",
    "    accuracy_train_k, accuracy_test_k = k_fold_cross_validation(y_, polynomial_x, k, lambda_, predict_threshold)\n",
    "    # Find optimal weights and loss with ridge regression\n",
    "    w, loss = ridge_regression(y_, polynomial_x, lambda_)\n",
    "\n",
    "    \n",
    "    print(\"\\t Predicted -1 but was 1 :\", get_false(polynomial_x, y_, w, predict_threshold))\n",
    "\n",
    "    \n",
    "    return w, predict_threshold, accuracy_train_k, accuracy_test_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-10-26 22:29:36.613819 combinations 2 : 0 / 28\n",
      "2017-10-26 22:29:38.751821 combinations 3 : 0 / 56\n",
      "2017-10-26 22:29:43.789818 combinations 3 : 50 / 56\n",
      "2017-10-26 22:29:44.447860 combinations 4 : 0 / 70\n",
      "2017-10-26 22:29:50.586821 combinations 4 : 50 / 70\n",
      "2017-10-26 22:29:53.109858 combinations 5 : 0 / 56\n",
      "2017-10-26 22:30:00.656818 combinations 5 : 50 / 56\n",
      "2017-10-26 22:30:01.581887 combinations 6 : 0 / 28\n",
      "2017-10-26 22:30:06.337820 combinations 7 : 0 / 8\n",
      "\t Predicted -1 but was 1 : 0.6371915393654524\n",
      "0 0 Train Accuracy: 0.815523783711\n",
      "0 0 Test Accuracy: 0.814459953923\n",
      "2017-10-26 22:30:22.922819 combinations 2 : 0 / 28\n",
      "2017-10-26 22:30:25.097820 combinations 3 : 0 / 56\n",
      "2017-10-26 22:30:30.212820 combinations 3 : 50 / 56\n",
      "2017-10-26 22:30:30.867837 combinations 4 : 0 / 70\n",
      "2017-10-26 22:30:37.037821 combinations 4 : 50 / 70\n",
      "2017-10-26 22:30:39.531819 combinations 5 : 0 / 56\n",
      "2017-10-26 22:30:46.766111 combinations 5 : 50 / 56\n",
      "2017-10-26 22:30:47.659113 combinations 6 : 0 / 28\n",
      "2017-10-26 22:30:51.995114 combinations 7 : 0 / 8\n",
      "2017-10-26 22:30:55.795151 combinations 2 : 0 / 28\n",
      "2017-10-26 22:31:01.104115 combinations 3 : 0 / 56\n",
      "2017-10-26 22:31:12.230083 combinations 3 : 50 / 56\n",
      "2017-10-26 22:31:14.259087 combinations 4 : 0 / 70\n",
      "2017-10-26 22:31:28.916083 combinations 4 : 50 / 70\n",
      "2017-10-26 22:31:34.893678 combinations 5 : 0 / 56\n",
      "2017-10-26 22:31:53.085628 combinations 5 : 50 / 56\n",
      "2017-10-26 22:31:56.203530 combinations 6 : 0 / 28\n",
      "2017-10-26 22:32:09.198544 combinations 7 : 0 / 8\n",
      "\t Predicted -1 but was 1 : 0.9561200923787528\n",
      "0 1 Train Accuracy: 0.95047856049\n",
      "0 1 Test Accuracy: 0.949961715161\n",
      "2017-10-26 22:32:16.165500 combinations 2 : 0 / 45\n",
      "2017-10-26 22:32:20.542509 combinations 3 : 0 / 120\n",
      "2017-10-26 22:32:26.201545 combinations 3 : 50 / 120\n",
      "2017-10-26 22:32:32.812548 combinations 3 : 100 / 120\n",
      "2017-10-26 22:32:35.899514 combinations 4 : 0 / 210\n",
      "2017-10-26 22:32:43.974546 combinations 4 : 50 / 210\n",
      "2017-10-26 22:32:52.432547 combinations 4 : 100 / 210\n",
      "2017-10-26 22:33:02.337540 combinations 4 : 150 / 210\n",
      "2017-10-26 22:33:13.797836 combinations 4 : 200 / 210\n",
      "\t Predicted -1 but was 1 : 0.5616155660377359\n",
      "1 0 Train Accuracy: 0.806441126036\n",
      "1 0 Test Accuracy: 0.802829379823\n",
      "2017-10-26 22:33:35.630316 combinations 2 : 0 / 45\n",
      "2017-10-26 22:33:40.033892 combinations 3 : 0 / 120\n",
      "2017-10-26 22:33:45.791863 combinations 3 : 50 / 120\n",
      "2017-10-26 22:33:52.920861 combinations 3 : 100 / 120\n",
      "2017-10-26 22:33:56.257895 combinations 4 : 0 / 210\n",
      "2017-10-26 22:34:04.247909 combinations 4 : 50 / 210\n",
      "2017-10-26 22:34:12.952899 combinations 4 : 100 / 210\n",
      "2017-10-26 22:34:22.557860 combinations 4 : 150 / 210\n",
      "2017-10-26 22:34:33.126860 combinations 4 : 200 / 210\n",
      "2017-10-26 22:34:38.258912 combinations 2 : 0 / 45\n",
      "2017-10-26 22:34:47.797903 combinations 3 : 0 / 120\n",
      "2017-10-26 22:35:00.038891 combinations 3 : 50 / 120\n",
      "2017-10-26 22:35:14.766900 combinations 3 : 100 / 120\n",
      "2017-10-26 22:35:21.059909 combinations 4 : 0 / 210\n",
      "2017-10-26 22:35:39.177861 combinations 4 : 50 / 210\n",
      "2017-10-26 22:36:03.784374 combinations 4 : 100 / 210\n",
      "2017-10-26 22:36:27.463196 combinations 4 : 150 / 210\n",
      "2017-10-26 22:36:53.856674 combinations 4 : 200 / 210\n",
      "2017-10-26 22:36:58.895640 combinations 2 : 0 / 45\n",
      "2017-10-26 22:36:59.333638 combinations 3 : 0 / 120\n",
      "2017-10-26 22:36:59.885638 combinations 3 : 50 / 120\n",
      "2017-10-26 22:37:00.623329 combinations 3 : 100 / 120\n",
      "2017-10-26 22:37:00.997344 combinations 4 : 0 / 210\n",
      "2017-10-26 22:37:02.105313 combinations 4 : 50 / 210\n",
      "2017-10-26 22:37:03.162313 combinations 4 : 100 / 210\n",
      "2017-10-26 22:37:04.276351 combinations 4 : 150 / 210\n",
      "2017-10-26 22:37:05.439351 combinations 4 : 200 / 210\n",
      "2017-10-26 22:37:06.177435 combinations 5 : 0 / 252\n",
      "2017-10-26 22:37:07.480410 combinations 5 : 50 / 252\n",
      "2017-10-26 22:37:08.923410 combinations 5 : 100 / 252\n",
      "2017-10-26 22:37:10.436408 combinations 5 : 150 / 252\n",
      "2017-10-26 22:37:12.159331 combinations 5 : 200 / 252\n",
      "2017-10-26 22:37:13.705376 combinations 5 : 250 / 252\n",
      "\t Predicted -1 but was 1 : 0.9713740458015268\n",
      "1 1 Train Accuracy: 0.93167989418\n",
      "1 1 Test Accuracy: 0.921560846561\n",
      "2017-10-26 22:37:17.110331 combinations 2 : 0 / 45\n",
      "2017-10-26 22:37:17.521375 combinations 3 : 0 / 120\n",
      "2017-10-26 22:37:18.110333 combinations 3 : 50 / 120\n",
      "2017-10-26 22:37:18.818366 combinations 3 : 100 / 120\n",
      "2017-10-26 22:37:19.172718 combinations 4 : 0 / 210\n",
      "2017-10-26 22:37:20.046228 combinations 4 : 50 / 210\n",
      "2017-10-26 22:37:21.009255 combinations 4 : 100 / 210\n",
      "2017-10-26 22:37:22.097266 combinations 4 : 150 / 210\n",
      "2017-10-26 22:37:23.309222 combinations 4 : 200 / 210\n",
      "2017-10-26 22:37:23.549227 combinations 5 : 0 / 252\n",
      "2017-10-26 22:37:24.805222 combinations 5 : 50 / 252\n",
      "2017-10-26 22:37:26.213183 combinations 5 : 100 / 252\n",
      "2017-10-26 22:37:27.571183 combinations 5 : 150 / 252\n",
      "2017-10-26 22:37:28.975199 combinations 5 : 200 / 252\n",
      "2017-10-26 22:37:30.628182 combinations 5 : 250 / 252\n",
      "2017-10-26 22:37:31.038181 combinations 2 : 0 / 45\n",
      "2017-10-26 22:37:31.938211 combinations 3 : 0 / 120\n",
      "2017-10-26 22:37:33.933332 combinations 3 : 50 / 120\n",
      "2017-10-26 22:37:36.609317 combinations 3 : 100 / 120\n",
      "2017-10-26 22:37:37.488296 combinations 4 : 0 / 210\n",
      "2017-10-26 22:37:40.168544 combinations 4 : 50 / 210\n",
      "2017-10-26 22:37:44.213542 combinations 4 : 100 / 210\n",
      "2017-10-26 22:37:47.537615 combinations 4 : 150 / 210\n",
      "2017-10-26 22:37:50.666379 combinations 4 : 200 / 210\n",
      "2017-10-26 22:37:51.243391 combinations 5 : 0 / 252\n",
      "2017-10-26 22:37:54.164895 combinations 5 : 50 / 252\n",
      "2017-10-26 22:37:57.313430 combinations 5 : 100 / 252\n",
      "2017-10-26 22:38:00.985163 combinations 5 : 150 / 252\n",
      "2017-10-26 22:38:04.654911 combinations 5 : 200 / 252\n",
      "2017-10-26 22:38:08.728904 combinations 5 : 250 / 252\n",
      "2017-10-26 22:38:10.263876 combinations 2 : 0 / 45\n",
      "2017-10-26 22:38:14.852878 combinations 3 : 0 / 120\n",
      "2017-10-26 22:38:19.955916 combinations 3 : 50 / 120\n",
      "2017-10-26 22:38:25.511875 combinations 3 : 100 / 120\n",
      "2017-10-26 22:38:27.874879 combinations 4 : 0 / 210\n",
      "2017-10-26 22:38:34.141874 combinations 4 : 50 / 210\n",
      "2017-10-26 22:38:40.638914 combinations 4 : 100 / 210\n",
      "2017-10-26 22:38:48.185877 combinations 4 : 150 / 210\n",
      "2017-10-26 22:38:57.235875 combinations 4 : 200 / 210\n",
      "\t Predicted -1 but was 1 : 0.46076568015204994\n",
      "2 0 Train Accuracy: 0.845313653137\n",
      "2 0 Test Accuracy: 0.839599367422\n",
      "2017-10-26 22:39:18.520885 combinations 2 : 0 / 45\n",
      "2017-10-26 22:39:22.245877 combinations 3 : 0 / 120\n",
      "2017-10-26 22:39:26.786914 combinations 3 : 50 / 120\n",
      "2017-10-26 22:39:32.060874 combinations 3 : 100 / 120\n",
      "2017-10-26 22:39:34.419916 combinations 4 : 0 / 210\n",
      "2017-10-26 22:39:40.372876 combinations 4 : 50 / 210\n",
      "2017-10-26 22:39:48.646029 combinations 4 : 100 / 210\n",
      "2017-10-26 22:39:56.668849 combinations 4 : 150 / 210\n",
      "2017-10-26 22:40:04.810827 combinations 4 : 200 / 210\n",
      "2017-10-26 22:40:09.694602 combinations 2 : 0 / 45\n",
      "2017-10-26 22:40:19.019747 combinations 3 : 0 / 120\n",
      "2017-10-26 22:40:30.348325 combinations 3 : 50 / 120\n",
      "2017-10-26 22:40:42.363508 combinations 3 : 100 / 120\n",
      "2017-10-26 22:40:48.228594 combinations 4 : 0 / 210\n",
      "2017-10-26 22:41:05.063652 combinations 4 : 50 / 210\n",
      "2017-10-26 22:41:21.792652 combinations 4 : 100 / 210\n",
      "2017-10-26 22:41:40.554399 combinations 4 : 150 / 210\n",
      "2017-10-26 22:41:59.995418 combinations 4 : 200 / 210\n",
      "2017-10-26 22:42:03.733412 combinations 2 : 0 / 45\n",
      "2017-10-26 22:42:03.906433 combinations 3 : 0 / 120\n",
      "2017-10-26 22:42:04.154412 combinations 3 : 50 / 120\n",
      "2017-10-26 22:42:04.402413 combinations 3 : 100 / 120\n",
      "2017-10-26 22:42:04.503414 combinations 4 : 0 / 210\n",
      "2017-10-26 22:42:04.812414 combinations 4 : 50 / 210\n",
      "2017-10-26 22:42:05.155414 combinations 4 : 100 / 210\n",
      "2017-10-26 22:42:05.525412 combinations 4 : 150 / 210\n",
      "2017-10-26 22:42:05.920455 combinations 4 : 200 / 210\n",
      "2017-10-26 22:42:06.007453 combinations 5 : 0 / 252\n",
      "2017-10-26 22:42:06.451414 combinations 5 : 50 / 252\n",
      "2017-10-26 22:42:06.920414 combinations 5 : 100 / 252\n",
      "2017-10-26 22:42:07.448414 combinations 5 : 150 / 252\n",
      "2017-10-26 22:42:07.985414 combinations 5 : 200 / 252\n",
      "2017-10-26 22:42:08.555411 combinations 5 : 250 / 252\n",
      "\t Predicted -1 but was 1 : 0.8138297872340425\n",
      "2 1 Train Accuracy: 0.939491525424\n",
      "2 1 Test Accuracy: 0.904406779661\n",
      "2017-10-26 22:42:09.937411 combinations 2 : 0 / 45\n",
      "2017-10-26 22:42:10.081411 combinations 3 : 0 / 120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-10-26 22:42:10.282412 combinations 3 : 50 / 120\n",
      "2017-10-26 22:42:10.499411 combinations 3 : 100 / 120\n",
      "2017-10-26 22:42:10.604417 combinations 4 : 0 / 210\n",
      "2017-10-26 22:42:10.899412 combinations 4 : 50 / 210\n",
      "2017-10-26 22:42:11.357415 combinations 4 : 100 / 210\n",
      "2017-10-26 22:42:11.796447 combinations 4 : 150 / 210\n",
      "2017-10-26 22:42:12.221419 combinations 4 : 200 / 210\n",
      "2017-10-26 22:42:12.315456 combinations 5 : 0 / 252\n",
      "2017-10-26 22:42:12.804412 combinations 5 : 50 / 252\n",
      "2017-10-26 22:42:13.307457 combinations 5 : 100 / 252\n",
      "2017-10-26 22:42:13.819427 combinations 5 : 150 / 252\n",
      "2017-10-26 22:42:14.472412 combinations 5 : 200 / 252\n",
      "2017-10-26 22:42:15.103417 combinations 5 : 250 / 252\n",
      "2017-10-26 22:42:15.279413 combinations 2 : 0 / 45\n",
      "2017-10-26 22:42:15.613414 combinations 3 : 0 / 120\n",
      "2017-10-26 22:42:16.141413 combinations 3 : 50 / 120\n",
      "2017-10-26 22:42:16.823453 combinations 3 : 100 / 120\n",
      "2017-10-26 22:42:17.066440 combinations 4 : 0 / 210\n",
      "2017-10-26 22:42:17.753443 combinations 4 : 50 / 210\n",
      "2017-10-26 22:42:18.497413 combinations 4 : 100 / 210\n",
      "2017-10-26 22:42:19.320414 combinations 4 : 150 / 210\n",
      "2017-10-26 22:42:20.581418 combinations 4 : 200 / 210\n",
      "2017-10-26 22:42:20.920415 combinations 5 : 0 / 252\n",
      "2017-10-26 22:42:22.347152 combinations 5 : 50 / 252\n",
      "2017-10-26 22:42:23.901711 combinations 5 : 100 / 252\n",
      "2017-10-26 22:42:25.375713 combinations 5 : 150 / 252\n",
      "2017-10-26 22:42:26.663712 combinations 5 : 200 / 252\n",
      "2017-10-26 22:42:28.097710 combinations 5 : 250 / 252\n",
      "2017-10-26 22:42:29.149747 combinations 2 : 0 / 28\n",
      "2017-10-26 22:42:30.498749 combinations 3 : 0 / 56\n",
      "2017-10-26 22:42:33.226718 combinations 3 : 50 / 56\n",
      "2017-10-26 22:42:33.589713 combinations 4 : 0 / 70\n",
      "2017-10-26 22:42:37.091710 combinations 4 : 50 / 70\n",
      "2017-10-26 22:42:38.570712 combinations 5 : 0 / 56\n",
      "2017-10-26 22:42:42.190741 combinations 5 : 50 / 56\n",
      "2017-10-26 22:42:42.669762 combinations 6 : 0 / 28\n",
      "\t Predicted -1 but was 1 : 0.5922480620155038\n",
      "3 0 Train Accuracy: 0.845177664975\n",
      "3 0 Test Accuracy: 0.832680686488\n",
      "2017-10-26 22:42:53.682583 combinations 2 : 0 / 28\n",
      "2017-10-26 22:42:55.251624 combinations 3 : 0 / 56\n",
      "2017-10-26 22:42:58.124612 combinations 3 : 50 / 56\n",
      "2017-10-26 22:42:58.452622 combinations 4 : 0 / 70\n",
      "2017-10-26 22:43:01.415596 combinations 4 : 50 / 70\n",
      "2017-10-26 22:43:02.676590 combinations 5 : 0 / 56\n",
      "2017-10-26 22:43:06.095630 combinations 5 : 50 / 56\n",
      "2017-10-26 22:43:06.518622 combinations 6 : 0 / 28\n",
      "2017-10-26 22:43:10.975584 combinations 2 : 0 / 28\n",
      "2017-10-26 22:43:14.898742 combinations 3 : 0 / 56\n",
      "2017-10-26 22:43:21.816738 combinations 3 : 50 / 56\n",
      "2017-10-26 22:43:22.588743 combinations 4 : 0 / 70\n",
      "2017-10-26 22:43:29.731353 combinations 4 : 50 / 70\n",
      "2017-10-26 22:43:32.812920 combinations 5 : 0 / 56\n",
      "2017-10-26 22:43:40.970921 combinations 5 : 50 / 56\n",
      "2017-10-26 22:43:42.029920 combinations 6 : 0 / 28\n",
      "(1477, 28)\n",
      "(1477, 1)\n",
      "2017-10-26 22:43:47.173950 combinations 2 : 0 / 28\n",
      "2017-10-26 22:43:47.230942 combinations 3 : 0 / 56\n",
      "2017-10-26 22:43:47.354962 combinations 3 : 50 / 56\n",
      "2017-10-26 22:43:47.370969 combinations 4 : 0 / 70\n",
      "2017-10-26 22:43:47.521034 combinations 4 : 50 / 70\n",
      "2017-10-26 22:43:47.597048 combinations 5 : 0 / 56\n",
      "2017-10-26 22:43:47.785379 combinations 5 : 50 / 56\n",
      "2017-10-26 22:43:47.810383 combinations 6 : 0 / 28\n",
      "2017-10-26 22:43:47.920454 combinations 7 : 0 / 8\n",
      "\t Predicted -1 but was 1 : 0.9647058823529412\n",
      "3 1 Train Accuracy: 0.945084745763\n",
      "3 1 Test Accuracy: 0.928813559322\n",
      "(1477, 28)\n",
      "(1477, 1)\n",
      "2017-10-26 22:43:48.374571 combinations 2 : 0 / 28\n",
      "2017-10-26 22:43:48.435070 combinations 3 : 0 / 56\n",
      "2017-10-26 22:43:48.560569 combinations 3 : 50 / 56\n",
      "2017-10-26 22:43:48.578573 combinations 4 : 0 / 70\n",
      "2017-10-26 22:43:48.726668 combinations 4 : 50 / 70\n",
      "2017-10-26 22:43:48.785752 combinations 5 : 0 / 56\n",
      "2017-10-26 22:43:48.948789 combinations 5 : 50 / 56\n",
      "2017-10-26 22:43:48.972814 combinations 6 : 0 / 28\n",
      "2017-10-26 22:43:49.087312 combinations 7 : 0 / 8\n",
      "(1477, 28)\n",
      "(1477, 1)\n",
      "2017-10-26 22:43:49.249860 combinations 2 : 0 / 28\n",
      "2017-10-26 22:43:49.381395 combinations 3 : 0 / 56\n",
      "2017-10-26 22:43:49.660552 combinations 3 : 50 / 56\n",
      "2017-10-26 22:43:49.704536 combinations 4 : 0 / 70\n",
      "2017-10-26 22:43:50.086241 combinations 4 : 50 / 70\n",
      "2017-10-26 22:43:50.237889 combinations 5 : 0 / 56\n",
      "2017-10-26 22:43:50.642139 combinations 5 : 50 / 56\n",
      "2017-10-26 22:43:50.690646 combinations 6 : 0 / 28\n",
      "2017-10-26 22:43:50.939959 combinations 7 : 0 / 8\n",
      "Count: 250000\n",
      "Train Accuracy: 0.840930911252\n",
      "Test Accuracy: 0.836617238666\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Initialize variables to submit data, this includes the id.\n",
    "It is important as the data will be separated depending on its features and category\n",
    "\"\"\"\n",
    "count = 0\n",
    "\n",
    "accuracy_train = 0\n",
    "accuracy_test = 0\n",
    "\n",
    "submission_ids = []\n",
    "submission_y = []\n",
    "\n",
    "result_y = []\n",
    "result_ids = []\n",
    "\n",
    "# For each category in PRI_num_jet and if they have or not NA\n",
    "for numjet in range(0, 4):\n",
    "    for index in range(0, 2):\n",
    "        # Get the x, y and ID\n",
    "        x_, y_, ids_ = get_data_numjet(jet_num_x_dict, jet_num_y_dict, jet_num_ids_dict, numjet, index)\n",
    "        \n",
    "        # Get the optimal weights and accuracy\n",
    "        w, predict_threshold, accuracy_train_k, accuracy_test_k = build_best_model(x_, y_, numjet, index)\n",
    "        \n",
    "        # Get the number of elements in that category\n",
    "        number_of_el = len(y_)\n",
    "\n",
    "        # Add the accuracy in proportion to the number of elements (max 1 if all elements in 1 category)\n",
    "        accuracy_train += accuracy_train_k * number_of_el\n",
    "        accuracy_test += accuracy_test_k * number_of_el\n",
    "        \n",
    "        # PRint training and testing accuracy\n",
    "        print(numjet, index, \"Train Accuracy: \" + str(accuracy_train_k))\n",
    "        print(numjet, index, \"Test Accuracy: \" + str(accuracy_test_k))\n",
    "        \n",
    "        # Count the number of elements\n",
    "        count += number_of_el\n",
    "  \n",
    "        # Predict local\n",
    "        removed_col_key = list(jet_num_x_dict[numjet])[index]\n",
    "        sub_x2 = jet_num_x_dict[numjet][removed_col_key]\n",
    "        sub_ids2 = jet_num_ids_dict[numjet][removed_col_key]\n",
    "\n",
    "        sub_x2 = build_features(sub_x2, numjet, index)\n",
    "        pred_y2 = predict_labels(w, sub_x2, predict_threshold)\n",
    "        \n",
    "        for sub_index, sub_id in enumerate(sub_ids2):\n",
    "            result_ids.append(sub_id)\n",
    "            result_y.append(pred_y2[sub_index])\n",
    "        \n",
    "        \n",
    "        # Predict submission\n",
    "        removed_col_key = list(jet_num_x_dict[numjet])[index]\n",
    "        sub_x = sub_jet_num_x_dict[numjet][removed_col_key]\n",
    "        sub_ids = sub_jet_num_ids_dict[numjet][removed_col_key]\n",
    "        \n",
    "        sub_x = build_features(sub_x, numjet, index)\n",
    "        pred_y = predict_labels(w, sub_x, predict_threshold)\n",
    "        for sub_index, sub_id in enumerate(sub_ids):\n",
    "            submission_ids.append(sub_id)\n",
    "            submission_y.append(pred_y[sub_index])\n",
    "        \n",
    "print(\"Count:\", count)\n",
    "print(\"Train Accuracy: \" + str(accuracy_train / count))\n",
    "print(\"Test Accuracy: \" + str(accuracy_test / count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250000 250000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.84051600000000004"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get total accuracy in our train set\n",
    "def get_accuracy_ids(result_y, result_ids, y, ids):\n",
    "    stacked = np.column_stack((ids, y))\n",
    "    stacked = stacked[stacked[:,0].argsort()]\n",
    "    stacked_pred = np.column_stack((result_ids, result_y))\n",
    "    stacked_pred = stacked_pred[stacked_pred[:,0].argsort()]\n",
    "    \n",
    "    print(len(stacked_pred), len(stacked))\n",
    "    unique, counts = np.unique((stacked == stacked_pred)[:, 1], return_counts=True)\n",
    "    return dict(zip(unique, counts))[True] / len(y)\n",
    "\n",
    "get_accuracy_ids(result_y, result_ids, y, ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission file created !\n"
     ]
    }
   ],
   "source": [
    "# Create submission csv file\n",
    "submission_stacked = np.column_stack((submission_ids, submission_y))\n",
    "submission_stacked = submission_stacked[submission_stacked[:,0].argsort()]\n",
    "create_csv_submission(submission_stacked[:,0], submission_stacked[:,1], \"datas/submission.csv\")\n",
    "print('Submission file created !')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find the best model for specific classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(73790, 18)\n",
      "(73790, 1)\n",
      "2017-10-26 22:52:20.054175 combinations 2 : 0 / 28\n",
      "2017-10-26 22:52:23.519219 combinations 3 : 0 / 56\n",
      "2017-10-26 22:52:30.130171 combinations 3 : 50 / 56\n",
      "2017-10-26 22:52:31.007173 combinations 4 : 0 / 70\n",
      "2017-10-26 22:52:39.506176 combinations 4 : 50 / 70\n",
      "2017-10-26 22:52:43.244216 combinations 5 : 0 / 56\n"
     ]
    }
   ],
   "source": [
    "k = 5\n",
    "\n",
    "# Manually choose the categories\n",
    "x_, y_, ids_ = get_data_numjet(jet_num_x_dict, jet_num_y_dict, jet_num_ids_dict, 0, 0)\n",
    "\n",
    "def build_polynomial2(x, max_degree):\n",
    "    polynomial_x = x\n",
    "    # Create new features with the tanh of the original data\n",
    "    polynomial_x = np.concatenate((polynomial_x, np.tanh(x)), axis=1)\n",
    "    # Create new features with the ln of the original data\n",
    "    polynomial_x = np.concatenate((polynomial_x, np.log(np.abs(x))), axis=1)\n",
    "    # Create new features with the square root of the original data\n",
    "    polynomial_x = np.concatenate((polynomial_x, np.sqrt(np.abs(x))), axis=1)\n",
    "    \n",
    "    # Create polynomials of max_degree of the new data\n",
    "    for degree in range(2, max_degree + 1):\n",
    "        polynomial_x = np.concatenate((polynomial_x, np.power(x, degree)), axis=1)\n",
    "        polynomial_x = np.concatenate((polynomial_x, np.power(np.tanh(x), degree)), axis=1)\n",
    "        polynomial_x = np.concatenate((polynomial_x, np.power(np.log(np.abs(x)), degree)), axis=1)\n",
    "\n",
    "    return polynomial_x\n",
    "\n",
    "x_ = reorder_mi(x_, x_, y_)\n",
    "\n",
    "# Build combinations\n",
    "polynomial_x = normalize(x_)\n",
    "polynomial_x = build_polynomial(polynomial_x, 5)\n",
    "polynomial_x = build_combinations_lvl(polynomial_x, 2, 8)\n",
    "polynomial_x = build_combinations_lvl(polynomial_x, 3, 8)\n",
    "polynomial_x = build_combinations_lvl(polynomial_x, 4, 8)\n",
    "polynomial_x = build_combinations_lvl(polynomial_x, 5, 8)\n",
    "polynomial_x = build_combinations_lvl(polynomial_x, 6, 8)\n",
    "polynomial_x = build_combinations_lvl(polynomial_x, 7, 8)\n",
    "\n",
    "predict_threshold = -0.00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Here to test the accuracy of one specific classifier\n",
    "# Find best lambdas\n",
    "lambdas = np.logspace(-8, 0, 30)\n",
    "best_accuracy = 0\n",
    "best_lambda = 0\n",
    "for lambda_ in lambdas:\n",
    "    \n",
    "    accuracy_train_k, accuracy_test_k = k_fold_cross_validation(y_, polynomial_x, k, lambda_, predict_threshold)\n",
    "\n",
    "    if accuracy_test_k > best_accuracy:\n",
    "        best_accuracy = accuracy_test_k\n",
    "        best_lambda = lambda_\n",
    "    print(\"Lambdas:\", lambda_, \"Train:\", accuracy_train_k, \" Test:\", accuracy_test_k)\n",
    "\n",
    "w, loss = ridge_regression(y_, polynomial_x, best_lambda)\n",
    "\n",
    "print(\"\\t Predicted -1 but was 1 :\", get_false(polynomial_x, y_, w, predict_threshold))\n",
    "\n",
    "print(\"BEST:\", best_lambda, best_accuracy)\n",
    "lambda_ = best_lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thresh: 0.0 Train: 0.945084745763  Test: 0.928813559322\n",
      "Thresh: -0.0040404040404 Train: 0.945084745763  Test: 0.928813559322\n",
      "Thresh: -0.00808080808081 Train: 0.945084745763  Test: 0.928813559322\n",
      "Thresh: -0.0121212121212 Train: 0.945084745763  Test: 0.928813559322\n",
      "Thresh: -0.0161616161616 Train: 0.944915254237  Test: 0.92813559322\n",
      "Thresh: -0.020202020202 Train: 0.944915254237  Test: 0.92813559322\n",
      "Thresh: -0.0242424242424 Train: 0.944915254237  Test: 0.92813559322\n",
      "Thresh: -0.0282828282828 Train: 0.944745762712  Test: 0.92813559322\n",
      "Thresh: -0.0323232323232 Train: 0.944745762712  Test: 0.92813559322\n",
      "Thresh: -0.0363636363636 Train: 0.944915254237  Test: 0.92813559322\n",
      "Thresh: -0.040404040404 Train: 0.945084745763  Test: 0.92813559322\n",
      "Thresh: -0.0444444444444 Train: 0.945084745763  Test: 0.92813559322\n",
      "Thresh: -0.0484848484848 Train: 0.945084745763  Test: 0.92813559322\n",
      "Thresh: -0.0525252525253 Train: 0.944915254237  Test: 0.927457627119\n",
      "Thresh: -0.0565656565657 Train: 0.944915254237  Test: 0.927457627119\n",
      "Thresh: -0.0606060606061 Train: 0.944915254237  Test: 0.927457627119\n",
      "Thresh: -0.0646464646465 Train: 0.944915254237  Test: 0.927457627119\n",
      "Thresh: -0.0686868686869 Train: 0.945084745763  Test: 0.927457627119\n",
      "Thresh: -0.0727272727273 Train: 0.945254237288  Test: 0.927457627119\n",
      "Thresh: -0.0767676767677 Train: 0.945254237288  Test: 0.927457627119\n",
      "Thresh: -0.0808080808081 Train: 0.945254237288  Test: 0.927457627119\n",
      "Thresh: -0.0848484848485 Train: 0.945423728814  Test: 0.927457627119\n",
      "Thresh: -0.0888888888889 Train: 0.945423728814  Test: 0.926779661017\n",
      "Thresh: -0.0929292929293 Train: 0.945423728814  Test: 0.926779661017\n",
      "Thresh: -0.0969696969697 Train: 0.945423728814  Test: 0.926779661017\n",
      "Thresh: -0.10101010101 Train: 0.945593220339  Test: 0.926779661017\n",
      "Thresh: -0.105050505051 Train: 0.945593220339  Test: 0.926779661017\n",
      "Thresh: -0.109090909091 Train: 0.945593220339  Test: 0.926101694915\n",
      "Thresh: -0.113131313131 Train: 0.945593220339  Test: 0.926101694915\n",
      "Thresh: -0.117171717172 Train: 0.945762711864  Test: 0.926101694915\n",
      "Thresh: -0.121212121212 Train: 0.94593220339  Test: 0.926101694915\n",
      "Thresh: -0.125252525253 Train: 0.94593220339  Test: 0.925423728814\n",
      "Thresh: -0.129292929293 Train: 0.946101694915  Test: 0.924745762712\n",
      "Thresh: -0.133333333333 Train: 0.946101694915  Test: 0.924745762712\n",
      "Thresh: -0.137373737374 Train: 0.946101694915  Test: 0.924745762712\n",
      "Thresh: -0.141414141414 Train: 0.946101694915  Test: 0.924745762712\n",
      "Thresh: -0.145454545455 Train: 0.946101694915  Test: 0.92406779661\n",
      "Thresh: -0.149494949495 Train: 0.94593220339  Test: 0.92406779661\n",
      "Thresh: -0.153535353535 Train: 0.946101694915  Test: 0.923389830508\n",
      "Thresh: -0.157575757576 Train: 0.946101694915  Test: 0.923389830508\n",
      "Thresh: -0.161616161616 Train: 0.946101694915  Test: 0.923389830508\n",
      "Thresh: -0.165656565657 Train: 0.946101694915  Test: 0.923389830508\n",
      "Thresh: -0.169696969697 Train: 0.946101694915  Test: 0.923389830508\n",
      "Thresh: -0.173737373737 Train: 0.946101694915  Test: 0.923389830508\n",
      "Thresh: -0.177777777778 Train: 0.946101694915  Test: 0.923389830508\n",
      "Thresh: -0.181818181818 Train: 0.94593220339  Test: 0.923389830508\n",
      "Thresh: -0.185858585859 Train: 0.946101694915  Test: 0.923389830508\n",
      "Thresh: -0.189898989899 Train: 0.94593220339  Test: 0.92406779661\n",
      "Thresh: -0.193939393939 Train: 0.94593220339  Test: 0.92406779661\n",
      "Thresh: -0.19797979798 Train: 0.946271186441  Test: 0.92406779661\n",
      "Thresh: -0.20202020202 Train: 0.946440677966  Test: 0.92406779661\n",
      "Thresh: -0.206060606061 Train: 0.946440677966  Test: 0.92406779661\n",
      "Thresh: -0.210101010101 Train: 0.946440677966  Test: 0.92406779661\n",
      "Thresh: -0.214141414141 Train: 0.946271186441  Test: 0.92406779661\n",
      "Thresh: -0.218181818182 Train: 0.946271186441  Test: 0.92406779661\n",
      "Thresh: -0.222222222222 Train: 0.946271186441  Test: 0.92406779661\n",
      "Thresh: -0.226262626263 Train: 0.946271186441  Test: 0.92406779661\n",
      "Thresh: -0.230303030303 Train: 0.946440677966  Test: 0.923389830508\n",
      "Thresh: -0.234343434343 Train: 0.946101694915  Test: 0.923389830508\n",
      "Thresh: -0.238383838384 Train: 0.94593220339  Test: 0.923389830508\n",
      "Thresh: -0.242424242424 Train: 0.945762711864  Test: 0.923389830508\n",
      "Thresh: -0.246464646465 Train: 0.94593220339  Test: 0.923389830508\n",
      "Thresh: -0.250505050505 Train: 0.946101694915  Test: 0.92406779661\n",
      "Thresh: -0.254545454545 Train: 0.946101694915  Test: 0.92406779661\n",
      "Thresh: -0.258585858586 Train: 0.946101694915  Test: 0.923389830508\n",
      "Thresh: -0.262626262626 Train: 0.946271186441  Test: 0.922711864407\n",
      "Thresh: -0.266666666667 Train: 0.946271186441  Test: 0.922711864407\n",
      "Thresh: -0.270707070707 Train: 0.946610169492  Test: 0.922711864407\n",
      "Thresh: -0.274747474747 Train: 0.946779661017  Test: 0.922033898305\n",
      "Thresh: -0.278787878788 Train: 0.946779661017  Test: 0.921355932203\n",
      "Thresh: -0.282828282828 Train: 0.946779661017  Test: 0.921355932203\n",
      "Thresh: -0.286868686869 Train: 0.946949152542  Test: 0.921355932203\n",
      "Thresh: -0.290909090909 Train: 0.947118644068  Test: 0.920677966102\n",
      "Thresh: -0.294949494949 Train: 0.947288135593  Test: 0.920677966102\n",
      "Thresh: -0.29898989899 Train: 0.947288135593  Test: 0.92\n",
      "Thresh: -0.30303030303 Train: 0.947288135593  Test: 0.92\n",
      "Thresh: -0.307070707071 Train: 0.947627118644  Test: 0.920677966102\n",
      "Thresh: -0.311111111111 Train: 0.947627118644  Test: 0.919322033898\n",
      "Thresh: -0.315151515152 Train: 0.947457627119  Test: 0.919322033898\n",
      "Thresh: -0.319191919192 Train: 0.947627118644  Test: 0.919322033898\n",
      "Thresh: -0.323232323232 Train: 0.947627118644  Test: 0.919322033898\n",
      "Thresh: -0.327272727273 Train: 0.947627118644  Test: 0.919322033898\n",
      "Thresh: -0.331313131313 Train: 0.947288135593  Test: 0.919322033898\n",
      "Thresh: -0.335353535354 Train: 0.947627118644  Test: 0.919322033898\n",
      "Thresh: -0.339393939394 Train: 0.947457627119  Test: 0.919322033898\n",
      "Thresh: -0.343434343434 Train: 0.947627118644  Test: 0.919322033898\n",
      "Thresh: -0.347474747475 Train: 0.947288135593  Test: 0.919322033898\n",
      "Thresh: -0.351515151515 Train: 0.947457627119  Test: 0.919322033898\n",
      "Thresh: -0.355555555556 Train: 0.947457627119  Test: 0.919322033898\n",
      "Thresh: -0.359595959596 Train: 0.947118644068  Test: 0.919322033898\n",
      "Thresh: -0.363636363636 Train: 0.947457627119  Test: 0.92\n",
      "Thresh: -0.367676767677 Train: 0.947288135593  Test: 0.919322033898\n",
      "Thresh: -0.371717171717 Train: 0.947627118644  Test: 0.919322033898\n",
      "Thresh: -0.375757575758 Train: 0.947457627119  Test: 0.919322033898\n",
      "Thresh: -0.379797979798 Train: 0.947457627119  Test: 0.917966101695\n",
      "Thresh: -0.383838383838 Train: 0.947118644068  Test: 0.917288135593\n",
      "Thresh: -0.387878787879 Train: 0.947118644068  Test: 0.917288135593\n",
      "Thresh: -0.391919191919 Train: 0.947288135593  Test: 0.916610169492\n",
      "Thresh: -0.39595959596 Train: 0.947457627119  Test: 0.91593220339\n",
      "Thresh: -0.4 Train: 0.947457627119  Test: 0.916610169492\n",
      "\t Predicted -1 but was 1 : 0.8488372093023255\n",
      "BEST: 0.0 0.928813559322\n"
     ]
    }
   ],
   "source": [
    "# Find best threshold\n",
    "threshs = np.linspace(0, -0.4, num=100)\n",
    "best_accuracy = 0\n",
    "best_thresh = 0\n",
    "for thresh in threshs:\n",
    "    predict_threshold = thresh\n",
    "    \n",
    "    accuracy_train_k, accuracy_test_k = k_fold_cross_validation(y_, polynomial_x, k, lambda_, predict_threshold)\n",
    "\n",
    "    if accuracy_test_k > best_accuracy:\n",
    "        best_accuracy = accuracy_test_k\n",
    "        best_thresh = thresh\n",
    "    print(\"Thresh:\", thresh, \"Train:\", accuracy_train_k, \" Test:\", accuracy_test_k)\n",
    "\n",
    "w, loss = ridge_regression(y_, polynomial_x, best_lambda)\n",
    "\n",
    "print(\"\\t Predicted -1 but was 1 :\", get_false(polynomial_x, y_, w, predict_threshold))\n",
    "\n",
    "print(\"BEST:\", best_thresh, best_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions to implement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_, y_, ids_ = get_data_numjet(jet_num_x_dict, jet_num_y_dict, jet_num_ids_dict, 3, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2670.6732406203905"
      ]
     },
     "execution_count": 577,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = 0\n",
    "w = 0\n",
    "x = 0\n",
    "tx = 0\n",
    "\n",
    "y__ = []\n",
    "for i in y_:\n",
    "    y__.append(i[0] if i == 1 else 0)\n",
    "    \n",
    "\n",
    "x_ = normalize(x_) \n",
    "initial_w = np.random.rand(x_.shape[1], 1)\n",
    "calculate_loss(y_, x_, initial_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularized logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 629,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, loss=2670.673555720307\n",
      "Current iteration=100, loss=1992.5838713970966\n",
      "Current iteration=200, loss=1680.3961374405621\n",
      "Current iteration=300, loss=1493.5706444530838\n",
      "Current iteration=400, loss=1367.686225251261\n",
      "Current iteration=500, loss=1276.9139479615926\n",
      "Current iteration=600, loss=1208.5358819086453\n",
      "Current iteration=700, loss=1155.4359046451773\n",
      "Current iteration=800, loss=1113.2674569534631\n",
      "Current iteration=900, loss=1079.2014276754742\n",
      "Current iteration=1000, loss=1051.306276554273\n",
      "Current iteration=1100, loss=1028.213535890061\n",
      "Current iteration=1200, loss=1008.9245087144211\n",
      "Current iteration=1300, loss=992.6921936663058\n",
      "Current iteration=1400, loss=978.9459293278553\n",
      "Current iteration=1500, loss=967.2417222869964\n",
      "Current iteration=1600, loss=957.2287390256013\n",
      "Current iteration=1700, loss=948.6262380103396\n",
      "Current iteration=1800, loss=941.2073014830423\n",
      "Current iteration=1900, loss=934.7870272084164\n",
      "Current iteration=2000, loss=929.2137104136083\n",
      "Current iteration=2100, loss=924.3621082775786\n",
      "Current iteration=2200, loss=920.1282161432421\n",
      "Current iteration=2300, loss=916.4251771423893\n",
      "Current iteration=2400, loss=913.1800587003754\n",
      "Current iteration=2500, loss=910.3312993864935\n",
      "Current iteration=2600, loss=907.8266770614821\n",
      "Current iteration=2700, loss=905.6216840499955\n",
      "Current iteration=2800, loss=903.6782211708678\n",
      "Current iteration=2900, loss=901.963542462933\n",
      "Current iteration=3000, loss=900.4493977932235\n",
      "Current iteration=3100, loss=899.1113323361291\n",
      "Current iteration=3200, loss=897.928110989941\n",
      "Current iteration=3300, loss=896.8812427887784\n",
      "Current iteration=3400, loss=895.9545857510336\n",
      "Current iteration=3500, loss=895.1340167749621\n",
      "Current iteration=3600, loss=894.4071544150544\n",
      "Current iteration=3700, loss=893.7631248821205\n",
      "Current iteration=3800, loss=893.1923635663084\n",
      "Current iteration=3900, loss=892.6864459135795\n",
      "Current iteration=4000, loss=892.2379426863654\n",
      "Current iteration=4100, loss=891.8402955915412\n",
      "Current iteration=4200, loss=891.4877100014506\n",
      "Current iteration=4300, loss=891.1750621033783\n",
      "Current iteration=4400, loss=890.8978182766496\n",
      "Current iteration=4500, loss=890.6519648859271\n",
      "Current iteration=4600, loss=890.4339469838512\n",
      "Current iteration=4700, loss=890.2406146696973\n",
      "Current iteration=4800, loss=890.0691760448206\n",
      "Current iteration=4900, loss=889.9171558800342\n",
      "loss=889.7817529411287\n"
     ]
    }
   ],
   "source": [
    "w, loss = reg_logistic_regression(y_, x_, 0.1, initial_w, 5000, 0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 636,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, loss=[[ 3123.24252767]]\n",
      "Current iteration=100, loss=[[ 888.71125654]]\n",
      "Current iteration=200, loss=[[ 888.71125642]]\n",
      "Current iteration=300, loss=[[ 888.71125642]]\n",
      "Current iteration=400, loss=[[ 888.71125642]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 888.71125642]])"
      ]
     },
     "execution_count": 636,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w, loss = logistic_regression(y_, x_, initial_w, 500, 0.1)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/99): loss=16.31680577135001\t\t0.3744075829383886\n",
      "Gradient Descent(1/99): loss=50.736759376294046\t\t0.6269465132024373\n",
      "Gradient Descent(2/99): loss=181.81340506469255\t\t0.37576167907921465\n",
      "Gradient Descent(3/99): loss=660.8932795968924\t\t0.6276235612728503\n",
      "Gradient Descent(4/99): loss=2408.6660456413883\t\t0.3744075829383886\n",
      "Gradient Descent(5/99): loss=8783.604796420854\t\t0.6262694651320244\n",
      "Gradient Descent(6/99): loss=32035.334534264457\t\t0.3744075829383886\n",
      "Gradient Descent(7/99): loss=116842.53028677976\t\t0.6262694651320244\n",
      "Gradient Descent(8/99): loss=426163.8384898581\t\t0.37373053486797564\n",
      "Gradient Descent(9/99): loss=1554365.9815739973\t\t0.6262694651320244\n",
      "Gradient Descent(10/99): loss=5669310.581684775\t\t0.37373053486797564\n",
      "Gradient Descent(11/99): loss=20677940.987653714\t\t0.6262694651320244\n",
      "Gradient Descent(12/99): loss=75419622.43471941\t\t0.37373053486797564\n",
      "Gradient Descent(13/99): loss=275081524.0263116\t\t0.6262694651320244\n",
      "Gradient Descent(14/99): loss=1003317739.8673092\t\t0.37373053486797564\n",
      "Gradient Descent(15/99): loss=3659447836.952078\t\t0.6262694651320244\n",
      "Gradient Descent(16/99): loss=13347275686.163767\t\t0.37373053486797564\n",
      "Gradient Descent(17/99): loss=48682144463.46543\t\t0.6262694651320244\n",
      "Gradient Descent(18/99): loss=177560668209.182\t\t0.37373053486797564\n",
      "Gradient Descent(19/99): loss=647625350989.1755\t\t0.6262694651320244\n",
      "Gradient Descent(20/99): loss=2362114309855.7534\t\t0.37373053486797564\n",
      "Gradient Descent(21/99): loss=8615450282027.002\t\t0.6262694651320244\n",
      "Gradient Descent(22/99): loss=31423535792651.773\t\t0.37373053486797564\n",
      "Gradient Descent(23/99): loss=114612535548145.03\t\t0.6262694651320244\n",
      "Gradient Descent(24/99): loss=418031675093886.5\t\t0.37373053486797564\n",
      "Gradient Descent(25/99): loss=1524706530101972.2\t\t0.6262694651320244\n",
      "Gradient Descent(26/99): loss=5561133620827851.0\t\t0.37373053486797564\n",
      "Gradient Descent(27/99): loss=2.028338341715736e+16\t\t0.6262694651320244\n",
      "Gradient Descent(28/99): loss=7.398053542654661e+16\t\t0.37373053486797564\n",
      "Gradient Descent(29/99): loss=2.6983267581329168e+17\t\t0.6262694651320244\n",
      "Gradient Descent(30/99): loss=9.841733709652837e+17\t\t0.37373053486797564\n",
      "Gradient Descent(31/99): loss=3.5896216838741284e+18\t\t0.6262694651320244\n",
      "Gradient Descent(32/99): loss=1.3092595485184956e+19\t\t0.37373053486797564\n",
      "Gradient Descent(33/99): loss=4.775323742575663e+19\t\t0.6262694651320244\n",
      "Gradient Descent(34/99): loss=1.7417262201532596e+20\t\t0.37373053486797564\n",
      "Gradient Descent(35/99): loss=6.352679712419089e+20\t\t0.6262694651320244\n",
      "Gradient Descent(36/99): loss=2.317042659266506e+21\t\t0.37373053486797564\n",
      "Gradient Descent(37/99): loss=8.451058337421542e+21\t\t0.6262694651320244\n",
      "Gradient Descent(38/99): loss=3.082394134474474e+22\t\t0.37373053486797564\n",
      "Gradient Descent(39/99): loss=1.124256066032729e+23\t\t0.6262694651320244\n",
      "Gradient Descent(40/99): loss=4.100551866080172e+23\t\t0.37373053486797564\n",
      "Gradient Descent(41/99): loss=1.4956135096294056e+24\t\t0.6262694651320244\n",
      "Gradient Descent(42/99): loss=5.455021283084664e+24\t\t0.37373053486797564\n",
      "Gradient Descent(43/99): loss=1.989635491209232e+25\t\t0.6262694651320244\n",
      "Gradient Descent(44/99): loss=7.256890821223146e+25\t\t0.37373053486797564\n",
      "Gradient Descent(45/99): loss=2.6468398168322962e+26\t\t0.6262694651320244\n",
      "Gradient Descent(46/99): loss=9.653942974421112e+26\t\t0.37373053486797564\n",
      "Gradient Descent(47/99): loss=3.5211278884611e+27\t\t0.6262694651320244\n",
      "Gradient Descent(48/99): loss=1.2842774853496558e+28\t\t0.37373053486797564\n",
      "Gradient Descent(49/99): loss=4.684205492169409e+28\t\t0.6262694651320244\n",
      "Gradient Descent(50/99): loss=1.7084922334285263e+29\t\t0.37373053486797564\n",
      "Gradient Descent(51/99): loss=6.2314638342942035e+29\t\t0.6262694651320244\n",
      "Gradient Descent(52/99): loss=2.2728310236559865e+30\t\t0.37373053486797564\n",
      "Gradient Descent(53/99): loss=8.289803165772864e+30\t\t0.6262694651320244\n",
      "Gradient Descent(54/99): loss=3.0235787795925195e+31\t\t0.37373053486797564\n",
      "Gradient Descent(55/99): loss=1.1028040658610579e+32\t\t0.6262694651320244\n",
      "Gradient Descent(56/99): loss=4.0223089799683744e+32\t\t0.37373053486797564\n",
      "Gradient Descent(57/99): loss=1.4670756148964553e+33\t\t0.6262694651320244\n",
      "Gradient Descent(58/99): loss=5.350933681481461e+33\t\t0.37373053486797564\n",
      "Gradient Descent(59/99): loss=1.9516711322090655e+34\t\t0.6262694651320244\n",
      "Gradient Descent(60/99): loss=7.118421634490612e+34\t\t0.37373053486797564\n",
      "Gradient Descent(61/99): loss=2.5963353010724337e+35\t\t0.6262694651320244\n",
      "Gradient Descent(62/99): loss=9.46973548593018e+35\t\t0.37373053486797564\n",
      "Gradient Descent(63/99): loss=3.4539410274337075e+36\t\t0.6262694651320244\n",
      "Gradient Descent(64/99): loss=1.2597721064875127e+37\t\t0.37373053486797564\n",
      "Gradient Descent(65/99): loss=4.594825874786729e+37\t\t0.6262694651320244\n",
      "Gradient Descent(66/99): loss=1.6758923864789429e+38\t\t0.37373053486797564\n",
      "Gradient Descent(67/99): loss=6.112560883906073e+38\t\t0.6262694651320244\n",
      "Gradient Descent(68/99): loss=2.229462993024228e+39\t\t0.37373053486797564\n",
      "Gradient Descent(69/99): loss=8.131624914119924e+39\t\t0.6262694651320244\n",
      "Gradient Descent(70/99): loss=2.965885684168306e+40\t\t0.37373053486797564\n",
      "Gradient Descent(71/99): loss=1.08176139264369e+41\t\t0.6262694651320244\n",
      "Gradient Descent(72/99): loss=3.945559051250373e+41\t\t0.37373053486797564\n",
      "Gradient Descent(73/99): loss=1.4390822535142315e+42\t\t0.6262694651320244\n",
      "Gradient Descent(74/99): loss=5.248832181901566e+42\t\t0.37373053486797564\n",
      "Gradient Descent(75/99): loss=1.914431173512704e+43\t\t0.6262694651320244\n",
      "Gradient Descent(76/99): loss=6.982594586953325e+43\t\t0.37373053486797564\n",
      "Gradient Descent(77/99): loss=2.546794465130263e+44\t\t0.6262694651320244\n",
      "Gradient Descent(78/99): loss=9.289042872025326e+44\t\t0.37373053486797564\n",
      "Gradient Descent(79/99): loss=3.388036163095366e+45\t\t0.6262694651320244\n",
      "Gradient Descent(80/99): loss=1.2357343162890582e+46\t\t0.37373053486797564\n",
      "Gradient Descent(81/99): loss=4.507151715462383e+46\t\t0.6262694651320244\n",
      "Gradient Descent(82/99): loss=1.6439145792438786e+47\t\t0.37373053486797564\n",
      "Gradient Descent(83/99): loss=5.995926728136193e+47\t\t0.6262694651320244\n",
      "Gradient Descent(84/99): loss=2.186922470491976e+48\t\t0.37373053486797564\n",
      "Gradient Descent(85/99): loss=7.976464871560206e+48\t\t0.6262694651320244\n",
      "Gradient Descent(86/99): loss=2.9092934342990647e+49\t\t0.37373053486797564\n",
      "Gradient Descent(87/99): loss=1.0611202359874595e+50\t\t0.6262694651320244\n",
      "Gradient Descent(88/99): loss=3.8702735927129445e+50\t\t0.37373053486797564\n",
      "Gradient Descent(89/99): loss=1.4116230352078762e+51\t\t0.6262694651320244\n",
      "Gradient Descent(90/99): loss=5.148678887408291e+51\t\t0.37373053486797564\n",
      "Gradient Descent(91/99): loss=1.877901792792732e+52\t\t0.6262694651320244\n",
      "Gradient Descent(92/99): loss=6.849359263788369e+52\t\t0.37373053486797564\n",
      "Gradient Descent(93/99): loss=2.4981989209710236e+53\t\t0.6262694651320244\n",
      "Gradient Descent(94/99): loss=9.111798065165152e+53\t\t0.37373053486797564\n",
      "Gradient Descent(95/99): loss=3.32338883358722e+54\t\t0.6262694651320244\n",
      "Gradient Descent(96/99): loss=1.21215519266581e+55\t\t0.37373053486797564\n",
      "Gradient Descent(97/99): loss=4.4211504722446904e+55\t\t0.6262694651320244\n",
      "Gradient Descent(98/99): loss=1.6125469425447092e+56\t\t0.37373053486797564\n",
      "Gradient Descent(99/99): loss=5.881518075972819e+56\t\t0.6262694651320244\n"
     ]
    }
   ],
   "source": [
    "w, loss = least_squares_GD(y_, x_, initial_w, 100, 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD (0/99): loss=103.7220312797626\t\t0.49085985104942453\n",
      "SGD (1/99): loss=634.9095254158576\t\t0.4617467840216655\n",
      "SGD (2/99): loss=712.8161550605626\t\t0.4813811780636425\n",
      "SGD (3/99): loss=22242.202312406975\t\t0.5639810426540285\n",
      "SGD (4/99): loss=36484.35925564048\t\t0.5260663507109005\n",
      "SGD (5/99): loss=365371.7919403375\t\t0.45091401489505756\n",
      "SGD (6/99): loss=667213.738941194\t\t0.4752877454299255\n",
      "SGD (7/99): loss=1678835.1118862384\t\t0.45023696682464454\n",
      "SGD (8/99): loss=30857173.477063492\t\t0.5199729180771835\n",
      "SGD (9/99): loss=33983658.9360351\t\t0.5186188219363574\n",
      "SGD (10/99): loss=61329227.89667664\t\t0.5016926201760324\n",
      "SGD (11/99): loss=1160543243.6444895\t\t0.5497630331753555\n",
      "SGD (12/99): loss=1493404993.3918626\t\t0.4901828029790115\n",
      "SGD (13/99): loss=24582257029.171535\t\t0.5660121868652674\n",
      "SGD (14/99): loss=153587856742.85718\t\t0.4163845633039946\n",
      "SGD (15/99): loss=159003153532.88318\t\t0.4197698036560596\n",
      "SGD (16/99): loss=431754571373.21985\t\t0.44888287068381855\n",
      "SGD (17/99): loss=3367053747642.421\t\t0.4109681787406906\n",
      "SGD (18/99): loss=22052084584018.336\t\t0.4996614759647935\n",
      "SGD (19/99): loss=1042920778120398.2\t\t0.5646580907244414\n",
      "SGD (20/99): loss=1311042385244817.2\t\t0.5849695328368314\n",
      "SGD (21/99): loss=2.508834384557883e+16\t\t0.4048747461069736\n",
      "SGD (22/99): loss=5.367357529487592e+16\t\t0.5260663507109005\n",
      "SGD (23/99): loss=8.488197234295928e+17\t\t0.47935003385240355\n",
      "SGD (24/99): loss=1.3643251729938324e+18\t\t0.45565335138794855\n",
      "SGD (25/99): loss=1.8490575885996462e+18\t\t0.47257955314827355\n",
      "SGD (26/99): loss=2.2831965480216937e+18\t\t0.4922139471902505\n",
      "SGD (27/99): loss=2.3488855414157107e+18\t\t0.4922139471902505\n",
      "SGD (28/99): loss=6.073475931056781e+18\t\t0.5172647257955315\n",
      "SGD (29/99): loss=6.442800152764658e+19\t\t0.5172647257955315\n",
      "SGD (30/99): loss=5.975749681484294e+20\t\t0.5010155721056195\n",
      "SGD (31/99): loss=2.1041227189600995e+21\t\t0.4949221394719025\n",
      "SGD (32/99): loss=4.52882829200159e+22\t\t0.4082599864590386\n",
      "SGD (33/99): loss=1.667144486645453e+23\t\t0.5565335138794855\n",
      "SGD (34/99): loss=2.0052706821595623e+24\t\t0.4895057549085985\n",
      "SGD (35/99): loss=1.8698789473189738e+25\t\t0.46445497630331756\n",
      "SGD (36/99): loss=2.014733311131743e+26\t\t0.41367637102234256\n",
      "SGD (37/99): loss=2.7825292246565322e+26\t\t0.47257955314827355\n",
      "SGD (38/99): loss=3.67202308062464e+26\t\t0.4969532836831415\n",
      "SGD (39/99): loss=4.970545643176643e+26\t\t0.5077860528097495\n",
      "SGD (40/99): loss=6.921640605334232e+27\t\t0.4177386594448206\n",
      "SGD (41/99): loss=3.0538687866839815e+29\t\t0.5937711577522004\n",
      "SGD (42/99): loss=3.372236969919859e+29\t\t0.5795531482735274\n",
      "SGD (43/99): loss=3.592358619172082e+29\t\t0.5098171970209885\n",
      "SGD (44/99): loss=1.3546051399292373e+30\t\t0.5375761679079215\n",
      "SGD (45/99): loss=1.3210867918747765e+30\t\t0.5341909275558565\n",
      "SGD (46/99): loss=1.1562930476879854e+31\t\t0.5098171970209885\n",
      "SGD (47/99): loss=1.3972439522412256e+31\t\t0.47935003385240355\n",
      "SGD (48/99): loss=4.878588906169809e+31\t\t0.5382532159783344\n",
      "SGD (49/99): loss=3.594716152215775e+32\t\t0.46377792823290453\n",
      "SGD (50/99): loss=1.063069494958782e+33\t\t0.4129993229519296\n",
      "SGD (51/99): loss=1.7986920276794597e+34\t\t0.5734597156398105\n",
      "SGD (52/99): loss=5.4247317296248876e+35\t\t0.44008124576844954\n",
      "SGD (53/99): loss=4.9507372087885424e+36\t\t0.5402843601895735\n",
      "SGD (54/99): loss=5.67065015076626e+36\t\t0.5030467163168585\n",
      "SGD (55/99): loss=1.977526272137279e+37\t\t0.5098171970209885\n",
      "SGD (56/99): loss=1.9946642488106768e+37\t\t0.5084631008801624\n",
      "SGD (57/99): loss=1.4548956049008777e+38\t\t0.41706161137440756\n",
      "SGD (58/99): loss=4.476453235886737e+38\t\t0.40216655382532157\n",
      "SGD (59/99): loss=3.373222073590032e+39\t\t0.5057549085985105\n",
      "SGD (60/99): loss=2.856595645050824e+39\t\t0.4983073798239675\n",
      "SGD (61/99): loss=3.2569781754418815e+40\t\t0.5680433310765064\n",
      "SGD (62/99): loss=3.7019903793835585e+41\t\t0.43195666892349355\n",
      "SGD (63/99): loss=2.1894063675408954e+42\t\t0.4922139471902505\n",
      "SGD (64/99): loss=1.335391831933947e+43\t\t0.4265402843601896\n",
      "SGD (65/99): loss=6.643120629384631e+43\t\t0.45700744752877454\n",
      "SGD (66/99): loss=2.177326995230265e+44\t\t0.4881516587677725\n",
      "SGD (67/99): loss=1.0429824639159066e+45\t\t0.44549763033175355\n",
      "SGD (68/99): loss=9.795121821899313e+44\t\t0.48747461069735953\n",
      "SGD (69/99): loss=4.151182873915304e+45\t\t0.5673662830060935\n",
      "SGD (70/99): loss=2.491028820033401e+46\t\t0.5639810426540285\n",
      "SGD (71/99): loss=8.282803341850198e+48\t\t0.3832092078537576\n",
      "SGD (72/99): loss=3.4349839344916973e+50\t\t0.5917400135409614\n",
      "SGD (73/99): loss=5.661769270216508e+53\t\t0.3764387271496276\n",
      "SGD (74/99): loss=5.5868095903179316e+53\t\t0.4631008801624915\n",
      "SGD (75/99): loss=5.025780218659698e+53\t\t0.5409614082599865\n",
      "SGD (76/99): loss=1.2369672956688747e+54\t\t0.5044008124576845\n",
      "SGD (77/99): loss=2.012530731359645e+54\t\t0.5240352064996615\n",
      "SGD (78/99): loss=5.632491505265596e+55\t\t0.4245091401489506\n",
      "SGD (79/99): loss=1.1380808387525578e+57\t\t0.5538253215978335\n",
      "SGD (80/99): loss=5.13248013449302e+58\t\t0.3947190250507786\n",
      "SGD (81/99): loss=7.720885145376256e+58\t\t0.3811780636425186\n",
      "SGD (82/99): loss=1.016918475266409e+60\t\t0.5165876777251185\n",
      "SGD (83/99): loss=1.1459966367868789e+60\t\t0.5321597833446174\n",
      "SGD (84/99): loss=6.799406259671735e+61\t\t0.3913337846987136\n",
      "SGD (85/99): loss=1.3409572078455599e+63\t\t0.5964793500338524\n",
      "SGD (86/99): loss=3.315816395480883e+64\t\t0.5355450236966824\n",
      "SGD (87/99): loss=3.237291426264388e+64\t\t0.5355450236966824\n",
      "SGD (88/99): loss=2.3222901249859855e+65\t\t0.3960731211916046\n",
      "SGD (89/99): loss=3.833670879320931e+65\t\t0.5104942450914015\n",
      "SGD (90/99): loss=8.725428213416975e+65\t\t0.47393364928909953\n",
      "SGD (91/99): loss=7.738735589709079e+65\t\t0.4888287068381855\n",
      "SGD (92/99): loss=9.221088326661237e+65\t\t0.45362220717670954\n",
      "SGD (93/99): loss=9.856960465265224e+65\t\t0.4617467840216655\n",
      "SGD (94/99): loss=1.7310977435435625e+66\t\t0.42247799593771157\n",
      "SGD (95/99): loss=2.7430887309023562e+66\t\t0.48408937034529453\n",
      "SGD (96/99): loss=1.9568518372740942e+68\t\t0.5788761002031144\n",
      "SGD (97/99): loss=3.9356792697189916e+68\t\t0.5368991198375085\n",
      "SGD (98/99): loss=4.2160568986531477e+68\t\t0.5619498984427894\n",
      "SGD (99/99): loss=1.5959937300301776e+69\t\t0.5260663507109005\n"
     ]
    }
   ],
   "source": [
    "w, loss = least_squares_SGD(y_, x_, initial_w, 100, 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank: 28\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(28, 1)"
      ]
     },
     "execution_count": 490,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w, loss = least_squares(y_, x_)\n",
    "w.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 691,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29, 1)"
      ]
     },
     "execution_count": 691,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w, loss = ridge_regression(y_, x_, 0.5)\n",
    "w.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_, y_, ids_ = get_data_numjet(jet_num_x_dict, jet_num_y_dict, jet_num_ids_dict, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13:55:49 0/5225\n",
      "13:58:21 500/5225\n",
      "14:00:54 1000/5225\n",
      "14:03:29 1500/5225\n",
      "14:06:05 2000/5225\n",
      "14:08:38 2500/5225\n",
      "14:17:57 3000/5225\n",
      "14:20:43 3500/5225\n",
      "14:23:21 4000/5225\n",
      "14:25:57 4500/5225\n",
      "14:28:39 5000/5225\n"
     ]
    }
   ],
   "source": [
    "train_x, train_y, test_x, test_y = separate_set(x_, y_)\n",
    "pred_y = knn_predict(train_x, train_y, test_x, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9483253588516747\n"
     ]
    }
   ],
   "source": [
    "correct_count = 0\n",
    "for index, yi in enumerate(test_y):\n",
    "    pred_yi = pred_y[index]\n",
    "    if pred_yi == yi:\n",
    "        correct_count += 1\n",
    "        \n",
    "print(correct_count / len(test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def k_fold_cross_validation_knn(y, x, k, knn_k):\n",
    "    \"\"\"return the accuracy of ridge regression.\"\"\"\n",
    "\n",
    "    # Return the array of which indeces go in each k interval\n",
    "    k_indices = build_k_indices(y, k)\n",
    "\n",
    "    # Create empty lists for the accuracy test data\n",
    "    accuracy_te = []\n",
    "\n",
    "    # Loop through each interval\n",
    "    for i in range(0, k):\n",
    "\n",
    "        # get k'th subgroup in test, others in train:\n",
    "        x_test = x[k_indices[i]]\n",
    "        y_test = y[k_indices[i]]\n",
    "        x_train = np.array([]).reshape(0, x.shape[1])\n",
    "        y_train = np.array([]).reshape(0, 1)\n",
    "\n",
    "        for j in range(0, k):\n",
    "            # If the index interval is different from test, put it in train\n",
    "            if j != i:\n",
    "                x_train = np.concatenate((x_train, x[k_indices[j]]))\n",
    "                y_train = np.concatenate((y_train, y[k_indices[j]]))\n",
    "\n",
    "        # KNN:\n",
    "        pred_y = knn_predict(x_train, y_train, x_test, 100)\n",
    "        \n",
    "        # Calculate the accuracy\n",
    "        correct_count = 0\n",
    "        for index, yi in enumerate(y_test):\n",
    "            pred_yi = pred_y[index]\n",
    "            if pred_yi == yi:\n",
    "                correct_count += 1\n",
    "        accuracy = correct_count / len(y_test)\n",
    "\n",
    "        # Append it to the list\n",
    "        accuracy_te.append(accuracy)\n",
    "\n",
    "    return np.mean(accuracy_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13:52:54 0/5224\n",
      "13:55:27 500/5224\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-297-76dc3e2a58f1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mk_fold_cross_validation_knn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-286-69c185eb25b5>\u001b[0m in \u001b[0;36mk_fold_cross_validation_knn\u001b[1;34m(y, x, k, knn_k)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[1;31m# KNN:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m         \u001b[0mpred_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mknn_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[0mcorrect_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Coac\\Documents\\PersonalProjects\\epfl\\epfl-ml-projects\\epfl-ml-project1\\knn.py\u001b[0m in \u001b[0;36mknn_predict\u001b[1;34m(x, y, x_to_predict, k)\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mdistances\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m             \u001b[0mdist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0meuclidean_distance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnot_label_x\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m             \u001b[0mdistances\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Coac\\Documents\\PersonalProjects\\epfl\\epfl-ml-projects\\epfl-ml-project1\\knn.py\u001b[0m in \u001b[0;36meuclidean_distance\u001b[1;34m(x1, x2)\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mdistance\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[0mdistance\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mpow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mx2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdistance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "k_fold_cross_validation_knn(y_, x_, 5, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 3.1 work better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not iterable",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-327-d07ef0330b26>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mx3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mids3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mremove_NaN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelete_columns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelete_rows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mlogistic_regression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Coac\\Documents\\PersonalProjects\\epfl\\epfl-ml-projects\\epfl-ml-project1\\pre_processing.py\u001b[0m in \u001b[0;36mremove_NaN\u001b[1;34m(x, y, ids, delete_columns, delete_rows)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;31m# If a sample is a NaN, add its column number and row number to the vector\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mrow_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mcol_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mfeature\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m999\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'int' object is not iterable"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "x3, y3, ids3 = remove_NaN(x, y, ids, delete_columns=True, delete_rows=False)\n",
    "logistic_regression(y3, x3, np.zeros((x3.shape[1], 1)), 1000, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 675,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-10-26 17:55:48.870554 combinations 2 : 0 / 45\n",
      "2017-10-26 17:55:48.935553 combinations 3 : 0 / 120\n",
      "2017-10-26 17:55:49.021552 combinations 3 : 50 / 120\n",
      "2017-10-26 17:55:49.137594 combinations 3 : 100 / 120\n",
      "2017-10-26 17:55:49.189589 combinations 4 : 0 / 210\n",
      "2017-10-26 17:55:49.317554 combinations 4 : 50 / 210\n",
      "2017-10-26 17:55:49.481579 combinations 4 : 100 / 210\n",
      "2017-10-26 17:55:49.672552 combinations 4 : 150 / 210\n",
      "2017-10-26 17:55:49.916554 combinations 4 : 200 / 210\n"
     ]
    }
   ],
   "source": [
    "x_, y_, ids_ = get_data_numjet(jet_num_x_dict, jet_num_y_dict, jet_num_ids_dict, 3, 1)\n",
    "y__ = []\n",
    "for i in y_:\n",
    "    y__.append(i[0] if i == 1 else 0)\n",
    "y__ = np.array(y__)\n",
    "\n",
    "\n",
    "\n",
    "polynomial_x = normalize(x_)\n",
    "polynomial_x = build_polynomial(polynomial_x, 2)\n",
    "polynomial_x = build_combinations_lvl(polynomial_x, 2, 10)\n",
    "polynomial_x = build_combinations_lvl(polynomial_x, 3, 10)\n",
    "polynomial_x = build_combinations_lvl(polynomial_x, 4, 10)\n",
    "polynomial_x = normalize(polynomial_x)\n",
    "\n",
    "\n",
    "initial_w = np.random.rand(polynomial_x.shape[1], 1)\n",
    "train_x, train_y, test_x, test_y = separate_set(polynomial_x, y_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 685,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, loss=[ 7561.11082329]\n",
      "Current iteration=100, loss=[ 6558.76591724]\n",
      "Current iteration=200, loss=[ 6336.41035596]\n",
      "Current iteration=300, loss=[ 6135.32695857]\n",
      "Current iteration=400, loss=[ 5953.39156277]\n",
      "Current iteration=500, loss=[ 5818.11703599]\n",
      "Current iteration=600, loss=[ 5681.33488939]\n",
      "Current iteration=700, loss=[ 5549.97436477]\n",
      "Current iteration=800, loss=[ 5430.76801074]\n",
      "Current iteration=900, loss=[ 5335.13873051]\n",
      "loss=[ 5249.88691813]\n"
     ]
    }
   ],
   "source": [
    "w, loss = reg_logistic_regression(train_y, train_x, 0.01, initial_w, 1000, 0.0000000005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 686,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.44256756756756754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Coac\\Documents\\PersonalProjects\\epfl\\epfl-ml-projects\\epfl-ml-project1\\implementation.py:110: RuntimeWarning: overflow encountered in exp\n",
      "  return np.exp(t) / (1 + np.exp(t))\n",
      "C:\\Users\\Coac\\Documents\\PersonalProjects\\epfl\\epfl-ml-projects\\epfl-ml-project1\\implementation.py:110: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(t) / (1 + np.exp(t))\n"
     ]
    }
   ],
   "source": [
    "pred_y = sigmoid(np.dot(test_x, w))\n",
    "pred_y[np.where(y_pred <= 0.5)] = -1\n",
    "pred_y[np.where(y_pred > 0.5)] = 1\n",
    "\n",
    "correct_count = 0\n",
    "for index, yi in enumerate(test_y):\n",
    "    pred_yi = pred_y[index]\n",
    "    if pred_yi == yi:\n",
    "        correct_count += 1\n",
    "print(correct_count / len(test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}