{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from helpers import *\n",
    "from costs import *\n",
    "from gradient_descent import *\n",
    "from stochastic_gradient_descent import *\n",
    "\n",
    "random.seed(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "y, x, ids = load_csv_data(data_path=\"datas/train.csv\", sub_sample=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set to 0 all column containing -999\n",
    "def remove_NaN(x):\n",
    "    columns_with_NaN = set(\"\")\n",
    "    for row in x:\n",
    "        for i,feature in enumerate(row):\n",
    "            if feature == -999:\n",
    "                columns_with_NaN.add(i)\n",
    "        \n",
    "    x = np.delete(x, [col for col in columns_with_NaN], axis=1)\n",
    "        \n",
    "    print(\"Cleaned \" + str(len(columns_with_NaN)) + \" columns\")\n",
    "        \n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned 11 columns\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.06833197,  0.40768027, -0.46996624, ...,  0.38684673,\n",
       "         1.04440205,  0.4125105 ],\n",
       "       [ 0.55250482,  0.54013641, -0.15316749, ..., -0.35771893,\n",
       "         0.02130497, -0.27381996],\n",
       "       [ 3.19515553,  1.09655998, -0.34970965, ...,  0.40013535,\n",
       "         0.02130497, -0.29396985],\n",
       "       ..., \n",
       "       [ 0.31931645, -0.13086367, -0.28495489, ..., -0.08608887,\n",
       "         0.02130497, -0.31701723],\n",
       "       [-0.84532397, -0.30297338, -0.69737766, ..., -0.76742886,\n",
       "        -1.00179211, -0.74543941],\n",
       "       [ 0.66533608, -0.25352276, -0.79202769, ..., -0.87267059,\n",
       "        -1.00179211, -0.74543941]])"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def normalize(x):\n",
    "    return (x - x.mean(axis=0)) / (x.std(axis=0) + 0.0000000001)\n",
    "\n",
    "\n",
    "def preprocess_data(x):\n",
    "    return normalize(remove_NaN(x))\n",
    "x = preprocess_data(x)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200000, 19)\n",
      "(200000,)\n",
      "(50000, 19)\n",
      "(50000,)\n"
     ]
    }
   ],
   "source": [
    "def separate_set(x, y):\n",
    "    x_and_y = np.concatenate((y.reshape((y.shape[0], 1)), x), axis=1)\n",
    "    np.random.shuffle(x_and_y)\n",
    "    \n",
    "    count = x_and_y.shape[0]\n",
    "    last_train_index = int(count * 0.8)\n",
    "    \n",
    "    train_set = x_and_y[0:last_train_index, :]\n",
    "    test_set = x_and_y[last_train_index:, :]\n",
    "    \n",
    "    train_y = train_set[:, 0]\n",
    "    test_y = test_set[:, 0]\n",
    "\n",
    "    train_x = train_set[:, 1:]\n",
    "    test_x = test_set[:, 1:]\n",
    "\n",
    "    return train_x, train_y, test_x, test_y\n",
    "\n",
    "train_x, train_y, test_x, test_y = separate_set(x, y)\n",
    "\n",
    "print(train_x.shape)\n",
    "print(train_y.shape)\n",
    "print(test_x.shape)\n",
    "print(test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/99): loss=5.59617539364443\t\t0.42319\n",
      "Gradient Descent(1/99): loss=2.5001047463576427\t\t0.61715\n",
      "Gradient Descent(2/99): loss=1.393074338097993\t\t0.46595\n",
      "Gradient Descent(3/99): loss=0.9010783906364982\t\t0.644165\n",
      "Gradient Descent(4/99): loss=0.6629962676706072\t\t0.56048\n",
      "Gradient Descent(5/99): loss=0.5434411941514207\t\t0.665315\n",
      "Gradient Descent(6/99): loss=0.48205495638705326\t\t0.65575\n",
      "Gradient Descent(7/99): loss=0.4499316523004009\t\t0.68387\n",
      "Gradient Descent(8/99): loss=0.4327644209422526\t\t0.69095\n",
      "Gradient Descent(9/99): loss=0.4233406696413214\t\t0.69537\n",
      "Gradient Descent(10/99): loss=0.4179787970595486\t\t0.700535\n",
      "Gradient Descent(11/99): loss=0.41478045311525763\t\t0.700715\n",
      "Gradient Descent(12/99): loss=0.4127571574707983\t\t0.703565\n",
      "Gradient Descent(13/99): loss=0.4113886040913678\t\t0.703005\n",
      "Gradient Descent(14/99): loss=0.4103973401057686\t\t0.70473\n",
      "Gradient Descent(15/99): loss=0.4096329887566649\t\t0.704565\n",
      "Gradient Descent(16/99): loss=0.409012344322813\t\t0.70557\n",
      "Gradient Descent(17/99): loss=0.40848814424849583\t\t0.705485\n",
      "Gradient Descent(18/99): loss=0.4080326505990167\t\t0.70618\n",
      "Gradient Descent(19/99): loss=0.4076289295886227\t\t0.706135\n",
      "Gradient Descent(20/99): loss=0.4072661627329231\t\t0.70668\n",
      "Gradient Descent(21/99): loss=0.4069370881645073\t\t0.70686\n",
      "Gradient Descent(22/99): loss=0.4066365793912423\t\t0.70696\n",
      "Gradient Descent(23/99): loss=0.4063608391361766\t\t0.70702\n",
      "Gradient Descent(24/99): loss=0.4061069308205478\t\t0.707145\n",
      "Gradient Descent(25/99): loss=0.4058724986868336\t\t0.70722\n",
      "Gradient Descent(26/99): loss=0.4056555954743584\t\t0.707225\n",
      "Gradient Descent(27/99): loss=0.40545457283211295\t\t0.707125\n",
      "Gradient Descent(28/99): loss=0.40526800925521345\t\t0.707265\n",
      "Gradient Descent(29/99): loss=0.4050946610719134\t\t0.707435\n",
      "Gradient Descent(30/99): loss=0.4049334279880021\t\t0.707455\n",
      "Gradient Descent(31/99): loss=0.40478332808569345\t\t0.70758\n",
      "Gradient Descent(32/99): loss=0.4046434791351244\t\t0.707485\n",
      "Gradient Descent(33/99): loss=0.4045130842356399\t\t0.70747\n",
      "Gradient Descent(34/99): loss=0.40439142050474913\t\t0.707475\n",
      "Gradient Descent(35/99): loss=0.40427782996606604\t\t0.707405\n",
      "Gradient Descent(36/99): loss=0.40417171206178537\t\t0.707435\n",
      "Gradient Descent(37/99): loss=0.40407251739256617\t\t0.707365\n",
      "Gradient Descent(38/99): loss=0.4039797424047378\t\t0.707275\n",
      "Gradient Descent(39/99): loss=0.40389292482346034\t\t0.70723\n",
      "Gradient Descent(40/99): loss=0.40381163968439476\t\t0.7073\n",
      "Gradient Descent(41/99): loss=0.4037354958539454\t\t0.70721\n",
      "Gradient Descent(42/99): loss=0.40366413295466747\t\t0.70722\n",
      "Gradient Descent(43/99): loss=0.40359721863144754\t\t0.70727\n",
      "Gradient Descent(44/99): loss=0.40353444610791606\t\t0.70721\n",
      "Gradient Descent(45/99): loss=0.40347553199276015\t\t0.707255\n",
      "Gradient Descent(46/99): loss=0.4034202143032559\t\t0.707215\n",
      "Gradient Descent(47/99): loss=0.4033682506791423\t\t0.70726\n",
      "Gradient Descent(48/99): loss=0.40331941676443844\t\t0.707245\n",
      "Gradient Descent(49/99): loss=0.40327350473829554\t\t0.70713\n",
      "Gradient Descent(50/99): loss=0.4032303219787558\t\t0.707055\n",
      "Gradient Descent(51/99): loss=0.40318968984551423\t\t0.70711\n",
      "Gradient Descent(52/99): loss=0.4031514425696052\t\t0.70714\n",
      "Gradient Descent(53/99): loss=0.40311542623943714\t\t0.707135\n",
      "Gradient Descent(54/99): loss=0.403081497873852\t\t0.70724\n",
      "Gradient Descent(55/99): loss=0.40304952457395776\t\t0.707225\n",
      "Gradient Descent(56/99): loss=0.403019382746384\t\t0.707175\n",
      "Gradient Descent(57/99): loss=0.4029909573913972\t\t0.70718\n",
      "Gradient Descent(58/99): loss=0.4029641414499915\t\t0.707175\n",
      "Gradient Descent(59/99): loss=0.4029388352046683\t\t0.707185\n",
      "Gradient Descent(60/99): loss=0.4029149457291385\t\t0.70725\n",
      "Gradient Descent(61/99): loss=0.40289238638264846\t\t0.70718\n",
      "Gradient Descent(62/99): loss=0.40287107634503816\t\t0.70718\n",
      "Gradient Descent(63/99): loss=0.4028509401890118\t\t0.707225\n",
      "Gradient Descent(64/99): loss=0.4028319074864258\t\t0.70721\n",
      "Gradient Descent(65/99): loss=0.4028139124456937\t\t0.70723\n",
      "Gradient Descent(66/99): loss=0.4027968935776758\t\t0.70724\n",
      "Gradient Descent(67/99): loss=0.4027807933876538\t\t0.70729\n",
      "Gradient Descent(68/99): loss=0.40276555809121173\t\t0.70728\n",
      "Gradient Descent(69/99): loss=0.4027511373520336\t\t0.70725\n",
      "Gradient Descent(70/99): loss=0.4027374840398082\t\t0.707295\n",
      "Gradient Descent(71/99): loss=0.4027245540065866\t\t0.70729\n",
      "Gradient Descent(72/99): loss=0.4027123058800855\t\t0.70728\n",
      "Gradient Descent(73/99): loss=0.4027007008725588\t\t0.70721\n",
      "Gradient Descent(74/99): loss=0.4026897026039783\t\t0.70725\n",
      "Gradient Descent(75/99): loss=0.40267927693837446\t\t0.70719\n",
      "Gradient Descent(76/99): loss=0.40266939183228206\t\t0.707205\n",
      "Gradient Descent(77/99): loss=0.402660017194332\t\t0.7072\n",
      "Gradient Descent(78/99): loss=0.4026511247551032\t\t0.70725\n",
      "Gradient Descent(79/99): loss=0.4026426879464304\t\t0.707275\n",
      "Gradient Descent(80/99): loss=0.40263468178942685\t\t0.70724\n",
      "Gradient Descent(81/99): loss=0.4026270827905423\t\t0.707165\n",
      "Gradient Descent(82/99): loss=0.4026198688450369\t\t0.707205\n",
      "Gradient Descent(83/99): loss=0.4026130191472964\t\t0.70718\n",
      "Gradient Descent(84/99): loss=0.4026065141074686\t\t0.70717\n",
      "Gradient Descent(85/99): loss=0.4026003352739363\t\t0.7071\n",
      "Gradient Descent(86/99): loss=0.40259446526118753\t\t0.707115\n",
      "Gradient Descent(87/99): loss=0.40258888768267365\t\t0.707075\n",
      "Gradient Descent(88/99): loss=0.40258358708828434\t\t0.70705\n",
      "Gradient Descent(89/99): loss=0.40257854890609446\t\t0.70706\n",
      "Gradient Descent(90/99): loss=0.4025737593880656\t\t0.707105\n",
      "Gradient Descent(91/99): loss=0.4025692055594123\t\t0.707055\n",
      "Gradient Descent(92/99): loss=0.4025648751713649\t\t0.70704\n",
      "Gradient Descent(93/99): loss=0.402560756657079\t\t0.70704\n",
      "Gradient Descent(94/99): loss=0.40255683909046924\t\t0.706995\n",
      "Gradient Descent(95/99): loss=0.4025531121477481\t\t0.70697\n",
      "Gradient Descent(96/99): loss=0.40254956607148734\t\t0.70699\n",
      "Gradient Descent(97/99): loss=0.4025461916370119\t\t0.706985\n",
      "Gradient Descent(98/99): loss=0.40254298012096884\t\t0.706955\n",
      "Gradient Descent(99/99): loss=0.40253992327191473\t\t0.70693\n"
     ]
    }
   ],
   "source": [
    "w_init = np.random.rand(x.shape[1])\n",
    "w, loss = least_squares_GD(train_y, train_x, w_init, max_iters=100, gamma=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD (0/99): loss=9.491306913659226\t\t0.57564\n",
      "SGD (1/99): loss=8.834314810848172\t\t0.571265\n",
      "SGD (2/99): loss=7.816103257852662\t\t0.57043\n",
      "SGD (3/99): loss=7.823711805249579\t\t0.57096\n",
      "SGD (4/99): loss=7.218297244495725\t\t0.570445\n",
      "SGD (5/99): loss=7.305700470608432\t\t0.574485\n",
      "SGD (6/99): loss=7.264329729422425\t\t0.5738\n",
      "SGD (7/99): loss=7.006150239762559\t\t0.57053\n",
      "SGD (8/99): loss=6.960262243015965\t\t0.57068\n",
      "SGD (9/99): loss=5.802231588486342\t\t0.55542\n",
      "SGD (10/99): loss=5.824283840074395\t\t0.555815\n",
      "SGD (11/99): loss=2.1752018422353223\t\t0.51689\n",
      "SGD (12/99): loss=2.172737107948579\t\t0.51491\n",
      "SGD (13/99): loss=2.1742030083267343\t\t0.515055\n",
      "SGD (14/99): loss=2.2248484937244957\t\t0.501445\n",
      "SGD (15/99): loss=2.226512379714472\t\t0.501965\n",
      "SGD (16/99): loss=2.2126747616654177\t\t0.50305\n",
      "SGD (17/99): loss=2.14389957215831\t\t0.50632\n",
      "SGD (18/99): loss=2.0235792541988276\t\t0.509495\n",
      "SGD (19/99): loss=2.006647473992341\t\t0.513785\n",
      "SGD (20/99): loss=2.0176038854358604\t\t0.511625\n",
      "SGD (21/99): loss=2.012635899832488\t\t0.512605\n",
      "SGD (22/99): loss=1.998425376428446\t\t0.513975\n",
      "SGD (23/99): loss=1.8310729004396733\t\t0.50621\n",
      "SGD (24/99): loss=1.8265765821434747\t\t0.505855\n",
      "SGD (25/99): loss=1.74779028472411\t\t0.504865\n",
      "SGD (26/99): loss=1.75505112540094\t\t0.506165\n",
      "SGD (27/99): loss=1.7541179006421348\t\t0.504535\n",
      "SGD (28/99): loss=1.7084498275554687\t\t0.508335\n",
      "SGD (29/99): loss=1.7086038314834773\t\t0.50734\n",
      "SGD (30/99): loss=1.6363621647065851\t\t0.50802\n",
      "SGD (31/99): loss=1.6282819381987068\t\t0.502495\n",
      "SGD (32/99): loss=1.551238849497806\t\t0.5121\n",
      "SGD (33/99): loss=1.5531508793018676\t\t0.51162\n",
      "SGD (34/99): loss=1.5548230516735144\t\t0.51208\n",
      "SGD (35/99): loss=1.5540831053248383\t\t0.51215\n",
      "SGD (36/99): loss=1.5237759840392064\t\t0.520235\n",
      "SGD (37/99): loss=1.5204919984441416\t\t0.52159\n",
      "SGD (38/99): loss=1.5037437603108212\t\t0.526875\n",
      "SGD (39/99): loss=1.4423360442164068\t\t0.52925\n",
      "SGD (40/99): loss=1.4426642069390363\t\t0.5292\n",
      "SGD (41/99): loss=1.4334814312722743\t\t0.527685\n",
      "SGD (42/99): loss=1.4329153321594446\t\t0.532055\n",
      "SGD (43/99): loss=1.4281409691800473\t\t0.53553\n",
      "SGD (44/99): loss=1.4304949541104934\t\t0.53758\n",
      "SGD (45/99): loss=1.3110038561187374\t\t0.545035\n",
      "SGD (46/99): loss=1.3244844312117148\t\t0.548855\n",
      "SGD (47/99): loss=1.3445177451198704\t\t0.517115\n",
      "SGD (48/99): loss=1.3119433729801138\t\t0.52839\n",
      "SGD (49/99): loss=1.3182194175107123\t\t0.52752\n",
      "SGD (50/99): loss=1.3056232414972329\t\t0.529185\n",
      "SGD (51/99): loss=1.3056444394321212\t\t0.52916\n",
      "SGD (52/99): loss=1.3211238518824975\t\t0.52562\n",
      "SGD (53/99): loss=1.3117254704040933\t\t0.528515\n",
      "SGD (54/99): loss=1.30566507594532\t\t0.528025\n",
      "SGD (55/99): loss=1.2994137019892704\t\t0.532885\n",
      "SGD (56/99): loss=1.2454492517229079\t\t0.53851\n",
      "SGD (57/99): loss=1.2452519815731258\t\t0.538445\n",
      "SGD (58/99): loss=1.244704138389013\t\t0.53832\n",
      "SGD (59/99): loss=1.2359636241395089\t\t0.537085\n",
      "SGD (60/99): loss=1.2388026741471188\t\t0.53711\n",
      "SGD (61/99): loss=1.2614385160751387\t\t0.53453\n",
      "SGD (62/99): loss=1.2673376408307444\t\t0.53746\n",
      "SGD (63/99): loss=1.2484295808698234\t\t0.53484\n",
      "SGD (64/99): loss=1.2488495276100928\t\t0.53734\n",
      "SGD (65/99): loss=1.2449155386869\t\t0.536445\n",
      "SGD (66/99): loss=1.2293901187292737\t\t0.53444\n",
      "SGD (67/99): loss=1.228415653246827\t\t0.539155\n",
      "SGD (68/99): loss=1.1970492188831625\t\t0.54789\n",
      "SGD (69/99): loss=1.19432935451409\t\t0.551575\n",
      "SGD (70/99): loss=1.1977136442942502\t\t0.551295\n",
      "SGD (71/99): loss=1.1951277127152893\t\t0.550305\n",
      "SGD (72/99): loss=1.0925920229769535\t\t0.547015\n",
      "SGD (73/99): loss=1.0877648190055322\t\t0.55367\n",
      "SGD (74/99): loss=1.095416808531404\t\t0.55525\n",
      "SGD (75/99): loss=1.0477792745288097\t\t0.55527\n",
      "SGD (76/99): loss=1.056639929351283\t\t0.555975\n",
      "SGD (77/99): loss=1.047137669611469\t\t0.553795\n",
      "SGD (78/99): loss=1.0634324101946866\t\t0.554125\n",
      "SGD (79/99): loss=0.9820383130287893\t\t0.54247\n",
      "SGD (80/99): loss=0.925164888289004\t\t0.555935\n",
      "SGD (81/99): loss=0.9393063265352564\t\t0.5563\n",
      "SGD (82/99): loss=0.948735226091569\t\t0.553955\n",
      "SGD (83/99): loss=0.9188513252598416\t\t0.550165\n",
      "SGD (84/99): loss=0.7404798742515274\t\t0.55812\n",
      "SGD (85/99): loss=0.7403639299696547\t\t0.55943\n",
      "SGD (86/99): loss=0.7393426275476275\t\t0.558245\n",
      "SGD (87/99): loss=0.7356010664553926\t\t0.56664\n",
      "SGD (88/99): loss=0.7282742433227611\t\t0.566005\n",
      "SGD (89/99): loss=0.7235484438302573\t\t0.56505\n",
      "SGD (90/99): loss=0.7201379731662889\t\t0.55904\n",
      "SGD (91/99): loss=0.7187193705855648\t\t0.560845\n",
      "SGD (92/99): loss=0.7115314037084115\t\t0.55991\n",
      "SGD (93/99): loss=0.6917373069091092\t\t0.561025\n",
      "SGD (94/99): loss=0.6950564359051395\t\t0.556425\n",
      "SGD (95/99): loss=0.6914502809909117\t\t0.56335\n",
      "SGD (96/99): loss=0.6878124282701098\t\t0.56347\n",
      "SGD (97/99): loss=0.6882005916585182\t\t0.570255\n",
      "SGD (98/99): loss=0.6748745334407464\t\t0.579605\n",
      "SGD (99/99): loss=0.6651721271493043\t\t0.574385\n"
     ]
    }
   ],
   "source": [
    "w, loss = least_squares_SGD(train_y, train_x, w_init, 100, gamma=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.641"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_accuracy(test_x, test_y, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned 11 columns\n"
     ]
    }
   ],
   "source": [
    "_, submission_x, submission_ids = load_csv_data(data_path=\"datas/test.csv\", sub_sample=False)\n",
    "submission_x = preprocess_data(submission_x)\n",
    "pred_y = predict_labels(w, submission_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done !\n"
     ]
    }
   ],
   "source": [
    "create_csv_submission(submission_ids, pred_y, \"datas/submission.csv\")\n",
    "print('Done !')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# https://stackoverflow.com/a/7941594/4810319\n",
    "def main():\n",
    "    np.random.seed(1977)\n",
    "    numvars, numdata = 5, 100\n",
    "    data = 10 * np.random.random((numvars, numdata))\n",
    "    data = x[0:300, 0:7].T\n",
    "    print(x[0:200, 7])\n",
    "    fig = scatterplot_matrix(data, ['DER_mass_MMC', 'DER_mass_transverse_met_lep', 'DER_mass_vis', 'DER_pt_h', 'DER_deltaeta_jet_jet', 'DER_mass_jet_jet', 'DER_prodeta_jet_jet'],\n",
    "            linestyle='none', marker='o', color='black', mfc='none')\n",
    "    fig.suptitle('Simple Scatterplot Matrix')\n",
    "    plt.show()\n",
    "\n",
    "def scatterplot_matrix(data, names, **kwargs):\n",
    "    \"\"\"Plots a scatterplot matrix of subplots.  Each row of \"data\" is plotted\n",
    "    against other rows, resulting in a nrows by nrows grid of subplots with the\n",
    "    diagonal subplots labeled with \"names\".  Additional keyword arguments are\n",
    "    passed on to matplotlib's \"plot\" command. Returns the matplotlib figure\n",
    "    object containg the subplot grid.\"\"\"\n",
    "    numvars, numdata = data.shape\n",
    "    fig, axes = plt.subplots(nrows=numvars, ncols=numvars, figsize=(8,8))\n",
    "    fig.subplots_adjust(hspace=0.05, wspace=0.05)\n",
    "\n",
    "    for ax in axes.flat:\n",
    "        # Hide all ticks and labels\n",
    "        ax.xaxis.set_visible(False)\n",
    "        ax.yaxis.set_visible(False)\n",
    "\n",
    "        # Set up ticks only on one side for the \"edge\" subplots...\n",
    "        if ax.is_first_col():\n",
    "            ax.yaxis.set_ticks_position('left')\n",
    "        if ax.is_last_col():\n",
    "            ax.yaxis.set_ticks_position('right')\n",
    "        if ax.is_first_row():\n",
    "            ax.xaxis.set_ticks_position('top')\n",
    "        if ax.is_last_row():\n",
    "            ax.xaxis.set_ticks_position('bottom')\n",
    "\n",
    "    # Plot the data.\n",
    "    for i, j in zip(*np.triu_indices_from(axes, k=1)):\n",
    "        for x, y in [(i,j), (j,i)]:\n",
    "            axes[x,y].plot(data[x], data[y], **kwargs)\n",
    "\n",
    "    # Label the diagonal subplots...\n",
    "    for i, label in enumerate(names):\n",
    "        axes[i,i].annotate(label, (0.5, 0.5), xycoords='axes fraction',\n",
    "                ha='center', va='center')\n",
    "\n",
    "    # Turn on the proper x or y axes ticks.\n",
    "    for i, j in zip(range(numvars), itertools.cycle((-1, 0))):\n",
    "        axes[j,i].xaxis.set_visible(True)\n",
    "        axes[i,j].yaxis.set_visible(True)\n",
    "\n",
    "    return fig\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank: 19\n",
      "Loss: 0.402476348718\n",
      "Accuracy: 0.706655\n",
      "Accuracy: 0.71032\n"
     ]
    }
   ],
   "source": [
    "def least_squares(y, tx):\n",
    "    gram = tx.T.dot(tx)\n",
    "    print(\"Rank: \" + str(np.linalg.matrix_rank(gram)))\n",
    "    w = np.linalg.inv(gram).dot(tx.T).dot(y)\n",
    "    return w, compute_loss(y, tx, w)\n",
    "    \n",
    "w, loss = least_squares(train_y, train_x)\n",
    "print(\"Loss: \" + str(loss))\n",
    "print(\"Accuracy: \" + str(get_accuracy(train_x, train_y, w)))\n",
    "print(\"Accuracy: \" + str(get_accuracy(test_x, test_y, w)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
