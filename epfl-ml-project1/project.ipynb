{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from helpers import *\n",
    "from costs import *\n",
    "from gradient_descent import *\n",
    "from stochastic_gradient_descent import *\n",
    "\n",
    "random.seed(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "y, x, ids = load_csv_data(data_path=\"datas/train.csv\", sub_sample=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set to 0 all column containing -999\n",
    "def remove_NaN(x):\n",
    "    columns_with_NaN = set(\"\")\n",
    "    for row in x:\n",
    "        for i,feature in enumerate(row):\n",
    "            if feature == -999:\n",
    "                columns_with_NaN.add(i)\n",
    "\n",
    "    for col in columns_with_NaN:\n",
    "        x[:, col] = 0\n",
    "        \n",
    "    return x\n",
    "        \n",
    "x = remove_NaN(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.06833197,  0.40768027, ...,  0.        ,\n",
       "         0.        ,  0.4125105 ],\n",
       "       [ 0.        ,  0.55250482,  0.54013641, ...,  0.        ,\n",
       "         0.        , -0.27381996],\n",
       "       [ 0.        ,  3.19515553,  1.09655998, ...,  0.        ,\n",
       "         0.        , -0.29396985],\n",
       "       ..., \n",
       "       [ 0.        ,  0.31931645, -0.13086367, ...,  0.        ,\n",
       "         0.        , -0.31701723],\n",
       "       [ 0.        , -0.84532397, -0.30297338, ...,  0.        ,\n",
       "         0.        , -0.74543941],\n",
       "       [ 0.        ,  0.66533608, -0.25352276, ...,  0.        ,\n",
       "         0.        , -0.74543941]])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def normalize(x):\n",
    "    return (x - x.mean(axis=0)) / (x.std(axis=0) + 0.0000000001)\n",
    "x = normalize(x)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200000, 30)\n",
      "(200000,)\n",
      "(50000, 30)\n",
      "(50000,)\n"
     ]
    }
   ],
   "source": [
    "def separate_set(x, y):\n",
    "    x_and_y = np.concatenate((y.reshape((y.shape[0], 1)), x), axis=1)\n",
    "    np.random.shuffle(x_and_y)\n",
    "    \n",
    "    count = x_and_y.shape[0]\n",
    "    last_train_index = int(count * 0.8)\n",
    "    \n",
    "    train_set = x_and_y[0:last_train_index, :]\n",
    "    test_set = x_and_y[last_train_index:, :]\n",
    "    \n",
    "    train_y = train_set[:, 0]\n",
    "    test_y = test_set[:, 0]\n",
    "\n",
    "    train_x = train_set[:, 1:]\n",
    "    test_x = test_set[:, 1:]\n",
    "\n",
    "    return train_x, train_y, test_x, test_y\n",
    "\n",
    "train_x, train_y, test_x, test_y = separate_set(x, y)\n",
    "\n",
    "print(train_x.shape)\n",
    "print(train_y.shape)\n",
    "print(test_x.shape)\n",
    "print(test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/99): loss=9.606167510643457\t\t0.407925\n",
      "Gradient Descent(1/99): loss=4.437332362676688\t\t0.622815\n",
      "Gradient Descent(2/99): loss=2.4190050649254657\t\t0.4459\n",
      "Gradient Descent(3/99): loss=1.451067205450863\t\t0.642675\n",
      "Gradient Descent(4/99): loss=0.9587158184431638\t\t0.5155\n",
      "Gradient Descent(5/99): loss=0.7037169310322476\t\t0.65858\n",
      "Gradient Descent(6/99): loss=0.5704191801056481\t\t0.612865\n",
      "Gradient Descent(7/99): loss=0.5001257063758184\t\t0.67442\n",
      "Gradient Descent(8/99): loss=0.46263584727603985\t\t0.681305\n",
      "Gradient Descent(9/99): loss=0.44231537320308273\t\t0.688985\n",
      "Gradient Descent(10/99): loss=0.4310359407342719\t\t0.700995\n",
      "Gradient Descent(11/99): loss=0.42455438801652823\t\t0.69714\n",
      "Gradient Descent(12/99): loss=0.4206458103531692\t\t0.704405\n",
      "Gradient Descent(13/99): loss=0.41813739479820144\t\t0.70142\n",
      "Gradient Descent(14/99): loss=0.41640684319586313\t\t0.705505\n",
      "Gradient Descent(15/99): loss=0.4151211715622359\t\t0.703835\n",
      "Gradient Descent(16/99): loss=0.4141002293712916\t\t0.70598\n",
      "Gradient Descent(17/99): loss=0.4132451134477236\t\t0.705225\n",
      "Gradient Descent(18/99): loss=0.41250045155138776\t\t0.70636\n",
      "Gradient Descent(19/99): loss=0.41183442444741325\t\t0.70612\n",
      "Gradient Descent(20/99): loss=0.41122811545200283\t\t0.70693\n",
      "Gradient Descent(21/99): loss=0.4106697879034205\t\t0.706885\n",
      "Gradient Descent(22/99): loss=0.41015178085465015\t\t0.707335\n",
      "Gradient Descent(23/99): loss=0.40966880523599614\t\t0.70736\n",
      "Gradient Descent(24/99): loss=0.4092169952083872\t\t0.70775\n",
      "Gradient Descent(25/99): loss=0.40879337073005495\t\t0.70769\n",
      "Gradient Descent(26/99): loss=0.40839552666584394\t\t0.707845\n",
      "Gradient Descent(27/99): loss=0.408021448452361\t\t0.70788\n",
      "Gradient Descent(28/99): loss=0.4076693996352451\t\t0.708\n",
      "Gradient Descent(29/99): loss=0.40733785101328757\t\t0.7081\n",
      "Gradient Descent(30/99): loss=0.4070254344031654\t\t0.70806\n",
      "Gradient Descent(31/99): loss=0.40673091133580247\t\t0.708025\n",
      "Gradient Descent(32/99): loss=0.40645315105480745\t\t0.708275\n",
      "Gradient Descent(33/99): loss=0.4061911144779233\t\t0.70847\n",
      "Gradient Descent(34/99): loss=0.4059438420958541\t\t0.70858\n",
      "Gradient Descent(35/99): loss=0.4057104445497974\t\t0.70861\n",
      "Gradient Descent(36/99): loss=0.4054900950858448\t\t0.708625\n",
      "Gradient Descent(37/99): loss=0.4052820233624168\t\t0.708705\n",
      "Gradient Descent(38/99): loss=0.4050855102598548\t\t0.70863\n",
      "Gradient Descent(39/99): loss=0.4048998834513861\t\t0.708695\n",
      "Gradient Descent(40/99): loss=0.40472451356633066\t\t0.708825\n",
      "Gradient Descent(41/99): loss=0.40455881082409406\t\t0.70895\n",
      "Gradient Descent(42/99): loss=0.40440222204987497\t\t0.708855\n",
      "Gradient Descent(43/99): loss=0.40425422800544875\t\t0.70887\n",
      "Gradient Descent(44/99): loss=0.4041143409842258\t\t0.70878\n",
      "Gradient Descent(45/99): loss=0.40398210263114975\t\t0.708875\n",
      "Gradient Descent(46/99): loss=0.4038570819563024\t\t0.708945\n",
      "Gradient Descent(47/99): loss=0.40373887351723386\t\t0.708915\n",
      "Gradient Descent(48/99): loss=0.4036270957496697\t\t0.709\n",
      "Gradient Descent(49/99): loss=0.4035213894297827\t\t0.708905\n",
      "Gradient Descent(50/99): loss=0.4034214162539601\t\t0.70883\n",
      "Gradient Descent(51/99): loss=0.40332685752414466\t\t0.708775\n",
      "Gradient Descent(52/99): loss=0.40323741292854515\t\t0.70869\n",
      "Gradient Descent(53/99): loss=0.40315279940888943\t\t0.70878\n",
      "Gradient Descent(54/99): loss=0.403072750106523\t\t0.70888\n",
      "Gradient Descent(55/99): loss=0.4029970133805878\t\t0.70887\n",
      "Gradient Descent(56/99): loss=0.4029253518922902\t\t0.70889\n",
      "Gradient Descent(57/99): loss=0.40285754174992777\t\t0.708865\n",
      "Gradient Descent(58/99): loss=0.40279337170990087\t\t0.708785\n",
      "Gradient Descent(59/99): loss=0.40273264242941653\t\t0.708695\n",
      "Gradient Descent(60/99): loss=0.40267516576700846\t\t0.708755\n",
      "Gradient Descent(61/99): loss=0.40262076412735814\t\t0.70882\n",
      "Gradient Descent(62/99): loss=0.40256926984722174\t\t0.708745\n",
      "Gradient Descent(63/99): loss=0.40252052461955135\t\t0.70876\n",
      "Gradient Descent(64/99): loss=0.4024743789531422\t\t0.70878\n",
      "Gradient Descent(65/99): loss=0.4024306916653653\t\t0.7088\n",
      "Gradient Descent(66/99): loss=0.4023893294057426\t\t0.708825\n",
      "Gradient Descent(67/99): loss=0.4023501662082995\t\t0.708895\n",
      "Gradient Descent(68/99): loss=0.402313083070792\t\t0.708865\n",
      "Gradient Descent(69/99): loss=0.40227796755904927\t\t0.7088\n",
      "Gradient Descent(70/99): loss=0.40224471343480966\t\t0.708835\n",
      "Gradient Descent(71/99): loss=0.4022132203055408\t\t0.70881\n",
      "Gradient Descent(72/99): loss=0.40218339329485003\t\t0.70873\n",
      "Gradient Descent(73/99): loss=0.4021551427321898\t\t0.708695\n",
      "Gradient Descent(74/99): loss=0.4021283838606525\t\t0.70869\n",
      "Gradient Descent(75/99): loss=0.40210303656173274\t\t0.70868\n",
      "Gradient Descent(76/99): loss=0.40207902509601656\t\t0.708605\n",
      "Gradient Descent(77/99): loss=0.4020562778588212\t\t0.708565\n",
      "Gradient Descent(78/99): loss=0.40203472714988064\t\t0.708575\n",
      "Gradient Descent(79/99): loss=0.40201430895622725\t\t0.7085\n",
      "Gradient Descent(80/99): loss=0.40199496274747837\t\t0.708545\n",
      "Gradient Descent(81/99): loss=0.40197663128278793\t\t0.708485\n",
      "Gradient Descent(82/99): loss=0.4019592604287686\t\t0.70847\n",
      "Gradient Descent(83/99): loss=0.4019427989877392\t\t0.708505\n",
      "Gradient Descent(84/99): loss=0.40192719853568504\t\t0.708535\n",
      "Gradient Descent(85/99): loss=0.40191241326936544\t\t0.708515\n",
      "Gradient Descent(86/99): loss=0.40189839986203346\t\t0.708475\n",
      "Gradient Descent(87/99): loss=0.40188511732726534\t\t0.708465\n",
      "Gradient Descent(88/99): loss=0.4018725268904325\t\t0.708455\n",
      "Gradient Descent(89/99): loss=0.40186059186737055\t\t0.708465\n",
      "Gradient Descent(90/99): loss=0.40184927754983424\t\t0.708435\n",
      "Gradient Descent(91/99): loss=0.4018385510973468\t\t0.70838\n",
      "Gradient Descent(92/99): loss=0.40182838143507693\t\t0.708335\n",
      "Gradient Descent(93/99): loss=0.4018187391574011\t\t0.708325\n",
      "Gradient Descent(94/99): loss=0.40180959643682446\t\t0.708295\n",
      "Gradient Descent(95/99): loss=0.40180092693795755\t\t0.70834\n",
      "Gradient Descent(96/99): loss=0.4017927057362617\t\t0.708305\n",
      "Gradient Descent(97/99): loss=0.4017849092412915\t\t0.708275\n",
      "Gradient Descent(98/99): loss=0.401777515124183\t\t0.70822\n",
      "Gradient Descent(99/99): loss=0.4017705022491456\t\t0.708185\n"
     ]
    }
   ],
   "source": [
    "w_init = np.random.rand(x.shape[1])\n",
    "w, loss = least_squares_GD(train_y, train_x, w_init, max_iters=100, gamma=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w, loss = least_squares_SGD(y, x, w_init, 100, gamma=0.03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.70532"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_accuracy(test_x, test_y, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test, x_test, ids_test = load_csv_data(data_path=\"datas/test.csv\", sub_sample=False)\n",
    "pred_y = predict_labels(w, x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_csv_submission(ids_test, pred_y, \"datas/submission.csv\")\n",
    "print('Done !')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# https://stackoverflow.com/a/7941594/4810319\n",
    "def main():\n",
    "    np.random.seed(1977)\n",
    "    numvars, numdata = 5, 100\n",
    "    data = 10 * np.random.random((numvars, numdata))\n",
    "    data = x[0:300, 0:7].T\n",
    "    print(x[0:200, 7])\n",
    "    fig = scatterplot_matrix(data, ['DER_mass_MMC', 'DER_mass_transverse_met_lep', 'DER_mass_vis', 'DER_pt_h', 'DER_deltaeta_jet_jet', 'DER_mass_jet_jet', 'DER_prodeta_jet_jet'],\n",
    "            linestyle='none', marker='o', color='black', mfc='none')\n",
    "    fig.suptitle('Simple Scatterplot Matrix')\n",
    "    plt.show()\n",
    "\n",
    "def scatterplot_matrix(data, names, **kwargs):\n",
    "    \"\"\"Plots a scatterplot matrix of subplots.  Each row of \"data\" is plotted\n",
    "    against other rows, resulting in a nrows by nrows grid of subplots with the\n",
    "    diagonal subplots labeled with \"names\".  Additional keyword arguments are\n",
    "    passed on to matplotlib's \"plot\" command. Returns the matplotlib figure\n",
    "    object containg the subplot grid.\"\"\"\n",
    "    numvars, numdata = data.shape\n",
    "    fig, axes = plt.subplots(nrows=numvars, ncols=numvars, figsize=(8,8))\n",
    "    fig.subplots_adjust(hspace=0.05, wspace=0.05)\n",
    "\n",
    "    for ax in axes.flat:\n",
    "        # Hide all ticks and labels\n",
    "        ax.xaxis.set_visible(False)\n",
    "        ax.yaxis.set_visible(False)\n",
    "\n",
    "        # Set up ticks only on one side for the \"edge\" subplots...\n",
    "        if ax.is_first_col():\n",
    "            ax.yaxis.set_ticks_position('left')\n",
    "        if ax.is_last_col():\n",
    "            ax.yaxis.set_ticks_position('right')\n",
    "        if ax.is_first_row():\n",
    "            ax.xaxis.set_ticks_position('top')\n",
    "        if ax.is_last_row():\n",
    "            ax.xaxis.set_ticks_position('bottom')\n",
    "\n",
    "    # Plot the data.\n",
    "    for i, j in zip(*np.triu_indices_from(axes, k=1)):\n",
    "        for x, y in [(i,j), (j,i)]:\n",
    "            axes[x,y].plot(data[x], data[y], **kwargs)\n",
    "\n",
    "    # Label the diagonal subplots...\n",
    "    for i, label in enumerate(names):\n",
    "        axes[i,i].annotate(label, (0.5, 0.5), xycoords='axes fraction',\n",
    "                ha='center', va='center')\n",
    "\n",
    "    # Turn on the proper x or y axes ticks.\n",
    "    for i, j in zip(range(numvars), itertools.cycle((-1, 0))):\n",
    "        axes[j,i].xaxis.set_visible(True)\n",
    "        axes[i,j].yaxis.set_visible(True)\n",
    "\n",
    "    return fig\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
