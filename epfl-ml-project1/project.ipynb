{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from helpers import *\n",
    "from costs import *\n",
    "from gradient_descent import *\n",
    "from stochastic_gradient_descent import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 30)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y, x, ids = load_csv_data(data_path=\"datas/train.csv\", sub_sample=True)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8255096.3165090261"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = np.random.rand(x.shape[1])\n",
    "compute_loss(y, x, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30,)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient = compute_gradient(y, x, w)\n",
    "gradient.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/9999): loss=6479839.830317005, w0=0.843728636592973, w1=0.49732034221062926\n",
      "Gradient Descent(1/9999): loss=1361350.2303653355, w0=0.8121159300785203, w1=0.5036303784651381\n",
      "Gradient Descent(2/9999): loss=408712.7853882518, w0=0.7900406672049812, w1=0.5059753579000098\n",
      "Gradient Descent(3/9999): loss=227115.9598260727, w0=0.7722519242855079, w1=0.5066181850191881\n",
      "Gradient Descent(4/9999): loss=188427.9610869171, w0=0.7564861431592869, w1=0.5065322009330597\n",
      "Gradient Descent(5/9999): loss=176414.76341160072, w0=0.7417649255863538, w1=0.5061360515524895\n",
      "Gradient Descent(6/9999): loss=169555.9135835394, w0=0.7276628365123493, w1=0.5056097850928716\n",
      "Gradient Descent(7/9999): loss=163856.2006180753, w0=0.7139924489551348, w1=0.505030818826448\n",
      "Gradient Descent(8/9999): loss=158564.02038586428, w0=0.7006688622356293, w1=0.5044324240795603\n",
      "Gradient Descent(9/9999): loss=153528.86673917415, w0=0.6876514194790173, w1=0.5038288830922166\n",
      "Gradient Descent(10/9999): loss=148712.3247716239, w0=0.6749186320248258, w1=0.5032263100535342\n",
      "Gradient Descent(11/9999): loss=144097.51704161192, w0=0.6624573877818091, w1=0.5026273056997903\n",
      "Gradient Descent(12/9999): loss=139672.2794609774, w0=0.6502583039582913, w1=0.5020329594693018\n",
      "Gradient Descent(13/9999): loss=135425.98524715783, w0=0.6383137229873327, w1=0.5014437107297564\n",
      "Gradient Descent(14/9999): loss=131348.89991402737, w0=0.6266168456312302, w1=0.5008597192439942\n",
      "Gradient Descent(15/9999): loss=127432.00985459874, w0=0.6151613534542282, w1=0.5002810245378229\n",
      "Gradient Descent(16/9999): loss=123666.94334203658, w0=0.6039412420023557, w1=0.4997076144648882\n",
      "Gradient Descent(17/9999): loss=120045.91297049409, w0=0.5929507448166595, w1=0.49913945471255766\n",
      "Gradient Descent(18/9999): loss=116561.66599337793, w0=0.5821842967076505, w1=0.4985765015060942\n",
      "Gradient Descent(19/9999): loss=113207.43968265643, w0=0.5716365140985892, w1=0.49801870708494583\n",
      "Gradient Descent(20/9999): loss=109976.92084160996, w0=0.5613021828831571, w1=0.4974660220692901\n",
      "Gradient Descent(21/9999): loss=106864.20900704725, w0=0.5511762496795725, w1=0.49691839648825664\n",
      "Gradient Descent(22/9999): loss=103863.78298059269, w0=0.5412538147020759, w1=0.49637578023182377\n",
      "Gradient Descent(23/9999): loss=100970.47037395043, w0=0.5315301254771365, w1=0.4958381232541897\n",
      "Gradient Descent(24/9999): loss=98179.4198850708, w0=0.522000571064987, w1=0.49530537566965116\n",
      "Gradient Descent(25/9999): loss=95486.0760494473, w0=0.5126606766338048, w1=0.49477748780168335\n",
      "Gradient Descent(26/9999): loss=92886.15623515843, w0=0.5035060983144886, w1=0.49425441021135713\n",
      "Gradient Descent(27/9999): loss=90375.62967226712, w0=0.4945326182989735, w1=0.4937360937163632\n",
      "Gradient Descent(28/9999): loss=87950.69832707789, w0=0.48573614016037153, w1=0.4932224894055159\n",
      "Gradient Descent(29/9999): loss=85607.77944973503, w0=0.4771126843801055, w1=0.4927135486508592\n",
      "Gradient Descent(30/9999): loss=83343.48963991321, w0=0.4686583840704279, w1=0.49220922311830895\n",
      "Gradient Descent(31/9999): loss=81154.63029006332, w0=0.4603694808823568, w1=0.4917094647772577\n",
      "Gradient Descent(32/9999): loss=79038.17427898641, w0=0.45224232109000473, w1=0.49121422590934516\n",
      "Gradient Descent(33/9999): loss=76991.25380054553, w0=0.4442733518429138, w1=0.49072345911650317\n",
      "Gradient Descent(34/9999): loss=75011.14922321576, w0=0.4364591175784984, w1=0.4902371173283398\n",
      "Gradient Descent(35/9999): loss=73095.27888602327, w0=0.4287962565871165, w1=0.48975515380890994\n",
      "Gradient Descent(36/9999): loss=71241.1897453359, w0=0.4212814977226645, w1=0.48927752216290943\n",
      "Gradient Descent(37/9999): loss=69446.54879502958, w0=0.41391165725194246, w1=0.48880417634132506\n",
      "Gradient Descent(38/9999): loss=67709.13518984926, w0=0.4066836358363615, w1=0.4883350706465708\n",
      "Gradient Descent(39/9999): loss=66026.83300838164, w0=0.3995944156398772, w1=0.487870159737137\n",
      "Gradient Descent(40/9999): loss=64397.62459802796, w0=0.3926410575573263, w1=0.48740939863177973\n",
      "Gradient Descent(41/9999): loss=62819.58444976795, w0=0.38582069855762535, w1=0.48695274271327427\n",
      "Gradient Descent(42/9999): loss=61290.8735553941, w0=0.3791305491365529, w1=0.48650014773175604\n",
      "Gradient Descent(43/9999): loss=59809.73420432009, w0=0.37256789087409364, w1=0.48605156980767195\n",
      "Gradient Descent(44/9999): loss=58374.48518106998, w0=0.36613007409156006, w1=0.4856069654343629\n",
      "Gradient Descent(45/9999): loss=56983.5173281786, w0=0.35981451560393884, w1=0.48516629148029716\n",
      "Gradient Descent(46/9999): loss=55635.28944251316, w0=0.3536186965631249, w1=0.48472950519097463\n",
      "Gradient Descent(47/9999): loss=54328.3244759936, w0=0.347540160387914, w1=0.4842965641905192\n",
      "Gradient Descent(48/9999): loss=53061.20601437773, w0=0.34157651077682316, w1=0.4838674264829771\n",
      "Gradient Descent(49/9999): loss=51832.57501020858, w0=0.3357254097999922, w1=0.4834420504533375\n",
      "Gradient Descent(50/9999): loss=50641.12674822471, w0=0.3299845760666029, w1=0.4830203948682907\n",
      "Gradient Descent(51/9999): loss=49485.60802352775, w0=0.32435178296441713, w1=0.482602418876739\n",
      "Gradient Descent(52/9999): loss=48364.81451460797, w0=0.31882485696820045, w1=0.4821880820100741\n",
      "Gradient Descent(53/9999): loss=47277.588334963766, w0=0.31340167601394964, w1=0.4817773441822348\n",
      "Gradient Descent(54/9999): loss=46222.815748532346, w0=0.3080801679359886, w1=0.4813701656895568\n",
      "Gradient Descent(55/9999): loss=45199.42503549079, w0=0.3028583089641372, w1=0.4809665072104282\n",
      "Gradient Descent(56/9999): loss=44206.38449620244, w0=0.2977341222782892, w1=0.4805663298047604\n",
      "Gradient Descent(57/9999): loss=43242.700582184734, w0=0.2927056766178616, w1=0.4801695949132865\n",
      "Gradient Descent(58/9999): loss=42307.41614397348, w0=0.2877710849436972, w1=0.47977626435669735\n",
      "Gradient Descent(59/9999): loss=41399.60878666243, w0=0.2829285031501168, w1=0.47938630033462465\n",
      "Gradient Descent(60/9999): loss=40518.38932471785, w0=0.27817612882492604, w1=0.47899966542448075\n",
      "Gradient Descent(61/9999): loss=39662.900328411226, w0=0.27351220005528376, w1=0.4786163225801641\n",
      "Gradient Descent(62/9999): loss=38832.314754887215, w0=0.2689349942774388, w1=0.4782362351306387\n",
      "Gradient Descent(63/9999): loss=38025.83465749605, w0=0.2644428271684347, w1=0.47785936677839486\n",
      "Gradient Descent(64/9999): loss=37242.68996757412, w0=0.2600340515779708, w1=0.4774856815978006\n",
      "Gradient Descent(65/9999): loss=36482.13734336005, w0=0.2557070564986933, w1=0.47711514403334865\n",
      "Gradient Descent(66/9999): loss=35743.45908119055, w0=0.2514602660732696, w1=0.47674771889780787\n",
      "Gradient Descent(67/9999): loss=35025.96208453511, w0=0.24729213863667773, w1=0.4763833713702842\n",
      "Gradient Descent(68/9999): loss=34328.97688680538, w0=0.24320116579221313, w1=0.476022066994198\n",
      "Gradient Descent(69/9999): loss=33651.85672421727, w0=0.2391858715197871, w1=0.47566377167518387\n",
      "Gradient Descent(70/9999): loss=32993.976655294595, w0=0.23524481131515557, w1=0.4753084516789171\n",
      "Gradient Descent(71/9999): loss=32354.73272388586, w0=0.2313765713587808, w1=0.4749560736288743\n",
      "Gradient Descent(72/9999): loss=31733.541162822592, w0=0.22757976771308822, w1=0.474606604504031\n",
      "Gradient Descent(73/9999): loss=31129.83763558139, w0=0.2238530455469374, w1=0.4742600116365021\n",
      "Gradient Descent(74/9999): loss=30543.076513524225, w0=0.2201950783861816, w1=0.47391626270912957\n",
      "Gradient Descent(75/9999): loss=29972.730186485733, w0=0.2166045673892401, w1=0.4735753257530217\n",
      "Gradient Descent(76/9999): loss=29418.288404651597, w0=0.21308024064665912, w1=0.47323716914504727\n",
      "Gradient Descent(77/9999): loss=28879.2576498333, w0=0.20962085250368184, w1=0.4729017616052896\n",
      "Gradient Descent(78/9999): loss=28355.16053439082, w0=0.2062251829048944, w1=0.4725690721944628\n",
      "Gradient Descent(79/9999): loss=27845.53522618786, w0=0.2028920367600559, w1=0.4722390703112953\n",
      "Gradient Descent(80/9999): loss=27349.934898086667, w0=0.19962024333026213, w1=0.4719117256898819\n",
      "Gradient Descent(81/9999): loss=26867.927200599792, w0=0.19640865563362997, w1=0.47158700839700923\n",
      "Gradient Descent(82/9999): loss=26399.09375641857, w0=0.19325614986972764, w1=0.4712648888294567\n",
      "Gradient Descent(83/9999): loss=25943.02967563009, w0=0.19016162486200935, w1=0.4709453377112758\n",
      "Gradient Descent(84/9999): loss=25499.34309051981, w0=0.1871240015175476, w1=0.47062832609105\n",
      "Gradient Descent(85/9999): loss=25067.654708934395, w0=0.18414222230338703, w1=0.4703138253391389\n",
      "Gradient Descent(86/9999): loss=24647.59738525051, w0=0.1812152507388745, w1=0.47000180714490764\n",
      "Gradient Descent(87/9999): loss=24238.815708060465, w0=0.17834207090334891, w1=0.46969224351394434\n",
      "Gradient Descent(88/9999): loss=23840.965603745433, w0=0.17552168695860096, w1=0.46938510676526807\n",
      "Gradient Descent(89/9999): loss=23453.713955161656, w0=0.17275312268554036, w1=0.4690803695285287\n",
      "Gradient Descent(90/9999): loss=23076.73823471569, w0=0.1700354210345316, w1=0.46877800474120074\n",
      "Gradient Descent(91/9999): loss=22709.72615115077, w0=0.16736764368888407, w1=0.46847798564577303\n",
      "Gradient Descent(92/9999): loss=22352.375309409243, w0=0.16474887064100432, w1=0.46818028578693577\n",
      "Gradient Descent(93/9999): loss=22004.392882975157, w0=0.16217819978074016, w1=0.4678848790087668\n",
      "Gradient Descent(94/9999): loss=21665.495298137193, w0=0.1596547464954664, w1=0.4675917394519182\n",
      "Gradient Descent(95/9999): loss=21335.407929645782, w0=0.15717764328148207, w1=0.4673008415508048\n",
      "Gradient Descent(96/9999): loss=21013.864807268714, w0=0.1547460393663071, w1=0.4670121600307961\n",
      "Gradient Descent(97/9999): loss=20700.608332778367, w0=0.15235910034148467, w1=0.46672566990541237\n",
      "Gradient Descent(98/9999): loss=20395.389006929934, w0=0.15001600780551183, w1=0.4664413464735269\n",
      "Gradient Descent(99/9999): loss=20097.965166014434, w0=0.14771595901653775, w1=0.4661591653165745\n",
      "Gradient Descent(100/9999): loss=19808.10272759303, w0=0.14545816655448368, w1=0.465879102295768\n",
      "Gradient Descent(101/9999): loss=19525.57494504018, w0=0.14324185799225408, w1=0.46560113354932386\n",
      "Gradient Descent(102/9999): loss=19250.16217054281, w0=0.14106627557572174, w1=0.4653252354896968\n",
      "Gradient Descent(103/9999): loss=18981.65162622066, w0=0.13893067591218336, w1=0.46505138480082553\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(104/9999): loss=18719.837183050266, w0=0.13683432966699485, w1=0.46477955843538976\n",
      "Gradient Descent(105/9999): loss=18464.519147290477, w0=0.13477652126810755, w1=0.46450973361207903\n",
      "Gradient Descent(106/9999): loss=18215.504054122528, w0=0.13275654861823813, w1=0.46424188781287473\n",
      "Gradient Descent(107/9999): loss=17972.604468231213, w0=0.13077372281441638, w1=0.46397599878034534\n",
      "Gradient Descent(108/9999): loss=17735.638791066754, w0=0.12882736787466498, w1=0.46371204451495596\n",
      "Gradient Descent(109/9999): loss=17504.431074539134, w0=0.12691682047157596, w1=0.46345000327239233\n",
      "Gradient Descent(110/9999): loss=17278.810840907823, w0=0.12504142967255794, w1=0.46318985356090026\n",
      "Gradient Descent(111/9999): loss=17058.612908640713, w0=0.12320055668653732, w1=0.4629315741386406\n",
      "Gradient Descent(112/9999): loss=16843.677224025894, w0=0.12139357461690538, w1=0.4626751440110605\n",
      "Gradient Descent(113/9999): loss=16633.84869832953, w0=0.11961986822051181, w1=0.4624205424282812\n",
      "Gradient Descent(114/9999): loss=16428.977050301903, w0=0.11787883367251277, w1=0.4621677488825027\n",
      "Gradient Descent(115/9999): loss=16228.916653842003, w0=0.11616987833688949, w1=0.46191674310542635\n",
      "Gradient Descent(116/9999): loss=16033.526390639134, w0=0.1144924205424606, w1=0.46166750506569415\n",
      "Gradient Descent(117/9999): loss=15842.669507617404, w0=0.11284588936421817, w1=0.46142001496634716\n",
      "Gradient Descent(118/9999): loss=15656.213479016003, w0=0.11122972440982438, w1=0.4611742532423013\n",
      "Gradient Descent(119/9999): loss=15474.029872945035, w0=0.10964337561111177, w1=0.4609302005578425\n",
      "Gradient Descent(120/9999): loss=15295.994222262902, w0=0.1080863030204362, w1=0.46068783780413974\n",
      "Gradient Descent(121/9999): loss=15121.985899627329, w0=0.10655797661173737, w1=0.46044714609677795\n",
      "Gradient Descent(122/9999): loss=14951.887996577907, w0=0.10505787608616748, w1=0.46020810677330964\n",
      "Gradient Descent(123/9999): loss=14785.587206513483, w0=0.10358549068215357, w1=0.459970701390826\n",
      "Gradient Descent(124/9999): loss=14622.973711432749, w0=0.10214031898976426, w1=0.45973491172354763\n",
      "Gradient Descent(125/9999): loss=14463.941072311694, w0=0.10072186876925669, w1=0.45950071976043455\n",
      "Gradient Descent(126/9999): loss=14308.386122995893, w0=0.09932965677368352, w1=0.4592681077028166\n",
      "Gradient Descent(127/9999): loss=14156.208867490464, w0=0.09796320857544494, w1=0.4590370579620433\n",
      "Gradient Descent(128/9999): loss=14007.312380534571, w0=0.09662205839667416, w1=0.4588075531571536\n",
      "Gradient Descent(129/9999): loss=13861.602711351574, w0=0.09530574894334959, w1=0.45857957611256606\n",
      "Gradient Descent(130/9999): loss=13718.9887904699, w0=0.09401383124303012, w1=0.45835310985578875\n",
      "Gradient Descent(131/9999): loss=13579.382339513353, w0=0.09274586448611416, w1=0.45812813761514964\n",
      "Gradient Descent(132/9999): loss=13442.697783863227, w0=0.09150141587052632, w1=0.45790464281754684\n",
      "Gradient Descent(133/9999): loss=13308.852168098132, w0=0.09028006044973906, w1=0.45768260908621927\n",
      "Gradient Descent(134/9999): loss=13177.765074120558, w0=0.08908138098403998, w1=0.4574620202385375\n",
      "Gradient Descent(135/9999): loss=13049.358541882571, w0=0.0879049677949584, w1=0.4572428602838147\n",
      "Gradient Descent(136/9999): loss=12923.556992625914, w0=0.08675041862276806, w1=0.45702511342113805\n",
      "Gradient Descent(137/9999): loss=12800.287154554842, w0=0.08561733848698522, w1=0.45680876403721987\n",
      "Gradient Descent(138/9999): loss=12679.477990862719, w0=0.08450533954978481, w1=0.45659379670426964\n",
      "Gradient Descent(139/9999): loss=12561.060630036203, w0=0.08341404098225924, w1=0.4563801961778854\n",
      "Gradient Descent(140/9999): loss=12444.968298363308, w0=0.08234306883344736, w1=0.45616794739496563\n",
      "Gradient Descent(141/9999): loss=12331.13625457429, w0=0.08129205590206351, w1=0.4559570354716413\n",
      "Gradient Descent(142/9999): loss=12219.501726546565, w0=0.08026064161085868, w1=0.4557474457012275\n",
      "Gradient Descent(143/9999): loss=12110.003850007239, w0=0.07924847188354818, w1=0.45553916355219537\n",
      "Gradient Descent(144/9999): loss=12002.583609169074, w0=0.07825519902424236, w1=0.4553321746661635\n",
      "Gradient Descent(145/9999): loss=11897.183779237785, w0=0.07728048159931869, w1=0.4551264648559096\n",
      "Gradient Descent(146/9999): loss=11793.74887073073, w0=0.07632398432167611, w1=0.4549220201034013\n",
      "Gradient Descent(147/9999): loss=11692.22507554893, w0=0.07538537793731354, w1=0.4547188265578472\n",
      "Gradient Descent(148/9999): loss=11592.56021474638, w0=0.07446433911417696, w1=0.4545168705337666\n",
      "Gradient Descent(149/9999): loss=11494.703687942389, w0=0.07356055033322116, w1=0.4543161385090797\n",
      "Gradient Descent(150/9999): loss=11398.606424324502, w0=0.07267369978163327, w1=0.45411661712321616\n",
      "Gradient Descent(151/9999): loss=11304.220835191287, w0=0.07180348124816771, w1=0.45391829317524357\n",
      "Gradient Descent(152/9999): loss=11211.500767985908, w0=0.07094959402054303, w1=0.45372115362201454\n",
      "Gradient Descent(153/9999): loss=11120.401461773012, w0=0.07011174278485291, w1=0.45352518557633326\n",
      "Gradient Descent(154/9999): loss=11030.879504113038, w0=0.06928963752694497, w1=0.45333037630514056\n",
      "Gradient Descent(155/9999): loss=10942.892789289503, w0=0.06848299343572237, w1=0.4531367132277181\n",
      "Gradient Descent(156/9999): loss=10856.400477846293, w0=0.06769153080832452, w1=0.4529441839139109\n",
      "Gradient Descent(157/9999): loss=10771.362957393394, w0=0.06691497495714455, w1=0.45275277608236864\n",
      "Gradient Descent(158/9999): loss=10687.74180464078, w0=0.0661530561186422, w1=0.4525624775988054\n",
      "Gradient Descent(159/9999): loss=10605.499748621583, w0=0.06540550936391236, w1=0.4523732764742775\n",
      "Gradient Descent(160/9999): loss=10524.600635066818, w0=0.06467207451097011, w1=0.4521851608634796\n",
      "Gradient Descent(161/9999): loss=10445.009391895224, w0=0.06395249603871471, w1=0.451998119063059\n",
      "Gradient Descent(162/9999): loss=10366.691995782909, w0=0.06324652300253567, w1=0.4518121395099476\n",
      "Gradient Descent(163/9999): loss=10289.615439778689, w0=0.0625539089515252, w1=0.45162721077971185\n",
      "Gradient Descent(164/9999): loss=10213.747701932007, w0=0.06187441184726239, w1=0.4514433215849203\n",
      "Gradient Descent(165/9999): loss=10139.05771490148, w0=0.06120779398413523, w1=0.45126046077352844\n",
      "Gradient Descent(166/9999): loss=10065.515336513097, w0=0.06055382191116757, w1=0.45107861732728133\n",
      "Gradient Descent(167/9999): loss=9993.091321238058, w0=0.05991226635531916, w1=0.45089778036013306\n",
      "Gradient Descent(168/9999): loss=9921.757292561293, w0=0.059282902146227404, w1=0.45071793911668356\n",
      "Gradient Descent(169/9999): loss=9851.48571621252, w0=0.058665508142360684, w1=0.45053908297063244\n",
      "Gradient Descent(170/9999): loss=9782.249874232662, w0=0.05805986715855359, w1=0.45036120142324926\n",
      "Gradient Descent(171/9999): loss=9714.023839849327, w0=0.057465765894895285, w1=0.45018428410186107\n",
      "Gradient Descent(172/9999): loss=9646.782453135826, w0=0.05688299486694302, w1=0.45000832075835595\n",
      "Gradient Descent(173/9999): loss=9580.501297429055, w0=0.056311348337233356, w1=0.4498333012677034\n",
      "Gradient Descent(174/9999): loss=9515.156676482431, w0=0.05575062424806462, w1=0.44965921562649047\n",
      "Gradient Descent(175/9999): loss=9450.725592330633, w0=0.055200624155524416, w1=0.4494860539514748\n",
      "Gradient Descent(176/9999): loss=9387.185723843846, w0=0.05466115316473702, w1=0.4493138064781528\n",
      "Gradient Descent(177/9999): loss=9324.515405949813, w0=0.054132019866305905, w1=0.44914246355934423\n",
      "Gradient Descent(178/9999): loss=9262.693609502692, w0=0.05361303627392727, w1=0.44897201566379236\n",
      "Gradient Descent(179/9999): loss=9201.699921778374, w0=0.05310401776315109, w1=0.44880245337477953\n",
      "Gradient Descent(180/9999): loss=9141.514527576635, w0=0.05260478301126669, w1=0.4486337673887583\n",
      "Gradient Descent(181/9999): loss=9082.118190911036, w0=0.052115153938290575, w1=0.44846594851399796\n",
      "Gradient Descent(182/9999): loss=9023.492237268112, w0=0.05163495564903446, w1=0.44829898766924586\n",
      "Gradient Descent(183/9999): loss=8965.618536418006, w0=0.0511640163762323, w1=0.4481328758824041\n",
      "Gradient Descent(184/9999): loss=8908.479485759242, w0=0.050702167424705416, w1=0.44796760428922083\n",
      "Gradient Descent(185/9999): loss=8852.057994180883, w0=0.05024924311654531, w1=0.44780316413199645\n",
      "Gradient Descent(186/9999): loss=8796.337466425843, w0=0.04980508073729437, w1=0.4476395467583042\n",
      "Gradient Descent(187/9999): loss=8741.301787939678, w0=0.04936952048310496, w1=0.44747674361972545\n",
      "Gradient Descent(188/9999): loss=8686.93531018959, w0=0.04894240540885787, w1=0.44731474627059903\n",
      "Gradient Descent(189/9999): loss=8633.222836438978, w0=0.04852358137722166, w1=0.4471535463667849\n",
      "Gradient Descent(190/9999): loss=8580.149607963214, w0=0.048112897008634636, w1=0.4469931356644418\n",
      "Gradient Descent(191/9999): loss=8527.701290692868, w0=0.04771020363219182, w1=0.446833506018819\n",
      "Gradient Descent(192/9999): loss=8475.863962270978, w0=0.04731535523741947, w1=0.4466746493830612\n",
      "Gradient Descent(193/9999): loss=8424.624099511433, w0=0.04692820842692027, w1=0.4465165578070277\n",
      "Gradient Descent(194/9999): loss=8373.9685662459, w0=0.04654862236987251, w1=0.44635922343612505\n",
      "Gradient Descent(195/9999): loss=8323.884601547159, w0=0.04617645875636712, w1=0.4462026385101525\n",
      "Gradient Descent(196/9999): loss=8274.35980831707, w0=0.045811581752566555, w1=0.44604679536216113\n",
      "Gradient Descent(197/9999): loss=8225.382142227783, w0=0.045453857956670146, w1=0.4458916864173259\n",
      "Gradient Descent(198/9999): loss=8176.9399010051375, w0=0.04510315635567056, w1=0.44573730419183094\n",
      "Gradient Descent(199/9999): loss=8129.021714043582, w0=0.04475934828288659, w1=0.44558364129176714\n",
      "Gradient Descent(200/9999): loss=8081.616532342244, w0=0.04442230737625765, w1=0.445430690412043\n",
      "Gradient Descent(201/9999): loss=8034.713618752126, w0=0.04409190953738579, w1=0.44527844433530755\n",
      "Gradient Descent(202/9999): loss=7988.302538524738, w0=0.04376803289131114, w1=0.4451268959308862\n",
      "Gradient Descent(203/9999): loss=7942.373150152714, w0=0.04345055774700728, w1=0.44497603815372855\n",
      "Gradient Descent(204/9999): loss=7896.915596493342, w0=0.043139366558583005, w1=0.4448258640433684\n",
      "Gradient Descent(205/9999): loss=7851.92029616618, w0=0.04283434388717744, w1=0.444676366722896\n",
      "Gradient Descent(206/9999): loss=7807.377935216197, w0=0.0425353763635357, w1=0.4445275393979424\n",
      "Gradient Descent(207/9999): loss=7763.279459034155, w0=0.04224235265125245, w1=0.44437937535567484\n",
      "Gradient Descent(208/9999): loss=7719.6160645262435, w0=0.04195516341067106, w1=0.44423186796380487\n",
      "Gradient Descent(209/9999): loss=7676.379192525192, w0=0.041673701263426344, w1=0.4440850106696072\n",
      "Gradient Descent(210/9999): loss=7633.5605204353005, w0=0.04139786075761893, w1=0.4439387969989505\n",
      "Gradient Descent(211/9999): loss=7591.151955104168, w0=0.04112753833360984, w1=0.44379322055533926\n",
      "Gradient Descent(212/9999): loss=7549.145625914008, w0=0.04086263229042378, w1=0.4436482750189669\n",
      "Gradient Descent(213/9999): loss=7507.533878085733, w0=0.04060304275275003, w1=0.4435039541457802\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(214/9999): loss=7466.309266189215, w0=0.04034867163853013, w1=0.4433602517665542\n",
      "Gradient Descent(215/9999): loss=7425.464547853256, w0=0.0400994226271215, w1=0.4432171617859786\n",
      "Gradient Descent(216/9999): loss=7384.992677669096, w0=0.0398552011280267, w1=0.4430746781817543\n",
      "Gradient Descent(217/9999): loss=7344.886801281433, w0=0.03961591425017782, w1=0.44293279500370086\n",
      "Gradient Descent(218/9999): loss=7305.140249661115, w0=0.039381470771766174, w1=0.4427915063728746\n",
      "Gradient Descent(219/9999): loss=7265.7465335538745, w0=0.039151781110607214, w1=0.4426508064806967\n",
      "Gradient Descent(220/9999): loss=7226.699338099627, w0=0.038926757295031096, w1=0.442510689588092\n",
      "Gradient Descent(221/9999): loss=7187.992517617024, w0=0.03870631293528931, w1=0.44237115002463745\n",
      "Gradient Descent(222/9999): loss=7149.620090548159, w0=0.03849036319546813, w1=0.4422321821877214\n",
      "Gradient Descent(223/9999): loss=7111.5762345584335, w0=0.03827882476589971, w1=0.44209378054171206\n",
      "Gradient Descent(224/9999): loss=7073.8552817867585, w0=0.038071615836061834, w1=0.4419559396171361\n",
      "Gradient Descent(225/9999): loss=7036.4517142414525, w0=0.0378686560679576, w1=0.4418186540098673\n",
      "Gradient Descent(226/9999): loss=6999.36015933731, w0=0.03766986656996629, w1=0.4416819183803243\n",
      "Gradient Descent(227/9999): loss=6962.575385569436, w0=0.03747516987115715, w1=0.44154572745267817\n",
      "Gradient Descent(228/9999): loss=6926.0922983196415, w0=0.037284489896057524, w1=0.44141007601406945\n",
      "Gradient Descent(229/9999): loss=6889.905935791278, w0=0.037097751939867474, w1=0.4412749589138343\n",
      "Gradient Descent(230/9999): loss=6854.011465068523, w0=0.036914882644112676, w1=0.44114037106273984\n",
      "Gradient Descent(231/9999): loss=6818.404178296269, w0=0.03673580997272787, w1=0.4410063074322292\n",
      "Gradient Descent(232/9999): loss=6783.079488976884, w0=0.03656046318856311, w1=0.4408727630536745\n",
      "Gradient Descent(233/9999): loss=6748.032928380212, w0=0.03638877283030528, w1=0.4407397330176398\n",
      "Gradient Descent(234/9999): loss=6713.260142063339, w0=0.03622067068980751, w1=0.4406072124731522\n",
      "Gradient Descent(235/9999): loss=6678.756886496673, w0=0.03605608978981914, w1=0.440475196626982\n",
      "Gradient Descent(236/9999): loss=6644.51902579311, w0=0.035894964362109136, w1=0.4403436807429312\n",
      "Gradient Descent(237/9999): loss=6610.542528537054, w0=0.035737229825975984, w1=0.4402126601411307\n",
      "Gradient Descent(238/9999): loss=6576.823464710228, w0=0.03558282276713713, w1=0.44008213019734604\n",
      "Gradient Descent(239/9999): loss=6543.358002711274, w0=0.03543168091699123, w1=0.4399520863422911\n",
      "Gradient Descent(240/9999): loss=6510.142406466234, w0=0.03528374313224662, w1=0.43982252406095046\n",
      "Gradient Descent(241/9999): loss=6477.17303262713, w0=0.03513894937490948, w1=0.4396934388919095\n",
      "Gradient Descent(242/9999): loss=6444.446327855898, w0=0.034997240692625275, w1=0.4395648264266929\n",
      "Gradient Descent(243/9999): loss=6411.958826191053, w0=0.03485855919936729, w1=0.439436682309111\n",
      "Gradient Descent(244/9999): loss=6379.707146494531, w0=0.03472284805646602, w1=0.4393090022346138\n",
      "Gradient Descent(245/9999): loss=6347.687989976214, w0=0.03459005145397341, w1=0.43918178194965307\n",
      "Gradient Descent(246/9999): loss=6315.898137793782, w0=0.03446011459235605, w1=0.4390550172510521\n",
      "Gradient Descent(247/9999): loss=6284.3344487255235, w0=0.03433298366451146, w1=0.43892870398538275\n",
      "Gradient Descent(248/9999): loss=6252.993856913889, w0=0.0342086058381017, w1=0.43880283804835046\n",
      "Gradient Descent(249/9999): loss=6221.873369677585, w0=0.03408692923819888, w1=0.4386774153841863\n",
      "Gradient Descent(250/9999): loss=6190.970065390109, w0=0.03396790293023678, w1=0.43855243198504684\n",
      "Gradient Descent(251/9999): loss=6160.281091422671, w0=0.033851476903263435, w1=0.43842788389042064\n",
      "Gradient Descent(252/9999): loss=6129.803662149528, w0=0.03373760205348922, w1=0.43830376718654257\n",
      "Gradient Descent(253/9999): loss=6099.5350570137925, w0=0.033626230168125215, w1=0.4381800780058149\n",
      "Gradient Descent(254/9999): loss=6069.472618651879, w0=0.033517313909506845, w1=0.4380568125262353\n",
      "Gradient Descent(255/9999): loss=6039.613751074748, w0=0.0334108067994976, w1=0.4379339669708322\n",
      "Gradient Descent(256/9999): loss=6009.95591790425, w0=0.03330666320416804, w1=0.43781153760710656\n",
      "Gradient Descent(257/9999): loss=5980.496640662815, w0=0.033204838318745135, w1=0.43768952074648065\n",
      "Gradient Descent(258/9999): loss=5951.233497114905, w0=0.03310528815282725, w1=0.43756791274375356\n",
      "Gradient Descent(259/9999): loss=5922.164119658594, w0=0.03300796951585998, w1=0.43744670999656327\n",
      "Gradient Descent(260/9999): loss=5893.286193765764, w0=0.032912840002868406, w1=0.4373259089448553\n",
      "Gradient Descent(261/9999): loss=5864.597456469406, w0=0.0328198579804411, w1=0.43720550607035763\n",
      "Gradient Descent(262/9999): loss=5836.09569489659, w0=0.032728982572961536, w1=0.4370854978960625\n",
      "Gradient Descent(263/9999): loss=5807.778744845696, w0=0.03264017364908252, w1=0.43696588098571393\n",
      "Gradient Descent(264/9999): loss=5779.64448940655, w0=0.03255339180843938, w1=0.43684665194330213\n",
      "Gradient Descent(265/9999): loss=5751.690857622149, w0=0.03246859836859769, w1=0.43672780741256345\n",
      "Gradient Descent(266/9999): loss=5723.915823190694, w0=0.032385755352231434, w1=0.4366093440764869\n",
      "Gradient Descent(267/9999): loss=5696.317403206708, w0=0.032304825474527524, w1=0.43649125865682653\n",
      "Gradient Descent(268/9999): loss=5668.893656940022, w0=0.03222577213081271, w1=0.4363735479136199\n",
      "Gradient Descent(269/9999): loss=5641.642684651485, w0=0.032148559384398985, w1=0.43625620864471204\n",
      "Gradient Descent(270/9999): loss=5614.562626444265, w0=0.032073151954643615, w1=0.43613923768528606\n",
      "Gradient Descent(271/9999): loss=5587.6516611496545, w0=0.031999515205220035, w1=0.4360226319073985\n",
      "Gradient Descent(272/9999): loss=5560.908005246318, w0=0.031927615132595906, w1=0.43590638821952105\n",
      "Gradient Descent(273/9999): loss=5534.329911811963, w0=0.03185741835471469, w1=0.4357905035660879\n",
      "Gradient Descent(274/9999): loss=5507.915669506445, w0=0.03178889209987718, w1=0.43567497492704826\n",
      "Gradient Descent(275/9999): loss=5481.663601585341, w0=0.03172200419581947, w1=0.4355597993174247\n",
      "Gradient Descent(276/9999): loss=5455.572064943055, w0=0.031656723058983864, w1=0.4354449737868768\n",
      "Gradient Descent(277/9999): loss=5429.6394491845585, w0=0.031593017683979416, w1=0.43533049541927005\n",
      "Gradient Descent(278/9999): loss=5403.864175724906, w0=0.03153085763322872, w1=0.43521636133225033\n",
      "Gradient Descent(279/9999): loss=5378.24469691563, w0=0.03147021302679771, w1=0.43510256867682345\n",
      "Gradient Descent(280/9999): loss=5352.7794951972655, w0=0.03141105453240516, w1=0.43498911463693984\n",
      "Gradient Descent(281/9999): loss=5327.467082277126, w0=0.031353353355608946, w1=0.43487599642908453\n",
      "Gradient Descent(282/9999): loss=5302.305998331641, w0=0.03129708123016567, w1=0.43476321130187195\n",
      "Gradient Descent(283/9999): loss=5277.294811232431, w0=0.03124221040856087, w1=0.4346507565356459\n",
      "Gradient Descent(284/9999): loss=5252.432115795459, w0=0.03118871365270666, w1=0.43453862944208443\n",
      "Gradient Descent(285/9999): loss=5227.716533052501, w0=0.03113656422480393, w1=0.43442682736380955\n",
      "Gradient Descent(286/9999): loss=5203.1467095443095, w0=0.031085735878366175, w1=0.4343153476740019\n",
      "Gradient Descent(287/9999): loss=5178.721316634751, w0=0.031036202849402155, w1=0.4342041877760198\n",
      "Gradient Descent(288/9999): loss=5154.439049845335, w0=0.03098793984775457, w1=0.4340933451030238\n",
      "Gradient Descent(289/9999): loss=5130.298628209469, w0=0.03094092204859204, w1=0.4339828171176046\n",
      "Gradient Descent(290/9999): loss=5106.298793645864, w0=0.03089512508405165, w1=0.43387260131141725\n",
      "Gradient Descent(291/9999): loss=5082.4383103505015, w0=0.030850525035029476, w1=0.43376269520481814\n",
      "Gradient Descent(292/9999): loss=5058.715964206597, w0=0.030807098423116446, w1=0.43365309634650784\n",
      "Gradient Descent(293/9999): loss=5035.130562212008, w0=0.030764822202677008, w1=0.43354380231317774\n",
      "Gradient Descent(294/9999): loss=5011.680931923567, w0=0.030723673753068108, w1=0.43343481070916107\n",
      "Gradient Descent(295/9999): loss=4988.365920917819, w0=0.03068363087099597, w1=0.4333261191660884\n",
      "Gradient Descent(296/9999): loss=4965.1843962676585, w0=0.03064467176300833, w1=0.43321772534254743\n",
      "Gradient Descent(297/9999): loss=4942.135244034403, w0=0.030606775038119663, w1=0.43310962692374705\n",
      "Gradient Descent(298/9999): loss=4919.217368774808, w0=0.030569919700567143, w1=0.4330018216211852\n",
      "Gradient Descent(299/9999): loss=4896.429693062595, w0=0.030534085142694997, w1=0.43289430717232147\n",
      "Gradient Descent(300/9999): loss=4873.771157024038, w0=0.030499251137964988, w1=0.43278708134025307\n",
      "Gradient Descent(301/9999): loss=4851.240717887183, w0=0.030465397834090874, w1=0.43268014191339554\n",
      "Gradient Descent(302/9999): loss=4828.837349544288, w0=0.030432505746294598, w1=0.432573486705167\n",
      "Gradient Descent(303/9999): loss=4806.560042127107, w0=0.030400555750682096, w1=0.4324671135536763\n",
      "Gradient Descent(304/9999): loss=4784.407801594569, w0=0.03036952907773665, w1=0.43236102032141527\n",
      "Gradient Descent(305/9999): loss=4762.3796493325535, w0=0.030339407305927657, w1=0.43225520489495484\n",
      "Gradient Descent(306/9999): loss=4740.474621765331, w0=0.03031017235543287, w1=0.43214966518464465\n",
      "Gradient Descent(307/9999): loss=4718.691769978361, w0=0.030281806481972037, w1=0.43204439912431664\n",
      "Gradient Descent(308/9999): loss=4697.0301593520735, w0=0.030254292270750043, w1=0.4319394046709923\n",
      "Gradient Descent(309/9999): loss=4675.48886920632, w0=0.03022761263050759, w1=0.4318346798045936\n",
      "Gradient Descent(310/9999): loss=4654.066992455167, w0=0.03020175078767755, w1=0.4317302225276574\n",
      "Gradient Descent(311/9999): loss=4632.763635271723, w0=0.030176690280645114, w1=0.4316260308650537\n",
      "Gradient Descent(312/9999): loss=4611.577916762675, w0=0.030152414954109943, w1=0.43152210286370685\n",
      "Gradient Descent(313/9999): loss=4590.508968652287, w0=0.03012890895354849, w1=0.43141843659232115\n",
      "Gradient Descent(314/9999): loss=4569.55593497552, w0=0.030106156719774773, w1=0.43131503014110895\n",
      "Gradient Descent(315/9999): loss=4548.717971780039, w0=0.030084142983597822, w1=0.4312118816215227\n",
      "Gradient Descent(316/9999): loss=4527.994246836828, w0=0.030062852760574146, w1=0.4311089891659902\n",
      "Gradient Descent(317/9999): loss=4507.3839393591425, w0=0.03004227134585352, w1=0.4310063509276533\n",
      "Gradient Descent(318/9999): loss=4486.886239729564, w0=0.03002238430911646, w1=0.43090396508010964\n",
      "Gradient Descent(319/9999): loss=4466.500349234904, w0=0.030003177489601792, w1=0.4308018298171576\n",
      "Gradient Descent(320/9999): loss=4446.225479808725, w0=0.029984636991222675, w1=0.4306999433525449\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(321/9999): loss=4426.060853781246, w0=0.02996674917776959, w1=0.4305983039197198\n",
      "Gradient Descent(322/9999): loss=4406.005703636414, w0=0.02994950066819873, w1=0.43049690977158583\n",
      "Gradient Descent(323/9999): loss=4386.059271775927, w0=0.029932878332004267, w1=0.4303957591802593\n",
      "Gradient Descent(324/9999): loss=4366.220810289993, w0=0.029916869284673103, w1=0.4302948504368301\n",
      "Gradient Descent(325/9999): loss=4346.489580734633, w0=0.02990146088322053, w1=0.4301941818511252\n",
      "Gradient Descent(326/9999): loss=4326.864853915323, w0=0.029886640721805476, w1=0.4300937517514755\n",
      "Gradient Descent(327/9999): loss=4307.345909676789, w0=0.029872396627423914, w1=0.4299935584844855\n",
      "Gradient Descent(328/9999): loss=4287.93203669877, w0=0.02985871665567901, w1=0.42989360041480545\n",
      "Gradient Descent(329/9999): loss=4268.622532297576, w0=0.029845589086626722, w1=0.42979387592490703\n",
      "Gradient Descent(330/9999): loss=4249.416702233246, w0=0.02983300242069548, w1=0.4296943834148614\n",
      "Gradient Descent(331/9999): loss=4230.313860522178, w0=0.029820945374678658, w1=0.4295951213021201\n",
      "Gradient Descent(332/9999): loss=4211.31332925502, w0=0.029809406877798563, w1=0.4294960880212989\n",
      "Gradient Descent(333/9999): loss=4192.414438419707, w0=0.02979837606784066, w1=0.4293972820239642\n",
      "Gradient Descent(334/9999): loss=4173.61652572946, w0=0.02978784228735681, w1=0.42929870177842216\n",
      "Gradient Descent(335/9999): loss=4154.9189364556105, w0=0.0297777950799363, w1=0.4292003457695105\n",
      "Gradient Descent(336/9999): loss=4136.321023265124, w0=0.02976822418654348, w1=0.42910221249839287\n",
      "Gradient Descent(337/9999): loss=4117.822146062637, w0=0.02975911954192082, w1=0.429004300482356\n",
      "Gradient Descent(338/9999): loss=4099.421671836926, w0=0.029750471271056237, w1=0.42890660825460913\n",
      "Gradient Descent(339/9999): loss=4081.118974511636, w0=0.029742269685713557, w1=0.42880913436408624\n",
      "Gradient Descent(340/9999): loss=4062.9134348001676, w0=0.02973450528102502, w1=0.4287118773752505\n",
      "Gradient Descent(341/9999): loss=4044.804440064574, w0=0.02972716873214469, w1=0.4286148358679015\n",
      "Gradient Descent(342/9999): loss=4026.791384178387, w0=0.02972025089096174, w1=0.4285180084369846\n",
      "Gradient Descent(343/9999): loss=4008.873667393204, w0=0.029713742782872525, w1=0.42842139369240295\n",
      "Gradient Descent(344/9999): loss=3991.0506962089703, w0=0.029707635603610432, w1=0.42832499025883164\n",
      "Gradient Descent(345/9999): loss=3973.3218832478274, w0=0.029701920716132455, w1=0.4282287967755345\n",
      "Gradient Descent(346/9999): loss=3955.6866471314097, w0=0.029696589647561528, w1=0.42813281189618296\n",
      "Gradient Descent(347/9999): loss=3938.1444123615156, w0=0.029691634086183593, w1=0.4280370342886773\n",
      "Gradient Descent(348/9999): loss=3920.694609204028, w0=0.029687045878498484, w1=0.4279414626349701\n",
      "Gradient Descent(349/9999): loss=3903.336673576002, w0=0.029682817026323638, w1=0.42784609563089215\n",
      "Gradient Descent(350/9999): loss=3886.07004693582, w0=0.029678939683949707, w1=0.42775093198598013\n",
      "Gradient Descent(351/9999): loss=3868.8941761763267, w0=0.02967540615534717, w1=0.4276559704233069\n",
      "Gradient Descent(352/9999): loss=3851.8085135208535, w0=0.02967220889142303, w1=0.4275612096793137\n",
      "Gradient Descent(353/9999): loss=3834.812516422047, w0=0.029669340487326713, w1=0.42746664850364463\n",
      "Gradient Descent(354/9999): loss=3817.905647463423, w0=0.029666793679804293, w1=0.427372285658983\n",
      "Gradient Descent(355/9999): loss=3801.0873742635627, w0=0.029664561344600222, w1=0.42727811992089\n",
      "Gradient Descent(356/9999): loss=3784.357169382866, w0=0.02966263649390567, w1=0.4271841500776452\n",
      "Gradient Descent(357/9999): loss=3767.714510232805, w0=0.029661012273852688, w1=0.4270903749300894\n",
      "Gradient Descent(358/9999): loss=3751.1588789875827, w0=0.029659681962053392, w1=0.42699679329146895\n",
      "Gradient Descent(359/9999): loss=3734.689762498145, w0=0.02965863896518333, w1=0.4269034039872826\n",
      "Gradient Descent(360/9999): loss=3718.3066522084596, w0=0.02965787681660829, w1=0.4268102058551299\n",
      "Gradient Descent(361/9999): loss=3702.009044074011, w0=0.02965738917405376, w1=0.42671719774456174\n",
      "Gradient Descent(362/9999): loss=3685.796438482438, w0=0.02965716981731629, w1=0.4266243785169326\n",
      "Gradient Descent(363/9999): loss=3669.6683401762502, w0=0.029657212646016024, w1=0.42653174704525487\n",
      "Gradient Descent(364/9999): loss=3653.624258177565, w0=0.029657511677389635, w1=0.42643930221405496\n",
      "Gradient Descent(365/9999): loss=3637.6637057148114, w0=0.02965806104412301, w1=0.42634704291923115\n",
      "Gradient Descent(366/9999): loss=3621.7862001513267, w0=0.02965885499222292, w1=0.4262549680679133\n",
      "Gradient Descent(367/9999): loss=3605.991262915816, w0=0.029659887878927024, w1=0.4261630765783244\n",
      "Gradient Descent(368/9999): loss=3590.2784194345936, w0=0.029661154170651505, w1=0.42607136737964385\n",
      "Gradient Descent(369/9999): loss=3574.6471990655737, w0=0.029662648440975693, w1=0.4259798394118724\n",
      "Gradient Descent(370/9999): loss=3559.0971350339487, w0=0.029664365368662987, w1=0.4258884916256988\n",
      "Gradient Descent(371/9999): loss=3543.6277643695116, w0=0.029666299735717463, w1=0.42579732298236844\n",
      "Gradient Descent(372/9999): loss=3528.238627845575, w0=0.02966844642547552, w1=0.4257063324535532\n",
      "Gradient Descent(373/9999): loss=3512.9292699194298, w0=0.029670800420731953, w1=0.42561551902122335\n",
      "Gradient Descent(374/9999): loss=3497.699238674322, w0=0.029673356801899812, w1=0.4255248816775207\n",
      "Gradient Descent(375/9999): loss=3482.5480857628704, w0=0.0296761107452035, w1=0.4254344194246339\n",
      "Gradient Descent(376/9999): loss=3467.4753663519177, w0=0.029679057520904474, w1=0.42534413127467463\n",
      "Gradient Descent(377/9999): loss=3452.4806390687495, w0=0.029682192491558977, w1=0.4252540162495561\n",
      "Gradient Descent(378/9999): loss=3437.563465948653, w0=0.029685511110307258, w1=0.42516407338087253\n",
      "Gradient Descent(379/9999): loss=3422.7234123837734, w0=0.0296890089191937, w1=0.42507430170978044\n",
      "Gradient Descent(380/9999): loss=3407.960047073229, w0=0.029692681547517297, w1=0.4249847002868814\n",
      "Gradient Descent(381/9999): loss=3393.272941974451, w0=0.02969652471021198, w1=0.42489526817210627\n",
      "Gradient Descent(382/9999): loss=3378.661672255716, w0=0.029700534206256207, w1=0.42480600443460076\n",
      "Gradient Descent(383/9999): loss=3364.125816249831, w0=0.029704705917111336, w1=0.42471690815261287\n",
      "Gradient Descent(384/9999): loss=3349.6649554089377, w0=0.029709035805188275, w1=0.42462797841338124\n",
      "Gradient Descent(385/9999): loss=3335.27867426041, w0=0.02971351991234187, w1=0.42453921431302527\n",
      "Gradient Descent(386/9999): loss=3320.966560363807, w0=0.029718154358392555, w1=0.4244506149564366\n",
      "Gradient Descent(387/9999): loss=3306.728204268858, w0=0.029722935339674798, w1=0.42436217945717186\n",
      "Gradient Descent(388/9999): loss=3292.5631994744444, w0=0.029727859127611837, w1=0.4242739069373468\n",
      "Gradient Descent(389/9999): loss=3278.471142388549, w0=0.029732922067316255, w1=0.42418579652753186\n",
      "Gradient Descent(390/9999): loss=3264.451632289152, w0=0.02973812057621592, w1=0.42409784736664907\n",
      "Gradient Descent(391/9999): loss=3250.5042712860413, w0=0.02974345114270486, w1=0.4240100586018702\n",
      "Gradient Descent(392/9999): loss=3236.628664283512, w0=0.029748910324818606, w1=0.4239224293885161\n",
      "Gradient Descent(393/9999): loss=3222.8244189439342, w0=0.029754494748933588, w1=0.4238349588899577\n",
      "Gradient Descent(394/9999): loss=3209.091145652152, w0=0.02976020110849013, w1=0.4237476462775179\n",
      "Gradient Descent(395/9999): loss=3195.42845748071, w0=0.029766026162738643, w1=0.42366049073037476\n",
      "Gradient Descent(396/9999): loss=3181.8359701558657, w0=0.02977196673550861, w1=0.4235734914354662\n",
      "Gradient Descent(397/9999): loss=3168.313302024376, w0=0.029778019713999918, w1=0.4234866475873955\n",
      "Gradient Descent(398/9999): loss=3154.8600740210272, w0=0.02978418204759619, w1=0.4233999583883385\n",
      "Gradient Descent(399/9999): loss=3141.475909636905, w0=0.029790450746699674, w1=0.42331342304795144\n",
      "Gradient Descent(400/9999): loss=3128.160434888358, w0=0.029796822881587356, w1=0.4232270407832805\n",
      "Gradient Descent(401/9999): loss=3114.913278286659, w0=0.029803295581287855, w1=0.423140810818672\n",
      "Gradient Descent(402/9999): loss=3101.734070808334, w0=0.029809866032478805, w1=0.4230547323856843\n",
      "Gradient Descent(403/9999): loss=3088.6224458661322, w0=0.029816531478404285, w1=0.42296880472300025\n",
      "Gradient Descent(404/9999): loss=3075.5780392806405, w0=0.029823289217811987, w1=0.4228830270763411\n",
      "Gradient Descent(405/9999): loss=3062.6004892525034, w0=0.02983013660390975, w1=0.4227973986983817\n",
      "Gradient Descent(406/9999): loss=3049.6894363352435, w0=0.029837071043341108, w1=0.42271191884866594\n",
      "Gradient Descent(407/9999): loss=3036.8445234086594, w0=0.029844089995179558, w1=0.42262658679352455\n",
      "Gradient Descent(408/9999): loss=3024.065395652795, w0=0.029851190969941113, w1=0.42254140180599276\n",
      "Gradient Descent(409/9999): loss=3011.351700522449, w0=0.02985837152861494, w1=0.4224563631657297\n",
      "Gradient Descent(410/9999): loss=2998.7030877222237, w0=0.02986562928171167, w1=0.42237147015893867\n",
      "Gradient Descent(411/9999): loss=2986.119209182092, w0=0.02987296188832907, w1=0.4222867220782882\n",
      "Gradient Descent(412/9999): loss=2973.599719033466, w0=0.02988036705523483, w1=0.42220211822283465\n",
      "Gradient Descent(413/9999): loss=2961.1442735857645, w0=0.02988784253596606, w1=0.4221176578979451\n",
      "Gradient Descent(414/9999): loss=2948.752531303445, w0=0.029895386129945325, w1=0.42203334041522184\n",
      "Gradient Descent(415/9999): loss=2936.4241527835147, w0=0.02990299568161276, w1=0.4219491650924275\n",
      "Gradient Descent(416/9999): loss=2924.1588007334835, w0=0.029910669079574154, w1=0.4218651312534113\n",
      "Gradient Descent(417/9999): loss=2911.9561399497584, w0=0.02991840425576455, w1=0.42178123822803587\n",
      "Gradient Descent(418/9999): loss=2899.8158372964676, w0=0.02992619918462723, w1=0.4216974853521057\n",
      "Gradient Descent(419/9999): loss=2887.7375616847003, w0=0.029934051882307695, w1=0.42161387196729583\n",
      "Gradient Descent(420/9999): loss=2875.7209840521477, w0=0.029941960405862424, w1=0.4215303974210817\n",
      "Gradient Descent(421/9999): loss=2863.7657773431465, w0=0.029949922852482158, w1=0.4214470610666702\n",
      "Gradient Descent(422/9999): loss=2851.871616489092, w0=0.029957937358729418, w1=0.42136386226293093\n",
      "Gradient Descent(423/9999): loss=2840.0381783892385, w0=0.02996600209979, w1=0.42128080037432913\n",
      "Gradient Descent(424/9999): loss=2828.2651418918545, w0=0.029974115288738244, w1=0.4211978747708587\n",
      "Gradient Descent(425/9999): loss=2816.5521877757305, w0=0.029982275175815747, w1=0.42111508482797666\n",
      "Gradient Descent(426/9999): loss=2804.8989987320383, w0=0.02999048004772337, w1=0.4210324299265382\n",
      "Gradient Descent(427/9999): loss=2793.305259346516, w0=0.02999872822692623, w1=0.4209499094527326\n",
      "Gradient Descent(428/9999): loss=2781.7706560819834, w0=0.030007018070971482, w1=0.42086752279801987\n",
      "Gradient Descent(429/9999): loss=2770.2948772611803, w0=0.030015347971818644, w1=0.4207852693590684\n",
      "Gradient Descent(430/9999): loss=2758.8776130498995, w0=0.030023716355182244, w1=0.4207031485376933\n",
      "Gradient Descent(431/9999): loss=2747.5185554404356, w0=0.030032121679886562, w1=0.4206211597407954\n",
      "Gradient Descent(432/9999): loss=2736.217398235319, w0=0.030040562437232275, w1=0.42053930238030135\n",
      "Gradient Descent(433/9999): loss=2724.9738370313335, w0=0.03004903715037474, w1=0.4204575758731041\n",
      "Gradient Descent(434/9999): loss=2713.7875692038215, w0=0.030057544373713767, w1=0.42037597964100454\n",
      "Gradient Descent(435/9999): loss=2702.6582938912475, w0=0.030066082692294615, w1=0.42029451311065347\n",
      "Gradient Descent(436/9999): loss=2691.585711980036, w0=0.03007465072122006, w1=0.4202131757134947\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(437/9999): loss=2680.5695260896596, w0=0.03008324710507331, w1=0.42013196688570864\n",
      "Gradient Descent(438/9999): loss=2669.6094405579797, w0=0.030091870517351557, w1=0.4200508860681566\n",
      "Gradient Descent(439/9999): loss=2658.705161426831, w0=0.030100519659910008, w1=0.41996993270632604\n",
      "Gradient Descent(440/9999): loss=2647.8563964278474, w0=0.03010919326241619, w1=0.4198891062502762\n",
      "Gradient Descent(441/9999): loss=2637.06285496851, w0=0.030117890081814334, w1=0.41980840615458465\n",
      "Gradient Descent(442/9999): loss=2626.324248118432, w0=0.030126608901799676, w1=0.4197278318782945\n",
      "Gradient Descent(443/9999): loss=2615.640288595857, w0=0.03013534853230247, w1=0.41964738288486214\n",
      "Gradient Descent(444/9999): loss=2605.010690754374, w0=0.030144107808981566, w1=0.41956705864210586\n",
      "Gradient Descent(445/9999): loss=2594.4351705698396, w0=0.03015288559272737, w1=0.4194868586221549\n",
      "Gradient Descent(446/9999): loss=2583.9134456275083, w0=0.030161680769174002, w1=0.41940678230139933\n",
      "Gradient Descent(447/9999): loss=2573.4452351093582, w0=0.030170492248220493, w1=0.41932682916044056\n",
      "Gradient Descent(448/9999): loss=2563.0302597816094, w0=0.030179318963560884, w1=0.41924699868404236\n",
      "Gradient Descent(449/9999): loss=2552.6682419824397, w0=0.03018815987222302, w1=0.4191672903610825\n",
      "Gradient Descent(450/9999): loss=2542.3589056098704, w0=0.030197013954115916, w1=0.4190877036845054\n",
      "Gradient Descent(451/9999): loss=2532.101976109849, w0=0.03020588021158554, w1=0.4190082381512745\n",
      "Gradient Descent(452/9999): loss=2521.8971804644934, w0=0.030214757668978826, w1=0.4189288932623265\n",
      "Gradient Descent(453/9999): loss=2511.744247180519, w0=0.030223645372215807, w1=0.418849668522525\n",
      "Gradient Descent(454/9999): loss=2501.6429062778197, w0=0.030232542388369702, w1=0.4187705634406153\n",
      "Gradient Descent(455/9999): loss=2491.592889278228, w0=0.03024144780525482, w1=0.4186915775291799\n",
      "Gradient Descent(456/9999): loss=2481.593929194418, w0=0.030250360731022125, w1=0.4186127103045942\n",
      "Gradient Descent(457/9999): loss=2471.645760518979, w0=0.030259280293762354, w1=0.4185339612869829\n",
      "Gradient Descent(458/9999): loss=2461.748119213631, w0=0.030268205641116507, w1=0.4184553300001772\n",
      "Gradient Descent(459/9999): loss=2451.9007426985895, w0=0.030277135939893608, w1=0.4183768159716722\n",
      "Gradient Descent(460/9999): loss=2442.10336984208, w0=0.030286070375695612, w1=0.418298418732585\n",
      "Gradient Descent(461/9999): loss=2432.355740949991, w0=0.030295008152549284, w1=0.41822013781761325\n",
      "Gradient Descent(462/9999): loss=2422.6575977556586, w0=0.030303948492544967, w1=0.4181419727649944\n",
      "Gradient Descent(463/9999): loss=2413.0086834098006, w0=0.030312890635482103, w1=0.41806392311646534\n",
      "Gradient Descent(464/9999): loss=2403.4087424705695, w0=0.030321833838521373, w1=0.4179859884172227\n",
      "Gradient Descent(465/9999): loss=2393.8575208937364, w0=0.030330777375843332, w1=0.41790816821588317\n",
      "Gradient Descent(466/9999): loss=2384.3547660230074, w0=0.030339720538313453, w1=0.4178304620644452\n",
      "Gradient Descent(467/9999): loss=2374.9002265804575, w0=0.030348662633153424, w1=0.4177528695182502\n",
      "Gradient Descent(468/9999): loss=2365.4936526570814, w0=0.03035760298361861, w1=0.4176753901359451\n",
      "Gradient Descent(469/9999): loss=2356.134795703471, w0=0.030366540928681556, w1=0.4175980234794449\n",
      "Gradient Descent(470/9999): loss=2346.8234085205986, w0=0.03037547582272142, w1=0.4175207691138956\n",
      "Gradient Descent(471/9999): loss=2337.5592452507212, w0=0.030384407035219246, w1=0.417443626607638\n",
      "Gradient Descent(472/9999): loss=2328.3420613683843, w0=0.03039333395045894, w1=0.4173665955321718\n",
      "Gradient Descent(473/9999): loss=2319.1716136715427, w0=0.030402255967233876, w1=0.41728967546212004\n",
      "Gradient Descent(474/9999): loss=2310.0476602727795, w0=0.030411172498559007, w1=0.417212865975194\n",
      "Gradient Descent(475/9999): loss=2300.9699605906326, w0=0.030420082971388387, w1=0.4171361666521589\n",
      "Gradient Descent(476/9999): loss=2291.938275341019, w0=0.030428986826338014, w1=0.4170595770767994\n",
      "Gradient Descent(477/9999): loss=2282.9523665287584, w0=0.03043788351741388, w1=0.4169830968358862\n",
      "Gradient Descent(478/9999): loss=2274.0119974391932, w0=0.030446772511745167, w1=0.41690672551914254\n",
      "Gradient Descent(479/9999): loss=2265.1169326299037, w0=0.030455653289322423, w1=0.41683046271921154\n",
      "Gradient Descent(480/9999): loss=2256.2669379225085, w0=0.03046452534274072, w1=0.41675430803162367\n",
      "Gradient Descent(481/9999): loss=2247.461780394573, w0=0.030473388176947648, w1=0.41667826105476474\n",
      "Gradient Descent(482/9999): loss=2238.7012283715817, w0=0.03048224130899604, w1=0.4166023213898443\n",
      "Gradient Descent(483/9999): loss=2229.985051419019, w0=0.030491084267801404, w1=0.4165264886408645\n",
      "Gradient Descent(484/9999): loss=2221.313020334523, w0=0.030499916593903907, w1=0.416450762414589\n",
      "Gradient Descent(485/9999): loss=2212.684907140125, w0=0.03050873783923488, w1=0.416375142320513\n",
      "Gradient Descent(486/9999): loss=2204.1004850745703, w0=0.030517547566887737, w1=0.41629962797083275\n",
      "Gradient Descent(487/9999): loss=2195.559528585724, w0=0.030526345350893217, w1=0.4162242189804162\n",
      "Gradient Descent(488/9999): loss=2187.061813323044, w0=0.030535130775998908, w1=0.41614891496677375\n",
      "Gradient Descent(489/9999): loss=2178.6071161301434, w0=0.030543903437452936, w1=0.416073715550029\n",
      "Gradient Descent(490/9999): loss=2170.195215037418, w0=0.03055266294079175, w1=0.4159986203528905\n",
      "Gradient Descent(491/9999): loss=2161.8258892547533, w0=0.03056140890163195, w1=0.4159236290006236\n",
      "Gradient Descent(492/9999): loss=2153.4989191643017, w0=0.030570140945466055, w1=0.4158487411210224\n",
      "Gradient Descent(493/9999): loss=2145.2140863133313, w0=0.030578858707462162, w1=0.4157739563443826\n",
      "Gradient Descent(494/9999): loss=2136.9711734071434, w0=0.03058756183226741, w1=0.41569927430347403\n",
      "Gradient Descent(495/9999): loss=2128.7699643020615, w0=0.03059624997381518, w1=0.4156246946335142\n",
      "Gradient Descent(496/9999): loss=2120.6102439984784, w0=0.03060492279513597, w1=0.4155502169721416\n",
      "Gradient Descent(497/9999): loss=2112.4917986339847, w0=0.030613579968171878, w1=0.4154758409593898\n",
      "Gradient Descent(498/9999): loss=2104.4144154765445, w0=0.030622221173594602, w1=0.41540156623766145\n",
      "Gradient Descent(499/9999): loss=2096.377882917745, w0=0.030630846100626944, w1=0.4153273924517032\n",
      "Gradient Descent(500/9999): loss=2088.381990466107, w0=0.030639454446867685, w1=0.4152533192485802\n",
      "Gradient Descent(501/9999): loss=2080.426528740455, w0=0.030648045918119832, w1=0.4151793462776515\n",
      "Gradient Descent(502/9999): loss=2072.5112894633453, w0=0.03065662022822214, w1=0.4151054731905455\n",
      "Gradient Descent(503/9999): loss=2064.6360654545583, w0=0.030665177098883854, w1=0.4150316996411359\n",
      "Gradient Descent(504/9999): loss=2056.8006506246434, w0=0.030673716259522605, w1=0.4149580252855175\n",
      "Gradient Descent(505/9999): loss=2049.0048399685234, w0=0.030682237447105434, w1=0.41488444978198297\n",
      "Gradient Descent(506/9999): loss=2041.2484295591557, w0=0.030690740405992848, w1=0.41481097279099943\n",
      "Gradient Descent(507/9999): loss=2033.5312165412456, w0=0.030699224887785866, w1=0.41473759397518545\n",
      "Gradient Descent(508/9999): loss=2025.852999125015, w0=0.030707690651176, w1=0.4146643129992884\n",
      "Gradient Descent(509/9999): loss=2018.213576580027, w0=0.030716137461798135, w1=0.41459112953016203\n",
      "Gradient Descent(510/9999): loss=2010.612749229058, w0=0.030724565092086206, w1=0.4145180432367443\n",
      "Gradient Descent(511/9999): loss=2003.0503184420213, w0=0.03073297332113168, w1=0.41444505379003566\n",
      "Gradient Descent(512/9999): loss=1995.5260866299482, w0=0.030741361934544742, w1=0.4143721608630773\n",
      "Gradient Descent(513/9999): loss=1988.03985723901, w0=0.03074973072431817, w1=0.4142993641309301\n",
      "Gradient Descent(514/9999): loss=1980.5914347445926, w0=0.030758079488693826, w1=0.4142266632706533\n",
      "Gradient Descent(515/9999): loss=1973.180624645421, w0=0.03076640803203172, w1=0.41415405796128385\n",
      "Gradient Descent(516/9999): loss=1965.8072334577294, w0=0.030774716164681602, w1=0.4140815478838162\n",
      "Gradient Descent(517/9999): loss=1958.4710687094764, w0=0.030783003702857054, w1=0.4140091327211815\n",
      "Gradient Descent(518/9999): loss=1951.1719389346076, w0=0.030791270468511982, w1=0.4139368121582281\n",
      "Gradient Descent(519/9999): loss=1943.909653667365, w0=0.030799516289219533, w1=0.41386458588170144\n",
      "Gradient Descent(520/9999): loss=1936.6840234366389, w0=0.030807740998053328, w1=0.4137924535802249\n",
      "Gradient Descent(521/9999): loss=1929.4948597603616, w0=0.030815944433471014, w1=0.41372041494428025\n",
      "Gradient Descent(522/9999): loss=1922.341975139951, w0=0.03082412643920007, w1=0.41364846966618884\n",
      "Gradient Descent(523/9999): loss=1915.2251830547877, w0=0.03083228686412583, w1=0.4135766174400927\n",
      "Gradient Descent(524/9999): loss=1908.1442979567425, w0=0.030840425562181684, w1=0.41350485796193615\n",
      "Gradient Descent(525/9999): loss=1901.0991352647384, w0=0.030848542392241402, w1=0.4134331909294473\n",
      "Gradient Descent(526/9999): loss=1894.0895113593594, w0=0.030856637218013565, w1=0.4133616160421202\n",
      "Gradient Descent(527/9999): loss=1887.1152435774934, w0=0.03086470990793805, w1=0.41329013300119677\n",
      "Gradient Descent(528/9999): loss=1880.1761502070196, w0=0.030872760335084536, w1=0.41321874150964943\n",
      "Gradient Descent(529/9999): loss=1873.2720504815304, w0=0.030880788377052968, w1=0.4131474412721634\n",
      "Gradient Descent(530/9999): loss=1866.4027645750982, w0=0.030888793915875998, w1=0.41307623199511984\n",
      "Gradient Descent(531/9999): loss=1859.5681135970729, w0=0.030896776837923308, w1=0.4130051133865785\n",
      "Gradient Descent(532/9999): loss=1852.7679195869198, w0=0.030904737033807812, w1=0.41293408515626134\n",
      "Gradient Descent(533/9999): loss=1846.002005509098, w0=0.030912674398293694, w1=0.4128631470155357\n",
      "Gradient Descent(534/9999): loss=1839.270195247969, w0=0.03092058883020624, w1=0.4127922986773982\n",
      "Gradient Descent(535/9999): loss=1832.5723136027452, w0=0.03092848023234345, w1=0.41272153985645826\n",
      "Gradient Descent(536/9999): loss=1825.9081862824733, w0=0.030936348511389388, w1=0.4126508702689225\n",
      "Gradient Descent(537/9999): loss=1819.277639901051, w0=0.030944193577829203, w1=0.4125802896325788\n",
      "Gradient Descent(538/9999): loss=1812.680501972282, w0=0.030952015345865856, w1=0.41250979766678086\n",
      "Gradient Descent(539/9999): loss=1806.1166009049596, w0=0.03095981373333848, w1=0.4124393940924328\n",
      "Gradient Descent(540/9999): loss=1799.5857659979895, w0=0.03096758866164232, w1=0.4123690786319739\n",
      "Gradient Descent(541/9999): loss=1793.0878274355427, w0=0.030975340055650297, w1=0.4122988510093638\n",
      "Gradient Descent(542/9999): loss=1786.6226162822416, w0=0.030983067843636067, w1=0.41222871095006763\n",
      "Gradient Descent(543/9999): loss=1780.1899644783803, w0=0.03099077195719864, w1=0.41215865818104136\n",
      "Gradient Descent(544/9999): loss=1773.7897048351751, w0=0.03099845233118848, w1=0.4120886924307175\n",
      "Gradient Descent(545/9999): loss=1767.4216710300468, w0=0.03100610890363506, w1=0.41201881342899066\n",
      "Gradient Descent(546/9999): loss=1761.0856976019368, w0=0.031013741615675848, w1=0.4119490209072037\n",
      "Gradient Descent(547/9999): loss=1754.7816199466497, w0=0.03102135041148672, w1=0.4118793145981337\n",
      "Gradient Descent(548/9999): loss=1748.5092743122311, w0=0.03102893523821373, w1=0.4118096942359783\n",
      "Gradient Descent(549/9999): loss=1742.2684977943727, w0=0.03103649604590626, w1=0.411740159556342\n",
      "Gradient Descent(550/9999): loss=1736.0591283318488, w0=0.031044032787451485, w1=0.41167071029622304\n",
      "Gradient Descent(551/9999): loss=1729.881004701981, w0=0.031051545418510133, w1=0.4116013461939999\n",
      "Gradient Descent(552/9999): loss=1723.7339665161353, w0=0.031059033897453545, w1=0.4115320669894184\n",
      "Gradient Descent(553/9999): loss=1717.6178542152427, w0=0.031066498185301994, w1=0.4114628724235788\n",
      "Gradient Descent(554/9999): loss=1711.5325090653537, w0=0.031073938245664203, w1=0.411393762238923\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(555/9999): loss=1705.4777731532172, w0=0.031081354044678104, w1=0.4113247361792219\n",
      "Gradient Descent(556/9999): loss=1699.4534893818916, w0=0.03108874555095277, w1=0.41125579398956313\n",
      "Gradient Descent(557/9999): loss=1693.4595014663794, w0=0.03109611273551152, w1=0.4111869354163387\n",
      "Gradient Descent(558/9999): loss=1687.4956539292903, w0=0.031103455571736133, w1=0.4111181602072329\n",
      "Gradient Descent(559/9999): loss=1681.5617920965344, w0=0.031110774035312233, w1=0.41104946811121024\n",
      "Gradient Descent(560/9999): loss=1675.6577620930382, w0=0.031118068104175726, w1=0.4109808588785036\n",
      "Gradient Descent(561/9999): loss=1669.7834108384889, w0=0.03112533775846034, w1=0.41091233226060275\n",
      "Gradient Descent(562/9999): loss=1663.9385860431062, w0=0.031132582980446206, w1=0.4108438880102426\n",
      "Gradient Descent(563/9999): loss=1658.1231362034378, w0=0.0311398037545095, w1=0.4107755258813919\n",
      "Gradient Descent(564/9999): loss=1652.3369105981817, w0=0.031147000067073074, w1=0.4107072456292419\n",
      "Gradient Descent(565/9999): loss=1646.5797592840358, w0=0.031154171906558097, w1=0.41063904701019527\n",
      "Gradient Descent(566/9999): loss=1640.851533091569, w0=0.03116131926333668, w1=0.41057092978185517\n",
      "Gradient Descent(567/9999): loss=1635.1520836211218, w0=0.03116844212968544, w1=0.4105028937030143\n",
      "Gradient Descent(568/9999): loss=1629.4812632387261, w0=0.031175540499740053, w1=0.4104349385336442\n",
      "Gradient Descent(569/9999): loss=1623.8389250720568, w0=0.03118261436945066, w1=0.4103670640348847\n",
      "Gradient Descent(570/9999): loss=1618.2249230064006, w0=0.031189663736538236, w1=0.41029926996903343\n",
      "Gradient Descent(571/9999): loss=1612.6391116806537, w0=0.031196688600451838, w1=0.4102315560995354\n",
      "Gradient Descent(572/9999): loss=1607.0813464833407, w0=0.031203688962326694, w1=0.41016392219097286\n",
      "Gradient Descent(573/9999): loss=1601.5514835486595, w0=0.031210664824943186, w1=0.41009636800905536\n",
      "Gradient Descent(574/9999): loss=1596.0493797525473, w0=0.031217616192686638, w1=0.41002889332060943\n",
      "Gradient Descent(575/9999): loss=1590.5748927087714, w0=0.031224543071507953, w1=0.40996149789356906\n",
      "Gradient Descent(576/9999): loss=1585.1278807650428, w0=0.031231445468885045, w1=0.4098941814969658\n",
      "Gradient Descent(577/9999): loss=1579.7082029991504, w0=0.031238323393785065, w1=0.4098269439009193\n",
      "Gradient Descent(578/9999): loss=1574.3157192151243, w0=0.03124517685662739, w1=0.4097597848766275\n",
      "Gradient Descent(579/9999): loss=1568.9502899394108, w0=0.03125200586924739, w1=0.40969270419635767\n",
      "Gradient Descent(580/9999): loss=1563.611776417081, w0=0.03125881044486094, w1=0.40962570163343687\n",
      "Gradient Descent(581/9999): loss=1558.300040608055, w0=0.03126559059802965, w1=0.4095587769622428\n",
      "Gradient Descent(582/9999): loss=1553.014945183347, w0=0.031272346344626833, w1=0.4094919299581949\n",
      "Gradient Descent(583/9999): loss=1547.7563535213371, w0=0.03127907770180416, w1=0.4094251603977452\n",
      "Gradient Descent(584/9999): loss=1542.5241297040604, w0=0.031285784687959, w1=0.40935846805836973\n",
      "Gradient Descent(585/9999): loss=1537.31813851352, w0=0.031292467322702466, w1=0.40929185271855967\n",
      "Gradient Descent(586/9999): loss=1532.1382454280183, w0=0.03129912562682811, w1=0.40922531415781266\n",
      "Gradient Descent(587/9999): loss=1526.9843166185133, w0=0.03130575962228124, w1=0.4091588521566244\n",
      "Gradient Descent(588/9999): loss=1521.8562189449908, w0=0.03131236933212894, w1=0.40909246649648023\n",
      "Gradient Descent(589/9999): loss=1516.753819952861, w0=0.03131895478053065, w1=0.40902615695984673\n",
      "Gradient Descent(590/9999): loss=1511.6769878693758, w0=0.031325515992709385, w1=0.4089599233301635\n",
      "Gradient Descent(591/9999): loss=1506.6255916000632, w0=0.03133205299492359, w1=0.4088937653918352\n",
      "Gradient Descent(592/9999): loss=1501.5995007251838, w0=0.03133856581443952, w1=0.40882768293022337\n",
      "Gradient Descent(593/9999): loss=1496.5985854962073, w0=0.031345054479504274, w1=0.4087616757316385\n",
      "Gradient Descent(594/9999): loss=1491.6227168323098, w0=0.03135151901931931, w1=0.4086957435833323\n",
      "Gradient Descent(595/9999): loss=1486.6717663168881, w0=0.0313579594640146, w1=0.4086298862734899\n",
      "Gradient Descent(596/9999): loss=1481.7456061940957, w0=0.03136437584462328, w1=0.40856410359122214\n",
      "Gradient Descent(597/9999): loss=1476.8441093653985, w0=0.03137076819305684, w1=0.4084983953265582\n",
      "Gradient Descent(598/9999): loss=1471.9671493861472, w0=0.03137713654208087, w1=0.4084327612704378\n",
      "Gradient Descent(599/9999): loss=1467.1146004621714, w0=0.031383480925291286, w1=0.40836720121470427\n",
      "Gradient Descent(600/9999): loss=1462.2863374463927, w0=0.03138980137709106, w1=0.40830171495209683\n",
      "Gradient Descent(601/9999): loss=1457.4822358354534, w0=0.03139609793266748, w1=0.4082363022762436\n",
      "Gradient Descent(602/9999): loss=1452.7021717663677, w0=0.03140237062796987, w1=0.40817096298165434\n",
      "Gradient Descent(603/9999): loss=1447.9460220131893, w0=0.03140861949968777, w1=0.4081056968637137\n",
      "Gradient Descent(604/9999): loss=1443.2136639836986, w0=0.03141484458522961, w1=0.4080405037186739\n",
      "Gradient Descent(605/9999): loss=1438.5049757161066, w0=0.03142104592270185, w1=0.407975383343648\n",
      "Gradient Descent(606/9999): loss=1433.8198358757786, w0=0.03142722355088849, w1=0.4079103355366033\n",
      "Gradient Descent(607/9999): loss=1429.158123751975, w0=0.03143337750923116, w1=0.40784536009635414\n",
      "Gradient Descent(608/9999): loss=1424.5197192546104, w0=0.03143950783780949, w1=0.40778045682255576\n",
      "Gradient Descent(609/9999): loss=1419.9045029110293, w0=0.03144561457732199, w1=0.4077156255156975\n",
      "Gradient Descent(610/9999): loss=1415.3123558628026, w0=0.03145169776906735, w1=0.4076508659770963\n",
      "Gradient Descent(611/9999): loss=1410.743159862537, w0=0.031457757454926086, w1=0.40758617800889035\n",
      "Gradient Descent(612/9999): loss=1406.1967972707064, w0=0.03146379367734262, w1=0.4075215614140327\n",
      "Gradient Descent(613/9999): loss=1401.6731510524962, w0=0.03146980647930777, w1=0.40745701599628503\n",
      "Gradient Descent(614/9999): loss=1397.172104774669, w0=0.03147579590434157, w1=0.40739254156021154\n",
      "Gradient Descent(615/9999): loss=1392.693542602445, w0=0.03148176199647651, w1=0.40732813791117267\n",
      "Gradient Descent(616/9999): loss=1388.2373492963986, w0=0.03148770480024113, w1=0.40726380485531916\n",
      "Gradient Descent(617/9999): loss=1383.8034102093732, w0=0.031493624360643936, w1=0.40719954219958593\n",
      "Gradient Descent(618/9999): loss=1379.3916112834142, w0=0.03149952072315775, w1=0.4071353497516864\n",
      "Gradient Descent(619/9999): loss=1375.0018390467153, w0=0.03150539393370432, w1=0.4070712273201065\n",
      "Gradient Descent(620/9999): loss=1370.6339806105814, w0=0.03151124403863932, w1=0.4070071747140988\n",
      "Gradient Descent(621/9999): loss=1366.2879236664123, w0=0.03151707108473766, w1=0.4069431917436771\n",
      "Gradient Descent(622/9999): loss=1361.9635564826976, w0=0.03152287511917915, w1=0.4068792782196104\n",
      "Gradient Descent(623/9999): loss=1357.6607679020321, w0=0.03152865618953444, w1=0.4068154339534176\n",
      "Gradient Descent(624/9999): loss=1353.3794473381413, w0=0.03153441434375129, w1=0.4067516587573619\n",
      "Gradient Descent(625/9999): loss=1349.1194847729291, w0=0.031540149630141165, w1=0.40668795244444533\n",
      "Gradient Descent(626/9999): loss=1344.8807707535389, w0=0.03154586209736612, w1=0.40662431482840333\n",
      "Gradient Descent(627/9999): loss=1340.6631963894301, w0=0.03155155179442597, w1=0.40656074572369943\n",
      "Gradient Descent(628/9999): loss=1336.4666533494697, w0=0.03155721877064572, w1=0.4064972449455201\n",
      "Gradient Descent(629/9999): loss=1332.2910338590411, w0=0.03156286307566336, w1=0.4064338123097693\n",
      "Gradient Descent(630/9999): loss=1328.136230697167, w0=0.03156848475941786, w1=0.4063704476330636\n",
      "Gradient Descent(631/9999): loss=1324.0021371936489, w0=0.03157408387213747, w1=0.40630715073272683\n",
      "Gradient Descent(632/9999): loss=1319.888647226221, w0=0.03157966046432826, w1=0.40624392142678517\n",
      "Gradient Descent(633/9999): loss=1315.795655217719, w0=0.03158521458676296, w1=0.4061807595339622\n",
      "Gradient Descent(634/9999): loss=1311.723056133264, w0=0.031590746290470026, w1=0.4061176648736739\n",
      "Gradient Descent(635/9999): loss=1307.6707454774642, w0=0.03159625562672296, w1=0.40605463726602375\n",
      "Gradient Descent(636/9999): loss=1303.6386192916252, w0=0.03160174264702988, w1=0.405991676531798\n",
      "Gradient Descent(637/9999): loss=1299.6265741509828, w0=0.03160720740312333, w1=0.4059287824924609\n",
      "Gradient Descent(638/9999): loss=1295.6345071619448, w0=0.03161264994695032, w1=0.40586595497014993\n",
      "Gradient Descent(639/9999): loss=1291.6623159593491, w0=0.031618070330662607, w1=0.40580319378767116\n",
      "Gradient Descent(640/9999): loss=1287.709898703738, w0=0.03162346860660716, w1=0.40574049876849466\n",
      "Gradient Descent(641/9999): loss=1283.777154078643, w0=0.03162884482731694, w1=0.4056778697367499\n",
      "Gradient Descent(642/9999): loss=1279.8639812878903, w0=0.03163419904550178, w1=0.40561530651722133\n",
      "Gradient Descent(643/9999): loss=1275.9702800529126, w0=0.03163953131403955, w1=0.40555280893534384\n",
      "Gradient Descent(644/9999): loss=1272.0959506100817, w0=0.031644841685967526, w1=0.40549037681719846\n",
      "Gradient Descent(645/9999): loss=1268.240893708053, w0=0.03165013021447394, w1=0.4054280099895078\n",
      "Gradient Descent(646/9999): loss=1264.4050106051216, w0=0.03165539695288972, w1=0.4053657082796319\n",
      "Gradient Descent(647/9999): loss=1260.5882030665962, w0=0.03166064195468048, w1=0.405303471515564\n",
      "Gradient Descent(648/9999): loss=1256.7903733621856, w0=0.03166586527343863, w1=0.4052412995259262\n",
      "Gradient Descent(649/9999): loss=1253.011424263396, w0=0.03167106696287572, w1=0.4051791921399653\n",
      "Gradient Descent(650/9999): loss=1249.2512590409476, w0=0.031676247076814976, w1=0.4051171491875487\n",
      "Gradient Descent(651/9999): loss=1245.5097814621993, w0=0.031681405669183954, w1=0.40505517049916046\n",
      "Gradient Descent(652/9999): loss=1241.7868957885903, w0=0.031686542794007434, w1=0.404993255905897\n",
      "Gradient Descent(653/9999): loss=1238.0825067730939, w0=0.03169165850540047, w1=0.40493140523946336\n",
      "Gradient Descent(654/9999): loss=1234.3965196576858, w0=0.03169675285756158, w1=0.40486961833216895\n",
      "Gradient Descent(655/9999): loss=1230.7288401708238, w0=0.03170182590476615, w1=0.40480789501692394\n",
      "Gradient Descent(656/9999): loss=1227.079374524942, w0=0.03170687770135993, w1=0.40474623512723523\n",
      "Gradient Descent(657/9999): loss=1223.4480294139585, w0=0.03171190830175278, w1=0.4046846384972027\n",
      "Gradient Descent(658/9999): loss=1219.8347120107949, w0=0.03171691776041249, w1=0.4046231049615154\n",
      "Gradient Descent(659/9999): loss=1216.239329964909, w0=0.031721906131858775, w1=0.4045616343554477\n",
      "Gradient Descent(660/9999): loss=1212.6617913998432, w0=0.03172687347065748, w1=0.40450022651485573\n",
      "Gradient Descent(661/9999): loss=1209.1020049107801, w0=0.03173181983141482, w1=0.4044388812761737\n",
      "Gradient Descent(662/9999): loss=1205.559879562117, w0=0.031736745268771835, w1=0.40437759847641025\n",
      "Gradient Descent(663/9999): loss=1202.0353248850492, w0=0.031741649837399, w1=0.40431637795314473\n",
      "Gradient Descent(664/9999): loss=1198.5282508751668, w0=0.0317465335919909, w1=0.4042552195445239\n",
      "Gradient Descent(665/9999): loss=1195.0385679900642, w0=0.031751396587261116, w1=0.4041941230892583\n",
      "Gradient Descent(666/9999): loss=1191.5661871469633, w0=0.031756238877937173, w1=0.4041330884266186\n",
      "Gradient Descent(667/9999): loss=1188.111019720346, w0=0.03176106051875568, w1=0.4040721153964325\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(668/9999): loss=1184.6729775396038, w0=0.03176586156445754, w1=0.40401120383908107\n",
      "Gradient Descent(669/9999): loss=1181.251972886694, w0=0.03177064206978334, w1=0.4039503535954954\n",
      "Gradient Descent(670/9999): loss=1177.847918493813, w0=0.03177540208946878, w1=0.4038895645071535\n",
      "Gradient Descent(671/9999): loss=1174.460727541079, w0=0.03178014167824032, w1=0.4038288364160767\n",
      "Gradient Descent(672/9999): loss=1171.0903136542279, w0=0.03178486089081087, w1=0.40376816916482644\n",
      "Gradient Descent(673/9999): loss=1167.7365909023194, w0=0.03178955978187559, w1=0.4037075625965013\n",
      "Gradient Descent(674/9999): loss=1164.3994737954588, w0=0.03179423840610786, w1=0.40364701655473356\n",
      "Gradient Descent(675/9999): loss=1161.0788772825265, w0=0.03179889681815531, w1=0.403586530883686\n",
      "Gradient Descent(676/9999): loss=1157.774716748922, w0=0.031803535072635954, w1=0.403526105428049\n",
      "Gradient Descent(677/9999): loss=1154.486908014318, w0=0.03180815322413447, w1=0.4034657400330372\n",
      "Gradient Descent(678/9999): loss=1151.2153673304283, w0=0.031812751327198505, w1=0.40340543454438654\n",
      "Gradient Descent(679/9999): loss=1147.9600113787844, w0=0.03181732943633519, w1=0.40334518880835124\n",
      "Gradient Descent(680/9999): loss=1144.7207572685265, w0=0.03182188760600762, w1=0.4032850026717008\n",
      "Gradient Descent(681/9999): loss=1141.4975225342039, w0=0.031826425890631546, w1=0.40322487598171697\n",
      "Gradient Descent(682/9999): loss=1138.2902251335872, w0=0.0318309443445721, w1=0.4031648085861908\n",
      "Gradient Descent(683/9999): loss=1135.0987834454943, w0=0.03183544302214059, w1=0.4031048003334198\n",
      "Gradient Descent(684/9999): loss=1131.923116267623, w0=0.03183992197759147, w1=0.4030448510722049\n",
      "Gradient Descent(685/9999): loss=1128.763142814399, w0=0.03184438126511929, w1=0.40298496065184786\n",
      "Gradient Descent(686/9999): loss=1125.6187827148337, w0=0.03184882093885581, w1=0.4029251289221482\n",
      "Gradient Descent(687/9999): loss=1122.4899560103927, w0=0.03185324105286716, w1=0.4028653557334005\n",
      "Gradient Descent(688/9999): loss=1119.3765831528751, w0=0.03185764166115109, w1=0.40280564093639154\n",
      "Gradient Descent(689/9999): loss=1116.2785850023051, w0=0.03186202281763431, w1=0.40274598438239767\n",
      "Gradient Descent(690/9999): loss=1113.195882824833, w0=0.031866384576169894, w1=0.40268638592318207\n",
      "Gradient Descent(691/9999): loss=1110.1283982906484, w0=0.03187072699053475, w1=0.402626845410992\n",
      "Gradient Descent(692/9999): loss=1107.0760534719043, w0=0.03187505011442721, w1=0.40256736269855614\n",
      "Gradient Descent(693/9999): loss=1104.0387708406492, w0=0.03187935400146462, w1=0.402507937639082\n",
      "Gradient Descent(694/9999): loss=1101.0164732667745, w0=0.031883638705181065, w1=0.4024485700862533\n",
      "Gradient Descent(695/9999): loss=1098.009084015968, w0=0.031887904279025145, w1=0.40238925989422736\n",
      "Gradient Descent(696/9999): loss=1095.0165267476812, w0=0.031892150776357805, w1=0.40233000691763254\n",
      "Gradient Descent(697/9999): loss=1092.0387255131054, w0=0.03189637825045025, w1=0.4022708110115657\n",
      "Gradient Descent(698/9999): loss=1089.0756047531597, w0=0.031900586754481895, w1=0.40221167203158964\n",
      "Gradient Descent(699/9999): loss=1086.1270892964867, w0=0.03190477634153843, w1=0.4021525898337308\n",
      "Gradient Descent(700/9999): loss=1083.193104357462, w0=0.031908947064609894, w1=0.40209356427447657\n",
      "Gradient Descent(701/9999): loss=1080.273575534213, w0=0.03191309897658883, w1=0.4020345952107729\n",
      "Gradient Descent(702/9999): loss=1077.368428806644, w0=0.03191723213026852, w1=0.40197568250002197\n",
      "Gradient Descent(703/9999): loss=1074.4775905344793, w0=0.031921346578341246, w1=0.4019168260000798\n",
      "Gradient Descent(704/9999): loss=1071.6009874553083, w0=0.03192544237339662, w1=0.4018580255692537\n",
      "Gradient Descent(705/9999): loss=1068.738546682646, w0=0.03192951956791998, w1=0.4017992810663002\n",
      "Gradient Descent(706/9999): loss=1065.8901957040023, w0=0.03193357821429082, w1=0.4017405923504224\n",
      "Gradient Descent(707/9999): loss=1063.0558623789598, w0=0.031937618364781296, w1=0.40168195928126804\n",
      "Gradient Descent(708/9999): loss=1060.2354749372623, w0=0.03194164007155476, w1=0.4016233817189268\n",
      "Gradient Descent(709/9999): loss=1057.4289619769154, w0=0.03194564338666436, w1=0.40156485952392834\n",
      "Gradient Descent(710/9999): loss=1054.636252462292, w0=0.03194962836205169, w1=0.40150639255724\n",
      "Gradient Descent(711/9999): loss=1051.8572757222544, w0=0.03195359504954546, w1=0.40144798068026455\n",
      "Gradient Descent(712/9999): loss=1049.0919614482773, w0=0.03195754350086027, w1=0.40138962375483783\n",
      "Gradient Descent(713/9999): loss=1046.3402396925903, w0=0.03196147376759537, w1=0.40133132164322693\n",
      "Gradient Descent(714/9999): loss=1043.602040866322, w0=0.03196538590123352, w1=0.40127307420812774\n",
      "Gradient Descent(715/9999): loss=1040.8772957376582, w0=0.031969279953139806, w1=0.4012148813126629\n",
      "Gradient Descent(716/9999): loss=1038.1659354300066, w0=0.031973155974560644, w1=0.4011567428203797\n",
      "Gradient Descent(717/9999): loss=1035.4678914201747, w0=0.03197701401662267, w1=0.4010986585952479\n",
      "Gradient Descent(718/9999): loss=1032.7830955365537, w0=0.031980854130331755, w1=0.4010406285016578\n",
      "Gradient Descent(719/9999): loss=1030.1114799573127, w0=0.03198467636657208, w1=0.400982652404418\n",
      "Gradient Descent(720/9999): loss=1027.4529772086028, w0=0.03198848077610517, w1=0.4009247301687535\n",
      "Gradient Descent(721/9999): loss=1024.8075201627714, w0=0.031992267409569036, w1=0.40086686166030366\n",
      "Gradient Descent(722/9999): loss=1022.1750420365822, w0=0.03199603631747732, w1=0.4008090467451201\n",
      "Gradient Descent(723/9999): loss=1019.5554763894492, w0=0.031999787550218504, w1=0.4007512852896647\n",
      "Gradient Descent(724/9999): loss=1016.9487571216748, w0=0.03200352115805511, w1=0.4006935771608078\n",
      "Gradient Descent(725/9999): loss=1014.354818472701, w0=0.03200723719112298, w1=0.400635922225826\n",
      "Gradient Descent(726/9999): loss=1011.7735950193685, w0=0.03201093569943056, w1=0.40057832035240054\n",
      "Gradient Descent(727/9999): loss=1009.2050216741823, w0=0.032014616732858235, w1=0.40052077140861514\n",
      "Gradient Descent(728/9999): loss=1006.6490336835896, w0=0.032018280341157675, w1=0.40046327526295405\n",
      "Gradient Descent(729/9999): loss=1004.1055666262655, w0=0.03202192657395124, w1=0.40040583178430045\n",
      "Gradient Descent(730/9999): loss=1001.5745564114072, w0=0.0320255554807314, w1=0.40034844084193427\n",
      "Gradient Descent(731/9999): loss=999.0559392770358, w0=0.032029167110860186, w1=0.40029110230553056\n",
      "Gradient Descent(732/9999): loss=996.5496517883095, w0=0.032032761513568646, w1=0.4002338160451575\n",
      "Gradient Descent(733/9999): loss=994.0556308358435, w0=0.03203633873795639, w1=0.4001765819312747\n",
      "Gradient Descent(734/9999): loss=991.5738136340395, w0=0.0320398988329911, w1=0.4001193998347313\n",
      "Gradient Descent(735/9999): loss=989.1041377194226, w0=0.032043441847508104, w1=0.40006226962676433\n",
      "Gradient Descent(736/9999): loss=986.6465409489886, w0=0.03204696783020997, w1=0.4000051911789967\n",
      "Gradient Descent(737/9999): loss=984.2009614985578, w0=0.032050476829666115, w1=0.39994816436343555\n",
      "Gradient Descent(738/9999): loss=981.7673378611396, w0=0.03205396889431243, w1=0.39989118905247073\n",
      "Gradient Descent(739/9999): loss=979.3456088453026, w0=0.032057444072450966, w1=0.39983426511887266\n",
      "Gradient Descent(740/9999): loss=976.9357135735561, w0=0.03206090241224962, w1=0.39977739243579086\n",
      "Gradient Descent(741/9999): loss=974.5375914807375, w0=0.03206434396174182, w1=0.39972057087675233\n",
      "Gradient Descent(742/9999): loss=972.15118231241, w0=0.032067768768826305, w1=0.3996638003156596\n",
      "Gradient Descent(743/9999): loss=969.7764261232661, w0=0.03207117688126683, w1=0.39960708062678935\n",
      "Gradient Descent(744/9999): loss=967.4132632755429, w0=0.03207456834669198, w1=0.3995504116847905\n",
      "Gradient Descent(745/9999): loss=965.0616344374413, w0=0.032077943212594946, w1=0.3994937933646827\n",
      "Gradient Descent(746/9999): loss=962.7214805815563, w0=0.03208130152633336, w1=0.39943722554185473\n",
      "Gradient Descent(747/9999): loss=960.3927429833149, w0=0.032084643335129115, w1=0.3993807080920628\n",
      "Gradient Descent(748/9999): loss=958.0753632194215, w0=0.03208796868606824, w1=0.3993242408914288\n",
      "Gradient Descent(749/9999): loss=955.7692831663113, w0=0.032091277626100756, w1=0.3992678238164392\n",
      "Gradient Descent(750/9999): loss=953.4744449986111, w0=0.03209457020204059, w1=0.399211456743943\n",
      "Gradient Descent(751/9999): loss=951.1907911876109, w0=0.03209784646056546, w1=0.3991551395511503\n",
      "Gradient Descent(752/9999): loss=948.9182644997381, w0=0.03210110644821684, w1=0.3990988721156307\n",
      "Gradient Descent(753/9999): loss=946.6568079950458, w0=0.03210435021139989, w1=0.3990426543153121\n",
      "Gradient Descent(754/9999): loss=944.4063650257036, w0=0.03210757779638339, w1=0.39898648602847864\n",
      "Gradient Descent(755/9999): loss=942.1668792344988, w0=0.032110789249299755, w1=0.3989303671337695\n",
      "Gradient Descent(756/9999): loss=939.9382945533449, w0=0.03211398461614503, w1=0.39887429751017744\n",
      "Gradient Descent(757/9999): loss=937.7205552017983, w0=0.032117163942778866, w1=0.39881827703704714\n",
      "Gradient Descent(758/9999): loss=935.513605685581, w0=0.032120327274924566, w1=0.3987623055940738\n",
      "Gradient Descent(759/9999): loss=933.3173907951117, w0=0.03212347465816912, w1=0.39870638306130163\n",
      "Gradient Descent(760/9999): loss=931.1318556040461, w0=0.03212660613796326, w1=0.3986505093191226\n",
      "Gradient Descent(761/9999): loss=928.9569454678211, w0=0.03212972175962151, w1=0.39859468424827466\n",
      "Gradient Descent(762/9999): loss=926.792606022211, w0=0.032132821568322284, w1=0.3985389077298405\n",
      "Gradient Descent(763/9999): loss=924.6387831818872, w0=0.032135905609107944, w1=0.3984831796452462\n",
      "Gradient Descent(764/9999): loss=922.4954231389862, w0=0.032138973926884945, w1=0.39842749987625964\n",
      "Gradient Descent(765/9999): loss=920.362472361688, w0=0.03214202656642392, w1=0.3983718683049891\n",
      "Gradient Descent(766/9999): loss=918.2398775927975, w0=0.032145063572359814, w1=0.39831628481388215\n",
      "Gradient Descent(767/9999): loss=916.1275858483347, w0=0.03214808498919202, w1=0.3982607492857239\n",
      "Gradient Descent(768/9999): loss=914.0255444161345, w0=0.03215109086128453, w1=0.39820526160363595\n",
      "Gradient Descent(769/9999): loss=911.9337008544501, w0=0.0321540812328661, w1=0.39814982165107476\n",
      "Gradient Descent(770/9999): loss=909.8520029905662, w0=0.03215705614803041, w1=0.3980944293118305\n",
      "Gradient Descent(771/9999): loss=907.780398919417, w0=0.032160015650736236, w1=0.3980390844700255\n",
      "Gradient Descent(772/9999): loss=905.7188370022145, w0=0.03216295978480766, w1=0.39798378701011333\n",
      "Gradient Descent(773/9999): loss=903.6672658650805, w0=0.03216588859393426, w1=0.39792853681687695\n",
      "Gradient Descent(774/9999): loss=901.6256343976878, w0=0.03216880212167131, w1=0.3978733337754279\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(775/9999): loss=899.5938917519074, w0=0.03217170041143999, w1=0.3978181777712045\n",
      "Gradient Descent(776/9999): loss=897.5719873404632, w0=0.03217458350652764, w1=0.3977630686899711\n",
      "Gradient Descent(777/9999): loss=895.5598708355934, w0=0.03217745145008797, w1=0.3977080064178163\n",
      "Gradient Descent(778/9999): loss=893.557492167718, w0=0.0321803042851413, w1=0.3976529908411521\n",
      "Gradient Descent(779/9999): loss=891.5648015241152, w0=0.03218314205457482, w1=0.3975980218467123\n",
      "Gradient Descent(780/9999): loss=889.5817493476019, w0=0.032185964801142844, w1=0.39754309932155146\n",
      "Gradient Descent(781/9999): loss=887.6082863352228, w0=0.03218877256746707, w1=0.39748822315304366\n",
      "Gradient Descent(782/9999): loss=885.6443634369473, w0=0.03219156539603684, w1=0.39743339322888105\n",
      "Gradient Descent(783/9999): loss=883.6899318543686, w0=0.032194343329209445, w1=0.39737860943707287\n",
      "Gradient Descent(784/9999): loss=881.744943039415, w0=0.03219710640921038, w1=0.3973238716659442\n",
      "Gradient Descent(785/9999): loss=879.809348693065, w0=0.03219985467813365, w1=0.3972691798041345\n",
      "Gradient Descent(786/9999): loss=877.8831007640673, w0=0.032202588177942075, w1=0.39721453374059684\n",
      "Gradient Descent(787/9999): loss=875.9661514476713, w0=0.03220530695046756, w1=0.39715993336459626\n",
      "Gradient Descent(788/9999): loss=874.0584531843625, w0=0.03220801103741144, w1=0.39710537856570893\n",
      "Gradient Descent(789/9999): loss=872.1599586586019, w0=0.03221070048034475, w1=0.39705086923382077\n",
      "Gradient Descent(790/9999): loss=870.2706207975768, w0=0.03221337532070861, w1=0.39699640525912644\n",
      "Gradient Descent(791/9999): loss=868.3903927699522, w0=0.032216035599814466, w1=0.39694198653212803\n",
      "Gradient Descent(792/9999): loss=866.5192279846344, w0=0.03221868135884449, w1=0.396887612943634\n",
      "Gradient Descent(793/9999): loss=864.6570800895369, w0=0.03222131263885187, w1=0.39683328438475807\n",
      "Gradient Descent(794/9999): loss=862.8039029703543, w0=0.032223929480761165, w1=0.3967790007469179\n",
      "Gradient Descent(795/9999): loss=860.9596507493414, w0=0.03222653192536864, w1=0.39672476192183426\n",
      "Gradient Descent(796/9999): loss=859.1242777841013, w0=0.03222912001334262, w1=0.39667056780152965\n",
      "Gradient Descent(797/9999): loss=857.2977386663748, w0=0.03223169378522383, w1=0.39661641827832733\n",
      "Gradient Descent(798/9999): loss=855.4799882208415, w0=0.032234253281425755, w1=0.39656231324485014\n",
      "Gradient Descent(799/9999): loss=853.6709815039238, w0=0.032236798542235, w1=0.3965082525940195\n",
      "Gradient Descent(800/9999): loss=851.8706738025973, w0=0.03223932960781164, w1=0.3964542362190542\n",
      "Gradient Descent(801/9999): loss=850.0790206332078, w0=0.03224184651818961, w1=0.39640026401346934\n",
      "Gradient Descent(802/9999): loss=848.2959777402953, w0=0.032244349313277035, w1=0.3963463358710754\n",
      "Gradient Descent(803/9999): loss=846.5215010954213, w0=0.03224683803285663, w1=0.39629245168597693\n",
      "Gradient Descent(804/9999): loss=844.7555468960059, w0=0.032249312716586064, w1=0.39623861135257177\n",
      "Gradient Descent(805/9999): loss=842.9980715641682, w0=0.03225177340399834, w1=0.3961848147655498\n",
      "Gradient Descent(806/9999): loss=841.2490317455716, w0=0.032254220134502166, w1=0.3961310618198918\n",
      "Gradient Descent(807/9999): loss=839.5083843082801, w0=0.03225665294738234, w1=0.39607735241086883\n",
      "Gradient Descent(808/9999): loss=837.7760863416141, w0=0.03225907188180015, w1=0.3960236864340407\n",
      "Gradient Descent(809/9999): loss=836.0520951550164, w0=0.03226147697679372, w1=0.3959700637852551\n",
      "Gradient Descent(810/9999): loss=834.3363682769233, w0=0.03226386827127845, w1=0.3959164843606469\n",
      "Gradient Descent(811/9999): loss=832.6288634536397, w0=0.032266245804047376, w1=0.3958629480566367\n",
      "Gradient Descent(812/9999): loss=830.9295386482212, w0=0.03226860961377156, w1=0.39580945476993\n",
      "Gradient Descent(813/9999): loss=829.2383520393636, w0=0.03227095973900049, w1=0.39575600439751624\n",
      "Gradient Descent(814/9999): loss=827.5552620202944, w0=0.03227329621816248, w1=0.39570259683666775\n",
      "Gradient Descent(815/9999): loss=825.8802271976737, w0=0.03227561908956508, w1=0.3956492319849388\n",
      "Gradient Descent(816/9999): loss=824.2132063904975, w0=0.03227792839139545, w1=0.3955959097401645\n",
      "Gradient Descent(817/9999): loss=822.5541586290094, w0=0.03228022416172076, w1=0.3955426300004601\n",
      "Gradient Descent(818/9999): loss=820.9030431536161, w0=0.03228250643848861, w1=0.39548939266421956\n",
      "Gradient Descent(819/9999): loss=819.2598194138093, w0=0.03228477525952743, w1=0.3954361976301151\n",
      "Gradient Descent(820/9999): loss=817.6244470670931, w0=0.03228703066254688, w1=0.3953830447970959\n",
      "Gradient Descent(821/9999): loss=815.9968859779161, w0=0.032289272685138246, w1=0.3953299340643872\n",
      "Gradient Descent(822/9999): loss=814.3770962166095, w0=0.032291501364774856, w1=0.3952768653314895\n",
      "Gradient Descent(823/9999): loss=812.7650380583317, w0=0.03229371673881249, w1=0.39522383849817744\n",
      "Gradient Descent(824/9999): loss=811.1606719820165, w0=0.032295918844489784, w1=0.395170853464499\n",
      "Gradient Descent(825/9999): loss=809.5639586693278, w0=0.032298107718928624, w1=0.3951179101307744\n",
      "Gradient Descent(826/9999): loss=807.9748590036189, w0=0.03230028339913458, w1=0.39506500839759545\n",
      "Gradient Descent(827/9999): loss=806.3933340688991, w0=0.0323024459219973, w1=0.39501214816582436\n",
      "Gradient Descent(828/9999): loss=804.8193451488024, w0=0.032304595324290905, w1=0.39495932933659295\n",
      "Gradient Descent(829/9999): loss=803.2528537255643, w0=0.03230673164267445, w1=0.39490655181130174\n",
      "Gradient Descent(830/9999): loss=801.693821479002, w0=0.032308854913692266, w1=0.394853815491619\n",
      "Gradient Descent(831/9999): loss=800.142210285502, w0=0.03231096517377442, w1=0.39480112027947994\n",
      "Gradient Descent(832/9999): loss=798.59798221701, w0=0.032313062459237124, w1=0.3947484660770857\n",
      "Gradient Descent(833/9999): loss=797.0610995400282, w0=0.032315146806283106, w1=0.39469585278690256\n",
      "Gradient Descent(834/9999): loss=795.5315247146175, w0=0.03231721825100207, w1=0.39464328031166096\n",
      "Gradient Descent(835/9999): loss=794.0092203934034, w0=0.032319276829371066, w1=0.3945907485543548\n",
      "Gradient Descent(836/9999): loss=792.4941494205885, w0=0.03232132257725493, w1=0.3945382574182403\n",
      "Gradient Descent(837/9999): loss=790.98627483097, w0=0.03232335553040668, w1=0.39448580680683537\n",
      "Gradient Descent(838/9999): loss=789.4855598489605, w0=0.03232537572446794, w1=0.3944333966239187\n",
      "Gradient Descent(839/9999): loss=787.9919678876169, w0=0.03232738319496933, w1=0.3943810267735287\n",
      "Gradient Descent(840/9999): loss=786.5054625476707, w0=0.032329377977330875, w1=0.39432869715996305\n",
      "Gradient Descent(841/9999): loss=785.0260076165662, w0=0.032331360106862446, w1=0.3942764076877774\n",
      "Gradient Descent(842/9999): loss=783.5535670675021, w0=0.03233332961876414, w1=0.39422415826178475\n",
      "Gradient Descent(843/9999): loss=782.088105058479, w0=0.032335286548126695, w1=0.39417194878705475\n",
      "Gradient Descent(844/9999): loss=780.62958593135, w0=0.032337230929931904, w1=0.39411977916891255\n",
      "Gradient Descent(845/9999): loss=779.177974210879, w0=0.032339162799053016, w1=0.39406764931293825\n",
      "Gradient Descent(846/9999): loss=777.7332346038005, w0=0.032341082190255145, w1=0.3940155591249659\n",
      "Gradient Descent(847/9999): loss=776.2953319978874, w0=0.03234298913819568, w1=0.3939635085110827\n",
      "Gradient Descent(848/9999): loss=774.8642314610215, w0=0.03234488367742469, w1=0.39391149737762826\n",
      "Gradient Descent(849/9999): loss=773.4398982402691, w0=0.032346765842385314, w1=0.3938595256311938\n",
      "Gradient Descent(850/9999): loss=772.0222977609624, w0=0.03234863566741419, w1=0.3938075931786212\n",
      "Gradient Descent(851/9999): loss=770.6113956257844, w0=0.03235049318674184, w1=0.39375569992700227\n",
      "Gradient Descent(852/9999): loss=769.2071576138588, w0=0.03235233843449308, w1=0.39370384578367806\n",
      "Gradient Descent(853/9999): loss=767.8095496798447, w0=0.03235417144468743, w1=0.39365203065623794\n",
      "Gradient Descent(854/9999): loss=766.4185379530371, w0=0.03235599225123948, w1=0.3936002544525188\n",
      "Gradient Descent(855/9999): loss=765.0340887364691, w0=0.032357800887959344, w1=0.3935485170806043\n",
      "Gradient Descent(856/9999): loss=763.6561685060218, w0=0.03235959738855301, w1=0.3934968184488241\n",
      "Gradient Descent(857/9999): loss=762.2847439095372, w0=0.03236138178662277, w1=0.3934451584657532\n",
      "Gradient Descent(858/9999): loss=760.9197817659355, w0=0.03236315411566759, w1=0.3933935370402108\n",
      "Gradient Descent(859/9999): loss=759.5612490643374, w0=0.03236491440908353, w1=0.39334195408125994\n",
      "Gradient Descent(860/9999): loss=758.2091129631921, w0=0.03236666270016414, w1=0.39329040949820654\n",
      "Gradient Descent(861/9999): loss=756.8633407894059, w0=0.03236839902210082, w1=0.3932389032005986\n",
      "Gradient Descent(862/9999): loss=755.5239000374809, w0=0.03237012340798325, w1=0.39318743509822557\n",
      "Gradient Descent(863/9999): loss=754.1907583686532, w0=0.03237183589079977, w1=0.3931360051011174\n",
      "Gradient Descent(864/9999): loss=752.8638836100375, w0=0.03237353650343775, w1=0.393084613119544\n",
      "Gradient Descent(865/9999): loss=751.5432437537775, w0=0.032375225278684014, w1=0.39303325906401443\n",
      "Gradient Descent(866/9999): loss=750.228806956197, w0=0.0323769022492252, w1=0.3929819428452761\n",
      "Gradient Descent(867/9999): loss=748.9205415369594, w0=0.03237856744764817, w1=0.392930664374314\n",
      "Gradient Descent(868/9999): loss=747.6184159782289, w0=0.03238022090644036, w1=0.39287942356235017\n",
      "Gradient Descent(869/9999): loss=746.3223989238368, w0=0.03238186265799021, w1=0.39282822032084264\n",
      "Gradient Descent(870/9999): loss=745.0324591784516, w0=0.032383492734587495, w1=0.392777054561485\n",
      "Gradient Descent(871/9999): loss=743.7485657067549, w0=0.03238511116842377, w1=0.3927259261962055\n",
      "Gradient Descent(872/9999): loss=742.4706876326194, w0=0.03238671799159268, w1=0.3926748351371665\n",
      "Gradient Descent(873/9999): loss=741.198794238293, w0=0.03238831323609039, w1=0.39262378129676345\n",
      "Gradient Descent(874/9999): loss=739.9328549635848, w0=0.032389896933815956, w1=0.3925727645876245\n",
      "Gradient Descent(875/9999): loss=738.6728394050587, w0=0.03239146911657167, w1=0.39252178492260964\n",
      "Gradient Descent(876/9999): loss=737.4187173152272, w0=0.032393029816063464, w1=0.3924708422148099\n",
      "Gradient Descent(877/9999): loss=736.1704586017529, w0=0.03239457906390128, w1=0.39241993637754674\n",
      "Gradient Descent(878/9999): loss=734.9280333266514, w0=0.03239611689159943, w1=0.39236906732437155\n",
      "Gradient Descent(879/9999): loss=733.6914117055, w0=0.03239764333057697, w1=0.39231823496906454\n",
      "Gradient Descent(880/9999): loss=732.4605641066491, w0=0.03239915841215809, w1=0.3922674392256344\n",
      "Gradient Descent(881/9999): loss=731.2354610504393, w0=0.03240066216757244, w1=0.3922166800083174\n",
      "Gradient Descent(882/9999): loss=730.016073208421, w0=0.03240215462795554, w1=0.39216595723157677\n",
      "Gradient Descent(883/9999): loss=728.8023714025786, w0=0.0324036358243491, w1=0.392115270810102\n",
      "Gradient Descent(884/9999): loss=727.5943266045589, w0=0.03240510578770144, w1=0.3920646206588082\n",
      "Gradient Descent(885/9999): loss=726.3919099349032, w0=0.032406564548867804, w1=0.3920140066928354\n",
      "Gradient Descent(886/9999): loss=725.1950926622832, w0=0.03240801213861072, w1=0.3919634288275479\n",
      "Gradient Descent(887/9999): loss=724.003846202741, w0=0.0324094485876004, w1=0.3919128869785335\n",
      "Gradient Descent(888/9999): loss=722.8181421189338, w0=0.032410873926415056, w1=0.39186238106160287\n",
      "Gradient Descent(889/9999): loss=721.6379521193801, w0=0.032412288185541274, w1=0.39181191099278895\n",
      "Gradient Descent(890/9999): loss=720.4632480577127, w0=0.032413691395374374, w1=0.39176147668834627\n",
      "Gradient Descent(891/9999): loss=719.2940019319337, w0=0.032415083586218744, w1=0.3917110780647502\n",
      "Gradient Descent(892/9999): loss=718.1301858836738, w0=0.032416464788288205, w1=0.3916607150386964\n",
      "Gradient Descent(893/9999): loss=716.9717721974554, w0=0.03241783503170635, w1=0.3916103875271002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(894/9999): loss=715.8187332999591, w0=0.03241919434650692, w1=0.39156009544709564\n",
      "Gradient Descent(895/9999): loss=714.6710417592956, w0=0.032420542762634105, w1=0.39150983871603523\n",
      "Gradient Descent(896/9999): loss=713.5286702842783, w0=0.032421880309942926, w1=0.3914596172514891\n",
      "Gradient Descent(897/9999): loss=712.3915917237027, w0=0.03242320701819956, w1=0.39140943097124437\n",
      "Gradient Descent(898/9999): loss=711.2597790656279, w0=0.0324245229170817, w1=0.3913592797933045\n",
      "Gradient Descent(899/9999): loss=710.1332054366619, w0=0.03242582803617886, w1=0.39130916363588863\n",
      "Gradient Descent(900/9999): loss=709.0118441012507, w0=0.03242712240499277, w1=0.39125908241743107\n",
      "Gradient Descent(901/9999): loss=707.895668460972, w0=0.03242840605293765, w1=0.39120903605658053\n",
      "Gradient Descent(902/9999): loss=706.7846520538301, w0=0.03242967900934059, w1=0.3911590244721995\n",
      "Gradient Descent(903/9999): loss=705.6787685535571, w0=0.03243094130344186, w1=0.39110904758336384\n",
      "Gradient Descent(904/9999): loss=704.577991768916, w0=0.03243219296439527, w1=0.39105910530936183\n",
      "Gradient Descent(905/9999): loss=703.4822956430086, w0=0.03243343402126847, w1=0.3910091975696937\n",
      "Gradient Descent(906/9999): loss=702.3916542525855, w0=0.0324346645030433, w1=0.3909593242840711\n",
      "Gradient Descent(907/9999): loss=701.3060418073607, w0=0.03243588443861611, w1=0.39090948537241627\n",
      "Gradient Descent(908/9999): loss=700.2254326493288, w0=0.03243709385679807, w1=0.39085968075486166\n",
      "Gradient Descent(909/9999): loss=699.1498012520882, w0=0.03243829278631554, w1=0.3908099103517492\n",
      "Gradient Descent(910/9999): loss=698.0791222201639, w0=0.03243948125581035, w1=0.39076017408362973\n",
      "Gradient Descent(911/9999): loss=697.0133702883363, w0=0.032440659293840136, w1=0.39071047187126223\n",
      "Gradient Descent(912/9999): loss=695.9525203209736, w0=0.03244182692887867, w1=0.39066080363561345\n",
      "Gradient Descent(913/9999): loss=694.8965473113662, w0=0.032442984189316154, w1=0.3906111692978573\n",
      "Gradient Descent(914/9999): loss=693.8454263810654, w0=0.032444131103459575, w1=0.3905615687793739\n",
      "Gradient Descent(915/9999): loss=692.7991327792256, w0=0.03244526769953297, w1=0.39051200200174957\n",
      "Gradient Descent(916/9999): loss=691.7576418819499, w0=0.0324463940056778, w1=0.3904624688867757\n",
      "Gradient Descent(917/9999): loss=690.7209291916374, w0=0.03244751004995321, w1=0.3904129693564485\n",
      "Gradient Descent(918/9999): loss=689.6889703363371, w0=0.03244861586033635, w1=0.3903635033329681\n",
      "Gradient Descent(919/9999): loss=688.6617410691019, w0=0.03244971146472272, w1=0.3903140707387383\n",
      "Gradient Descent(920/9999): loss=687.6392172673479, w0=0.032450796890926434, w1=0.39026467149636584\n",
      "Gradient Descent(921/9999): loss=686.621374932216, w0=0.032451872166680565, w1=0.39021530552865974\n",
      "Gradient Descent(922/9999): loss=685.608190187937, w0=0.03245293731963741, w1=0.3901659727586308\n",
      "Gradient Descent(923/9999): loss=684.5996392812012, w0=0.03245399237736881, w1=0.39011667310949105\n",
      "Gradient Descent(924/9999): loss=683.5956985805285, w0=0.03245503736736648, w1=0.3900674065046531\n",
      "Gradient Descent(925/9999): loss=682.5963445756444, w0=0.03245607231704225, w1=0.39001817286772966\n",
      "Gradient Descent(926/9999): loss=681.6015538768575, w0=0.032457097253728424, w1=0.3899689721225329\n",
      "Gradient Descent(927/9999): loss=680.6113032144419, w0=0.03245811220467804, w1=0.38991980419307387\n",
      "Gradient Descent(928/9999): loss=679.6255694380208, w0=0.03245911719706518, w1=0.389870669003562\n",
      "Gradient Descent(929/9999): loss=678.6443295159536, w0=0.032460112257985244, w1=0.38982156647840444\n",
      "Gradient Descent(930/9999): loss=677.6675605347289, w0=0.03246109741445528, w1=0.38977249654220564\n",
      "Gradient Descent(931/9999): loss=676.6952396983556, w0=0.03246207269341424, w1=0.38972345911976664\n",
      "Gradient Descent(932/9999): loss=675.7273443277624, w0=0.03246303812172329, w1=0.38967445413608465\n",
      "Gradient Descent(933/9999): loss=674.7638518601967, w0=0.0324639937261661, w1=0.38962548151635235\n",
      "Gradient Descent(934/9999): loss=673.8047398486276, w0=0.032464939533449116, w1=0.38957654118595747\n",
      "Gradient Descent(935/9999): loss=672.8499859611533, w0=0.03246587557020184, w1=0.38952763307048216\n",
      "Gradient Descent(936/9999): loss=671.8995679804094, w0=0.032466801862977145, w1=0.3894787570957025\n",
      "Gradient Descent(937/9999): loss=670.9534638029818, w0=0.032467718438251525, w1=0.38942991318758785\n",
      "Gradient Descent(938/9999): loss=670.0116514388217, w0=0.0324686253224254, w1=0.38938110127230047\n",
      "Gradient Descent(939/9999): loss=669.0741090106641, w0=0.03246952254182337, w1=0.38933232127619477\n",
      "Gradient Descent(940/9999): loss=668.1408147534497, w0=0.03247041012269453, w1=0.38928357312581696\n",
      "Gradient Descent(941/9999): loss=667.2117470137483, w0=0.03247128809121271, w1=0.3892348567479044\n",
      "Gradient Descent(942/9999): loss=666.2868842491869, w0=0.032472156473476774, w1=0.3891861720693852\n",
      "Gradient Descent(943/9999): loss=665.3662050278795, w0=0.03247301529551087, w1=0.3891375190173774\n",
      "Gradient Descent(944/9999): loss=664.4496880278599, w0=0.032473864583264726, w1=0.38908889751918874\n",
      "Gradient Descent(945/9999): loss=663.5373120365184, w0=0.03247470436261392, w1=0.389040307502316\n",
      "Gradient Descent(946/9999): loss=662.6290559500407, w0=0.03247553465936013, w1=0.38899174889444443\n",
      "Gradient Descent(947/9999): loss=661.7248987728495, w0=0.03247635549923142, w1=0.38894322162344735\n",
      "Gradient Descent(948/9999): loss=660.8248196170488, w0=0.0324771669078825, w1=0.3888947256173855\n",
      "Gradient Descent(949/9999): loss=659.9287977018726, w0=0.032477968910894994, w1=0.3888462608045065\n",
      "Gradient Descent(950/9999): loss=659.0368123531334, w0=0.0324787615337777, w1=0.3887978271132446\n",
      "Gradient Descent(951/9999): loss=658.148843002678, w0=0.03247954480196685, w1=0.38874942447221983\n",
      "Gradient Descent(952/9999): loss=657.2648691878403, w0=0.0324803187408264, w1=0.38870105281023754\n",
      "Gradient Descent(953/9999): loss=656.3848705509032, w0=0.03248108337564824, w1=0.3886527120562881\n",
      "Gradient Descent(954/9999): loss=655.508826838558, w0=0.032481838731652495, w1=0.38860440213954617\n",
      "Gradient Descent(955/9999): loss=654.6367179013698, w0=0.03248258483398775, w1=0.38855612298937026\n",
      "Gradient Descent(956/9999): loss=653.7685236932443, w0=0.03248332170773134, w1=0.3885078745353023\n",
      "Gradient Descent(957/9999): loss=652.9042242708972, w0=0.03248404937788959, w1=0.3884596567070669\n",
      "Gradient Descent(958/9999): loss=652.0437997933274, w0=0.03248476786939805, w1=0.38841146943457117\n",
      "Gradient Descent(959/9999): loss=651.1872305212918, w0=0.03248547720712178, w1=0.3883633126479039\n",
      "Gradient Descent(960/9999): loss=650.3344968167836, w0=0.032486177415855556, w1=0.38831518627733536\n",
      "Gradient Descent(961/9999): loss=649.4855791425126, w0=0.03248686852032418, w1=0.3882670902533165\n",
      "Gradient Descent(962/9999): loss=648.640458061388, w0=0.03248755054518268, w1=0.3882190245064787\n",
      "Gradient Descent(963/9999): loss=647.7991142360058, w0=0.032488223515016575, w1=0.3881709889676331\n",
      "Gradient Descent(964/9999): loss=646.9615284281357, w0=0.03248888745434211, w1=0.38812298356777014\n",
      "Gradient Descent(965/9999): loss=646.127681498213, w0=0.03248954238760651, w1=0.38807500823805924\n",
      "Gradient Descent(966/9999): loss=645.2975544048331, w0=0.03249018833918824, w1=0.388027062909848\n",
      "Gradient Descent(967/9999): loss=644.4711282042465, w0=0.032490825333397214, w1=0.387979147514662\n",
      "Gradient Descent(968/9999): loss=643.6483840498586, w0=0.03249145339447504, w1=0.3879312619842042\n",
      "Gradient Descent(969/9999): loss=642.8293031917317, w0=0.03249207254659529, w1=0.3878834062503543\n",
      "Gradient Descent(970/9999): loss=642.0138669760873, w0=0.032492682813863694, w1=0.3878355802451685\n",
      "Gradient Descent(971/9999): loss=641.2020568448156, w0=0.03249328422031842, w1=0.3877877839008789\n",
      "Gradient Descent(972/9999): loss=640.3938543349818, w0=0.032493876789930286, w1=0.3877400171498931\n",
      "Gradient Descent(973/9999): loss=639.5892410783392, w0=0.032494460546602995, w1=0.38769227992479355\n",
      "Gradient Descent(974/9999): loss=638.7881988008437, w0=0.03249503551417337, w1=0.3876445721583372\n",
      "Gradient Descent(975/9999): loss=637.99070932217, w0=0.03249560171641159, w1=0.38759689378345497\n",
      "Gradient Descent(976/9999): loss=637.1967545552303, w0=0.03249615917702142, w1=0.38754924473325136\n",
      "Gradient Descent(977/9999): loss=636.4063165056971, w0=0.03249670791964044, w1=0.38750162494100393\n",
      "Gradient Descent(978/9999): loss=635.6193772715264, w0=0.032497247967840266, w1=0.3874540343401628\n",
      "Gradient Descent(979/9999): loss=634.8359190424845, w0=0.03249777934512679, w1=0.3874064728643501\n",
      "Gradient Descent(980/9999): loss=634.0559240996769, w0=0.03249830207494041, w1=0.38735894044735975\n",
      "Gradient Descent(981/9999): loss=633.2793748150798, w0=0.03249881618065622, w1=0.38731143702315673\n",
      "Gradient Descent(982/9999): loss=632.5062536510738, w0=0.03249932168558429, w1=0.3872639625258768\n",
      "Gradient Descent(983/9999): loss=631.7365431599802, w0=0.032499818612969834, w1=0.3872165168898259\n",
      "Gradient Descent(984/9999): loss=630.9702259835998, w0=0.03250030698599347, w1=0.3871691000494798\n",
      "Gradient Descent(985/9999): loss=630.2072848527529, w0=0.03250078682777141, w1=0.3871217119394835\n",
      "Gradient Descent(986/9999): loss=629.4477025868233, w0=0.032501258161355726, w1=0.38707435249465105\n",
      "Gradient Descent(987/9999): loss=628.6914620933045, w0=0.03250172100973451, w1=0.38702702164996466\n",
      "Gradient Descent(988/9999): loss=627.9385463673459, w0=0.032502175395832125, w1=0.3869797193405746\n",
      "Gradient Descent(989/9999): loss=627.1889384913052, w0=0.03250262134250943, w1=0.38693244550179867\n",
      "Gradient Descent(990/9999): loss=626.4426216342994, w0=0.03250305887256397, w1=0.38688520006912164\n",
      "Gradient Descent(991/9999): loss=625.6995790517599, w0=0.03250348800873019, w1=0.3868379829781949\n",
      "Gradient Descent(992/9999): loss=624.9597940849906, w0=0.032503908773679674, w1=0.3867907941648359\n",
      "Gradient Descent(993/9999): loss=624.2232501607265, w0=0.032504321190021335, w1=0.3867436335650279\n",
      "Gradient Descent(994/9999): loss=623.4899307906956, w0=0.032504725280301634, w1=0.38669650111491927\n",
      "Gradient Descent(995/9999): loss=622.7598195711829, w0=0.03250512106700479, w1=0.38664939675082327\n",
      "Gradient Descent(996/9999): loss=622.0329001825963, w0=0.03250550857255297, w1=0.38660232040921744\n",
      "Gradient Descent(997/9999): loss=621.3091563890358, w0=0.032505887819306536, w1=0.3865552720267433\n",
      "Gradient Descent(998/9999): loss=620.5885720378626, w0=0.03250625882956421, w1=0.3865082515402057\n",
      "Gradient Descent(999/9999): loss=619.8711310592741, w0=0.03250662162556331, w1=0.3864612588865726\n",
      "Gradient Descent(1000/9999): loss=619.156817465877, w0=0.03250697622947993, w1=0.38641429400297467\n",
      "Gradient Descent(1001/9999): loss=618.445615352265, w0=0.032507322663429164, w1=0.3863673568267044\n",
      "Gradient Descent(1002/9999): loss=617.7375088945989, w0=0.0325076609494653, w1=0.3863204472952163\n",
      "Gradient Descent(1003/9999): loss=617.0324823501873, w0=0.03250799110958202, w1=0.386273565346126\n",
      "Gradient Descent(1004/9999): loss=616.3305200570708, w0=0.03250831316571259, w1=0.38622671091721\n",
      "Gradient Descent(1005/9999): loss=615.6316064336074, w0=0.0325086271397301, w1=0.3861798839464052\n",
      "Gradient Descent(1006/9999): loss=614.9357259780604, w0=0.032508933053447604, w1=0.38613308437180843\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(1007/9999): loss=614.2428632681892, w0=0.03250923092861836, w1=0.3860863121316762\n",
      "Gradient Descent(1008/9999): loss=613.5530029608407, w0=0.03250952078693602, w1=0.3860395671644241\n",
      "Gradient Descent(1009/9999): loss=612.8661297915432, w0=0.03250980265003482, w1=0.3859928494086263\n",
      "Gradient Descent(1010/9999): loss=612.1822285741041, w0=0.032510076539489764, w1=0.3859461588030154\n",
      "Gradient Descent(1011/9999): loss=611.501284200207, w0=0.03251034247681683, w1=0.3858994952864817\n",
      "Gradient Descent(1012/9999): loss=610.8232816390125, w0=0.03251060048347316, w1=0.38585285879807296\n",
      "Gradient Descent(1013/9999): loss=610.1482059367614, w0=0.03251085058085726, w1=0.38580624927699403\n",
      "Gradient Descent(1014/9999): loss=609.4760422163782, w0=0.03251109279030918, w1=0.38575966666260625\n",
      "Gradient Descent(1015/9999): loss=608.8067756770782, w0=0.0325113271331107, w1=0.38571311089442717\n",
      "Gradient Descent(1016/9999): loss=608.1403915939762, w0=0.03251155363048555, w1=0.3856665819121301\n",
      "Gradient Descent(1017/9999): loss=607.4768753176969, w0=0.03251177230359955, w1=0.3856200796555436\n",
      "Gradient Descent(1018/9999): loss=606.8162122739874, w0=0.03251198317356083, w1=0.38557360406465124\n",
      "Gradient Descent(1019/9999): loss=606.158387963332, w0=0.03251218626142, w1=0.385527155079591\n",
      "Gradient Descent(1020/9999): loss=605.5033879605687, w0=0.032512381588170366, w1=0.385480732640655\n",
      "Gradient Descent(1021/9999): loss=604.8511979145084, w0=0.03251256917474807, w1=0.3854343366882891\n",
      "Gradient Descent(1022/9999): loss=604.2018035475531, w0=0.0325127490420323, w1=0.38538796716309226\n",
      "Gradient Descent(1023/9999): loss=603.5551906553214, w0=0.03251292121084549, w1=0.3853416240058165\n",
      "Gradient Descent(1024/9999): loss=602.9113451062709, w0=0.03251308570195345, w1=0.38529530715736615\n",
      "Gradient Descent(1025/9999): loss=602.2702528413251, w0=0.0325132425360656, w1=0.3852490165587977\n",
      "Gradient Descent(1026/9999): loss=601.6318998735013, w0=0.03251339173383511, w1=0.3852027521513191\n",
      "Gradient Descent(1027/9999): loss=600.9962722875414, w0=0.03251353331585912, w1=0.3851565138762898\n",
      "Gradient Descent(1028/9999): loss=600.3633562395429, w0=0.03251366730267887, w1=0.38511030167521987\n",
      "Gradient Descent(1029/9999): loss=599.7331379565941, w0=0.03251379371477992, w1=0.38506411548976993\n",
      "Gradient Descent(1030/9999): loss=599.1056037364085, w0=0.03251391257259231, w1=0.3850179552617506\n",
      "Gradient Descent(1031/9999): loss=598.4807399469635, w0=0.03251402389649074, w1=0.3849718209331221\n",
      "Gradient Descent(1032/9999): loss=597.8585330261395, w0=0.032514127706794724, w1=0.384925712445994\n",
      "Gradient Descent(1033/9999): loss=597.2389694813614, w0=0.032514224023768794, w1=0.38487962974262463\n",
      "Gradient Descent(1034/9999): loss=596.6220358892417, w0=0.032514312867622665, w1=0.3848335727654208\n",
      "Gradient Descent(1035/9999): loss=596.0077188952253, w0=0.0325143942585114, w1=0.38478754145693733\n",
      "Gradient Descent(1036/9999): loss=595.396005213237, w0=0.03251446821653559, w1=0.3847415357598768\n",
      "Gradient Descent(1037/9999): loss=594.7868816253293, w0=0.03251453476174152, w1=0.38469555561708896\n",
      "Gradient Descent(1038/9999): loss=594.180334981334, w0=0.032514593914121336, w1=0.3846496009715705\n",
      "Gradient Descent(1039/9999): loss=593.5763521985136, w0=0.03251464569361322, w1=0.38460367176646465\n",
      "Gradient Descent(1040/9999): loss=592.9749202612151, w0=0.03251469012010156, w1=0.38455776794506064\n",
      "Gradient Descent(1041/9999): loss=592.3760262205269, w0=0.032514727213417124, w1=0.38451188945079345\n",
      "Gradient Descent(1042/9999): loss=591.7796571939353, w0=0.03251475699333721, w1=0.38446603622724346\n",
      "Gradient Descent(1043/9999): loss=591.185800364985, w0=0.03251477947958583, w1=0.38442020821813594\n",
      "Gradient Descent(1044/9999): loss=590.5944429829387, w0=0.03251479469183386, w1=0.3843744053673407\n",
      "Gradient Descent(1045/9999): loss=590.0055723624414, w0=0.03251480264969922, w1=0.3843286276188718\n",
      "Gradient Descent(1046/9999): loss=589.419175883184, w0=0.03251480337274703, w1=0.3842828749168871\n",
      "Gradient Descent(1047/9999): loss=588.8352409895703, w0=0.03251479688048977, w1=0.38423714720568786\n",
      "Gradient Descent(1048/9999): loss=588.2537551903839, w0=0.03251478319238746, w1=0.38419144442971837\n",
      "Gradient Descent(1049/9999): loss=587.6747060584592, w0=0.032514762327847786, w1=0.3841457665335656\n",
      "Gradient Descent(1050/9999): loss=587.0980812303522, w0=0.03251473430622631, w1=0.38410011346195877\n",
      "Gradient Descent(1051/9999): loss=586.5238684060131, w0=0.0325146991468266, w1=0.38405448515976914\n",
      "Gradient Descent(1052/9999): loss=585.9520553484618, w0=0.032514656868900385, w1=0.3840088815720094\n",
      "Gradient Descent(1053/9999): loss=585.3826298834646, w0=0.03251460749164774, w1=0.38396330264383344\n",
      "Gradient Descent(1054/9999): loss=584.8155798992107, w0=0.03251455103421721, w1=0.38391774832053593\n",
      "Gradient Descent(1055/9999): loss=584.250893345994, w0=0.03251448751570601, w1=0.383872218547552\n",
      "Gradient Descent(1056/9999): loss=583.6885582358924, w0=0.032514416955160136, w1=0.3838267132704568\n",
      "Gradient Descent(1057/9999): loss=583.128562642453, w0=0.032514339371574566, w1=0.3837812324349652\n",
      "Gradient Descent(1058/9999): loss=582.570894700375, w0=0.03251425478389339, w1=0.3837357759869313\n",
      "Gradient Descent(1059/9999): loss=582.0155426051967, w0=0.032514163211009944, w1=0.3836903438723483\n",
      "Gradient Descent(1060/9999): loss=581.462494612983, w0=0.03251406467176703, w1=0.38364493603734784\n",
      "Gradient Descent(1061/9999): loss=580.9117390400157, w0=0.032513959184957, w1=0.38359955242819993\n",
      "Gradient Descent(1062/9999): loss=580.3632642624837, w0=0.03251384676932196, w1=0.38355419299131227\n",
      "Gradient Descent(1063/9999): loss=579.8170587161758, w0=0.03251372744355387, w1=0.3835088576732302\n",
      "Gradient Descent(1064/9999): loss=579.2731108961748, w0=0.032513601226294764, w1=0.38346354642063607\n",
      "Gradient Descent(1065/9999): loss=578.7314093565547, w0=0.032513468136136835, w1=0.3834182591803491\n",
      "Gradient Descent(1066/9999): loss=578.1919427100755, w0=0.03251332819162263, w1=0.3833729958993249\n",
      "Gradient Descent(1067/9999): loss=577.6546996278845, w0=0.03251318141124517, w1=0.3833277565246551\n",
      "Gradient Descent(1068/9999): loss=577.1196688392155, w0=0.03251302781344813, w1=0.3832825410035671\n",
      "Gradient Descent(1069/9999): loss=576.586839131091, w0=0.03251286741662596, w1=0.38323734928342357\n",
      "Gradient Descent(1070/9999): loss=576.0561993480258, w0=0.03251270023912404, w1=0.38319218131172217\n",
      "Gradient Descent(1071/9999): loss=575.5277383917318, w0=0.032512526299238835, w1=0.3831470370360952\n",
      "Gradient Descent(1072/9999): loss=575.0014452208252, w0=0.03251234561521804, w1=0.38310191640430935\n",
      "Gradient Descent(1073/9999): loss=574.4773088505344, w0=0.032512158205260724, w1=0.38305681936426506\n",
      "Gradient Descent(1074/9999): loss=573.9553183524082, w0=0.03251196408751747, w1=0.38301174586399644\n",
      "Gradient Descent(1075/9999): loss=573.4354628540291, w0=0.03251176328009052, w1=0.38296669585167087\n",
      "Gradient Descent(1076/9999): loss=572.917731538724, w0=0.032511555801033924, w1=0.38292166927558846\n",
      "Gradient Descent(1077/9999): loss=572.4021136452792, w0=0.03251134166835369, w1=0.382876666084182\n",
      "Gradient Descent(1078/9999): loss=571.8885984676555, w0=0.03251112090000792, w1=0.38283168622601643\n",
      "Gradient Descent(1079/9999): loss=571.3771753547046, w0=0.03251089351390694, w1=0.38278672964978844\n",
      "Gradient Descent(1080/9999): loss=570.8678337098892, w0=0.03251065952791346, w1=0.3827417963043263\n",
      "Gradient Descent(1081/9999): loss=570.360562991001, w0=0.03251041895984272, w1=0.3826968861385894\n",
      "Gradient Descent(1082/9999): loss=569.8553527098825, w0=0.0325101718274626, w1=0.38265199910166786\n",
      "Gradient Descent(1083/9999): loss=569.3521924321502, w0=0.0325099181484938, w1=0.3826071351427824\n",
      "Gradient Descent(1084/9999): loss=568.8510717769182, w0=0.032509657940609935, w1=0.38256229421128374\n",
      "Gradient Descent(1085/9999): loss=568.351980416524, w0=0.032509391221437714, w1=0.38251747625665244\n",
      "Gradient Descent(1086/9999): loss=567.8549080762558, w0=0.03250911800855706, w1=0.38247268122849853\n",
      "Gradient Descent(1087/9999): loss=567.3598445340793, w0=0.03250883831950125, w1=0.38242790907656105\n",
      "Gradient Descent(1088/9999): loss=566.8667796203697, w0=0.03250855217175705, w1=0.3823831597507078\n",
      "Gradient Descent(1089/9999): loss=566.375703217641, w0=0.03250825958276485, w1=0.38233843320093513\n",
      "Gradient Descent(1090/9999): loss=565.8866052602791, w0=0.0325079605699188, w1=0.3822937293773674\n",
      "Gradient Descent(1091/9999): loss=565.3994757342762, w0=0.032507655150566965, w1=0.3822490482302567\n",
      "Gradient Descent(1092/9999): loss=564.9143046769652, w0=0.03250734334201143, w1=0.38220438970998255\n",
      "Gradient Descent(1093/9999): loss=564.4310821767571, w0=0.03250702516150845, w1=0.3821597537670516\n",
      "Gradient Descent(1094/9999): loss=563.9497983728778, w0=0.032506700626268585, w1=0.38211514035209726\n",
      "Gradient Descent(1095/9999): loss=563.4704434551093, w0=0.03250636975345683, w1=0.3820705494158793\n",
      "Gradient Descent(1096/9999): loss=562.9930076635275, w0=0.03250603256019275, w1=0.3820259809092837\n",
      "Gradient Descent(1097/9999): loss=562.5174812882474, w0=0.03250568906355061, w1=0.3819814347833221\n",
      "Gradient Descent(1098/9999): loss=562.0438546691639, w0=0.03250533928055949, w1=0.38193691098913163\n",
      "Gradient Descent(1099/9999): loss=561.5721181956981, w0=0.03250498322820347, w1=0.38189240947797454\n",
      "Gradient Descent(1100/9999): loss=561.1022623065413, w0=0.03250462092342169, w1=0.3818479302012378\n",
      "Gradient Descent(1101/9999): loss=560.6342774894044, w0=0.03250425238310853, w1=0.38180347311043294\n",
      "Gradient Descent(1102/9999): loss=560.1681542807646, w0=0.03250387762411372, w1=0.3817590381571956\n",
      "Gradient Descent(1103/9999): loss=559.7038832656166, w0=0.032503496663242475, w1=0.3817146252932851\n",
      "Gradient Descent(1104/9999): loss=559.2414550772228, w0=0.03250310951725562, w1=0.38167023447058446\n",
      "Gradient Descent(1105/9999): loss=558.7808603968658, w0=0.03250271620286972, w1=0.38162586564109974\n",
      "Gradient Descent(1106/9999): loss=558.3220899536024, w0=0.032502316736757214, w1=0.38158151875695984\n",
      "Gradient Descent(1107/9999): loss=557.8651345240178, w0=0.03250191113554653, w1=0.3815371937704163\n",
      "Gradient Descent(1108/9999): loss=557.4099849319831, w0=0.032501499415822206, w1=0.3814928906338427\n",
      "Gradient Descent(1109/9999): loss=556.956632048411, w0=0.03250108159412505, w1=0.3814486092997346\n",
      "Gradient Descent(1110/9999): loss=556.5050667910157, w0=0.032500657686952227, w1=0.3814043497207092\n",
      "Gradient Descent(1111/9999): loss=556.0552801240722, w0=0.03250022771075741, w1=0.3813601118495048\n",
      "Gradient Descent(1112/9999): loss=555.6072630581779, w0=0.03249979168195089, w1=0.3813158956389808\n",
      "Gradient Descent(1113/9999): loss=555.1610066500141, w0=0.03249934961689971, w1=0.3812717010421171\n",
      "Gradient Descent(1114/9999): loss=554.7165020021107, w0=0.032498901531927776, w1=0.381227528012014\n",
      "Gradient Descent(1115/9999): loss=554.2737402626103, w0=0.032498447443315995, w1=0.3811833765018917\n",
      "Gradient Descent(1116/9999): loss=553.832712625034, w0=0.03249798736730239, w1=0.38113924646509023\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(1117/9999): loss=553.3934103280491, w0=0.032497521320082226, w1=0.3810951378550688\n",
      "Gradient Descent(1118/9999): loss=552.9558246552373, w0=0.03249704931780813, w1=0.3810510506254059\n",
      "Gradient Descent(1119/9999): loss=552.5199469348642, w0=0.0324965713765902, w1=0.3810069847297986\n",
      "Gradient Descent(1120/9999): loss=552.0857685396498, w0=0.03249608751249615, w1=0.3809629401220625\n",
      "Gradient Descent(1121/9999): loss=551.6532808865413, w0=0.032495597741551425, w1=0.3809189167561313\n",
      "Gradient Descent(1122/9999): loss=551.2224754364856, w0=0.0324951020797393, w1=0.3808749145860566\n",
      "Gradient Descent(1123/9999): loss=550.7933436942028, w0=0.03249460054300103, w1=0.3808309335660074\n",
      "Gradient Descent(1124/9999): loss=550.3658772079634, w0=0.03249409314723593, w1=0.38078697365027003\n",
      "Gradient Descent(1125/9999): loss=549.9400675693631, w0=0.03249357990830155, w1=0.38074303479324767\n",
      "Gradient Descent(1126/9999): loss=549.5159064131019, w0=0.03249306084201373, w1=0.3806991169494602\n",
      "Gradient Descent(1127/9999): loss=549.0933854167622, w0=0.03249253596414676, w1=0.3806552200735437\n",
      "Gradient Descent(1128/9999): loss=548.6724963005887, w0=0.032492005290433494, w1=0.3806113441202503\n",
      "Gradient Descent(1129/9999): loss=548.2532308272703, w0=0.03249146883656545, w1=0.3805674890444478\n",
      "Gradient Descent(1130/9999): loss=547.8355808017213, w0=0.032490926618192945, w1=0.38052365480111955\n",
      "Gradient Descent(1131/9999): loss=547.4195380708659, w0=0.03249037865092519, w1=0.3804798413453638\n",
      "Gradient Descent(1132/9999): loss=547.0050945234217, w0=0.032489824950330426, w1=0.38043604863239366\n",
      "Gradient Descent(1133/9999): loss=546.5922420896858, w0=0.032489265531936026, w1=0.3803922766175369\n",
      "Gradient Descent(1134/9999): loss=546.1809727413212, w0=0.032488700411228616, w1=0.38034852525623525\n",
      "Gradient Descent(1135/9999): loss=545.7712784911448, w0=0.032488129603654194, w1=0.3803047945040446\n",
      "Gradient Descent(1136/9999): loss=545.3631513929166, w0=0.03248755312461823, w1=0.38026108431663425\n",
      "Gradient Descent(1137/9999): loss=544.9565835411285, w0=0.03248697098948578, w1=0.380217394649787\n",
      "Gradient Descent(1138/9999): loss=544.5515670707968, w0=0.03248638321358164, w1=0.38017372545939854\n",
      "Gradient Descent(1139/9999): loss=544.1480941572531, w0=0.032485789812190385, w1=0.3801300767014774\n",
      "Gradient Descent(1140/9999): loss=543.7461570159384, w0=0.032485190800556556, w1=0.3800864483321445\n",
      "Gradient Descent(1141/9999): loss=543.3457479021963, w0=0.03248458619388472, w1=0.38004284030763286\n",
      "Gradient Descent(1142/9999): loss=542.9468591110693, w0=0.0324839760073396, w1=0.37999925258428746\n",
      "Gradient Descent(1143/9999): loss=542.5494829770947, w0=0.0324833602560462, w1=0.3799556851185648\n",
      "Gradient Descent(1144/9999): loss=542.1536118741014, w0=0.03248273895508989, w1=0.3799121378670327\n",
      "Gradient Descent(1145/9999): loss=541.7592382150095, w0=0.032482112119516536, w1=0.3798686107863699\n",
      "Gradient Descent(1146/9999): loss=541.3663544516291, w0=0.0324814797643326, w1=0.37982510383336593\n",
      "Gradient Descent(1147/9999): loss=540.9749530744605, w0=0.03248084190450524, w1=0.37978161696492063\n",
      "Gradient Descent(1148/9999): loss=540.5850266124962, w0=0.03248019855496245, w1=0.37973815013804413\n",
      "Gradient Descent(1149/9999): loss=540.1965676330232, w0=0.03247954973059315, w1=0.3796947033098563\n",
      "Gradient Descent(1150/9999): loss=539.8095687414265, w0=0.03247889544624728, w1=0.3796512764375865\n",
      "Gradient Descent(1151/9999): loss=539.4240225809939, w0=0.03247823571673594, w1=0.37960786947857356\n",
      "Gradient Descent(1152/9999): loss=539.0399218327213, w0=0.03247757055683147, w1=0.37956448239026513\n",
      "Gradient Descent(1153/9999): loss=538.6572592151196, w0=0.032476899981267564, w1=0.3795211151302177\n",
      "Gradient Descent(1154/9999): loss=538.276027484021, w0=0.0324762240047394, w1=0.3794777676560961\n",
      "Gradient Descent(1155/9999): loss=537.8962194323897, w0=0.032475542641903714, w1=0.3794344399256734\n",
      "Gradient Descent(1156/9999): loss=537.5178278901293, w0=0.03247485590737892, w1=0.37939113189683044\n",
      "Gradient Descent(1157/9999): loss=537.140845723895, w0=0.03247416381574522, w1=0.37934784352755574\n",
      "Gradient Descent(1158/9999): loss=536.7652658369032, w0=0.03247346638154471, w1=0.3793045747759451\n",
      "Gradient Descent(1159/9999): loss=536.3910811687458, w0=0.03247276361928148, w1=0.37926132560020126\n",
      "Gradient Descent(1160/9999): loss=536.0182846952022, w0=0.03247205554342171, w1=0.3792180959586339\n",
      "Gradient Descent(1161/9999): loss=535.6468694280544, w0=0.03247134216839379, w1=0.3791748858096591\n",
      "Gradient Descent(1162/9999): loss=535.2768284149021, w0=0.032470623508588425, w1=0.37913169511179906\n",
      "Gradient Descent(1163/9999): loss=534.9081547389793, w0=0.03246989957835872, w1=0.379088523823682\n",
      "Gradient Descent(1164/9999): loss=534.5408415189701, w0=0.0324691703920203, w1=0.37904537190404186\n",
      "Gradient Descent(1165/9999): loss=534.1748819088286, w0=0.032468435963851405, w1=0.3790022393117178\n",
      "Gradient Descent(1166/9999): loss=533.8102690975974, w0=0.032467696308093014, w1=0.3789591260056542\n",
      "Gradient Descent(1167/9999): loss=533.4469963092264, w0=0.03246695143894891, w1=0.37891603194490026\n",
      "Gradient Descent(1168/9999): loss=533.0850568023959, w0=0.03246620137058581, w1=0.37887295708860974\n",
      "Gradient Descent(1169/9999): loss=532.7244438703362, w0=0.03246544611713346, w1=0.3788299013960407\n",
      "Gradient Descent(1170/9999): loss=532.3651508406518, w0=0.03246468569268472, w1=0.3787868648265553\n",
      "Gradient Descent(1171/9999): loss=532.0071710751447, w0=0.03246392011129571, w1=0.37874384733961936\n",
      "Gradient Descent(1172/9999): loss=531.6504979696388, w0=0.03246314938698584, w1=0.37870084889480216\n",
      "Gradient Descent(1173/9999): loss=531.2951249538052, w0=0.03246237353373799, w1=0.37865786945177626\n",
      "Gradient Descent(1174/9999): loss=530.9410454909888, w0=0.032461592565498534, w1=0.37861490897031724\n",
      "Gradient Descent(1175/9999): loss=530.5882530780359, w0=0.032460806496177504, w1=0.37857196741030325\n",
      "Gradient Descent(1176/9999): loss=530.2367412451217, w0=0.03246001533964865, w1=0.37852904473171495\n",
      "Gradient Descent(1177/9999): loss=529.8865035555796, w0=0.03245921910974955, w1=0.3784861408946351\n",
      "Gradient Descent(1178/9999): loss=529.5375336057306, w0=0.03245841782028171, w1=0.3784432558592485\n",
      "Gradient Descent(1179/9999): loss=529.1898250247153, w0=0.03245761148501066, w1=0.3784003895858413\n",
      "Gradient Descent(1180/9999): loss=528.8433714743243, w0=0.032456800117666065, w1=0.3783575420348013\n",
      "Gradient Descent(1181/9999): loss=528.4981666488313, w0=0.0324559837319418, w1=0.3783147131666173\n",
      "Gradient Descent(1182/9999): loss=528.154204274826, w0=0.032455162341496056, w1=0.3782719029418789\n",
      "Gradient Descent(1183/9999): loss=527.8114781110492, w0=0.032454335959951446, w1=0.37822911132127635\n",
      "Gradient Descent(1184/9999): loss=527.4699819482262, w0=0.032453504600895085, w1=0.37818633826560016\n",
      "Gradient Descent(1185/9999): loss=527.1297096089047, w0=0.03245266827787871, w1=0.378143583735741\n",
      "Gradient Descent(1186/9999): loss=526.7906549472904, w0=0.03245182700441876, w1=0.37810084769268926\n",
      "Gradient Descent(1187/9999): loss=526.4528118490847, w0=0.03245098079399647, w1=0.3780581300975349\n",
      "Gradient Descent(1188/9999): loss=526.116174231323, w0=0.03245012966005797, w1=0.3780154309114673\n",
      "Gradient Descent(1189/9999): loss=525.7807360422146, w0=0.032449273616014386, w1=0.37797275009577463\n",
      "Gradient Descent(1190/9999): loss=525.4464912609819, w0=0.03244841267524193, w1=0.37793008761184405\n",
      "Gradient Descent(1191/9999): loss=525.1134338977021, w0=0.03244754685108198, w1=0.37788744342116115\n",
      "Gradient Descent(1192/9999): loss=524.7815579931481, w0=0.03244667615684122, w1=0.37784481748530985\n",
      "Gradient Descent(1193/9999): loss=524.4508576186319, w0=0.03244580060579168, w1=0.377802209765972\n",
      "Gradient Descent(1194/9999): loss=524.121326875847, w0=0.03244492021117084, w1=0.3777596202249273\n",
      "Gradient Descent(1195/9999): loss=523.7929598967136, w0=0.032444034986181765, w1=0.37771704882405294\n",
      "Gradient Descent(1196/9999): loss=523.4657508432224, w0=0.032443144943993155, w1=0.37767449552532334\n",
      "Gradient Descent(1197/9999): loss=523.1396939072811, w0=0.032442250097739454, w1=0.37763196029081\n",
      "Gradient Descent(1198/9999): loss=522.8147833105608, w0=0.032441350460520935, w1=0.377589443082681\n",
      "Gradient Descent(1199/9999): loss=522.4910133043429, w0=0.032440446045403803, w1=0.37754694386320115\n",
      "Gradient Descent(1200/9999): loss=522.1683781693679, w0=0.032439536865420275, w1=0.3775044625947314\n",
      "Gradient Descent(1201/9999): loss=521.846872215683, w0=0.03243862293356869, w1=0.3774619992397288\n",
      "Gradient Descent(1202/9999): loss=521.5264897824939, w0=0.03243770426281357, w1=0.377419553760746\n",
      "Gradient Descent(1203/9999): loss=521.2072252380128, w0=0.03243678086608574, w1=0.37737712612043134\n",
      "Gradient Descent(1204/9999): loss=520.8890729793112, w0=0.03243585275628241, w1=0.3773347162815283\n",
      "Gradient Descent(1205/9999): loss=520.5720274321715, w0=0.03243491994626726, w1=0.37729232420687553\n",
      "Gradient Descent(1206/9999): loss=520.2560830509389, w0=0.032433982448870524, w1=0.3772499498594063\n",
      "Gradient Descent(1207/9999): loss=519.9412343183761, w0=0.032433040276889105, w1=0.3772075932021485\n",
      "Gradient Descent(1208/9999): loss=519.6274757455169, w0=0.03243209344308664, w1=0.3771652541982243\n",
      "Gradient Descent(1209/9999): loss=519.314801871521, w0=0.032431141960193595, w1=0.37712293281084985\n",
      "Gradient Descent(1210/9999): loss=519.0032072635296, w0=0.032430185840907365, w1=0.3770806290033353\n",
      "Gradient Descent(1211/9999): loss=518.6926865165228, w0=0.032429225097892346, w1=0.3770383427390841\n",
      "Gradient Descent(1212/9999): loss=518.3832342531757, w0=0.03242825974378004, w1=0.37699607398159324\n",
      "Gradient Descent(1213/9999): loss=518.0748451237171, w0=0.03242728979116913, w1=0.3769538226944527\n",
      "Gradient Descent(1214/9999): loss=517.7675138057871, w0=0.03242631525262558, w1=0.3769115888413453\n",
      "Gradient Descent(1215/9999): loss=517.4612350042977, w0=0.03242533614068271, w1=0.3768693723860464\n",
      "Gradient Descent(1216/9999): loss=517.1560034512926, w0=0.03242435246784128, w1=0.3768271732924238\n",
      "Gradient Descent(1217/9999): loss=516.8518139058069, w0=0.032423364246569615, w1=0.3767849915244375\n",
      "Gradient Descent(1218/9999): loss=516.5486611537304, w0=0.03242237148930364, w1=0.37674282704613926\n",
      "Gradient Descent(1219/9999): loss=516.2465400076683, w0=0.03242137420844699, w1=0.3767006798216725\n",
      "Gradient Descent(1220/9999): loss=515.945445306805, w0=0.032420372416371114, w1=0.3766585498152721\n",
      "Gradient Descent(1221/9999): loss=515.6453719167674, w0=0.03241936612541532, w1=0.37661643699126407\n",
      "Gradient Descent(1222/9999): loss=515.3463147294894, w0=0.032418355347886896, w1=0.3765743413140654\n",
      "Gradient Descent(1223/9999): loss=515.0482686630771, w0=0.03241734009606118, w1=0.3765322627481838\n",
      "Gradient Descent(1224/9999): loss=514.7512286616737, w0=0.03241632038218167, w1=0.3764902012582174\n",
      "Gradient Descent(1225/9999): loss=514.4551896953272, w0=0.032415296218460045, w1=0.3764481568088547\n",
      "Gradient Descent(1226/9999): loss=514.1601467598567, w0=0.03241426761707633, w1=0.376406129364874\n",
      "Gradient Descent(1227/9999): loss=513.8660948767199, w0=0.03241323459017893, w1=0.3763641188911436\n",
      "Gradient Descent(1228/9999): loss=513.5730290928824, w0=0.032412197149884726, w1=0.3763221253526211\n",
      "Gradient Descent(1229/9999): loss=513.2809444806865, w0=0.032411155308279156, w1=0.37628014871435367\n",
      "Gradient Descent(1230/9999): loss=512.9898361377205, w0=0.032410109077416316, w1=0.37623818894147737\n",
      "Gradient Descent(1231/9999): loss=512.6996991866904, w0=0.03240905846931902, w1=0.3761962459992172\n",
      "Gradient Descent(1232/9999): loss=512.4105287752893, w0=0.032408003495978895, w1=0.3761543198528868\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(1233/9999): loss=512.122320076071, w0=0.032406944169356466, w1=0.3761124104678881\n",
      "Gradient Descent(1234/9999): loss=511.83506828632113, w0=0.03240588050138123, w1=0.37607051780971146\n",
      "Gradient Descent(1235/9999): loss=511.5487686279312, w0=0.03240481250395175, w1=0.37602864184393486\n",
      "Gradient Descent(1236/9999): loss=511.2634163472716, w0=0.032403740188935735, w1=0.3759867825362242\n",
      "Gradient Descent(1237/9999): loss=510.9790067150666, w0=0.0324026635681701, w1=0.37594493985233274\n",
      "Gradient Descent(1238/9999): loss=510.6955350262689, w0=0.03240158265346108, w1=0.3759031137581011\n",
      "Gradient Descent(1239/9999): loss=510.4129965999359, w0=0.0324004974565843, w1=0.3758613042194569\n",
      "Gradient Descent(1240/9999): loss=510.131386779106, w0=0.03239940798928483, w1=0.37581951120241464\n",
      "Gradient Descent(1241/9999): loss=509.8507009306746, w0=0.03239831426327733, w1=0.3757777346730753\n",
      "Gradient Descent(1242/9999): loss=509.57093444527277, w0=0.03239721629024605, w1=0.3757359745976263\n",
      "Gradient Descent(1243/9999): loss=509.2920827371455, w0=0.03239611408184497, w1=0.3756942309423412\n",
      "Gradient Descent(1244/9999): loss=509.0141412440298, w0=0.03239500764969787, w1=0.37565250367357955\n",
      "Gradient Descent(1245/9999): loss=508.7371054270347, w0=0.032393897005398374, w1=0.3756107927577864\n",
      "Gradient Descent(1246/9999): loss=508.4609707705212, w0=0.03239278216051008, w1=0.3755690981614925\n",
      "Gradient Descent(1247/9999): loss=508.1857327819829, w0=0.03239166312656661, w1=0.37552741985131366\n",
      "Gradient Descent(1248/9999): loss=507.91138699192777, w0=0.032390539915071695, w1=0.375485757793951\n",
      "Gradient Descent(1249/9999): loss=507.63792895375957, w0=0.03238941253749925, w1=0.3754441119561902\n",
      "Gradient Descent(1250/9999): loss=507.36535424366053, w0=0.032388281005293465, w1=0.3754024823049017\n",
      "Gradient Descent(1251/9999): loss=507.0936584604746, w0=0.03238714532986887, w1=0.3753608688070402\n",
      "Gradient Descent(1252/9999): loss=506.82283722559123, w0=0.03238600552261042, w1=0.3753192714296447\n",
      "Gradient Descent(1253/9999): loss=506.552886182829, w0=0.03238486159487359, w1=0.37527769013983814\n",
      "Gradient Descent(1254/9999): loss=506.2838009983223, w0=0.03238371355798441, w1=0.3752361249048271\n",
      "Gradient Descent(1255/9999): loss=506.0155773604048, w0=0.03238256142323958, w1=0.37519457569190173\n",
      "Gradient Descent(1256/9999): loss=505.74821097949655, w0=0.032381405201906535, w1=0.3751530424684355\n",
      "Gradient Descent(1257/9999): loss=505.48169758799105, w0=0.03238024490522353, w1=0.375111525201885\n",
      "Gradient Descent(1258/9999): loss=505.2160329401418, w0=0.0323790805443997, w1=0.37507002385978966\n",
      "Gradient Descent(1259/9999): loss=504.95121281195014, w0=0.03237791213061516, w1=0.3750285384097715\n",
      "Gradient Descent(1260/9999): loss=504.68723300105455, w0=0.03237673967502105, w1=0.37498706881953514\n",
      "Gradient Descent(1261/9999): loss=504.42408932661857, w0=0.03237556318873964, w1=0.37494561505686735\n",
      "Gradient Descent(1262/9999): loss=504.16177762922126, w0=0.0323743826828644, w1=0.3749041770896369\n",
      "Gradient Descent(1263/9999): loss=503.90029377074666, w0=0.032373198168460064, w1=0.3748627548857945\n",
      "Gradient Descent(1264/9999): loss=503.63963363427473, w0=0.03237200965656273, w1=0.37482134841337233\n",
      "Gradient Descent(1265/9999): loss=503.379793123973, w0=0.032370817158179896, w1=0.374779957640484\n",
      "Gradient Descent(1266/9999): loss=503.1207681649877, w0=0.03236962068429058, w1=0.3747385825353244\n",
      "Gradient Descent(1267/9999): loss=502.8625547033366, w0=0.03236842024584536, w1=0.3746972230661692\n",
      "Gradient Descent(1268/9999): loss=502.60514870580215, w0=0.032367215853766465, w1=0.37465587920137505\n",
      "Gradient Descent(1269/9999): loss=502.3485461598239, w0=0.03236600751894785, w1=0.37461455090937906\n",
      "Gradient Descent(1270/9999): loss=502.0927430733939, w0=0.03236479525225527, w1=0.3745732381586987\n",
      "Gradient Descent(1271/9999): loss=501.8377354749502, w0=0.03236357906452634, w1=0.37453194091793157\n",
      "Gradient Descent(1272/9999): loss=501.58351941327277, w0=0.03236235896657064, w1=0.3744906591557552\n",
      "Gradient Descent(1273/9999): loss=501.33009095737845, w0=0.032361134969169744, w1=0.37444939284092693\n",
      "Gradient Descent(1274/9999): loss=501.0774461964173, w0=0.03235990708307734, w1=0.37440814194228356\n",
      "Gradient Descent(1275/9999): loss=500.8255812395699, w0=0.03235867531901927, w1=0.37436690642874126\n",
      "Gradient Descent(1276/9999): loss=500.5744922159436, w0=0.03235743968769363, w1=0.3743256862692953\n",
      "Gradient Descent(1277/9999): loss=500.32417527447143, w0=0.03235620019977081, w1=0.3742844814330198\n",
      "Gradient Descent(1278/9999): loss=500.0746265838092, w0=0.032354956865893604, w1=0.3742432918890677\n",
      "Gradient Descent(1279/9999): loss=499.82584233223554, w0=0.03235370969667724, w1=0.3742021176066704\n",
      "Gradient Descent(1280/9999): loss=499.57781872754975, w0=0.032352458702709495, w1=0.3741609585551376\n",
      "Gradient Descent(1281/9999): loss=499.33055199697316, w0=0.03235120389455074, w1=0.37411981470385713\n",
      "Gradient Descent(1282/9999): loss=499.084038387049, w0=0.03234994528273403, w1=0.37407868602229477\n",
      "Gradient Descent(1283/9999): loss=498.83827416354245, w0=0.03234868287776515, w1=0.3740375724799939\n",
      "Gradient Descent(1284/9999): loss=498.5932556113431, w0=0.03234741669012271, w1=0.3739964740465755\n",
      "Gradient Descent(1285/9999): loss=498.3489790343665, w0=0.032346146730258214, w1=0.3739553906917378\n",
      "Gradient Descent(1286/9999): loss=498.10544075545636, w0=0.03234487300859611, w1=0.37391432238525607\n",
      "Gradient Descent(1287/9999): loss=497.8626371162875, w0=0.03234359553553389, w1=0.3738732690969827\n",
      "Gradient Descent(1288/9999): loss=497.62056447726957, w0=0.03234231432144214, w1=0.37383223079684663\n",
      "Gradient Descent(1289/9999): loss=497.37921921745067, w0=0.03234102937666462, w1=0.37379120745485334\n",
      "Gradient Descent(1290/9999): loss=497.13859773442135, w0=0.03233974071151834, w1=0.3737501990410846\n",
      "Gradient Descent(1291/9999): loss=496.8986964442205, w0=0.0323384483362936, w1=0.37370920552569836\n",
      "Gradient Descent(1292/9999): loss=496.65951178123953, w0=0.03233715226125411, w1=0.3736682268789285\n",
      "Gradient Descent(1293/9999): loss=496.4210401981293, w0=0.032335852496637, w1=0.37362726307108457\n",
      "Gradient Descent(1294/9999): loss=496.18327816570553, w0=0.03233454905265294, w1=0.3735863140725517\n",
      "Gradient Descent(1295/9999): loss=495.94622217285644, w0=0.032333241939486196, w1=0.3735453798537904\n",
      "Gradient Descent(1296/9999): loss=495.7098687264491, w0=0.03233193116729467, w1=0.3735044603853362\n",
      "Gradient Descent(1297/9999): loss=495.47421435123834, w0=0.032330616746210024, w1=0.37346355563779965\n",
      "Gradient Descent(1298/9999): loss=495.23925558977373, w0=0.03232929868633769, w1=0.3734226655818661\n",
      "Gradient Descent(1299/9999): loss=495.0049890023094, w0=0.03232797699775697, w1=0.3733817901882954\n",
      "Gradient Descent(1300/9999): loss=494.7714111667126, w0=0.032326651690521103, w1=0.3733409294279219\n",
      "Gradient Descent(1301/9999): loss=494.5385186783733, w0=0.03232532277465734, w1=0.373300083271654\n",
      "Gradient Descent(1302/9999): loss=494.3063081501149, w0=0.03232399026016699, w1=0.37325925169047414\n",
      "Gradient Descent(1303/9999): loss=494.074776212104, w0=0.032322654157025496, w1=0.3732184346554386\n",
      "Gradient Descent(1304/9999): loss=493.8439195117619, w0=0.032321314475182515, w1=0.37317763213767713\n",
      "Gradient Descent(1305/9999): loss=493.613734713676, w0=0.03231997122456197, w1=0.37313684410839315\n",
      "Gradient Descent(1306/9999): loss=493.38421849951186, w0=0.03231862441506213, w1=0.3730960705388631\n",
      "Gradient Descent(1307/9999): loss=493.15536756792517, w0=0.03231727405655567, w1=0.3730553114004366\n",
      "Gradient Descent(1308/9999): loss=492.9271786344752, w0=0.03231592015888973, w1=0.37301456666453603\n",
      "Gradient Descent(1309/9999): loss=492.6996484315377, w0=0.032314562731886, w1=0.3729738363026565\n",
      "Gradient Descent(1310/9999): loss=492.4727737082187, w0=0.03231320178534078, w1=0.37293312028636566\n",
      "Gradient Descent(1311/9999): loss=492.2465512302683, w0=0.03231183732902502, w1=0.3728924185873033\n",
      "Gradient Descent(1312/9999): loss=492.02097777999614, w0=0.03231046937268443, w1=0.3728517311771815\n",
      "Gradient Descent(1313/9999): loss=491.7960501561857, w0=0.032309097926039525, w1=0.37281105802778414\n",
      "Gradient Descent(1314/9999): loss=491.57176517401007, w0=0.032307722998785676, w1=0.37277039911096693\n",
      "Gradient Descent(1315/9999): loss=491.3481196649483, w0=0.0323063446005932, w1=0.3727297543986571\n",
      "Gradient Descent(1316/9999): loss=491.1251104767005, w0=0.03230496274110742, w1=0.3726891238628532\n",
      "Gradient Descent(1317/9999): loss=490.9027344731061, w0=0.032303577429948716, w1=0.37264850747562506\n",
      "Gradient Descent(1318/9999): loss=490.6809885340603, w0=0.0323021886767126, w1=0.37260790520911347\n",
      "Gradient Descent(1319/9999): loss=490.4598695554316, w0=0.032300796490969785, w1=0.37256731703553003\n",
      "Gradient Descent(1320/9999): loss=490.23937444897996, w0=0.032299400882266245, w1=0.3725267429271571\n",
      "Gradient Descent(1321/9999): loss=490.0195001422758, w0=0.03229800186012327, w1=0.3724861828563473\n",
      "Gradient Descent(1322/9999): loss=489.80024357861805, w0=0.03229659943403756, w1=0.3724456367955237\n",
      "Gradient Descent(1323/9999): loss=489.5816017169541, w0=0.032295193613481245, w1=0.37240510471717947\n",
      "Gradient Descent(1324/9999): loss=489.36357153179915, w0=0.03229378440790199, w1=0.37236458659387756\n",
      "Gradient Descent(1325/9999): loss=489.14615001315684, w0=0.03229237182672303, w1=0.3723240823982507\n",
      "Gradient Descent(1326/9999): loss=488.9293341664391, w0=0.032290955879343254, w1=0.3722835921030013\n",
      "Gradient Descent(1327/9999): loss=488.71312101238726, w0=0.03228953657513725, w1=0.37224311568090107\n",
      "Gradient Descent(1328/9999): loss=488.49750758699435, w0=0.03228811392345539, w1=0.3722026531047909\n",
      "Gradient Descent(1329/9999): loss=488.2824909414254, w0=0.03228668793362387, w1=0.37216220434758074\n",
      "Gradient Descent(1330/9999): loss=488.06806814194084, w0=0.03228525861494479, w1=0.37212176938224933\n",
      "Gradient Descent(1331/9999): loss=487.854236269818, w0=0.032283825976696205, w1=0.3720813481818442\n",
      "Gradient Descent(1332/9999): loss=487.64099242127514, w0=0.0322823900281322, w1=0.37204094071948124\n",
      "Gradient Descent(1333/9999): loss=487.4283337073943, w0=0.03228095077848294, w1=0.3720005469683448\n",
      "Gradient Descent(1334/9999): loss=487.21625725404436, w0=0.03227950823695474, w1=0.37196016690168715\n",
      "Gradient Descent(1335/9999): loss=487.0047602018068, w0=0.03227806241273012, w1=0.3719198004928288\n",
      "Gradient Descent(1336/9999): loss=486.79383970589845, w0=0.032276613314967874, w1=0.3718794477151579\n",
      "Gradient Descent(1337/9999): loss=486.58349293609785, w0=0.03227516095280313, w1=0.3718391085421302\n",
      "Gradient Descent(1338/9999): loss=486.3737170766695, w0=0.03227370533534741, w1=0.371798782947269\n",
      "Gradient Descent(1339/9999): loss=486.1645093262904, w0=0.03227224647168869, w1=0.3717584709041647\n",
      "Gradient Descent(1340/9999): loss=485.9558668979754, w0=0.03227078437089147, w1=0.371718172386475\n",
      "Gradient Descent(1341/9999): loss=485.74778701900425, w0=0.03226931904199681, w1=0.37167788736792434\n",
      "Gradient Descent(1342/9999): loss=485.5402669308476, w0=0.03226785049402244, w1=0.3716376158223041\n",
      "Gradient Descent(1343/9999): loss=485.33330388909536, w0=0.03226637873596276, w1=0.371597357723472\n",
      "Gradient Descent(1344/9999): loss=485.1268951633834, w0=0.03226490377678894, w1=0.3715571130453525\n",
      "Gradient Descent(1345/9999): loss=484.9210380373216, w0=0.032263425625448976, w1=0.371516881761936\n",
      "Gradient Descent(1346/9999): loss=484.7157298084222, w0=0.03226194429086774, w1=0.37147666384727906\n",
      "Gradient Descent(1347/9999): loss=484.5109677880291, w0=0.03226045978194704, w1=0.3714364592755042\n",
      "Gradient Descent(1348/9999): loss=484.3067493012461, w0=0.032258972107565696, w1=0.3713962680207997\n",
      "Gradient Descent(1349/9999): loss=484.10307168686643, w0=0.032257481276579576, w1=0.37135609005741926\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(1350/9999): loss=483.8999322973028, w0=0.032255987297821684, w1=0.37131592535968205\n",
      "Gradient Descent(1351/9999): loss=483.69732849851755, w0=0.03225449018010219, w1=0.37127577390197253\n",
      "Gradient Descent(1352/9999): loss=483.49525766995293, w0=0.032252989932208494, w1=0.37123563565874007\n",
      "Gradient Descent(1353/9999): loss=483.2937172044622, w0=0.032251486562905315, w1=0.37119551060449907\n",
      "Gradient Descent(1354/9999): loss=483.09270450824044, w0=0.03224998008093472, w1=0.3711553987138286\n",
      "Gradient Descent(1355/9999): loss=482.89221700075626, w0=0.03224847049501619, w1=0.37111529996137216\n",
      "Gradient Descent(1356/9999): loss=482.6922521146843, w0=0.03224695781384668, w1=0.3710752143218379\n",
      "Gradient Descent(1357/9999): loss=482.49280729583637, w0=0.032245442046100664, w1=0.37103514176999797\n",
      "Gradient Descent(1358/9999): loss=482.29388000309507, w0=0.032243923200430236, w1=0.3709950822806888\n",
      "Gradient Descent(1359/9999): loss=482.095467708346, w0=0.032242401285465114, w1=0.37095503582881045\n",
      "Gradient Descent(1360/9999): loss=481.8975678964119, w0=0.03224087630981273, w1=0.3709150023893269\n",
      "Gradient Descent(1361/9999): loss=481.7001780649853, w0=0.03223934828205829, w1=0.37087498193726565\n",
      "Gradient Descent(1362/9999): loss=481.50329572456315, w0=0.032237817210764796, w1=0.37083497444771757\n",
      "Gradient Descent(1363/9999): loss=481.30691839838113, w0=0.03223628310447316, w1=0.3707949798958368\n",
      "Gradient Descent(1364/9999): loss=481.11104362234767, w0=0.03223474597170221, w1=0.3707549982568406\n",
      "Gradient Descent(1365/9999): loss=480.91566894497987, w0=0.032233205820948775, w1=0.3707150295060091\n",
      "Gradient Descent(1366/9999): loss=480.7207919273382, w0=0.032231662660687735, w1=0.3706750736186852\n",
      "Gradient Descent(1367/9999): loss=480.5264101429621, w0=0.03223011649937208, w1=0.3706351305702743\n",
      "Gradient Descent(1368/9999): loss=480.33252117780637, w0=0.03222856734543297, w1=0.37059520033624443\n",
      "Gradient Descent(1369/9999): loss=480.13912263017727, w0=0.03222701520727976, w1=0.37055528289212575\n",
      "Gradient Descent(1370/9999): loss=479.94621211066936, w0=0.032225460093300115, w1=0.37051537821351055\n",
      "Gradient Descent(1371/9999): loss=479.7537872421019, w0=0.03222390201186002, w1=0.37047548627605303\n",
      "Gradient Descent(1372/9999): loss=479.5618456594564, w0=0.032222340971303845, w1=0.37043560705546935\n",
      "Gradient Descent(1373/9999): loss=479.3703850098154, w0=0.032220776979954416, w1=0.3703957405275371\n",
      "Gradient Descent(1374/9999): loss=479.1794029522978, w0=0.032219210046113056, w1=0.3703558866680955\n",
      "Gradient Descent(1375/9999): loss=478.9888971579999, w0=0.03221764017805964, w1=0.37031604545304503\n",
      "Gradient Descent(1376/9999): loss=478.7988653099319, w0=0.032216067384052666, w1=0.37027621685834733\n",
      "Gradient Descent(1377/9999): loss=478.6093051029583, w0=0.032214491672329294, w1=0.370236400860025\n",
      "Gradient Descent(1378/9999): loss=478.42021424373576, w0=0.032212913051105414, w1=0.3701965974341616\n",
      "Gradient Descent(1379/9999): loss=478.2315904506535, w0=0.032211331528575686, w1=0.37015680655690125\n",
      "Gradient Descent(1380/9999): loss=478.0434314537732, w0=0.032209747112913606, w1=0.3701170282044487\n",
      "Gradient Descent(1381/9999): loss=477.8557349947679, w0=0.03220815981227157, w1=0.37007726235306904\n",
      "Gradient Descent(1382/9999): loss=477.6684988268642, w0=0.0322065696347809, w1=0.3700375089790876\n",
      "Gradient Descent(1383/9999): loss=477.4817207147817, w0=0.03220497658855192, w1=0.3699977680588898\n",
      "Gradient Descent(1384/9999): loss=477.2953984346744, w0=0.03220338068167401, w1=0.3699580395689209\n",
      "Gradient Descent(1385/9999): loss=477.10952977407226, w0=0.032201781922215664, w1=0.36991832348568604\n",
      "Gradient Descent(1386/9999): loss=476.9241125318224, w0=0.03220018031822452, w1=0.3698786197857498\n",
      "Gradient Descent(1387/9999): loss=476.73914451803165, w0=0.03219857587772743, w1=0.3698389284457364\n",
      "Gradient Descent(1388/9999): loss=476.5546235540084, w0=0.03219696860873054, w1=0.36979924944232917\n",
      "Gradient Descent(1389/9999): loss=476.37054747220503, w0=0.03219535851921928, w1=0.3697595827522708\n",
      "Gradient Descent(1390/9999): loss=476.18691411616095, w0=0.03219374561715848, w1=0.3697199283523628\n",
      "Gradient Descent(1391/9999): loss=476.00372134044574, w0=0.03219212991049239, w1=0.3696802862194656\n",
      "Gradient Descent(1392/9999): loss=475.82096701060254, w0=0.032190511407144747, w1=0.3696406563304983\n",
      "Gradient Descent(1393/9999): loss=475.63864900309187, w0=0.03218889011501882, w1=0.3696010386624386\n",
      "Gradient Descent(1394/9999): loss=475.45676520523523, w0=0.03218726604199745, w1=0.36956143319232254\n",
      "Gradient Descent(1395/9999): loss=475.2753135151599, w0=0.03218563919594315, w1=0.3695218398972444\n",
      "Gradient Descent(1396/9999): loss=475.0942918417428, w0=0.0321840095846981, w1=0.36948225875435664\n",
      "Gradient Descent(1397/9999): loss=474.9136981045564, w0=0.03218237721608424, w1=0.3694426897408695\n",
      "Gradient Descent(1398/9999): loss=474.733530233813, w0=0.03218074209790329, w1=0.36940313283405124\n",
      "Gradient Descent(1399/9999): loss=474.5537861703104, w0=0.03217910423793683, w1=0.36936358801122754\n",
      "Gradient Descent(1400/9999): loss=474.3744638653774, w0=0.03217746364394635, w1=0.3693240552497817\n",
      "Gradient Descent(1401/9999): loss=474.1955612808202, w0=0.03217582032367329, w1=0.3692845345271543\n",
      "Gradient Descent(1402/9999): loss=474.0170763888688, w0=0.03217417428483909, w1=0.36924502582084323\n",
      "Gradient Descent(1403/9999): loss=473.8390071721227, w0=0.03217252553514525, w1=0.3692055291084034\n",
      "Gradient Descent(1404/9999): loss=473.6613516234982, w0=0.03217087408227338, w1=0.36916604436744654\n",
      "Gradient Descent(1405/9999): loss=473.4841077461756, w0=0.032169219933885246, w1=0.36912657157564127\n",
      "Gradient Descent(1406/9999): loss=473.30727355354657, w0=0.03216756309762284, w1=0.36908711071071276\n",
      "Gradient Descent(1407/9999): loss=473.13084706916123, w0=0.03216590358110839, w1=0.3690476617504426\n",
      "Gradient Descent(1408/9999): loss=472.9548263266765, w0=0.03216424139194448, w1=0.3690082246726689\n",
      "Gradient Descent(1409/9999): loss=472.77920936980473, w0=0.03216257653771401, w1=0.36896879945528577\n",
      "Gradient Descent(1410/9999): loss=472.6039942522607, w0=0.032160909025980323, w1=0.36892938607624337\n",
      "Gradient Descent(1411/9999): loss=472.4291790377118, w0=0.032159238864287225, w1=0.3688899845135478\n",
      "Gradient Descent(1412/9999): loss=472.2547617997261, w0=0.032157566060159036, w1=0.36885059474526094\n",
      "Gradient Descent(1413/9999): loss=472.08074062172193, w0=0.03215589062110064, w1=0.36881121674950024\n",
      "Gradient Descent(1414/9999): loss=471.907113596917, w0=0.03215421255459755, w1=0.3687718505044386\n",
      "Gradient Descent(1415/9999): loss=471.73387882827876, w0=0.03215253186811593, w1=0.3687324959883043\n",
      "Gradient Descent(1416/9999): loss=471.5610344284737, w0=0.03215084856910267, w1=0.3686931531793808\n",
      "Gradient Descent(1417/9999): loss=471.3885785198178, w0=0.03214916266498543, w1=0.36865382205600644\n",
      "Gradient Descent(1418/9999): loss=471.21650923422766, w0=0.03214747416317267, w1=0.3686145025965747\n",
      "Gradient Descent(1419/9999): loss=471.04482471317, w0=0.032145783071053737, w1=0.36857519477953365\n",
      "Gradient Descent(1420/9999): loss=470.8735231076141, w0=0.03214408939599887, w1=0.36853589858338603\n",
      "Gradient Descent(1421/9999): loss=470.70260257798185, w0=0.0321423931453593, w1=0.36849661398668904\n",
      "Gradient Descent(1422/9999): loss=470.5320612940997, w0=0.032140694326467246, w1=0.3684573409680542\n",
      "Gradient Descent(1423/9999): loss=470.36189743515104, w0=0.03213899294663601, w1=0.3684180795061473\n",
      "Gradient Descent(1424/9999): loss=470.192109189627, w0=0.03213728901315999, w1=0.3683788295796881\n",
      "Gradient Descent(1425/9999): loss=470.02269475527964, w0=0.032135582533314756, w1=0.36833959116745035\n",
      "Gradient Descent(1426/9999): loss=469.8536523390744, w0=0.03213387351435708, w1=0.3683003642482615\n",
      "Gradient Descent(1427/9999): loss=469.68498015714215, w0=0.03213216196352499, w1=0.3682611488010027\n",
      "Gradient Descent(1428/9999): loss=469.5166764347328, w0=0.03213044788803782, w1=0.36822194480460857\n",
      "Gradient Descent(1429/9999): loss=469.34873940616836, w0=0.032128731295096274, w1=0.3681827522380671\n",
      "Gradient Descent(1430/9999): loss=469.18116731479654, w0=0.03212701219188244, w1=0.36814357108041956\n",
      "Gradient Descent(1431/9999): loss=469.0139584129438, w0=0.03212529058555985, w1=0.3681044013107603\n",
      "Gradient Descent(1432/9999): loss=468.8471109618702, w0=0.03212356648327356, w1=0.3680652429082365\n",
      "Gradient Descent(1433/9999): loss=468.680623231723, w0=0.03212183989215015, w1=0.3680260958520483\n",
      "Gradient Descent(1434/9999): loss=468.5144935014913, w0=0.032120110819297804, w1=0.36798696012144855\n",
      "Gradient Descent(1435/9999): loss=468.34872005896085, w0=0.032118379271806344, w1=0.36794783569574246\n",
      "Gradient Descent(1436/9999): loss=468.1833012006683, w0=0.032116645256747275, w1=0.36790872255428786\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(1437/9999): loss=468.01823523185686, w0=0.032114908781173845, w1=0.3678696206764947\n",
      "Gradient Descent(1438/9999): loss=467.85352046643163, w0=0.032113169852121086, w1=0.3678305300418252\n",
      "Gradient Descent(1439/9999): loss=467.6891552269147, w0=0.03211142847660585, w1=0.3677914506297935\n",
      "Gradient Descent(1440/9999): loss=467.52513784440146, w0=0.03210968466162687, w1=0.3677523824199657\n",
      "Gradient Descent(1441/9999): loss=467.36146665851663, w0=0.0321079384141648, w1=0.3677133253919596\n",
      "Gradient Descent(1442/9999): loss=467.1981400173702, w0=0.032106189741182276, w1=0.36767427952544457\n",
      "Gradient Descent(1443/9999): loss=467.0351562775138, w0=0.03210443864962394, w1=0.3676352448001415\n",
      "Gradient Descent(1444/9999): loss=466.8725138038979, w0=0.03210268514641649, w1=0.36759622119582264\n",
      "Gradient Descent(1445/9999): loss=466.7102109698284, w0=0.032100929238468745, w1=0.3675572086923114\n",
      "Gradient Descent(1446/9999): loss=466.5482461569235, w0=0.03209917093267167, w1=0.36751820726948237\n",
      "Gradient Descent(1447/9999): loss=466.38661775507194, w0=0.032097410235898446, w1=0.367479216907261\n",
      "Gradient Descent(1448/9999): loss=466.22532416238954, w0=0.03209564715500448, w1=0.36744023758562355\n",
      "Gradient Descent(1449/9999): loss=466.06436378517805, w0=0.03209388169682749, w1=0.3674012692845971\n",
      "Gradient Descent(1450/9999): loss=465.9037350378819, w0=0.03209211386818752, w1=0.3673623119842591\n",
      "Gradient Descent(1451/9999): loss=465.7434363430474, w0=0.032090343675887006, w1=0.3673233656647376\n",
      "Gradient Descent(1452/9999): loss=465.58346613128106, w0=0.032088571126710805, w1=0.3672844303062109\n",
      "Gradient Descent(1453/9999): loss=465.42382284120805, w0=0.03208679622742626, w1=0.36724550588890736\n",
      "Gradient Descent(1454/9999): loss=465.26450491943103, w0=0.03208501898478323, w1=0.36720659239310555\n",
      "Gradient Descent(1455/9999): loss=465.1055108204892, w0=0.03208323940551412, w1=0.3671676897991339\n",
      "Gradient Descent(1456/9999): loss=464.9468390068179, w0=0.03208145749633398, w1=0.36712879808737053\n",
      "Gradient Descent(1457/9999): loss=464.7884879487079, w0=0.03207967326394049, w1=0.36708991723824336\n",
      "Gradient Descent(1458/9999): loss=464.63045612426504, w0=0.032077886715014034, w1=0.3670510472322297\n",
      "Gradient Descent(1459/9999): loss=464.4727420193701, w0=0.03207609785621775, w1=0.36701218804985636\n",
      "Gradient Descent(1460/9999): loss=464.3153441276393, w0=0.032074306694197544, w1=0.36697333967169943\n",
      "Gradient Descent(1461/9999): loss=464.15826095038415, w0=0.03207251323558217, w1=0.36693450207838413\n",
      "Gradient Descent(1462/9999): loss=464.0014909965724, w0=0.03207071748698326, w1=0.36689567525058464\n",
      "Gradient Descent(1463/9999): loss=463.8450327827881, w0=0.032068919454995355, w1=0.36685685916902416\n",
      "Gradient Descent(1464/9999): loss=463.68888483319375, w0=0.03206711914619598, w1=0.3668180538144745\n",
      "Gradient Descent(1465/9999): loss=463.53304567949, w0=0.03206531656714564, w1=0.36677925916775633\n",
      "Gradient Descent(1466/9999): loss=463.3775138608778, w0=0.03206351172438793, w1=0.3667404752097386\n",
      "Gradient Descent(1467/9999): loss=463.2222879240199, w0=0.032061704624449516, w1=0.3667017019213389\n",
      "Gradient Descent(1468/9999): loss=463.0673664230026, w0=0.03205989527384021, w1=0.36666293928352284\n",
      "Gradient Descent(1469/9999): loss=462.91274791929726, w0=0.03205808367905301, w1=0.3666241872773044\n",
      "Gradient Descent(1470/9999): loss=462.7584309817225, w0=0.03205626984656414, w1=0.3665854458837455\n",
      "Gradient Descent(1471/9999): loss=462.60441418640727, w0=0.03205445378283309, w1=0.36654671508395587\n",
      "Gradient Descent(1472/9999): loss=462.4506961167524, w0=0.032052635494302664, w1=0.3665079948590932\n",
      "Gradient Descent(1473/9999): loss=462.29727536339374, w0=0.03205081498739903, w1=0.3664692851903627\n",
      "Gradient Descent(1474/9999): loss=462.14415052416496, w0=0.032048992268531744, w1=0.3664305860590172\n",
      "Gradient Descent(1475/9999): loss=461.9913202040608, w0=0.03204716734409382, w1=0.3663918974463568\n",
      "Gradient Descent(1476/9999): loss=461.83878301519997, w0=0.03204534022046173, w1=0.36635321933372905\n",
      "Gradient Descent(1477/9999): loss=461.68653757678925, w0=0.03204351090399551, w1=0.36631455170252863\n",
      "Gradient Descent(1478/9999): loss=461.53458251508624, w0=0.032041679401038727, w1=0.3662758945341972\n",
      "Gradient Descent(1479/9999): loss=461.38291646336404, w0=0.03203984571791858, w1=0.3662372478102234\n",
      "Gradient Descent(1480/9999): loss=461.2315380618746, w0=0.032038009860945925, w1=0.3661986115121426\n",
      "Gradient Descent(1481/9999): loss=461.0804459578136, w0=0.03203617183641531, w1=0.366159985621537\n",
      "Gradient Descent(1482/9999): loss=460.929638805284, w0=0.03203433165060501, w1=0.36612137012003526\n",
      "Gradient Descent(1483/9999): loss=460.7791152652617, w0=0.03203248930977709, w1=0.3660827649893125\n",
      "Gradient Descent(1484/9999): loss=460.6288740055588, w0=0.032030644820177444, w1=0.3660441702110902\n",
      "Gradient Descent(1485/9999): loss=460.47891370079014, w0=0.03202879818803581, w1=0.36600558576713604\n",
      "Gradient Descent(1486/9999): loss=460.3292330323376, w0=0.03202694941956586, w1=0.3659670116392637\n",
      "Gradient Descent(1487/9999): loss=460.1798306883149, w0=0.032025098520965176, w1=0.365928447809333\n",
      "Gradient Descent(1488/9999): loss=460.03070536353414, w0=0.032023245498415356, w1=0.3658898942592494\n",
      "Gradient Descent(1489/9999): loss=459.88185575947074, w0=0.03202139035808202, w1=0.36585135097096433\n",
      "Gradient Descent(1490/9999): loss=459.73328058422936, w0=0.03201953310611485, w1=0.36581281792647463\n",
      "Gradient Descent(1491/9999): loss=459.58497855251045, w0=0.03201767374864765, w1=0.3657742951078228\n",
      "Gradient Descent(1492/9999): loss=459.43694838557514, w0=0.03201581229179837, w1=0.3657357824970966\n",
      "Gradient Descent(1493/9999): loss=459.28918881121297, w0=0.03201394874166915, w1=0.36569728007642915\n",
      "Gradient Descent(1494/9999): loss=459.14169856370813, w0=0.032012083104346375, w1=0.36565878782799865\n",
      "Gradient Descent(1495/9999): loss=458.994476383805, w0=0.03201021538590069, w1=0.3656203057340284\n",
      "Gradient Descent(1496/9999): loss=458.84752101867656, w0=0.032008345592387064, w1=0.36558183377678655\n",
      "Gradient Descent(1497/9999): loss=458.7008312218905, w0=0.03200647372984482, w1=0.3655433719385861\n",
      "Gradient Descent(1498/9999): loss=458.55440575337656, w0=0.032004599804297665, w1=0.36550492020178477\n",
      "Gradient Descent(1499/9999): loss=458.408243379394, w0=0.032002723821753766, w1=0.3654664785487847\n",
      "Gradient Descent(1500/9999): loss=458.2623428724991, w0=0.032000845788205744, w1=0.36542804696203274\n",
      "Gradient Descent(1501/9999): loss=458.11670301151355, w0=0.03199896570963074, w1=0.36538962542401987\n",
      "Gradient Descent(1502/9999): loss=457.9713225814903, w0=0.03199708359199046, w1=0.36535121391728137\n",
      "Gradient Descent(1503/9999): loss=457.82620037368446, w0=0.031995199441231185, w1=0.36531281242439667\n",
      "Gradient Descent(1504/9999): loss=457.6813351855186, w0=0.031993313263283855, w1=0.3652744209279892\n",
      "Gradient Descent(1505/9999): loss=457.53672582055344, w0=0.03199142506406407, w1=0.36523603941072613\n",
      "Gradient Descent(1506/9999): loss=457.3923710884551, w0=0.031989534849472145, w1=0.3651976678553187\n",
      "Gradient Descent(1507/9999): loss=457.248269804964, w0=0.03198764262539316, w1=0.3651593062445216\n",
      "Gradient Descent(1508/9999): loss=457.10442079186396, w0=0.03198574839769696, w1=0.3651209545611331\n",
      "Gradient Descent(1509/9999): loss=456.960822876951, w0=0.03198385217223826, w1=0.3650826127879949\n",
      "Gradient Descent(1510/9999): loss=456.81747489400266, w0=0.03198195395485661, w1=0.36504428090799224\n",
      "Gradient Descent(1511/9999): loss=456.67437568274704, w0=0.03198005375137649, w1=0.36500595890405324\n",
      "Gradient Descent(1512/9999): loss=456.5315240888322, w0=0.03197815156760732, w1=0.3649676467591494\n",
      "Gradient Descent(1513/9999): loss=456.3889189637969, w0=0.03197624740934351, w1=0.3649293444562951\n",
      "Gradient Descent(1514/9999): loss=456.2465591650388, w0=0.031974341282364496, w1=0.3648910519785476\n",
      "Gradient Descent(1515/9999): loss=456.1044435557853, w0=0.031972433192434785, w1=0.36485276930900695\n",
      "Gradient Descent(1516/9999): loss=455.9625710050634, w0=0.03197052314530398, w1=0.3648144964308159\n",
      "Gradient Descent(1517/9999): loss=455.82094038767053, w0=0.031968611146706814, w1=0.3647762333271598\n",
      "Gradient Descent(1518/9999): loss=455.6795505841439, w0=0.03196669720236322, w1=0.36473797998126634\n",
      "Gradient Descent(1519/9999): loss=455.5384004807318, w0=0.03196478131797833, w1=0.36469973637640557\n",
      "Gradient Descent(1520/9999): loss=455.39748896936425, w0=0.03196286349924255, w1=0.3646615024958898\n",
      "Gradient Descent(1521/9999): loss=455.2568149476235, w0=0.03196094375183157, w1=0.3646232783230735\n",
      "Gradient Descent(1522/9999): loss=455.11637731871554, w0=0.031959022081406406, w1=0.3645850638413531\n",
      "Gradient Descent(1523/9999): loss=454.976174991441, w0=0.03195709849361345, w1=0.3645468590341669\n",
      "Gradient Descent(1524/9999): loss=454.8362068801661, w0=0.0319551729940845, w1=0.3645086638849951\n",
      "Gradient Descent(1525/9999): loss=454.6964719047947, w0=0.0319532455884368, w1=0.3644704783773595\n",
      "Gradient Descent(1526/9999): loss=454.5569689907399, w0=0.031951316282273066, w1=0.3644323024948236\n",
      "Gradient Descent(1527/9999): loss=454.41769706889517, w0=0.031949385081181544, w1=0.36439413622099226\n",
      "Gradient Descent(1528/9999): loss=454.27865507560654, w0=0.03194745199073604, w1=0.3643559795395117\n",
      "Gradient Descent(1529/9999): loss=454.13984195264516, w0=0.03194551701649593, w1=0.3643178324340695\n",
      "Gradient Descent(1530/9999): loss=454.0012566471785, w0=0.03194358016400625, w1=0.3642796948883944\n",
      "Gradient Descent(1531/9999): loss=453.8628981117435, w0=0.03194164143879769, w1=0.3642415668862561\n",
      "Gradient Descent(1532/9999): loss=453.72476530421864, w0=0.03193970084638664, w1=0.3642034484114653\n",
      "Gradient Descent(1533/9999): loss=453.5868571877967, w0=0.031937758392275244, w1=0.36416533944787355\n",
      "Gradient Descent(1534/9999): loss=453.4491727309572, w0=0.03193581408195141, w1=0.36412723997937313\n",
      "Gradient Descent(1535/9999): loss=453.3117109074398, w0=0.03193386792088887, w1=0.36408914998989694\n",
      "Gradient Descent(1536/9999): loss=453.17447069621704, w0=0.0319319199145472, w1=0.3640510694634184\n",
      "Gradient Descent(1537/9999): loss=453.037451081467, w0=0.031929970068371866, w1=0.3640129983839514\n",
      "Gradient Descent(1538/9999): loss=452.90065105254774, w0=0.031928018387794264, w1=0.36397493673555015\n",
      "Gradient Descent(1539/9999): loss=452.76406960396963, w0=0.03192606487823174, w1=0.363936884502309\n",
      "Gradient Descent(1540/9999): loss=452.6277057353698, w0=0.03192410954508765, w1=0.36389884166836245\n",
      "Gradient Descent(1541/9999): loss=452.49155845148493, w0=0.03192215239375136, w1=0.3638608082178851\n",
      "Gradient Descent(1542/9999): loss=452.3556267621261, w0=0.03192019342959833, w1=0.3638227841350913\n",
      "Gradient Descent(1543/9999): loss=452.2199096821518, w0=0.031918232657990105, w1=0.3637847694042353\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(1544/9999): loss=452.0844062314429, w0=0.031916270084274376, w1=0.36374676400961115\n",
      "Gradient Descent(1545/9999): loss=451.9491154348763, w0=0.03191430571378501, w1=0.3637087679355523\n",
      "Gradient Descent(1546/9999): loss=451.8140363222994, w0=0.031912339551842084, w1=0.3636707811664318\n",
      "Gradient Descent(1547/9999): loss=451.67916792850497, w0=0.031910371603751916, w1=0.3636328036866622\n",
      "Gradient Descent(1548/9999): loss=451.5445092932053, w0=0.03190840187480712, w1=0.3635948354806951\n",
      "Gradient Descent(1549/9999): loss=451.41005946100745, w0=0.03190643037028661, w1=0.3635568765330215\n",
      "Gradient Descent(1550/9999): loss=451.27581748138743, w0=0.03190445709545567, w1=0.36351892682817144\n",
      "Gradient Descent(1551/9999): loss=451.1417824086662, w0=0.03190248205556595, w1=0.3634809863507139\n",
      "Gradient Descent(1552/9999): loss=451.0079533019841, w0=0.03190050525585554, w1=0.36344305508525687\n",
      "Gradient Descent(1553/9999): loss=450.87432922527626, w0=0.03189852670154898, w1=0.363405133016447\n",
      "Gradient Descent(1554/9999): loss=450.74090924724817, w0=0.03189654639785731, w1=0.36336722012896977\n",
      "Gradient Descent(1555/9999): loss=450.6076924413511, w0=0.031894564349978075, w1=0.3633293164075492\n",
      "Gradient Descent(1556/9999): loss=450.47467788575796, w0=0.03189258056309541, w1=0.3632914218369477\n",
      "Gradient Descent(1557/9999): loss=450.3418646633387, w0=0.031890595042380024, w1=0.3632535364019662\n",
      "Gradient Descent(1558/9999): loss=450.2092518616366, w0=0.03188860779298927, w1=0.36321566008744405\n",
      "Gradient Descent(1559/9999): loss=450.0768385728439, w0=0.031886618820067165, w1=0.3631777928782585\n",
      "Gradient Descent(1560/9999): loss=449.9446238937786, w0=0.031884628128744416, w1=0.36313993475932516\n",
      "Gradient Descent(1561/9999): loss=449.8126069258603, w0=0.03188263572413847, w1=0.3631020857155975\n",
      "Gradient Descent(1562/9999): loss=449.6807867750859, w0=0.031880641611353545, w1=0.3630642457320669\n",
      "Gradient Descent(1563/9999): loss=449.5491625520079, w0=0.03187864579548065, w1=0.3630264147937627\n",
      "Gradient Descent(1564/9999): loss=449.41773337170923, w0=0.031876648281597635, w1=0.36298859288575175\n",
      "Gradient Descent(1565/9999): loss=449.2864983537814, w0=0.031874649074769223, w1=0.3629507799931387\n",
      "Gradient Descent(1566/9999): loss=449.1554566223001, w0=0.031872648180047036, w1=0.3629129761010656\n",
      "Gradient Descent(1567/9999): loss=449.0246073058034, w0=0.03187064560246963, w1=0.36287518119471196\n",
      "Gradient Descent(1568/9999): loss=448.89394953726793, w0=0.03186864134706255, w1=0.3628373952592946\n",
      "Gradient Descent(1569/9999): loss=448.76348245408684, w0=0.031866635418838316, w1=0.3627996182800676\n",
      "Gradient Descent(1570/9999): loss=448.6332051980469, w0=0.031864627822796504, w1=0.3627618502423222\n",
      "Gradient Descent(1571/9999): loss=448.5031169153053, w0=0.03186261856392376, w1=0.36272409113138665\n",
      "Gradient Descent(1572/9999): loss=448.37321675636866, w0=0.03186060764719382, w1=0.3626863409326261\n",
      "Gradient Descent(1573/9999): loss=448.2435038760692, w0=0.03185859507756758, w1=0.3626485996314426\n",
      "Gradient Descent(1574/9999): loss=448.11397743354337, w0=0.03185658085999309, w1=0.36261086721327496\n",
      "Gradient Descent(1575/9999): loss=447.98463659220994, w0=0.03185456499940561, w1=0.3625731436635986\n",
      "Gradient Descent(1576/9999): loss=447.8554805197477, w0=0.03185254750072763, w1=0.3625354289679256\n",
      "Gradient Descent(1577/9999): loss=447.7265083880733, w0=0.03185052836886891, w1=0.3624977231118045\n",
      "Gradient Descent(1578/9999): loss=447.59771937332033, w0=0.03184850760872652, w1=0.3624600260808201\n",
      "Gradient Descent(1579/9999): loss=447.4691126558172, w0=0.03184648522518486, w1=0.36242233786059364\n",
      "Gradient Descent(1580/9999): loss=447.3406874200651, w0=0.03184446122311569, w1=0.3623846584367824\n",
      "Gradient Descent(1581/9999): loss=447.21244285471795, w0=0.03184243560737818, w1=0.36234698779507996\n",
      "Gradient Descent(1582/9999): loss=447.0843781525598, w0=0.031840408382818916, w1=0.3623093259212157\n",
      "Gradient Descent(1583/9999): loss=446.95649251048457, w0=0.031838379554271976, w1=0.362271672800955\n",
      "Gradient Descent(1584/9999): loss=446.8287851294741, w0=0.0318363491265589, w1=0.36223402842009916\n",
      "Gradient Descent(1585/9999): loss=446.70125521457805, w0=0.03183431710448879, w1=0.36219639276448506\n",
      "Gradient Descent(1586/9999): loss=446.57390197489246, w0=0.03183228349285828, w1=0.3621587658199853\n",
      "Gradient Descent(1587/9999): loss=446.4467246235395, w0=0.03183024829645161, w1=0.36212114757250796\n",
      "Gradient Descent(1588/9999): loss=446.3197223776458, w0=0.03182821152004066, w1=0.3620835380079967\n",
      "Gradient Descent(1589/9999): loss=446.19289445832356, w0=0.03182617316838493, w1=0.36204593711243044\n",
      "Gradient Descent(1590/9999): loss=446.0662400906485, w0=0.03182413324623164, w1=0.36200834487182343\n",
      "Gradient Descent(1591/9999): loss=445.9397585036403, w0=0.03182209175831571, w1=0.36197076127222505\n",
      "Gradient Descent(1592/9999): loss=445.81344893024266, w0=0.03182004870935983, w1=0.3619331862997199\n",
      "Gradient Descent(1593/9999): loss=445.6873106073022, w0=0.031818004104074454, w1=0.36189561994042735\n",
      "Gradient Descent(1594/9999): loss=445.56134277554975, w0=0.03181595794715786, w1=0.3618580621805019\n",
      "Gradient Descent(1595/9999): loss=445.4355446795793, w0=0.03181391024329617, w1=0.3618205130061328\n",
      "Gradient Descent(1596/9999): loss=445.30991556782845, w0=0.03181186099716338, w1=0.36178297240354407\n",
      "Gradient Descent(1597/9999): loss=445.1844546925594, w0=0.0318098102134214, w1=0.36174544035899425\n",
      "Gradient Descent(1598/9999): loss=445.0591613098383, w0=0.03180775789672008, w1=0.3617079168587766\n",
      "Gradient Descent(1599/9999): loss=444.9340346795166, w0=0.03180570405169723, w1=0.3616704018892188\n",
      "Gradient Descent(1600/9999): loss=444.8090740652109, w0=0.031803648682978664, w1=0.36163289543668276\n",
      "Gradient Descent(1601/9999): loss=444.6842787342845, w0=0.03180159179517823, w1=0.3615953974875649\n",
      "Gradient Descent(1602/9999): loss=444.55964795782734, w0=0.03179953339289785, w1=0.3615579080282958\n",
      "Gradient Descent(1603/9999): loss=444.43518101063756, w0=0.03179747348072753, w1=0.36152042704534\n",
      "Gradient Descent(1604/9999): loss=444.31087717120164, w0=0.03179541206324538, w1=0.3614829545251962\n",
      "Gradient Descent(1605/9999): loss=444.18673572167665, w0=0.031793349145017705, w1=0.36144549045439706\n",
      "Gradient Descent(1606/9999): loss=444.0627559478704, w0=0.03179128473059896, w1=0.361408034819509\n",
      "Gradient Descent(1607/9999): loss=443.93893713922347, w0=0.03178921882453183, w1=0.3613705876071323\n",
      "Gradient Descent(1608/9999): loss=443.8152785887899, w0=0.031787151431347256, w1=0.3613331488039008\n",
      "Gradient Descent(1609/9999): loss=443.69177959321905, w0=0.03178508255556443, w1=0.3612957183964821\n",
      "Gradient Descent(1610/9999): loss=443.5684394527372, w0=0.031783012201690865, w1=0.3612582963715772\n",
      "Gradient Descent(1611/9999): loss=443.4452574711295, w0=0.03178094037422241, w1=0.36122088271592045\n",
      "Gradient Descent(1612/9999): loss=443.3222329557203, w0=0.03177886707764328, w1=0.3611834774162797\n",
      "Gradient Descent(1613/9999): loss=443.1993652173571, w0=0.031776792316426085, w1=0.36114608045945595\n",
      "Gradient Descent(1614/9999): loss=443.0766535703909, w0=0.03177471609503186, w1=0.3611086918322834\n",
      "Gradient Descent(1615/9999): loss=442.95409733265865, w0=0.031772638417910094, w1=0.36107131152162925\n",
      "Gradient Descent(1616/9999): loss=442.8316958254658, w0=0.03177055928949876, w1=0.36103393951439383\n",
      "Gradient Descent(1617/9999): loss=442.7094483735681, w0=0.031768478714224356, w1=0.3609965757975103\n",
      "Gradient Descent(1618/9999): loss=442.58735430515384, w0=0.031766396696501914, w1=0.36095922035794464\n",
      "Gradient Descent(1619/9999): loss=442.4654129518265, w0=0.03176431324073504, w1=0.3609218731826956\n",
      "Gradient Descent(1620/9999): loss=442.3436236485874, w0=0.031762228351315945, w1=0.3608845342587945\n",
      "Gradient Descent(1621/9999): loss=442.22198573381735, w0=0.031760142032625484, w1=0.3608472035733054\n",
      "Gradient Descent(1622/9999): loss=442.1004985492607, w0=0.03175805428903315, w1=0.36080988111332474\n",
      "Gradient Descent(1623/9999): loss=441.97916144000703, w0=0.03175596512489715, w1=0.3607725668659813\n",
      "Gradient Descent(1624/9999): loss=441.8579737544745, w0=0.0317538745445644, w1=0.3607352608184363\n",
      "Gradient Descent(1625/9999): loss=441.73693484439207, w0=0.031751782552370565, w1=0.36069796295788326\n",
      "Gradient Descent(1626/9999): loss=441.616044064784, w0=0.031749689152640086, w1=0.3606606732715476\n",
      "Gradient Descent(1627/9999): loss=441.4953007739515, w0=0.03174759434968622, w1=0.36062339174668706\n",
      "Gradient Descent(1628/9999): loss=441.37470433345624, w0=0.03174549814781105, w1=0.3605861183705913\n",
      "Gradient Descent(1629/9999): loss=441.2542541081044, w0=0.031743400551305534, w1=0.3605488531305818\n",
      "Gradient Descent(1630/9999): loss=441.13394946592916, w0=0.03174130156444951, w1=0.36051159601401206\n",
      "Gradient Descent(1631/9999): loss=441.0137897781745, w0=0.03173920119151174, w1=0.3604743470082671\n",
      "Gradient Descent(1632/9999): loss=440.8937744192786, w0=0.03173709943674994, w1=0.36043710610076374\n",
      "Gradient Descent(1633/9999): loss=440.7739027668579, w0=0.031734996304410804, w1=0.36039987327895034\n",
      "Gradient Descent(1634/9999): loss=440.65417420169007, w0=0.03173289179873003, w1=0.3603626485303068\n",
      "Gradient Descent(1635/9999): loss=440.5345881076984, w0=0.031730785923932366, w1=0.36032543184234445\n",
      "Gradient Descent(1636/9999): loss=440.41514387193587, w0=0.0317286786842316, w1=0.3602882232026059\n",
      "Gradient Descent(1637/9999): loss=440.2958408845679, w0=0.03172657008383061, w1=0.36025102259866504\n",
      "Gradient Descent(1638/9999): loss=440.1766785388577, w0=0.03172446012692142, w1=0.36021383001812696\n",
      "Gradient Descent(1639/9999): loss=440.05765623114985, w0=0.03172234881768519, w1=0.3601766454486279\n",
      "Gradient Descent(1640/9999): loss=439.9387733608544, w0=0.03172023616029224, w1=0.36013946887783493\n",
      "Gradient Descent(1641/9999): loss=439.82002933043134, w0=0.031718122158902094, w1=0.3601023002934463\n",
      "Gradient Descent(1642/9999): loss=439.70142354537484, w0=0.03171600681766354, w1=0.360065139683191\n",
      "Gradient Descent(1643/9999): loss=439.582955414198, w0=0.03171389014071458, w1=0.3600279870348288\n",
      "Gradient Descent(1644/9999): loss=439.4646243484167, w0=0.031711772132182535, w1=0.3599908423361502\n",
      "Gradient Descent(1645/9999): loss=439.346429762535, w0=0.031709652796184026, w1=0.35995370557497625\n",
      "Gradient Descent(1646/9999): loss=439.22837107402927, w0=0.03170753213682501, w1=0.35991657673915867\n",
      "Gradient Descent(1647/9999): loss=439.1104477033332, w0=0.03170541015820083, w1=0.3598794558165796\n",
      "Gradient Descent(1648/9999): loss=438.9926590738224, w0=0.031703286864396205, w1=0.3598423427951515\n",
      "Gradient Descent(1649/9999): loss=438.8750046117997, w0=0.031701162259485285, w1=0.3598052376628172\n",
      "Gradient Descent(1650/9999): loss=438.75748374647986, w0=0.03169903634753168, w1=0.3597681404075498\n",
      "Gradient Descent(1651/9999): loss=438.6400959099744, w0=0.031696909132588456, w1=0.3597310510173524\n",
      "Gradient Descent(1652/9999): loss=438.52284053727755, w0=0.0316947806186982, w1=0.35969396948025845\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(1653/9999): loss=438.4057170662505, w0=0.03169265080989303, w1=0.35965689578433113\n",
      "Gradient Descent(1654/9999): loss=438.28872493760736, w0=0.03169051971019461, w1=0.3596198299176637\n",
      "Gradient Descent(1655/9999): loss=438.17186359490046, w0=0.03168838732361421, w1=0.3595827718683792\n",
      "Gradient Descent(1656/9999): loss=438.05513248450524, w0=0.03168625365415269, w1=0.3595457216246305\n",
      "Gradient Descent(1657/9999): loss=437.93853105560675, w0=0.031684118705800554, w1=0.35950867917460005\n",
      "Gradient Descent(1658/9999): loss=437.82205876018435, w0=0.031681982482537986, w1=0.35947164450650004\n",
      "Gradient Descent(1659/9999): loss=437.70571505299773, w0=0.03167984498833484, w1=0.35943461760857215\n",
      "Gradient Descent(1660/9999): loss=437.58949939157276, w0=0.031677706227150715, w1=0.35939759846908753\n",
      "Gradient Descent(1661/9999): loss=437.4734112361872, w0=0.03167556620293492, w1=0.3593605870763467\n",
      "Gradient Descent(1662/9999): loss=437.35745004985654, w0=0.03167342491962657, w1=0.35932358341867954\n",
      "Gradient Descent(1663/9999): loss=437.24161529832, w0=0.03167128238115455, w1=0.35928658748444514\n",
      "Gradient Descent(1664/9999): loss=437.12590645002643, w0=0.031669138591437586, w1=0.3592495992620317\n",
      "Gradient Descent(1665/9999): loss=437.01032297612113, w0=0.031666993554384244, w1=0.35921261873985666\n",
      "Gradient Descent(1666/9999): loss=436.8948643504307, w0=0.031664847273892974, w1=0.3591756459063663\n",
      "Gradient Descent(1667/9999): loss=436.7795300494507, w0=0.031662699753852115, w1=0.35913868075003597\n",
      "Gradient Descent(1668/9999): loss=436.6643195523307, w0=0.031660550998139955, w1=0.3591017232593698\n",
      "Gradient Descent(1669/9999): loss=436.54923234086186, w0=0.03165840101062471, w1=0.3590647734229008\n",
      "Gradient Descent(1670/9999): loss=436.43426789946244, w0=0.0316562497951646, w1=0.3590278312291906\n",
      "Gradient Descent(1671/9999): loss=436.319425715165, w0=0.03165409735560783, w1=0.35899089666682954\n",
      "Gradient Descent(1672/9999): loss=436.2047052776024, w0=0.03165194369579265, w1=0.3589539697244365\n",
      "Gradient Descent(1673/9999): loss=436.0901060789948, w0=0.03164978881954735, w1=0.3589170503906589\n",
      "Gradient Descent(1674/9999): loss=435.9756276141366, w0=0.03164763273069033, w1=0.35888013865417256\n",
      "Gradient Descent(1675/9999): loss=435.86126938038274, w0=0.031645475433030076, w1=0.3588432345036816\n",
      "Gradient Descent(1676/9999): loss=435.7470308776361, w0=0.03164331693036521, w1=0.3588063379279185\n",
      "Gradient Descent(1677/9999): loss=435.6329116083338, w0=0.03164115722648452, w1=0.3587694489156439\n",
      "Gradient Descent(1678/9999): loss=435.51891107743495, w0=0.03163899632516697, w1=0.3587325674556465\n",
      "Gradient Descent(1679/9999): loss=435.4050287924072, w0=0.03163683423018174, w1=0.35869569353674324\n",
      "Gradient Descent(1680/9999): loss=435.2912642632141, w0=0.031634670945288226, w1=0.3586588271477789\n",
      "Gradient Descent(1681/9999): loss=435.1776170023019, w0=0.03163250647423611, w1=0.3586219682776263\n",
      "Gradient Descent(1682/9999): loss=435.06408652458754, w0=0.031630340820765344, w1=0.358585116915186\n",
      "Gradient Descent(1683/9999): loss=434.9506723474456, w0=0.031628173988606186, w1=0.35854827304938636\n",
      "Gradient Descent(1684/9999): loss=434.8373739906952, w0=0.03162600598147924, w1=0.3585114366691834\n",
      "Gradient Descent(1685/9999): loss=434.72419097658866, w0=0.031623836803095454, w1=0.3584746077635609\n",
      "Gradient Descent(1686/9999): loss=434.6111228297977, w0=0.031621666457156174, w1=0.35843778632153006\n",
      "Gradient Descent(1687/9999): loss=434.498169077402, w0=0.031619494947353145, w1=0.3584009723321297\n",
      "Gradient Descent(1688/9999): loss=434.38532924887653, w0=0.031617322277368555, w1=0.358364165784426\n",
      "Gradient Descent(1689/9999): loss=434.2726028760789, w0=0.03161514845087504, w1=0.3583273666675124\n",
      "Gradient Descent(1690/9999): loss=434.1599894932382, w0=0.03161297347153572, w1=0.35829057497050976\n",
      "Gradient Descent(1691/9999): loss=434.0474886369413, w0=0.031610797343004225, w1=0.3582537906825661\n",
      "Gradient Descent(1692/9999): loss=433.93509984612217, w0=0.03160862006892472, w1=0.3582170137928565\n",
      "Gradient Descent(1693/9999): loss=433.8228226620492, w0=0.03160644165293191, w1=0.3581802442905832\n",
      "Gradient Descent(1694/9999): loss=433.71065662831296, w0=0.031604262098651095, w1=0.3581434821649755\n",
      "Gradient Descent(1695/9999): loss=433.59860129081505, w0=0.031602081409698175, w1=0.3581067274052895\n",
      "Gradient Descent(1696/9999): loss=433.4866561977558, w0=0.03159989958967968, w1=0.35806998000080814\n",
      "Gradient Descent(1697/9999): loss=433.37482089962225, w0=0.03159771664219279, w1=0.35803323994084135\n",
      "Gradient Descent(1698/9999): loss=433.263094949177, w0=0.031595532570825355, w1=0.3579965072147256\n",
      "Gradient Descent(1699/9999): loss=433.15147790144607, w0=0.03159334737915594, w1=0.35795978181182403\n",
      "Gradient Descent(1700/9999): loss=433.0399693137077, w0=0.03159116107075381, w1=0.35792306372152644\n",
      "Gradient Descent(1701/9999): loss=432.92856874548033, w0=0.03158897364917901, w1=0.3578863529332491\n",
      "Gradient Descent(1702/9999): loss=432.8172757585114, w0=0.03158678511798233, w1=0.35784964943643477\n",
      "Gradient Descent(1703/9999): loss=432.7060899167661, w0=0.03158459548070537, w1=0.35781295322055257\n",
      "Gradient Descent(1704/9999): loss=432.59501078641546, w0=0.031582404740880546, w1=0.3577762642750979\n",
      "Gradient Descent(1705/9999): loss=432.4840379358257, w0=0.03158021290203111, w1=0.3577395825895925\n",
      "Gradient Descent(1706/9999): loss=432.3731709355465, w0=0.0315780199676712, w1=0.35770290815358413\n",
      "Gradient Descent(1707/9999): loss=432.2624093582999, w0=0.03157582594130581, w1=0.35766624095664684\n",
      "Gradient Descent(1708/9999): loss=432.15175277896935, w0=0.03157363082643088, w1=0.35762958098838066\n",
      "Gradient Descent(1709/9999): loss=432.0412007745883, w0=0.03157143462653327, w1=0.35759292823841154\n",
      "Gradient Descent(1710/9999): loss=431.93075292432974, w0=0.0315692373450908, w1=0.35755628269639145\n",
      "Gradient Descent(1711/9999): loss=431.82040880949427, w0=0.03156703898557228, w1=0.35751964435199807\n",
      "Gradient Descent(1712/9999): loss=431.71016801350044, w0=0.031564839551437525, w1=0.357483013194935\n",
      "Gradient Descent(1713/9999): loss=431.6000301218728, w0=0.031562639046137375, w1=0.3574463892149314\n",
      "Gradient Descent(1714/9999): loss=431.4899947222316, w0=0.031560437473113725, w1=0.3574097724017422\n",
      "Gradient Descent(1715/9999): loss=431.38006140428246, w0=0.031558234835799545, w1=0.3573731627451479\n",
      "Gradient Descent(1716/9999): loss=431.2702297598042, w0=0.0315560311376189, w1=0.3573365602349545\n",
      "Gradient Descent(1717/9999): loss=431.16049938264047, w0=0.031553826381986984, w1=0.3572999648609934\n",
      "Gradient Descent(1718/9999): loss=431.0508698686868, w0=0.031551620572310125, w1=0.35726337661312146\n",
      "Gradient Descent(1719/9999): loss=430.94134081588203, w0=0.03154941371198583, w1=0.35722679548122077\n",
      "Gradient Descent(1720/9999): loss=430.8319118241966, w0=0.03154720580440278, w1=0.3571902214551988\n",
      "Gradient Descent(1721/9999): loss=430.7225824956225, w0=0.03154499685294089, w1=0.3571536545249881\n",
      "Gradient Descent(1722/9999): loss=430.613352434163, w0=0.03154278686097129, w1=0.35711709468054637\n",
      "Gradient Descent(1723/9999): loss=430.5042212458227, w0=0.03154057583185638, w1=0.3570805419118564\n",
      "Gradient Descent(1724/9999): loss=430.3951885385961, w0=0.03153836376894982, w1=0.3570439962089259\n",
      "Gradient Descent(1725/9999): loss=430.28625392245874, w0=0.031536150675596605, w1=0.35700745756178764\n",
      "Gradient Descent(1726/9999): loss=430.1774170093561, w0=0.03153393655513302, w1=0.3569709259604992\n",
      "Gradient Descent(1727/9999): loss=430.0686774131943, w0=0.031531721410886726, w1=0.3569344013951429\n",
      "Gradient Descent(1728/9999): loss=429.9600347498292, w0=0.03152950524617673, w1=0.3568978838558258\n",
      "Gradient Descent(1729/9999): loss=429.85148863705695, w0=0.031527288064313444, w1=0.35686137333267987\n",
      "Gradient Descent(1730/9999): loss=429.74303869460397, w0=0.03152506986859869, w1=0.35682486981586137\n",
      "Gradient Descent(1731/9999): loss=429.6346845441169, w0=0.03152285066232572, w1=0.3567883732955513\n",
      "Gradient Descent(1732/9999): loss=429.526425809153, w0=0.03152063044877926, w1=0.3567518837619551\n",
      "Gradient Descent(1733/9999): loss=429.4182621151701, w0=0.03151840923123549, w1=0.3567154012053026\n",
      "Gradient Descent(1734/9999): loss=429.3101930895169, w0=0.03151618701296212, w1=0.3566789256158481\n",
      "Gradient Descent(1735/9999): loss=429.2022183614234, w0=0.03151396379721836, w1=0.35664245698387004\n",
      "Gradient Descent(1736/9999): loss=429.09433756199127, w0=0.03151173958725497, w1=0.35660599529967124\n",
      "Gradient Descent(1737/9999): loss=428.9865503241843, w0=0.03150951438631429, w1=0.35656954055357865\n",
      "Gradient Descent(1738/9999): loss=428.8788562828182, w0=0.03150728819763023, w1=0.35653309273594325\n",
      "Gradient Descent(1739/9999): loss=428.77125507455247, w0=0.03150506102442833, w1=0.3564966518371402\n",
      "Gradient Descent(1740/9999): loss=428.6637463378793, w0=0.03150283286992574, w1=0.3564602178475686\n",
      "Gradient Descent(1741/9999): loss=428.5563297131157, w0=0.031500603737331274, w1=0.35642379075765146\n",
      "Gradient Descent(1742/9999): loss=428.4490048423929, w0=0.03149837362984542, w1=0.35638737055783565\n",
      "Gradient Descent(1743/9999): loss=428.341771369648, w0=0.03149614255066037, w1=0.35635095723859184\n",
      "Gradient Descent(1744/9999): loss=428.23462894061424, w0=0.031493910502960014, w1=0.35631455079041446\n",
      "Gradient Descent(1745/9999): loss=428.1275772028115, w0=0.031491677489919996, w1=0.35627815120382167\n",
      "Gradient Descent(1746/9999): loss=428.020615805538, w0=0.03148944351470772, w1=0.3562417584693552\n",
      "Gradient Descent(1747/9999): loss=427.91374439986055, w0=0.03148720858048236, w1=0.35620537257758034\n",
      "Gradient Descent(1748/9999): loss=427.8069626386052, w0=0.0314849726903949, w1=0.3561689935190859\n",
      "Gradient Descent(1749/9999): loss=427.7002701763493, w0=0.03148273584758815, w1=0.3561326212844841\n",
      "Gradient Descent(1750/9999): loss=427.5936666694116, w0=0.03148049805519676, w1=0.3560962558644105\n",
      "Gradient Descent(1751/9999): loss=427.48715177584364, w0=0.03147825931634724, w1=0.3560598972495242\n",
      "Gradient Descent(1752/9999): loss=427.3807251554204, w0=0.031476019634158, w1=0.35602354543050724\n",
      "Gradient Descent(1753/9999): loss=427.2743864696325, w0=0.03147377901173934, w1=0.35598720039806514\n",
      "Gradient Descent(1754/9999): loss=427.16813538167685, w0=0.0314715374521935, w1=0.35595086214292637\n",
      "Gradient Descent(1755/9999): loss=427.0619715564471, w0=0.03146929495861466, w1=0.3559145306558426\n",
      "Gradient Descent(1756/9999): loss=426.95589466052644, w0=0.031467051534088984, w1=0.35587820592758845\n",
      "Gradient Descent(1757/9999): loss=426.84990436217777, w0=0.03146480718169461, w1=0.35584188794896154\n",
      "Gradient Descent(1758/9999): loss=426.7440003313358, w0=0.03146256190450168, w1=0.3558055767107824\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(1759/9999): loss=426.63818223959805, w0=0.031460315705572395, w1=0.35576927220389437\n",
      "Gradient Descent(1760/9999): loss=426.5324497602165, w0=0.031458068587960984, w1=0.3557329744191636\n",
      "Gradient Descent(1761/9999): loss=426.42680256808904, w0=0.031455820554713756, w1=0.35569668334747895\n",
      "Gradient Descent(1762/9999): loss=426.32124033975117, w0=0.031453571608869114, w1=0.355660398979752\n",
      "Gradient Descent(1763/9999): loss=426.21576275336747, w0=0.03145132175345757, w1=0.3556241213069169\n",
      "Gradient Descent(1764/9999): loss=426.1103694887232, w0=0.03144907099150177, w1=0.35558785031993034\n",
      "Gradient Descent(1765/9999): loss=426.0050602272164, w0=0.03144681932601651, w1=0.3555515860097716\n",
      "Gradient Descent(1766/9999): loss=425.8998346518489, w0=0.03144456676000877, w1=0.35551532836744226\n",
      "Gradient Descent(1767/9999): loss=425.7946924472188, w0=0.03144231329647771, w1=0.3554790773839664\n",
      "Gradient Descent(1768/9999): loss=425.6896332995121, w0=0.03144005893841471, w1=0.3554428330503903\n",
      "Gradient Descent(1769/9999): loss=425.584656896494, w0=0.031437803688803376, w1=0.35540659535778263\n",
      "Gradient Descent(1770/9999): loss=425.4797629275018, w0=0.03143554755061958, w1=0.35537036429723423\n",
      "Gradient Descent(1771/9999): loss=425.3749510834361, w0=0.031433290526831456, w1=0.355334139859858\n",
      "Gradient Descent(1772/9999): loss=425.2702210567527, w0=0.031431032620399445, w1=0.3552979220367891\n",
      "Gradient Descent(1773/9999): loss=425.1655725414557, w0=0.03142877383427629, w1=0.3552617108191846\n",
      "Gradient Descent(1774/9999): loss=425.06100523308777, w0=0.03142651417140706, w1=0.35522550619822363\n",
      "Gradient Descent(1775/9999): loss=424.95651882872426, w0=0.03142425363472919, w1=0.3551893081651072\n",
      "Gradient Descent(1776/9999): loss=424.85211302696393, w0=0.03142199222717249, w1=0.35515311671105815\n",
      "Gradient Descent(1777/9999): loss=424.74778752792145, w0=0.031419729951659155, w1=0.35511693182732124\n",
      "Gradient Descent(1778/9999): loss=424.64354203321994, w0=0.03141746681110378, w1=0.3550807535051629\n",
      "Gradient Descent(1779/9999): loss=424.53937624598325, w0=0.0314152028084134, w1=0.3550445817358713\n",
      "Gradient Descent(1780/9999): loss=424.4352898708278, w0=0.031412937946487515, w1=0.3550084165107562\n",
      "Gradient Descent(1781/9999): loss=424.33128261385525, w0=0.03141067222821807, w1=0.3549722578211491\n",
      "Gradient Descent(1782/9999): loss=424.2273541826451, w0=0.031408405656489516, w1=0.3549361056584029\n",
      "Gradient Descent(1783/9999): loss=424.12350428624677, w0=0.0314061382341788, w1=0.3548999600138919\n",
      "Gradient Descent(1784/9999): loss=424.0197326351718, w0=0.03140386996415541, w1=0.354863820879012\n",
      "Gradient Descent(1785/9999): loss=423.9160389413873, w0=0.03140160084928137, w1=0.35482768824518046\n",
      "Gradient Descent(1786/9999): loss=423.8124229183078, w0=0.03139933089241127, w1=0.3547915621038357\n",
      "Gradient Descent(1787/9999): loss=423.7088842807875, w0=0.031397060096392294, w1=0.3547554424464376\n",
      "Gradient Descent(1788/9999): loss=423.60542274511386, w0=0.031394788464064224, w1=0.3547193292644671\n",
      "Gradient Descent(1789/9999): loss=423.50203802899944, w0=0.031392515998259464, w1=0.3546832225494263\n",
      "Gradient Descent(1790/9999): loss=423.398729851575, w0=0.031390242701803066, w1=0.35464712229283846\n",
      "Gradient Descent(1791/9999): loss=423.2954979333823, w0=0.031387968577512734, w1=0.3546110284862478\n",
      "Gradient Descent(1792/9999): loss=423.19234199636634, w0=0.03138569362819886, w1=0.3545749411212196\n",
      "Gradient Descent(1793/9999): loss=423.08926176386876, w0=0.03138341785666453, w1=0.3545388601893401\n",
      "Gradient Descent(1794/9999): loss=422.9862569606205, w0=0.03138114126570554, w1=0.3545027856822163\n",
      "Gradient Descent(1795/9999): loss=422.88332731273437, w0=0.03137886385811045, w1=0.3544667175914762\n",
      "Gradient Descent(1796/9999): loss=422.7804725476985, w0=0.03137658563666054, w1=0.3544306559087684\n",
      "Gradient Descent(1797/9999): loss=422.67769239436893, w0=0.03137430660412989, w1=0.3543946006257623\n",
      "Gradient Descent(1798/9999): loss=422.5749865829624, w0=0.03137202676328535, w1=0.35435855173414793\n",
      "Gradient Descent(1799/9999): loss=422.47235484505035, w0=0.031369746116886595, w1=0.354322509225636\n",
      "Gradient Descent(1800/9999): loss=422.3697969135507, w0=0.031367464667686126, w1=0.35428647309195777\n",
      "Gradient Descent(1801/9999): loss=422.26731252272157, w0=0.0313651824184293, w1=0.35425044332486494\n",
      "Gradient Descent(1802/9999): loss=422.1649014081549, w0=0.03136289937185432, w1=0.35421441991612973\n",
      "Gradient Descent(1803/9999): loss=422.0625633067684, w0=0.03136061553069228, w1=0.35417840285754476\n",
      "Gradient Descent(1804/9999): loss=421.9602979568002, w0=0.0313583308976672, w1=0.3541423921409229\n",
      "Gradient Descent(1805/9999): loss=421.8581050978006, w0=0.03135604547549599, w1=0.3541063877580975\n",
      "Gradient Descent(1806/9999): loss=421.75598447062674, w0=0.0313537592668885, w1=0.35407038970092203\n",
      "Gradient Descent(1807/9999): loss=421.65393581743484, w0=0.03135147227454756, w1=0.3540343979612702\n",
      "Gradient Descent(1808/9999): loss=421.55195888167395, w0=0.031349184501168956, w1=0.35399841253103576\n",
      "Gradient Descent(1809/9999): loss=421.4500534080798, w0=0.031346895949441476, w1=0.3539624334021327\n",
      "Gradient Descent(1810/9999): loss=421.34821914266723, w0=0.03134460662204692, w1=0.35392646056649496\n",
      "Gradient Descent(1811/9999): loss=421.2464558327241, w0=0.031342316521660106, w1=0.35389049401607653\n",
      "Gradient Descent(1812/9999): loss=421.14476322680554, w0=0.03134002565094892, w1=0.3538545337428513\n",
      "Gradient Descent(1813/9999): loss=421.04314107472584, w0=0.03133773401257429, w1=0.35381857973881303\n",
      "Gradient Descent(1814/9999): loss=420.94158912755347, w0=0.03133544160919024, w1=0.3537826319959753\n",
      "Gradient Descent(1815/9999): loss=420.840107137604, w0=0.031333148443443884, w1=0.35374669050637153\n",
      "Gradient Descent(1816/9999): loss=420.73869485843335, w0=0.03133085451797547, w1=0.35371075526205487\n",
      "Gradient Descent(1817/9999): loss=420.63735204483237, w0=0.03132855983541838, w1=0.35367482625509805\n",
      "Gradient Descent(1818/9999): loss=420.5360784528199, w0=0.031326264398399126, w1=0.3536389034775936\n",
      "Gradient Descent(1819/9999): loss=420.43487383963634, w0=0.031323968209537416, w1=0.3536029869216535\n",
      "Gradient Descent(1820/9999): loss=420.3337379637379, w0=0.031321671271446146, w1=0.3535670765794093\n",
      "Gradient Descent(1821/9999): loss=420.23267058479007, w0=0.031319373586731404, w1=0.353531172443012\n",
      "Gradient Descent(1822/9999): loss=420.13167146366146, w0=0.031317075157992515, w1=0.35349527450463214\n",
      "Gradient Descent(1823/9999): loss=420.03074036241736, w0=0.031314775987822036, w1=0.35345938275645955\n",
      "Gradient Descent(1824/9999): loss=419.92987704431437, w0=0.03131247607880579, w1=0.3534234971907034\n",
      "Gradient Descent(1825/9999): loss=419.82908127379346, w0=0.03131017543352286, w1=0.35338761779959205\n",
      "Gradient Descent(1826/9999): loss=419.72835281647446, w0=0.031307874054545645, w1=0.3533517445753733\n",
      "Gradient Descent(1827/9999): loss=419.62769143914966, w0=0.03130557194443984, w1=0.353315877510314\n",
      "Gradient Descent(1828/9999): loss=419.52709690977787, w0=0.03130326910576448, w1=0.35328001659670016\n",
      "Gradient Descent(1829/9999): loss=419.4265689974789, w0=0.03130096554107193, w1=0.3532441618268368\n",
      "Gradient Descent(1830/9999): loss=419.3261074725269, w0=0.031298661252907915, w1=0.3532083131930482\n",
      "Gradient Descent(1831/9999): loss=419.2257121063452, w0=0.03129635624381156, w1=0.3531724706876773\n",
      "Gradient Descent(1832/9999): loss=419.1253826714996, w0=0.03129405051631536, w1=0.3531366343030863\n",
      "Gradient Descent(1833/9999): loss=419.02511894169277, w0=0.03129174407294524, w1=0.353100804031656\n",
      "Gradient Descent(1834/9999): loss=418.92492069175927, w0=0.03128943691622056, w1=0.3530649798657863\n",
      "Gradient Descent(1835/9999): loss=418.82478769765845, w0=0.031287129048654105, w1=0.3530291617978957\n",
      "Gradient Descent(1836/9999): loss=418.72471973646964, w0=0.03128482047275214, w1=0.3529933498204215\n",
      "Gradient Descent(1837/9999): loss=418.6247165863859, w0=0.031282511191014405, w1=0.3529575439258198\n",
      "Gradient Descent(1838/9999): loss=418.5247780267086, w0=0.031280201205934145, w1=0.3529217441065652\n",
      "Gradient Descent(1839/9999): loss=418.42490383784155, w0=0.0312778905199981, w1=0.35288595035515097\n",
      "Gradient Descent(1840/9999): loss=418.32509380128556, w0=0.03127557913568657, w1=0.3528501626640889\n",
      "Gradient Descent(1841/9999): loss=418.2253476996326, w0=0.03127326705547337, w1=0.3528143810259094\n",
      "Gradient Descent(1842/9999): loss=418.12566531656034, w0=0.0312709542818259, w1=0.35277860543316114\n",
      "Gradient Descent(1843/9999): loss=418.0260464368265, w0=0.03126864081720514, w1=0.3527428358784114\n",
      "Gradient Descent(1844/9999): loss=417.92649084626333, w0=0.03126632666406565, w1=0.35270707235424564\n",
      "Gradient Descent(1845/9999): loss=417.8269983317724, w0=0.03126401182485561, w1=0.3526713148532678\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(1846/9999): loss=417.7275686813186, w0=0.03126169630201684, w1=0.3526355633681\n",
      "Gradient Descent(1847/9999): loss=417.628201683925, w0=0.031259380097984804, w1=0.35259981789138256\n",
      "Gradient Descent(1848/9999): loss=417.5288971296675, w0=0.031257063215188625, w1=0.35256407841577403\n",
      "Gradient Descent(1849/9999): loss=417.42965480966933, w0=0.0312547456560511, w1=0.3525283449339511\n",
      "Gradient Descent(1850/9999): loss=417.3304745160956, w0=0.03125242742298873, w1=0.35249261743860844\n",
      "Gradient Descent(1851/9999): loss=417.231356042148, w0=0.03125010851841171, w1=0.3524568959224589\n",
      "Gradient Descent(1852/9999): loss=417.13229918205957, w0=0.03124778894472398, w1=0.3524211803782332\n",
      "Gradient Descent(1853/9999): loss=417.0333037310896, w0=0.031245468704323223, w1=0.3523854707986801\n",
      "Gradient Descent(1854/9999): loss=416.93436948551783, w0=0.03124314779960087, w1=0.3523497671765662\n",
      "Gradient Descent(1855/9999): loss=416.83549624263964, w0=0.031240826232942137, w1=0.35231406950467586\n",
      "Gradient Descent(1856/9999): loss=416.73668380076094, w0=0.031238504006726026, w1=0.35227837777581145\n",
      "Gradient Descent(1857/9999): loss=416.6379319591926, w0=0.03123618112332535, w1=0.35224269198279295\n",
      "Gradient Descent(1858/9999): loss=416.53924051824566, w0=0.031233857585106745, w1=0.3522070121184581\n",
      "Gradient Descent(1859/9999): loss=416.44060927922595, w0=0.03123153339443068, w1=0.3521713381756624\n",
      "Gradient Descent(1860/9999): loss=416.342038044429, w0=0.031229208553651494, w1=0.3521356701472787\n",
      "Gradient Descent(1861/9999): loss=416.2435266171354, w0=0.03122688306511738, w1=0.35210000802619773\n",
      "Gradient Descent(1862/9999): loss=416.1450748016051, w0=0.03122455693117043, w1=0.3520643518053276\n",
      "Gradient Descent(1863/9999): loss=416.04668240307313, w0=0.03122223015414663, w1=0.3520287014775939\n",
      "Gradient Descent(1864/9999): loss=415.94834922774356, w0=0.031219902736375888, w1=0.3519930570359398\n",
      "Gradient Descent(1865/9999): loss=415.8500750827859, w0=0.031217574680182048, w1=0.3519574184733257\n",
      "Gradient Descent(1866/9999): loss=415.75185977632873, w0=0.031215245987882896, w1=0.35192178578272937\n",
      "Gradient Descent(1867/9999): loss=415.6537031174562, w0=0.031212916661790192, w1=0.35188615895714603\n",
      "Gradient Descent(1868/9999): loss=415.5556049162018, w0=0.03121058670420967, w1=0.35185053798958804\n",
      "Gradient Descent(1869/9999): loss=415.45756498354433, w0=0.031208256117441064, w1=0.351814922873085\n",
      "Gradient Descent(1870/9999): loss=415.35958313140304, w0=0.031205924903778114, w1=0.3517793136006838\n",
      "Gradient Descent(1871/9999): loss=415.26165917263245, w0=0.031203593065508593, w1=0.3517437101654483\n",
      "Gradient Descent(1872/9999): loss=415.16379292101755, w0=0.03120126060491431, w1=0.3517081125604595\n",
      "Gradient Descent(1873/9999): loss=415.06598419126976, w0=0.031198927524271138, w1=0.3516725207788154\n",
      "Gradient Descent(1874/9999): loss=414.96823279902105, w0=0.031196593825849016, w1=0.3516369348136311\n",
      "Gradient Descent(1875/9999): loss=414.8705385608202, w0=0.031194259511911973, w1=0.3516013546580387\n",
      "Gradient Descent(1876/9999): loss=414.7729012941275, w0=0.03119192458471814, w1=0.35156578030518704\n",
      "Gradient Descent(1877/9999): loss=414.67532081731053, w0=0.031189589046519774, w1=0.35153021174824195\n",
      "Gradient Descent(1878/9999): loss=414.5777969496388, w0=0.031187252899563256, w1=0.35149464898038607\n",
      "Gradient Descent(1879/9999): loss=414.48032951128044, w0=0.031184916146089117, w1=0.3514590919948188\n",
      "Gradient Descent(1880/9999): loss=414.3829183232959, w0=0.031182578788332055, w1=0.35142354078475624\n",
      "Gradient Descent(1881/9999): loss=414.2855632076349, w0=0.031180240828520944, w1=0.3513879953434313\n",
      "Gradient Descent(1882/9999): loss=414.1882639871309, w0=0.031177902268878854, w1=0.35135245566409345\n",
      "Gradient Descent(1883/9999): loss=414.091020485497, w0=0.031175563111623058, w1=0.3513169217400088\n",
      "Gradient Descent(1884/9999): loss=413.99383252732133, w0=0.03117322335896506, w1=0.35128139356446003\n",
      "Gradient Descent(1885/9999): loss=413.89669993806245, w0=0.031170883013110592, w1=0.35124587113074635\n",
      "Gradient Descent(1886/9999): loss=413.7996225440454, w0=0.03116854207625965, w1=0.35121035443218346\n",
      "Gradient Descent(1887/9999): loss=413.7026001724564, w0=0.031166200550606486, w1=0.35117484346210354\n",
      "Gradient Descent(1888/9999): loss=413.6056326513393, w0=0.03116385843833965, w1=0.35113933821385507\n",
      "Gradient Descent(1889/9999): loss=413.5087198095906, w0=0.03116151574164197, w1=0.35110383868080297\n",
      "Gradient Descent(1890/9999): loss=413.4118614769554, w0=0.031159172462690603, w1=0.35106834485632843\n",
      "Gradient Descent(1891/9999): loss=413.31505748402293, w0=0.03115682860365702, w1=0.35103285673382895\n",
      "Gradient Descent(1892/9999): loss=413.2183076622223, w0=0.031154484166707044, w1=0.35099737430671824\n",
      "Gradient Descent(1893/9999): loss=413.1216118438179, w0=0.031152139154000842, w1=0.3509618975684262\n",
      "Gradient Descent(1894/9999): loss=413.02496986190556, w0=0.03114979356769296, w1=0.3509264265123989\n",
      "Gradient Descent(1895/9999): loss=412.9283815504083, w0=0.031147447409932326, w1=0.35089096113209856\n",
      "Gradient Descent(1896/9999): loss=412.83184674407164, w0=0.031145100682862262, w1=0.3508555014210033\n",
      "Gradient Descent(1897/9999): loss=412.7353652784594, w0=0.03114275338862051, w1=0.3508200473726074\n",
      "Gradient Descent(1898/9999): loss=412.6389369899505, w0=0.03114040552933923, w1=0.3507845989804212\n",
      "Gradient Descent(1899/9999): loss=412.5425617157332, w0=0.031138057107145043, w1=0.35074915623797076\n",
      "Gradient Descent(1900/9999): loss=412.44623929380236, w0=0.031135708124159008, w1=0.35071371913879823\n",
      "Gradient Descent(1901/9999): loss=412.3499695629546, w0=0.031133358582496662, w1=0.35067828767646153\n",
      "Gradient Descent(1902/9999): loss=412.2537523627841, w0=0.031131008484268027, w1=0.3506428618445344\n",
      "Gradient Descent(1903/9999): loss=412.15758753367936, w0=0.03112865783157762, w1=0.35060744163660645\n",
      "Gradient Descent(1904/9999): loss=412.06147491681776, w0=0.031126306626524478, w1=0.35057202704628293\n",
      "Gradient Descent(1905/9999): loss=411.965414354163, w0=0.031123954871202158, w1=0.3505366180671848\n",
      "Gradient Descent(1906/9999): loss=411.8694056884597, w0=0.03112160256769876, w1=0.3505012146929487\n",
      "Gradient Descent(1907/9999): loss=411.77344876323065, w0=0.031119249718096945, w1=0.35046581691722695\n",
      "Gradient Descent(1908/9999): loss=411.6775434227719, w0=0.03111689632447394, w1=0.35043042473368735\n",
      "Gradient Descent(1909/9999): loss=411.58168951214947, w0=0.03111454238890155, w1=0.3503950381360133\n",
      "Gradient Descent(1910/9999): loss=411.4858868771945, w0=0.031112187913446183, w1=0.3503596571179036\n",
      "Gradient Descent(1911/9999): loss=411.39013536450045, w0=0.031109832900168853, w1=0.3503242816730727\n",
      "Gradient Descent(1912/9999): loss=411.2944348214185, w0=0.031107477351125207, w1=0.35028891179525035\n",
      "Gradient Descent(1913/9999): loss=411.19878509605365, w0=0.031105121268365527, w1=0.3502535474781816\n",
      "Gradient Descent(1914/9999): loss=411.10318603726154, w0=0.031102764653934745, w1=0.35021818871562693\n",
      "Gradient Descent(1915/9999): loss=411.0076374946434, w0=0.03110040750987246, w1=0.3501828355013622\n",
      "Gradient Descent(1916/9999): loss=410.9121393185436, w0=0.031098049838212957, w1=0.35014748782917837\n",
      "Gradient Descent(1917/9999): loss=410.8166913600447, w0=0.031095691640985204, w1=0.35011214569288174\n",
      "Gradient Descent(1918/9999): loss=410.72129347096444, w0=0.031093332920212886, w1=0.35007680908629374\n",
      "Gradient Descent(1919/9999): loss=410.62594550385177, w0=0.031090973677914406, w1=0.35004147800325097\n",
      "Gradient Descent(1920/9999): loss=410.53064731198265, w0=0.031088613916102896, w1=0.3500061524376051\n",
      "Gradient Descent(1921/9999): loss=410.43539874935703, w0=0.031086253636786243, w1=0.34997083238322285\n",
      "Gradient Descent(1922/9999): loss=410.3401996706947, w0=0.03108389284196709, w1=0.34993551783398613\n",
      "Gradient Descent(1923/9999): loss=410.2450499314317, w0=0.03108153153364287, w1=0.34990020878379163\n",
      "Gradient Descent(1924/9999): loss=410.149949387717, w0=0.031079169713805775, w1=0.3498649052265511\n",
      "Gradient Descent(1925/9999): loss=410.05489789640814, w0=0.03107680738444283, w1=0.3498296071561912\n",
      "Gradient Descent(1926/9999): loss=409.95989531506814, w0=0.031074444547535857, w1=0.34979431456665344\n",
      "Gradient Descent(1927/9999): loss=409.86494150196177, w0=0.03107208120506151, w1=0.34975902745189413\n",
      "Gradient Descent(1928/9999): loss=409.77003631605163, w0=0.03106971735899129, w1=0.34972374580588445\n",
      "Gradient Descent(1929/9999): loss=409.67517961699554, w0=0.031067353011291546, w1=0.3496884696226103\n",
      "Gradient Descent(1930/9999): loss=409.5803712651418, w0=0.031064988163923498, w1=0.3496531988960724\n",
      "Gradient Descent(1931/9999): loss=409.4856111215261, w0=0.031062622818843248, w1=0.3496179336202859\n",
      "Gradient Descent(1932/9999): loss=409.39089904786874, w0=0.031060256978001795, w1=0.34958267378928093\n",
      "Gradient Descent(1933/9999): loss=409.2962349065695, w0=0.03105789064334504, w1=0.349547419397102\n",
      "Gradient Descent(1934/9999): loss=409.2016185607059, w0=0.031055523816813813, w1=0.34951217043780825\n",
      "Gradient Descent(1935/9999): loss=409.1070498740287, w0=0.03105315650034387, w1=0.3494769269054734\n",
      "Gradient Descent(1936/9999): loss=409.01252871095863, w0=0.031050788695865916, w1=0.3494416887941856\n",
      "Gradient Descent(1937/9999): loss=408.9180549365831, w0=0.03104842040530562, w1=0.34940645609804744\n",
      "Gradient Descent(1938/9999): loss=408.82362841665287, w0=0.031046051630583618, w1=0.34937122881117605\n",
      "Gradient Descent(1939/9999): loss=408.72924901757824, w0=0.031043682373615537, w1=0.34933600692770284\n",
      "Gradient Descent(1940/9999): loss=408.63491660642615, w0=0.031041312636312, w1=0.34930079044177365\n",
      "Gradient Descent(1941/9999): loss=408.5406310509166, w0=0.03103894242057864, w1=0.34926557934754854\n",
      "Gradient Descent(1942/9999): loss=408.4463922194193, w0=0.03103657172831612, w1=0.34923037363920195\n",
      "Gradient Descent(1943/9999): loss=408.3521999809502, w0=0.031034200561420137, w1=0.3491951733109225\n",
      "Gradient Descent(1944/9999): loss=408.2580542051688, w0=0.031031828921781437, w1=0.3491599783569129\n",
      "Gradient Descent(1945/9999): loss=408.16395476237386, w0=0.031029456811285827, w1=0.3491247887713903\n",
      "Gradient Descent(1946/9999): loss=408.0699015235009, w0=0.031027084231814193, w1=0.3490896045485858\n",
      "Gradient Descent(1947/9999): loss=407.975894360119, w0=0.03102471118524251, w1=0.34905442568274453\n",
      "Gradient Descent(1948/9999): loss=407.8819331444269, w0=0.031022337673441848, w1=0.3490192521681258\n",
      "Gradient Descent(1949/9999): loss=407.78801774925034, w0=0.031019963698278398, w1=0.34898408399900294\n",
      "Gradient Descent(1950/9999): loss=407.69414804803847, w0=0.03101758926161347, w1=0.3489489211696632\n",
      "Gradient Descent(1951/9999): loss=407.6003239148611, w0=0.031015214365303512, w1=0.3489137636744078\n",
      "Gradient Descent(1952/9999): loss=407.50654522440533, w0=0.03101283901120013, w1=0.3488786115075519\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(1953/9999): loss=407.4128118519719, w0=0.03101046320115009, w1=0.3488434646634245\n",
      "Gradient Descent(1954/9999): loss=407.3191236734729, w0=0.031008086936995333, w1=0.34880832313636845\n",
      "Gradient Descent(1955/9999): loss=407.2254805654281, w0=0.031005710220572986, w1=0.34877318692074044\n",
      "Gradient Descent(1956/9999): loss=407.1318824049619, w0=0.031003333053715377, w1=0.34873805601091096\n",
      "Gradient Descent(1957/9999): loss=407.0383290698006, w0=0.03100095543825005, w1=0.3487029304012641\n",
      "Gradient Descent(1958/9999): loss=406.9448204382685, w0=0.030998577375999776, w1=0.3486678100861978\n",
      "Gradient Descent(1959/9999): loss=406.8513563892859, w0=0.030996198868782555, w1=0.34863269506012357\n",
      "Gradient Descent(1960/9999): loss=406.75793680236507, w0=0.030993819918411644, w1=0.3485975853174666\n",
      "Gradient Descent(1961/9999): loss=406.66456155760824, w0=0.03099144052669556, w1=0.3485624808526656\n",
      "Gradient Descent(1962/9999): loss=406.5712305357033, w0=0.030989060695438095, w1=0.348527381660173\n",
      "Gradient Descent(1963/9999): loss=406.4779436179221, w0=0.030986680426438325, w1=0.34849228773445456\n",
      "Gradient Descent(1964/9999): loss=406.38470068611645, w0=0.030984299721490624, w1=0.3484571990699897\n",
      "Gradient Descent(1965/9999): loss=406.291501622716, w0=0.03098191858238468, w1=0.3484221156612711\n",
      "Gradient Descent(1966/9999): loss=406.19834631072445, w0=0.0309795370109055, w1=0.3483870375028052\n",
      "Gradient Descent(1967/9999): loss=406.10523463371726, w0=0.030977155008833426, w1=0.34835196458911144\n",
      "Gradient Descent(1968/9999): loss=406.0121664758386, w0=0.030974772577944152, w1=0.34831689691472284\n",
      "Gradient Descent(1969/9999): loss=405.91914172179804, w0=0.030972389720008722, w1=0.34828183447418565\n",
      "Gradient Descent(1970/9999): loss=405.82616025686787, w0=0.030970006436793557, w1=0.3482467772620595\n",
      "Gradient Descent(1971/9999): loss=405.7332219668809, w0=0.030967622730060457, w1=0.34821172527291716\n",
      "Gradient Descent(1972/9999): loss=405.6403267382261, w0=0.030965238601566615, w1=0.3481766785013447\n",
      "Gradient Descent(1973/9999): loss=405.54747445784756, w0=0.030962854053064634, w1=0.34814163694194133\n",
      "Gradient Descent(1974/9999): loss=405.4546650132401, w0=0.030960469086302532, w1=0.3481066005893194\n",
      "Gradient Descent(1975/9999): loss=405.36189829244734, w0=0.030958083703023755, w1=0.3480715694381044\n",
      "Gradient Descent(1976/9999): loss=405.26917418405856, w0=0.030955697904967196, w1=0.34803654348293495\n",
      "Gradient Descent(1977/9999): loss=405.176492577206, w0=0.0309533116938672, w1=0.3480015227184626\n",
      "Gradient Descent(1978/9999): loss=405.0838533615621, w0=0.030950925071453573, w1=0.34796650713935195\n",
      "Gradient Descent(1979/9999): loss=404.99125642733674, w0=0.0309485380394516, w1=0.34793149674028073\n",
      "Gradient Descent(1980/9999): loss=404.8987016652743, w0=0.03094615059958205, w1=0.3478964915159394\n",
      "Gradient Descent(1981/9999): loss=404.80618896665135, w0=0.0309437627535612, w1=0.34786149146103146\n",
      "Gradient Descent(1982/9999): loss=404.7137182232735, w0=0.03094137450310083, w1=0.34782649657027326\n",
      "Gradient Descent(1983/9999): loss=404.62128932747254, w0=0.030938985849908247, w1=0.347791506838394\n",
      "Gradient Descent(1984/9999): loss=404.52890217210467, w0=0.03093659679568629, w1=0.3477565222601357\n",
      "Gradient Descent(1985/9999): loss=404.43655665054695, w0=0.03093420734213335, w1=0.3477215428302532\n",
      "Gradient Descent(1986/9999): loss=404.3442526566946, w0=0.030931817490943366, w1=0.347686568543514\n",
      "Gradient Descent(1987/9999): loss=404.2519900849591, w0=0.030929427243805853, w1=0.34765159939469836\n",
      "Gradient Descent(1988/9999): loss=404.15976883026485, w0=0.0309270366024059, w1=0.3476166353785993\n",
      "Gradient Descent(1989/9999): loss=404.0675887880469, w0=0.030924645568424197, w1=0.3475816764900224\n",
      "Gradient Descent(1990/9999): loss=403.97544985424815, w0=0.030922254143537026, w1=0.3475467227237859\n",
      "Gradient Descent(1991/9999): loss=403.8833519253167, w0=0.030919862329416285, w1=0.34751177407472056\n",
      "Gradient Descent(1992/9999): loss=403.79129489820366, w0=0.030917470127729507, w1=0.34747683053766976\n",
      "Gradient Descent(1993/9999): loss=403.6992786703603, w0=0.03091507754013985, w1=0.3474418921074894\n",
      "Gradient Descent(1994/9999): loss=403.60730313973517, w0=0.030912684568306124, w1=0.34740695877904787\n",
      "Gradient Descent(1995/9999): loss=403.51536820477247, w0=0.0309102912138828, w1=0.347372030547226\n",
      "Gradient Descent(1996/9999): loss=403.4234737644083, w0=0.030907897478520015, w1=0.34733710740691703\n",
      "Gradient Descent(1997/9999): loss=403.33161971806913, w0=0.030905503363863596, w1=0.3473021893530267\n",
      "Gradient Descent(1998/9999): loss=403.2398059656689, w0=0.030903108871555052, w1=0.347267276380473\n",
      "Gradient Descent(1999/9999): loss=403.14803240760693, w0=0.0309007140032316, w1=0.34723236848418626\n",
      "Gradient Descent(2000/9999): loss=403.05629894476414, w0=0.030898318760526167, w1=0.34719746565910914\n",
      "Gradient Descent(2001/9999): loss=402.96460547850245, w0=0.03089592314506742, w1=0.34716256790019656\n",
      "Gradient Descent(2002/9999): loss=402.872951910661, w0=0.030893527158479744, w1=0.3471276752024157\n",
      "Gradient Descent(2003/9999): loss=402.7813381435541, w0=0.03089113080238328, w1=0.34709278756074596\n",
      "Gradient Descent(2004/9999): loss=402.6897640799689, w0=0.030888734078393935, w1=0.3470579049701788\n",
      "Gradient Descent(2005/9999): loss=402.598229623163, w0=0.03088633698812337, w1=0.34702302742571794\n",
      "Gradient Descent(2006/9999): loss=402.50673467686124, w0=0.030883939533179038, w1=0.34698815492237917\n",
      "Gradient Descent(2007/9999): loss=402.41527914525517, w0=0.030881541715164176, w1=0.34695328745519033\n",
      "Gradient Descent(2008/9999): loss=402.32386293299845, w0=0.03087914353567783, w1=0.3469184250191914\n",
      "Gradient Descent(2009/9999): loss=402.23248594520607, w0=0.03087674499631485, w1=0.3468835676094343\n",
      "Gradient Descent(2010/9999): loss=402.14114808745126, w0=0.03087434609866592, w1=0.34684871522098293\n",
      "Gradient Descent(2011/9999): loss=402.0498492657632, w0=0.030871946844317547, w1=0.3468138678489132\n",
      "Gradient Descent(2012/9999): loss=401.9585893866252, w0=0.03086954723485209, w1=0.34677902548831296\n",
      "Gradient Descent(2013/9999): loss=401.8673683569716, w0=0.03086714727184776, w1=0.3467441881342819\n",
      "Gradient Descent(2014/9999): loss=401.7761860841861, w0=0.03086474695687864, w1=0.3467093557819316\n",
      "Gradient Descent(2015/9999): loss=401.68504247609917, w0=0.030862346291514686, w1=0.34667452842638546\n",
      "Gradient Descent(2016/9999): loss=401.5939374409861, w0=0.030859945277321738, w1=0.34663970606277866\n",
      "Gradient Descent(2017/9999): loss=401.5028708875635, w0=0.030857543915861537, w1=0.3466048886862582\n",
      "Gradient Descent(2018/9999): loss=401.4118427249892, w0=0.030855142208691733, w1=0.34657007629198283\n",
      "Gradient Descent(2019/9999): loss=401.32085286285815, w0=0.030852740157365898, w1=0.346535268875123\n",
      "Gradient Descent(2020/9999): loss=401.2299012112007, w0=0.03085033776343353, w1=0.3465004664308608\n",
      "Gradient Descent(2021/9999): loss=401.13898768048085, w0=0.03084793502844007, w1=0.34646566895439007\n",
      "Gradient Descent(2022/9999): loss=401.0481121815936, w0=0.0308455319539269, w1=0.3464308764409162\n",
      "Gradient Descent(2023/9999): loss=400.9572746258627, w0=0.030843128541431383, w1=0.34639608888565615\n",
      "Gradient Descent(2024/9999): loss=400.86647492503846, w0=0.03084072479248683, w1=0.34636130628383854\n",
      "Gradient Descent(2025/9999): loss=400.7757129912961, w0=0.03083832070862255, w1=0.34632652863070346\n",
      "Gradient Descent(2026/9999): loss=400.68498873723274, w0=0.030835916291363834, w1=0.3462917559215025\n",
      "Gradient Descent(2027/9999): loss=400.594302075866, w0=0.03083351154223198, w1=0.3462569881514988\n",
      "Gradient Descent(2028/9999): loss=400.50365292063134, w0=0.0308311064627443, w1=0.3462222253159668\n",
      "Gradient Descent(2029/9999): loss=400.4130411853803, w0=0.03082870105441413, w1=0.3461874674101925\n",
      "Gradient Descent(2030/9999): loss=400.3224667843779, w0=0.03082629531875083, w1=0.3461527144294732\n",
      "Gradient Descent(2031/9999): loss=400.2319296323009, w0=0.030823889257259814, w1=0.3461179663691176\n",
      "Gradient Descent(2032/9999): loss=400.1414296442358, w0=0.030821482871442544, w1=0.3460832232244458\n",
      "Gradient Descent(2033/9999): loss=400.05096673567635, w0=0.030819076162796544, w1=0.346048484990789\n",
      "Gradient Descent(2034/9999): loss=399.9605408225216, w0=0.03081666913281542, w1=0.34601375166348985\n",
      "Gradient Descent(2035/9999): loss=399.8701518210741, w0=0.030814261782988853, w1=0.34597902323790214\n",
      "Gradient Descent(2036/9999): loss=399.7797996480375, w0=0.030811854114802622, w1=0.34594429970939095\n",
      "Gradient Descent(2037/9999): loss=399.68948422051477, w0=0.030809446129738606, w1=0.34590958107333253\n",
      "Gradient Descent(2038/9999): loss=399.5992054560059, w0=0.030807037829274804, w1=0.34587486732511424\n",
      "Gradient Descent(2039/9999): loss=399.50896327240605, w0=0.030804629214885335, w1=0.34584015846013466\n",
      "Gradient Descent(2040/9999): loss=399.41875758800336, w0=0.03080222028804045, w1=0.34580545447380334\n",
      "Gradient Descent(2041/9999): loss=399.3285883214772, w0=0.03079981105020655, w1=0.34577075536154095\n",
      "Gradient Descent(2042/9999): loss=399.2384553918959, w0=0.030797401502846187, w1=0.34573606111877925\n",
      "Gradient Descent(2043/9999): loss=399.148358718715, w0=0.030794991647418075, w1=0.345701371740961\n",
      "Gradient Descent(2044/9999): loss=399.058298221775, w0=0.030792581485377106, w1=0.3456666872235399\n",
      "Gradient Descent(2045/9999): loss=398.9682738213, w0=0.030790171018174346, w1=0.34563200756198065\n",
      "Gradient Descent(2046/9999): loss=398.8782854378946, w0=0.030787760247257063, w1=0.34559733275175886\n",
      "Gradient Descent(2047/9999): loss=398.78833299254325, w0=0.030785349174068725, w1=0.345562662788361\n",
      "Gradient Descent(2048/9999): loss=398.69841640660735, w0=0.030782937800049013, w1=0.34552799766728454\n",
      "Gradient Descent(2049/9999): loss=398.60853560182386, w0=0.03078052612663383, w1=0.34549333738403765\n",
      "Gradient Descent(2050/9999): loss=398.5186905003031, w0=0.030778114155255314, w1=0.3454586819341394\n",
      "Gradient Descent(2051/9999): loss=398.428881024527, w0=0.03077570188734184, w1=0.34542403131311966\n",
      "Gradient Descent(2052/9999): loss=398.33910709734704, w0=0.03077328932431804, w1=0.345389385516519\n",
      "Gradient Descent(2053/9999): loss=398.24936864198264, w0=0.0307708764676048, w1=0.34535474453988874\n",
      "Gradient Descent(2054/9999): loss=398.15966558201916, w0=0.030768463318619283, w1=0.34532010837879096\n",
      "Gradient Descent(2055/9999): loss=398.0699978414056, w0=0.03076604987877493, w1=0.34528547702879836\n",
      "Gradient Descent(2056/9999): loss=397.9803653444535, w0=0.030763636149481476, w1=0.3452508504854943\n",
      "Gradient Descent(2057/9999): loss=397.8907680158345, w0=0.03076122213214495, w1=0.34521622874447283\n",
      "Gradient Descent(2058/9999): loss=397.8012057805789, w0=0.030758807828167686, w1=0.34518161180133855\n",
      "Gradient Descent(2059/9999): loss=397.71167856407345, w0=0.030756393238948347, w1=0.3451469996517066\n",
      "Gradient Descent(2060/9999): loss=397.62218629205967, w0=0.030753978365881923, w1=0.34511239229120266\n",
      "Gradient Descent(2061/9999): loss=397.53272889063237, w0=0.030751563210359732, w1=0.34507778971546305\n",
      "Gradient Descent(2062/9999): loss=397.44330628623726, w0=0.03074914777376945, w1=0.3450431919201344\n",
      "Gradient Descent(2063/9999): loss=397.3539184056698, w0=0.030746732057495094, w1=0.34500859890087393\n",
      "Gradient Descent(2064/9999): loss=397.2645651760724, w0=0.030744316062917068, w1=0.34497401065334926\n",
      "Gradient Descent(2065/9999): loss=397.1752465249343, w0=0.030741899791412133, w1=0.3449394271732384\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(2066/9999): loss=397.08596238008784, w0=0.030739483244353445, w1=0.34490484845622976\n",
      "Gradient Descent(2067/9999): loss=396.9967126697084, w0=0.030737066423110544, w1=0.3448702744980221\n",
      "Gradient Descent(2068/9999): loss=396.90749732231154, w0=0.03073464932904938, w1=0.3448357052943245\n",
      "Gradient Descent(2069/9999): loss=396.81831626675194, w0=0.030732231963532315, w1=0.3448011408408564\n",
      "Gradient Descent(2070/9999): loss=396.729169432221, w0=0.03072981432791813, w1=0.3447665811333474\n",
      "Gradient Descent(2071/9999): loss=396.6400567482457, w0=0.030727396423562037, w1=0.3447320261675375\n",
      "Gradient Descent(2072/9999): loss=396.5509781446869, w0=0.030724978251815686, w1=0.34469747593917677\n",
      "Gradient Descent(2073/9999): loss=396.461933551737, w0=0.03072255981402718, w1=0.3446629304440256\n",
      "Gradient Descent(2074/9999): loss=396.37292289991893, w0=0.030720141111541074, w1=0.3446283896778545\n",
      "Gradient Descent(2075/9999): loss=396.28394612008435, w0=0.0307177221456984, w1=0.34459385363644407\n",
      "Gradient Descent(2076/9999): loss=396.19500314341144, w0=0.030715302917836657, w1=0.3445593223155851\n",
      "Gradient Descent(2077/9999): loss=396.1060939014039, w0=0.03071288342928983, w1=0.3445247957110785\n",
      "Gradient Descent(2078/9999): loss=396.01721832588885, w0=0.0307104636813884, w1=0.34449027381873526\n",
      "Gradient Descent(2079/9999): loss=395.92837634901554, w0=0.03070804367545936, w1=0.3444557566343762\n",
      "Gradient Descent(2080/9999): loss=395.83956790325317, w0=0.030705623412826198, w1=0.34442124415383246\n",
      "Gradient Descent(2081/9999): loss=395.75079292138986, w0=0.030703202894808934, w1=0.3443867363729449\n",
      "Gradient Descent(2082/9999): loss=395.6620513365306, w0=0.03070078212272412, w1=0.3443522332875646\n",
      "Gradient Descent(2083/9999): loss=395.57334308209596, w0=0.030698361097884836, w1=0.34431773489355233\n",
      "Gradient Descent(2084/9999): loss=395.48466809181997, w0=0.030695939821600727, w1=0.34428324118677894\n",
      "Gradient Descent(2085/9999): loss=395.39602629974917, w0=0.03069351829517798, w1=0.3442487521631251\n",
      "Gradient Descent(2086/9999): loss=395.3074176402405, w0=0.030691096519919355, w1=0.34421426781848136\n",
      "Gradient Descent(2087/9999): loss=395.2188420479599, w0=0.030688674497124188, w1=0.3441797881487481\n",
      "Gradient Descent(2088/9999): loss=395.130299457881, w0=0.030686252228088393, w1=0.34414531314983554\n",
      "Gradient Descent(2089/9999): loss=395.0417898052828, w0=0.030683829714104476, w1=0.34411084281766363\n",
      "Gradient Descent(2090/9999): loss=394.953313025749, w0=0.03068140695646155, w1=0.34407637714816214\n",
      "Gradient Descent(2091/9999): loss=394.86486905516614, w0=0.030678983956445333, w1=0.3440419161372706\n",
      "Gradient Descent(2092/9999): loss=394.7764578297214, w0=0.030676560715338162, w1=0.3440074597809381\n",
      "Gradient Descent(2093/9999): loss=394.68807928590206, w0=0.030674137234419002, w1=0.3439730080751236\n",
      "Gradient Descent(2094/9999): loss=394.59973336049364, w0=0.030671713514963457, w1=0.3439385610157956\n",
      "Gradient Descent(2095/9999): loss=394.5114199905778, w0=0.03066928955824377, w1=0.34390411859893233\n",
      "Gradient Descent(2096/9999): loss=394.4231391135316, w0=0.030666865365528838, w1=0.34386968082052155\n",
      "Gradient Descent(2097/9999): loss=394.3348906670257, w0=0.030664440938084223, w1=0.34383524767656065\n",
      "Gradient Descent(2098/9999): loss=394.2466745890228, w0=0.030662016277172154, w1=0.3438008191630566\n",
      "Gradient Descent(2099/9999): loss=394.15849081777617, w0=0.03065959138405154, w1=0.34376639527602587\n",
      "Gradient Descent(2100/9999): loss=394.0703392918281, w0=0.030657166259977976, w1=0.3437319760114944\n",
      "Gradient Descent(2101/9999): loss=393.9822199500088, w0=0.030654740906203754, w1=0.34369756136549773\n",
      "Gradient Descent(2102/9999): loss=393.8941327314342, w0=0.030652315323977875, w1=0.3436631513340808\n",
      "Gradient Descent(2103/9999): loss=393.80607757550564, w0=0.030649889514546044, w1=0.34362874591329806\n",
      "Gradient Descent(2104/9999): loss=393.7180544219069, w0=0.030647463479150695, w1=0.3435943450992132\n",
      "Gradient Descent(2105/9999): loss=393.63006321060425, w0=0.030645037219030988, w1=0.34355994888789954\n",
      "Gradient Descent(2106/9999): loss=393.5421038818439, w0=0.03064261073542282, w1=0.3435255572754396\n",
      "Gradient Descent(2107/9999): loss=393.45417637615134, w0=0.03064018402955884, w1=0.3434911702579253\n",
      "Gradient Descent(2108/9999): loss=393.3662806343291, w0=0.030637757102668445, w1=0.3434567878314579\n",
      "Gradient Descent(2109/9999): loss=393.2784165974563, w0=0.030635329955977803, w1=0.3434224099921479\n",
      "Gradient Descent(2110/9999): loss=393.1905842068867, w0=0.030632902590709846, w1=0.3433880367361152\n",
      "Gradient Descent(2111/9999): loss=393.102783404247, w0=0.030630475008084287, w1=0.3433536680594887\n",
      "Gradient Descent(2112/9999): loss=393.0150141314361, w0=0.030628047209317635, w1=0.3433193039584068\n",
      "Gradient Descent(2113/9999): loss=392.9272763306235, w0=0.030625619195623184, w1=0.34328494442901697\n",
      "Gradient Descent(2114/9999): loss=392.83956994424744, w0=0.03062319096821104, w1=0.3432505894674759\n",
      "Gradient Descent(2115/9999): loss=392.75189491501436, w0=0.030620762528288124, w1=0.34321623906994936\n",
      "Gradient Descent(2116/9999): loss=392.6642511858966, w0=0.03061833387705817, w1=0.3431818932326123\n",
      "Gradient Descent(2117/9999): loss=392.5766387001319, w0=0.03061590501572175, w1=0.3431475519516488\n",
      "Gradient Descent(2118/9999): loss=392.4890574012218, w0=0.030613475945476264, w1=0.343113215223252\n",
      "Gradient Descent(2119/9999): loss=392.4015072329299, w0=0.03061104666751597, w1=0.3430788830436241\n",
      "Gradient Descent(2120/9999): loss=392.31398813928075, w0=0.030608617183031966, w1=0.3430445554089764\n",
      "Gradient Descent(2121/9999): loss=392.22650006455876, w0=0.030606187493212218, w1=0.3430102323155291\n",
      "Gradient Descent(2122/9999): loss=392.1390429533068, w0=0.030603757599241565, w1=0.3429759137595114\n",
      "Gradient Descent(2123/9999): loss=392.0516167503242, w0=0.030601327502301722, w1=0.3429415997371616\n",
      "Gradient Descent(2124/9999): loss=391.96422140066653, w0=0.030598897203571288, w1=0.3429072902447268\n",
      "Gradient Descent(2125/9999): loss=391.8768568496438, w0=0.030596466704225757, w1=0.3428729852784632\n",
      "Gradient Descent(2126/9999): loss=391.7895230428188, w0=0.030594036005437523, w1=0.3428386848346357\n",
      "Gradient Descent(2127/9999): loss=391.70221992600636, w0=0.030591605108375896, w1=0.34280438890951814\n",
      "Gradient Descent(2128/9999): loss=391.61494744527175, w0=0.030589174014207094, w1=0.3427700974993933\n",
      "Gradient Descent(2129/9999): loss=391.5277055469295, w0=0.030586742724094267, w1=0.34273581060055275\n",
      "Gradient Descent(2130/9999): loss=391.4404941775423, w0=0.0305843112391975, w1=0.3427015282092968\n",
      "Gradient Descent(2131/9999): loss=391.3533132839192, w0=0.03058187956067382, w1=0.34266725032193457\n",
      "Gradient Descent(2132/9999): loss=391.2661628131152, w0=0.030579447689677196, w1=0.34263297693478406\n",
      "Gradient Descent(2133/9999): loss=391.1790427124291, w0=0.030577015627358563, w1=0.34259870804417186\n",
      "Gradient Descent(2134/9999): loss=391.09195292940285, w0=0.030574583374865814, w1=0.34256444364643335\n",
      "Gradient Descent(2135/9999): loss=391.00489341182003, w0=0.030572150933343825, w1=0.3425301837379126\n",
      "Gradient Descent(2136/9999): loss=390.91786410770504, w0=0.03056971830393444, w1=0.3424959283149624\n",
      "Gradient Descent(2137/9999): loss=390.830864965321, w0=0.030567285487776496, w1=0.34246167737394406\n",
      "Gradient Descent(2138/9999): loss=390.7438959331694, w0=0.030564852486005833, w1=0.3424274309112276\n",
      "Gradient Descent(2139/9999): loss=390.65695695998886, w0=0.030562419299755287, w1=0.34239318892319176\n",
      "Gradient Descent(2140/9999): loss=390.570047994753, w0=0.030559985930154708, w1=0.3423589514062237\n",
      "Gradient Descent(2141/9999): loss=390.4831689866705, w0=0.03055755237833097, w1=0.3423247183567192\n",
      "Gradient Descent(2142/9999): loss=390.39631988518244, w0=0.030555118645407966, w1=0.34229048977108256\n",
      "Gradient Descent(2143/9999): loss=390.3095006399632, w0=0.030552684732506625, w1=0.3422562656457267\n",
      "Gradient Descent(2144/9999): loss=390.2227112009166, w0=0.030550250640744923, w1=0.3422220459770729\n",
      "Gradient Descent(2145/9999): loss=390.1359515181772, w0=0.030547816371237886, w1=0.34218783076155096\n",
      "Gradient Descent(2146/9999): loss=390.0492215421075, w0=0.030545381925097592, w1=0.34215361999559923\n",
      "Gradient Descent(2147/9999): loss=389.9625212232979, w0=0.03054294730343319, w1=0.34211941367566434\n",
      "Gradient Descent(2148/9999): loss=389.8758505125643, w0=0.030540512507350893, w1=0.34208521179820145\n",
      "Gradient Descent(2149/9999): loss=389.7892093609483, w0=0.030538077537954005, w1=0.34205101435967405\n",
      "Gradient Descent(2150/9999): loss=389.7025977197148, w0=0.03053564239634291, w1=0.34201682135655403\n",
      "Gradient Descent(2151/9999): loss=389.6160155403519, w0=0.030533207083615094, w1=0.34198263278532165\n",
      "Gradient Descent(2152/9999): loss=389.529462774569, w0=0.030530771600865134, w1=0.34194844864246543\n",
      "Gradient Descent(2153/9999): loss=389.44293937429626, w0=0.030528335949184728, w1=0.3419142689244822\n",
      "Gradient Descent(2154/9999): loss=389.3564452916831, w0=0.030525900129662685, w1=0.3418800936278771\n",
      "Gradient Descent(2155/9999): loss=389.26998047909694, w0=0.030523464143384946, w1=0.34184592274916353\n",
      "Gradient Descent(2156/9999): loss=389.1835448891228, w0=0.03052102799143458, w1=0.34181175628486316\n",
      "Gradient Descent(2157/9999): loss=389.0971384745614, w0=0.03051859167489179, w1=0.34177759423150583\n",
      "Gradient Descent(2158/9999): loss=389.0107611884284, w0=0.030516155194833933, w1=0.34174343658562956\n",
      "Gradient Descent(2159/9999): loss=388.92441298395335, w0=0.030513718552335523, w1=0.34170928334378065\n",
      "Gradient Descent(2160/9999): loss=388.83809381457854, w0=0.03051128174846823, w1=0.3416751345025134\n",
      "Gradient Descent(2161/9999): loss=388.7518036339579, w0=0.030508844784300892, w1=0.34164099005839044\n",
      "Gradient Descent(2162/9999): loss=388.6655423959561, w0=0.03050640766089953, w1=0.3416068500079823\n",
      "Gradient Descent(2163/9999): loss=388.5793100546469, w0=0.03050397037932734, w1=0.3415727143478678\n",
      "Gradient Descent(2164/9999): loss=388.49310656431305, w0=0.030501532940644716, w1=0.3415385830746337\n",
      "Gradient Descent(2165/9999): loss=388.4069318794442, w0=0.03049909534590925, w1=0.3415044561848749\n",
      "Gradient Descent(2166/9999): loss=388.3207859547365, w0=0.03049665759617573, w1=0.34147033367519425\n",
      "Gradient Descent(2167/9999): loss=388.2346687450916, w0=0.030494219692496168, w1=0.34143621554220266\n",
      "Gradient Descent(2168/9999): loss=388.14858020561496, w0=0.030491781635919788, w1=0.341402101782519\n",
      "Gradient Descent(2169/9999): loss=388.06252029161556, w0=0.030489343427493047, w1=0.3413679923927702\n",
      "Gradient Descent(2170/9999): loss=387.9764889586043, w0=0.03048690506825963, w1=0.341333887369591\n",
      "Gradient Descent(2171/9999): loss=387.89048616229326, w0=0.03048446655926046, w1=0.34129978670962424\n",
      "Gradient Descent(2172/9999): loss=387.8045118585947, w0=0.030482027901533718, w1=0.34126569040952043\n",
      "Gradient Descent(2173/9999): loss=387.71856600361997, w0=0.030479589096114836, w1=0.3412315984659382\n",
      "Gradient Descent(2174/9999): loss=387.63264855367834, w0=0.030477150144036506, w1=0.34119751087554395\n",
      "Gradient Descent(2175/9999): loss=387.54675946527607, w0=0.030474711046328692, w1=0.3411634276350119\n",
      "Gradient Descent(2176/9999): loss=387.4608986951159, w0=0.030472271804018635, w1=0.34112934874102413\n",
      "Gradient Descent(2177/9999): loss=387.37506620009503, w0=0.030469832418130855, w1=0.3410952741902706\n",
      "Gradient Descent(2178/9999): loss=387.2892619373052, w0=0.030467392889687164, w1=0.3410612039794489\n",
      "Gradient Descent(2179/9999): loss=387.203485864031, w0=0.030464953219706672, w1=0.3410271381052645\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(2180/9999): loss=387.11773793774915, w0=0.030462513409205794, w1=0.3409930765644307\n",
      "Gradient Descent(2181/9999): loss=387.03201811612746, w0=0.030460073459198257, w1=0.34095901935366824\n",
      "Gradient Descent(2182/9999): loss=386.9463263570239, w0=0.0304576333706951, w1=0.3409249664697059\n",
      "Gradient Descent(2183/9999): loss=386.8606626184856, w0=0.030455193144704698, w1=0.34089091790927994\n",
      "Gradient Descent(2184/9999): loss=386.77502685874794, w0=0.030452752782232747, w1=0.34085687366913436\n",
      "Gradient Descent(2185/9999): loss=386.6894190362335, w0=0.030450312284282285, w1=0.34082283374602085\n",
      "Gradient Descent(2186/9999): loss=386.603839109551, w0=0.0304478716518537, w1=0.3407887981366986\n",
      "Gradient Descent(2187/9999): loss=386.5182870374949, w0=0.030445430885944726, w1=0.34075476683793454\n",
      "Gradient Descent(2188/9999): loss=386.43276277904374, w0=0.030442989987550458, w1=0.34072073984650314\n",
      "Gradient Descent(2189/9999): loss=386.34726629335955, w0=0.030440548957663363, w1=0.3406867171591864\n",
      "Gradient Descent(2190/9999): loss=386.2617975397871, w0=0.030438107797273274, w1=0.340652698772774\n",
      "Gradient Descent(2191/9999): loss=386.17635647785255, w0=0.030435666507367405, w1=0.3406186846840629\n",
      "Gradient Descent(2192/9999): loss=386.0909430672627, w0=0.030433225088930357, w1=0.34058467488985794\n",
      "Gradient Descent(2193/9999): loss=386.00555726790435, w0=0.030430783542944123, w1=0.34055066938697115\n",
      "Gradient Descent(2194/9999): loss=385.9201990398431, w0=0.030428341870388093, w1=0.34051666817222215\n",
      "Gradient Descent(2195/9999): loss=385.8348683433221, w0=0.03042590007223907, w1=0.340482671242438\n",
      "Gradient Descent(2196/9999): loss=385.7495651387621, w0=0.03042345814947127, w1=0.34044867859445316\n",
      "Gradient Descent(2197/9999): loss=385.6642893867596, w0=0.03042101610305632, w1=0.3404146902251096\n",
      "Gradient Descent(2198/9999): loss=385.57904104808637, w0=0.030418573933963283, w1=0.3403807061312567\n",
      "Gradient Descent(2199/9999): loss=385.49382008368866, w0=0.030416131643158647, w1=0.3403467263097511\n",
      "Gradient Descent(2200/9999): loss=385.4086264546861, w0=0.03041368923160635, w1=0.3403127507574568\n",
      "Gradient Descent(2201/9999): loss=385.3234601223709, w0=0.030411246700267763, w1=0.3402787794712453\n",
      "Gradient Descent(2202/9999): loss=385.23832104820684, w0=0.030408804050101722, w1=0.34024481244799526\n",
      "Gradient Descent(2203/9999): loss=385.15320919382873, w0=0.030406361282064514, w1=0.3402108496845927\n",
      "Gradient Descent(2204/9999): loss=385.06812452104134, w0=0.030403918397109892, w1=0.340176891177931\n",
      "Gradient Descent(2205/9999): loss=384.98306699181813, w0=0.03040147539618909, w1=0.34014293692491065\n",
      "Gradient Descent(2206/9999): loss=384.8980365683014, w0=0.030399032280250808, w1=0.3401089869224395\n",
      "Gradient Descent(2207/9999): loss=384.8130332128002, w0=0.030396589050241243, w1=0.34007504116743265\n",
      "Gradient Descent(2208/9999): loss=384.7280568877905, w0=0.03039414570710408, w1=0.34004109965681234\n",
      "Gradient Descent(2209/9999): loss=384.643107555914, w0=0.0303917022517805, w1=0.34000716238750806\n",
      "Gradient Descent(2210/9999): loss=384.55818517997693, w0=0.030389258685209187, w1=0.3399732293564564\n",
      "Gradient Descent(2211/9999): loss=384.4732897229496, w0=0.03038681500832634, w1=0.3399393005606012\n",
      "Gradient Descent(2212/9999): loss=384.3884211479656, w0=0.03038437122206568, w1=0.3399053759968933\n",
      "Gradient Descent(2213/9999): loss=384.3035794183208, w0=0.03038192732735844, w1=0.3398714556622908\n",
      "Gradient Descent(2214/9999): loss=384.2187644974725, w0=0.03037948332513339, w1=0.3398375395537589\n",
      "Gradient Descent(2215/9999): loss=384.1339763490387, w0=0.030377039216316844, w1=0.33980362766826977\n",
      "Gradient Descent(2216/9999): loss=384.0492149367975, w0=0.030374595001832646, w1=0.3397697200028027\n",
      "Gradient Descent(2217/9999): loss=383.96448022468564, w0=0.030372150682602194, w1=0.3397358165543441\n",
      "Gradient Descent(2218/9999): loss=383.87977217679844, w0=0.03036970625954444, w1=0.33970191731988725\n",
      "Gradient Descent(2219/9999): loss=383.7950907573885, w0=0.030367261733575908, w1=0.33966802229643256\n",
      "Gradient Descent(2220/9999): loss=383.7104359308652, w0=0.030364817105610675, w1=0.3396341314809874\n",
      "Gradient Descent(2221/9999): loss=383.62580766179343, w0=0.030362372376560403, w1=0.3396002448705662\n",
      "Gradient Descent(2222/9999): loss=383.54120591489357, w0=0.030359927547334332, w1=0.3395663624621902\n",
      "Gradient Descent(2223/9999): loss=383.45663065503993, w0=0.03035748261883929, w1=0.3395324842528876\n",
      "Gradient Descent(2224/9999): loss=383.3720818472605, w0=0.030355037591979692, w1=0.3394986102396937\n",
      "Gradient Descent(2225/9999): loss=383.28755945673595, w0=0.030352592467657563, w1=0.3394647404196504\n",
      "Gradient Descent(2226/9999): loss=383.2030634487986, w0=0.030350147246772523, w1=0.33943087478980677\n",
      "Gradient Descent(2227/9999): loss=383.1185937889324, w0=0.03034770193022181, w1=0.33939701334721856\n",
      "Gradient Descent(2228/9999): loss=383.03415044277136, w0=0.030345256518900278, w1=0.3393631560889485\n",
      "Gradient Descent(2229/9999): loss=382.94973337609906, w0=0.030342811013700403, w1=0.3393293030120661\n",
      "Gradient Descent(2230/9999): loss=382.8653425548481, w0=0.030340365415512292, w1=0.33929545411364764\n",
      "Gradient Descent(2231/9999): loss=382.7809779450992, w0=0.03033791972522369, w1=0.3392616093907762\n",
      "Gradient Descent(2232/9999): loss=382.69663951308036, w0=0.030335473943719986, w1=0.3392277688405418\n",
      "Gradient Descent(2233/9999): loss=382.6123272251662, w0=0.030333028071884212, w1=0.339193932460041\n",
      "Gradient Descent(2234/9999): loss=382.5280410478771, w0=0.030330582110597054, w1=0.3391601002463772\n",
      "Gradient Descent(2235/9999): loss=382.4437809478787, w0=0.030328136060736866, w1=0.33912627219666064\n",
      "Gradient Descent(2236/9999): loss=382.3595468919809, w0=0.03032568992317966, w1=0.3390924483080081\n",
      "Gradient Descent(2237/9999): loss=382.2753388471375, w0=0.030323243698799126, w1=0.3390586285775431\n",
      "Gradient Descent(2238/9999): loss=382.19115678044494, w0=0.03032079738846663, w1=0.3390248130023959\n",
      "Gradient Descent(2239/9999): loss=382.10700065914193, w0=0.030318350993051222, w1=0.33899100157970335\n",
      "Gradient Descent(2240/9999): loss=382.0228704506086, w0=0.03031590451341964, w1=0.33895719430660903\n",
      "Gradient Descent(2241/9999): loss=381.93876612236585, w0=0.030313457950436325, w1=0.33892339118026305\n",
      "Gradient Descent(2242/9999): loss=381.85468764207485, w0=0.030311011304963418, w1=0.3388895921978221\n",
      "Gradient Descent(2243/9999): loss=381.7706349775358, w0=0.030308564577860762, w1=0.33885579735644966\n",
      "Gradient Descent(2244/9999): loss=381.68660809668756, w0=0.030306117769985923, w1=0.3388220066533156\n",
      "Gradient Descent(2245/9999): loss=381.6026069676071, w0=0.030303670882194177, w1=0.33878822008559634\n",
      "Gradient Descent(2246/9999): loss=381.51863155850833, w0=0.030301223915338533, w1=0.33875443765047497\n",
      "Gradient Descent(2247/9999): loss=381.4346818377419, w0=0.03029877687026973, w1=0.338720659345141\n",
      "Gradient Descent(2248/9999): loss=381.35075777379416, w0=0.030296329747836248, w1=0.3386868851667905\n",
      "Gradient Descent(2249/9999): loss=381.2668593352867, w0=0.0302938825488843, w1=0.338653115112626\n",
      "Gradient Descent(2250/9999): loss=381.1829864909753, w0=0.030291435274257858, w1=0.3386193491798566\n",
      "Gradient Descent(2251/9999): loss=381.0991392097499, w0=0.030288987924798644, w1=0.33858558736569766\n",
      "Gradient Descent(2252/9999): loss=381.0153174606333, w0=0.030286540501346142, w1=0.3385518296673712\n",
      "Gradient Descent(2253/9999): loss=380.93152121278064, w0=0.030284093004737604, w1=0.3385180760821055\n",
      "Gradient Descent(2254/9999): loss=380.84775043547904, w0=0.030281645435808052, w1=0.33848432660713534\n",
      "Gradient Descent(2255/9999): loss=380.76400509814636, w0=0.030279197795390286, w1=0.3384505812397019\n",
      "Gradient Descent(2256/9999): loss=380.6802851703312, w0=0.03027675008431489, w1=0.33841683997705263\n",
      "Gradient Descent(2257/9999): loss=380.59659062171175, w0=0.030274302303410237, w1=0.33838310281644146\n",
      "Gradient Descent(2258/9999): loss=380.51292142209536, w0=0.030271854453502497, w1=0.33834936975512864\n",
      "Gradient Descent(2259/9999): loss=380.4292775414176, w0=0.030269406535415634, w1=0.3383156407903807\n",
      "Gradient Descent(2260/9999): loss=380.34565894974213, w0=0.030266958549971426, w1=0.33828191591947043\n",
      "Gradient Descent(2261/9999): loss=380.2620656172594, w0=0.030264510497989464, w1=0.3382481951396771\n",
      "Gradient Descent(2262/9999): loss=380.17849751428673, w0=0.03026206238028715, w1=0.338214478448286\n",
      "Gradient Descent(2263/9999): loss=380.0949546112669, w0=0.03025961419767971, w1=0.33818076584258894\n",
      "Gradient Descent(2264/9999): loss=380.0114368787682, w0=0.030257165950980203, w1=0.33814705731988387\n",
      "Gradient Descent(2265/9999): loss=379.9279442874835, w0=0.03025471764099952, w1=0.3381133528774749\n",
      "Gradient Descent(2266/9999): loss=379.8444768082292, w0=0.030252269268546392, w1=0.33807965251267247\n",
      "Gradient Descent(2267/9999): loss=379.76103441194556, w0=0.030249820834427395, w1=0.33804595622279315\n",
      "Gradient Descent(2268/9999): loss=379.67761706969515, w0=0.030247372339446958, w1=0.3380122640051597\n",
      "Gradient Descent(2269/9999): loss=379.59422475266274, w0=0.030244923784407365, w1=0.33797857585710106\n",
      "Gradient Descent(2270/9999): loss=379.51085743215464, w0=0.030242475170108765, w1=0.3379448917759523\n",
      "Gradient Descent(2271/9999): loss=379.4275150795977, w0=0.03024002649734917, w1=0.3379112117590547\n",
      "Gradient Descent(2272/9999): loss=379.3441976665392, w0=0.030237577766924466, w1=0.3378775358037555\n",
      "Gradient Descent(2273/9999): loss=379.26090516464615, w0=0.030235128979628424, w1=0.3378438639074082\n",
      "Gradient Descent(2274/9999): loss=379.17763754570433, w0=0.030232680136252688, w1=0.3378101960673724\n",
      "Gradient Descent(2275/9999): loss=379.0943947816179, w0=0.0302302312375868, w1=0.33777653228101356\n",
      "Gradient Descent(2276/9999): loss=379.0111768444091, w0=0.0302277822844182, w1=0.33774287254570345\n",
      "Gradient Descent(2277/9999): loss=378.927983706217, w0=0.030225333277532216, w1=0.3377092168588197\n",
      "Gradient Descent(2278/9999): loss=378.8448153392976, w0=0.03022288421771209, w1=0.33767556521774617\n",
      "Gradient Descent(2279/9999): loss=378.76167171602276, w0=0.030220435105738973, w1=0.33764191761987244\n",
      "Gradient Descent(2280/9999): loss=378.6785528088799, w0=0.030217985942391935, w1=0.33760827406259436\n",
      "Gradient Descent(2281/9999): loss=378.5954585904711, w0=0.030215536728447966, w1=0.3375746345433136\n",
      "Gradient Descent(2282/9999): loss=378.5123890335127, w0=0.030213087464681983, w1=0.33754099905943785\n",
      "Gradient Descent(2283/9999): loss=378.4293441108349, w0=0.03021063815186684, w1=0.3375073676083807\n",
      "Gradient Descent(2284/9999): loss=378.34632379538095, w0=0.03020818879077332, w1=0.3374737401875617\n",
      "Gradient Descent(2285/9999): loss=378.2633280602064, w0=0.030205739382170152, w1=0.33744011679440644\n",
      "Gradient Descent(2286/9999): loss=378.1803568784793, w0=0.03020328992682402, w1=0.3374064974263462\n",
      "Gradient Descent(2287/9999): loss=378.09741022347856, w0=0.03020084042549956, w1=0.33737288208081834\n",
      "Gradient Descent(2288/9999): loss=378.0144880685942, w0=0.03019839087895936, w1=0.33733927075526593\n",
      "Gradient Descent(2289/9999): loss=377.9315903873263, w0=0.030195941287963976, w1=0.33730566344713797\n",
      "Gradient Descent(2290/9999): loss=377.848717153285, w0=0.030193491653271936, w1=0.3372720601538894\n",
      "Gradient Descent(2291/9999): loss=377.76586834018923, w0=0.03019104197563974, w1=0.33723846087298076\n",
      "Gradient Descent(2292/9999): loss=377.6830439218668, w0=0.03018859225582187, w1=0.3372048656018787\n",
      "Gradient Descent(2293/9999): loss=377.6002438722533, w0=0.030186142494570793, w1=0.3371712743380554\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(2294/9999): loss=377.51746816539236, w0=0.030183692692636963, w1=0.33713768707898895\n",
      "Gradient Descent(2295/9999): loss=377.43471677543386, w0=0.030181242850768834, w1=0.33710410382216316\n",
      "Gradient Descent(2296/9999): loss=377.35198967663445, w0=0.030178792969712858, w1=0.3370705245650677\n",
      "Gradient Descent(2297/9999): loss=377.26928684335695, w0=0.030176343050213494, w1=0.33703694930519784\n",
      "Gradient Descent(2298/9999): loss=377.18660825006907, w0=0.030173893093013212, w1=0.3370033780400547\n",
      "Gradient Descent(2299/9999): loss=377.1039538713435, w0=0.0301714430988525, w1=0.336969810767145\n",
      "Gradient Descent(2300/9999): loss=377.02132368185715, w0=0.030168993068469856, w1=0.3369362474839812\n",
      "Gradient Descent(2301/9999): loss=376.938717656391, w0=0.030166543002601822, w1=0.3369026881880815\n",
      "Gradient Descent(2302/9999): loss=376.85613576982894, w0=0.03016409290198296, w1=0.3368691328769697\n",
      "Gradient Descent(2303/9999): loss=376.77357799715753, w0=0.03016164276734587, w1=0.3368355815481752\n",
      "Gradient Descent(2304/9999): loss=376.6910443134658, w0=0.030159192599421195, w1=0.3368020341992332\n",
      "Gradient Descent(2305/9999): loss=376.6085346939443, w0=0.030156742398937623, w1=0.3367684908276844\n",
      "Gradient Descent(2306/9999): loss=376.5260491138845, w0=0.030154292166621893, w1=0.33673495143107507\n",
      "Gradient Descent(2307/9999): loss=376.44358754867903, w0=0.030151841903198806, w1=0.3367014160069572\n",
      "Gradient Descent(2308/9999): loss=376.36114997382043, w0=0.03014939160939122, w1=0.33666788455288826\n",
      "Gradient Descent(2309/9999): loss=376.27873636490045, w0=0.030146941285920055, w1=0.33663435706643136\n",
      "Gradient Descent(2310/9999): loss=376.1963466976107, w0=0.030144490933504315, w1=0.3366008335451551\n",
      "Gradient Descent(2311/9999): loss=376.113980947741, w0=0.030142040552861068, w1=0.3365673139866336\n",
      "Gradient Descent(2312/9999): loss=376.0316390911793, w0=0.03013959014470547, w1=0.3365337983884466\n",
      "Gradient Descent(2313/9999): loss=375.9493211039112, w0=0.030137139709750765, w1=0.3365002867481793\n",
      "Gradient Descent(2314/9999): loss=375.8670269620195, w0=0.030134689248708282, w1=0.3364667790634223\n",
      "Gradient Descent(2315/9999): loss=375.78475664168366, w0=0.03013223876228745, w1=0.33643327533177186\n",
      "Gradient Descent(2316/9999): loss=375.70251011917935, w0=0.0301297882511958, w1=0.3363997755508296\n",
      "Gradient Descent(2317/9999): loss=375.6202873708776, w0=0.03012733771613897, w1=0.3363662797182026\n",
      "Gradient Descent(2318/9999): loss=375.5380883732451, w0=0.030124887157820704, w1=0.3363327878315034\n",
      "Gradient Descent(2319/9999): loss=375.45591310284294, w0=0.030122436576942864, w1=0.3362992998883499\n",
      "Gradient Descent(2320/9999): loss=375.37376153632647, w0=0.030119985974205433, w1=0.3362658158863655\n",
      "Gradient Descent(2321/9999): loss=375.29163365044474, w0=0.030117535350306524, w1=0.33623233582317896\n",
      "Gradient Descent(2322/9999): loss=375.2095294220403, w0=0.03011508470594237, w1=0.3361988596964244\n",
      "Gradient Descent(2323/9999): loss=375.1274488280484, w0=0.03011263404180735, w1=0.3361653875037413\n",
      "Gradient Descent(2324/9999): loss=375.0453918454964, w0=0.030110183358593973, w1=0.3361319192427746\n",
      "Gradient Descent(2325/9999): loss=374.9633584515039, w0=0.030107732656992903, w1=0.33609845491117446\n",
      "Gradient Descent(2326/9999): loss=374.88134862328184, w0=0.030105281937692945, w1=0.33606499450659644\n",
      "Gradient Descent(2327/9999): loss=374.79936233813174, w0=0.03010283120138106, w1=0.3360315380267014\n",
      "Gradient Descent(2328/9999): loss=374.7173995734462, w0=0.030100380448742373, w1=0.33599808546915544\n",
      "Gradient Descent(2329/9999): loss=374.6354603067073, w0=0.030097929680460166, w1=0.33596463683163\n",
      "Gradient Descent(2330/9999): loss=374.553544515487, w0=0.030095478897215894, w1=0.3359311921118019\n",
      "Gradient Descent(2331/9999): loss=374.4716521774462, w0=0.03009302809968918, w1=0.335897751307353\n",
      "Gradient Descent(2332/9999): loss=374.3897832703348, w0=0.030090577288557834, w1=0.3358643144159706\n",
      "Gradient Descent(2333/9999): loss=374.30793777199057, w0=0.03008812646449784, w1=0.33583088143534706\n",
      "Gradient Descent(2334/9999): loss=374.2261156603392, w0=0.030085675628183373, w1=0.33579745236318015\n",
      "Gradient Descent(2335/9999): loss=374.1443169133937, w0=0.030083224780286796, w1=0.3357640271971727\n",
      "Gradient Descent(2336/9999): loss=374.0625415092542, w0=0.030080773921478673, w1=0.3357306059350328\n",
      "Gradient Descent(2337/9999): loss=373.9807894261068, w0=0.03007832305242777, w1=0.33569718857447367\n",
      "Gradient Descent(2338/9999): loss=373.89906064222413, w0=0.030075872173801057, w1=0.33566377511321377\n",
      "Gradient Descent(2339/9999): loss=373.81735513596385, w0=0.030073421286263714, w1=0.3356303655489767\n",
      "Gradient Descent(2340/9999): loss=373.73567288576965, w0=0.03007097039047913, w1=0.33559695987949106\n",
      "Gradient Descent(2341/9999): loss=373.65401387016885, w0=0.030068519487108928, w1=0.33556355810249083\n",
      "Gradient Descent(2342/9999): loss=373.572378067774, w0=0.030066068576812943, w1=0.3355301602157149\n",
      "Gradient Descent(2343/9999): loss=373.49076545728093, w0=0.030063617660249245, w1=0.3354967662169073\n",
      "Gradient Descent(2344/9999): loss=373.4091760174692, w0=0.030061166738074133, w1=0.33546337610381727\n",
      "Gradient Descent(2345/9999): loss=373.3276097272014, w0=0.030058715810942147, w1=0.3354299898741989\n",
      "Gradient Descent(2346/9999): loss=373.24606656542267, w0=0.030056264879506068, w1=0.3353966075258116\n",
      "Gradient Descent(2347/9999): loss=373.1645465111601, w0=0.030053813944416925, w1=0.3353632290564196\n",
      "Gradient Descent(2348/9999): loss=373.0830495435231, w0=0.030051363006323998, w1=0.3353298544637923\n",
      "Gradient Descent(2349/9999): loss=373.00157564170195, w0=0.030048912065874823, w1=0.33529648374570414\n",
      "Gradient Descent(2350/9999): loss=372.92012478496815, w0=0.0300464611237152, w1=0.3352631168999345\n",
      "Gradient Descent(2351/9999): loss=372.83869695267373, w0=0.03004401018048919, w1=0.33522975392426774\n",
      "Gradient Descent(2352/9999): loss=372.7572921242509, w0=0.030041559236839117, w1=0.33519639481649327\n",
      "Gradient Descent(2353/9999): loss=372.67591027921117, w0=0.03003910829340559, w1=0.3351630395744054\n",
      "Gradient Descent(2354/9999): loss=372.5945513971461, w0=0.030036657350827493, w1=0.3351296881958035\n",
      "Gradient Descent(2355/9999): loss=372.51321545772566, w0=0.030034206409741995, w1=0.33509634067849187\n",
      "Gradient Descent(2356/9999): loss=372.4319024406986, w0=0.03003175547078454, w1=0.3350629970202796\n",
      "Gradient Descent(2357/9999): loss=372.35061232589175, w0=0.03002930453458888, w1=0.33502965721898087\n",
      "Gradient Descent(2358/9999): loss=372.26934509320955, w0=0.030026853601787053, w1=0.3349963212724147\n",
      "Gradient Descent(2359/9999): loss=372.18810072263403, w0=0.030024402673009404, w1=0.33496298917840495\n",
      "Gradient Descent(2360/9999): loss=372.1068791942239, w0=0.030021951748884574, w1=0.3349296609347805\n",
      "Gradient Descent(2361/9999): loss=372.02568048811503, w0=0.030019500830039517, w1=0.334896336539375\n",
      "Gradient Descent(2362/9999): loss=371.94450458451877, w0=0.030017049917099508, w1=0.334863015990027\n",
      "Gradient Descent(2363/9999): loss=371.8633514637227, w0=0.030014599010688127, w1=0.33482969928457984\n",
      "Gradient Descent(2364/9999): loss=371.78222110608976, w0=0.030012148111427286, w1=0.3347963864208818\n",
      "Gradient Descent(2365/9999): loss=371.70111349205774, w0=0.03000969721993722, w1=0.33476307739678585\n",
      "Gradient Descent(2366/9999): loss=371.6200286021396, w0=0.03000724633683649, w1=0.3347297722101499\n",
      "Gradient Descent(2367/9999): loss=371.538966416922, w0=0.030004795462742002, w1=0.3346964708588365\n",
      "Gradient Descent(2368/9999): loss=371.4579269170658, w0=0.030002344598268994, w1=0.3346631733407132\n",
      "Gradient Descent(2369/9999): loss=371.37691008330546, w0=0.02999989374403105, w1=0.3346298796536521\n",
      "Gradient Descent(2370/9999): loss=371.2959158964487, w0=0.029997442900640096, w1=0.3345965897955302\n",
      "Gradient Descent(2371/9999): loss=371.21494433737564, w0=0.029994992068706424, w1=0.33456330376422916\n",
      "Gradient Descent(2372/9999): loss=371.13399538703925, w0=0.029992541248838673, w1=0.33453002155763545\n",
      "Gradient Descent(2373/9999): loss=371.0530690264645, w0=0.02999009044164384, w1=0.33449674317364025\n",
      "Gradient Descent(2374/9999): loss=370.9721652367481, w0=0.029987639647727295, w1=0.3344634686101394\n",
      "Gradient Descent(2375/9999): loss=370.8912839990578, w0=0.02998518886769277, w1=0.3344301978650336\n",
      "Gradient Descent(2376/9999): loss=370.8104252946327, w0=0.029982738102142376, w1=0.33439693093622797\n",
      "Gradient Descent(2377/9999): loss=370.7295891047825, w0=0.029980287351676592, w1=0.3343636678216325\n",
      "Gradient Descent(2378/9999): loss=370.6487754108871, w0=0.02997783661689429, w1=0.3343304085191618\n",
      "Gradient Descent(2379/9999): loss=370.56798419439616, w0=0.02997538589839272, w1=0.33429715302673507\n",
      "Gradient Descent(2380/9999): loss=370.4872154368294, w0=0.02997293519676753, w1=0.3342639013422763\n",
      "Gradient Descent(2381/9999): loss=370.40646911977524, w0=0.02997048451261275, w1=0.33423065346371394\n",
      "Gradient Descent(2382/9999): loss=370.32574522489136, w0=0.02996803384652082, w1=0.3341974093889812\n",
      "Gradient Descent(2383/9999): loss=370.2450437339038, w0=0.029965583199082577, w1=0.3341641691160157\n",
      "Gradient Descent(2384/9999): loss=370.16436462860685, w0=0.029963132570887265, w1=0.3341309326427599\n",
      "Gradient Descent(2385/9999): loss=370.0837078908627, w0=0.029960681962522537, w1=0.33409769996716066\n",
      "Gradient Descent(2386/9999): loss=370.0030735026007, w0=0.029958231374574467, w1=0.3340644710871694\n",
      "Gradient Descent(2387/9999): loss=369.92246144581804, w0=0.02995578080762754, w1=0.3340312460007423\n",
      "Gradient Descent(2388/9999): loss=369.841871702578, w0=0.02995333026226466, w1=0.33399802470583984\n",
      "Gradient Descent(2389/9999): loss=369.76130425501094, w0=0.029950879739067175, w1=0.33396480720042715\n",
      "Gradient Descent(2390/9999): loss=369.680759085313, w0=0.02994842923861485, w1=0.3339315934824739\n",
      "Gradient Descent(2391/9999): loss=369.60023617574643, w0=0.029945978761485886, w1=0.33389838354995427\n",
      "Gradient Descent(2392/9999): loss=369.51973550863875, w0=0.02994352830825693, w1=0.33386517740084687\n",
      "Gradient Descent(2393/9999): loss=369.4392570663827, w0=0.029941077879503068, w1=0.33383197503313483\n",
      "Gradient Descent(2394/9999): loss=369.358800831436, w0=0.02993862747579783, w1=0.3337987764448058\n",
      "Gradient Descent(2395/9999): loss=369.2783667863208, w0=0.0299361770977132, w1=0.3337655816338519\n",
      "Gradient Descent(2396/9999): loss=369.19795491362345, w0=0.02993372674581962, w1=0.3337323905982696\n",
      "Gradient Descent(2397/9999): loss=369.1175651959942, w0=0.029931276420685982, w1=0.33369920333605985\n",
      "Gradient Descent(2398/9999): loss=369.0371976161466, w0=0.029928826122879653, w1=0.33366601984522815\n",
      "Gradient Descent(2399/9999): loss=368.95685215685785, w0=0.02992637585296646, w1=0.33363284012378425\n",
      "Gradient Descent(2400/9999): loss=368.8765288009677, w0=0.029923925611510697, w1=0.3335996641697424\n",
      "Gradient Descent(2401/9999): loss=368.7962275313789, w0=0.029921475399075144, w1=0.33356649198112126\n",
      "Gradient Descent(2402/9999): loss=368.715948331056, w0=0.02991902521622105, w1=0.33353332355594384\n",
      "Gradient Descent(2403/9999): loss=368.63569118302587, w0=0.029916575063508147, w1=0.3335001588922375\n",
      "Gradient Descent(2404/9999): loss=368.555456070377, w0=0.02991412494149466, w1=0.33346699798803403\n",
      "Gradient Descent(2405/9999): loss=368.4752429762589, w0=0.029911674850737303, w1=0.3334338408413695\n",
      "Gradient Descent(2406/9999): loss=368.3950518838827, w0=0.029909224791791277, w1=0.3334006874502844\n",
      "Gradient Descent(2407/9999): loss=368.31488277651965, w0=0.02990677476521029, w1=0.3333675378128234\n",
      "Gradient Descent(2408/9999): loss=368.2347356375022, w0=0.029904324771546552, w1=0.33333439192703573\n",
      "Gradient Descent(2409/9999): loss=368.154610450222, w0=0.02990187481135077, w1=0.33330124979097475\n",
      "Gradient Descent(2410/9999): loss=368.0745071981313, w0=0.02989942488517217, w1=0.33326811140269813\n",
      "Gradient Descent(2411/9999): loss=367.9944258647414, w0=0.029896974993558485, w1=0.33323497676026786\n",
      "Gradient Descent(2412/9999): loss=367.91436643362294, w0=0.029894525137055972, w1=0.3332018458617502\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(2413/9999): loss=367.834328888406, w0=0.029892075316209406, w1=0.3331687187052157\n",
      "Gradient Descent(2414/9999): loss=367.75431321277847, w0=0.029889625531562086, w1=0.3331355952887391\n",
      "Gradient Descent(2415/9999): loss=367.674319390487, w0=0.029887175783655837, w1=0.3331024756103994\n",
      "Gradient Descent(2416/9999): loss=367.5943474053365, w0=0.029884726073031026, w1=0.3330693596682799\n",
      "Gradient Descent(2417/9999): loss=367.51439724118916, w0=0.029882276400226553, w1=0.333036247460468\n",
      "Gradient Descent(2418/9999): loss=367.4344688819652, w0=0.02987982676577985, w1=0.33300313898505546\n",
      "Gradient Descent(2419/9999): loss=367.3545623116414, w0=0.029877377170226906, w1=0.3329700342401381\n",
      "Gradient Descent(2420/9999): loss=367.274677514252, w0=0.029874927614102247, w1=0.332936933223816\n",
      "Gradient Descent(2421/9999): loss=367.1948144738874, w0=0.029872478097938956, w1=0.3329038359341934\n",
      "Gradient Descent(2422/9999): loss=367.1149731746948, w0=0.02987002862226867, w1=0.3328707423693787\n",
      "Gradient Descent(2423/9999): loss=367.0351536008771, w0=0.029867579187621587, w1=0.33283765252748443\n",
      "Gradient Descent(2424/9999): loss=366.9553557366929, w0=0.029865129794526463, w1=0.33280456640662737\n",
      "Gradient Descent(2425/9999): loss=366.87557956645674, w0=0.029862680443510623, w1=0.33277148400492834\n",
      "Gradient Descent(2426/9999): loss=366.79582507453785, w0=0.029860231135099963, w1=0.3327384053205123\n",
      "Gradient Descent(2427/9999): loss=366.7160922453608, w0=0.029857781869818953, w1=0.3327053303515083\n",
      "Gradient Descent(2428/9999): loss=366.6363810634045, w0=0.029855332648190635, w1=0.3326722590960495\n",
      "Gradient Descent(2429/9999): loss=366.55669151320257, w0=0.02985288347073664, w1=0.3326391915522733\n",
      "Gradient Descent(2430/9999): loss=366.4770235793425, w0=0.02985043433797718, w1=0.33260612771832093\n",
      "Gradient Descent(2431/9999): loss=366.3973772464656, w0=0.02984798525043105, w1=0.33257306759233785\n",
      "Gradient Descent(2432/9999): loss=366.317752499267, w0=0.02984553620861565, w1=0.33254001117247356\n",
      "Gradient Descent(2433/9999): loss=366.23814932249485, w0=0.029843087213046966, w1=0.3325069584568816\n",
      "Gradient Descent(2434/9999): loss=366.1585677009505, w0=0.029840638264239584, w1=0.3324739094437196\n",
      "Gradient Descent(2435/9999): loss=366.07900761948827, w0=0.029838189362706696, w1=0.3324408641311491\n",
      "Gradient Descent(2436/9999): loss=365.99946906301466, w0=0.029835740508960096, w1=0.33240782251733575\n",
      "Gradient Descent(2437/9999): loss=365.91995201648837, w0=0.029833291703510192, w1=0.3323747846004492\n",
      "Gradient Descent(2438/9999): loss=365.84045646492046, w0=0.029830842946866006, w1=0.33234175037866315\n",
      "Gradient Descent(2439/9999): loss=365.76098239337347, w0=0.02982839423953517, w1=0.3323087198501551\n",
      "Gradient Descent(2440/9999): loss=365.68152978696145, w0=0.02982594558202395, w1=0.3322756930131067\n",
      "Gradient Descent(2441/9999): loss=365.60209863084975, w0=0.029823496974837226, w1=0.3322426698657036\n",
      "Gradient Descent(2442/9999): loss=365.5226889102544, w0=0.029821048418478508, w1=0.33220965040613526\n",
      "Gradient Descent(2443/9999): loss=365.4433006104426, w0=0.029818599913449935, w1=0.33217663463259517\n",
      "Gradient Descent(2444/9999): loss=365.3639337167315, w0=0.02981615146025229, w1=0.3321436225432808\n",
      "Gradient Descent(2445/9999): loss=365.2845882144888, w0=0.029813703059384987, w1=0.3321106141363934\n",
      "Gradient Descent(2446/9999): loss=365.20526408913196, w0=0.02981125471134608, w1=0.33207760941013836\n",
      "Gradient Descent(2447/9999): loss=365.12596132612833, w0=0.029808806416632274, w1=0.33204460836272476\n",
      "Gradient Descent(2448/9999): loss=365.0466799109945, w0=0.02980635817573892, w1=0.3320116109923657\n",
      "Gradient Descent(2449/9999): loss=364.96741982929643, w0=0.029803909989160025, w1=0.3319786172972782\n",
      "Gradient Descent(2450/9999): loss=364.88818106664894, w0=0.029801461857388245, w1=0.331945627275683\n",
      "Gradient Descent(2451/9999): loss=364.8089636087155, w0=0.0297990137809149, w1=0.33191264092580486\n",
      "Gradient Descent(2452/9999): loss=364.7297674412082, w0=0.029796565760229977, w1=0.33187965824587234\n",
      "Gradient Descent(2453/9999): loss=364.6505925498874, w0=0.02979411779582212, w1=0.33184667923411787\n",
      "Gradient Descent(2454/9999): loss=364.57143892056126, w0=0.02979166988817865, w1=0.3318137038887777\n",
      "Gradient Descent(2455/9999): loss=364.4923065390857, w0=0.029789222037785557, w1=0.33178073220809184\n",
      "Gradient Descent(2456/9999): loss=364.4131953913644, w0=0.02978677424512751, w1=0.33174776419030433\n",
      "Gradient Descent(2457/9999): loss=364.33410546334795, w0=0.029784326510687856, w1=0.33171479983366275\n",
      "Gradient Descent(2458/9999): loss=364.25503674103436, w0=0.02978187883494863, w1=0.3316818391364187\n",
      "Gradient Descent(2459/9999): loss=364.1759892104683, w0=0.02977943121839054, w1=0.3316488820968274\n",
      "Gradient Descent(2460/9999): loss=364.09696285774066, w0=0.029776983661493006, w1=0.33161592871314804\n",
      "Gradient Descent(2461/9999): loss=364.01795766898937, w0=0.029774536164734123, w1=0.33158297898364336\n",
      "Gradient Descent(2462/9999): loss=363.93897363039787, w0=0.02977208872859069, w1=0.33155003290658003\n",
      "Gradient Descent(2463/9999): loss=363.8600107281957, w0=0.02976964135353821, w1=0.3315170904802284\n",
      "Gradient Descent(2464/9999): loss=363.7810689486581, w0=0.02976719404005088, w1=0.3314841517028626\n",
      "Gradient Descent(2465/9999): loss=363.7021482781058, w0=0.02976474678860161, w1=0.33145121657276044\n",
      "Gradient Descent(2466/9999): loss=363.6232487029044, w0=0.029762299599662024, w1=0.3314182850882035\n",
      "Gradient Descent(2467/9999): loss=363.5443702094649, w0=0.029759852473702452, w1=0.331385357247477\n",
      "Gradient Descent(2468/9999): loss=363.4655127842426, w0=0.029757405411191942, w1=0.33135243304886997\n",
      "Gradient Descent(2469/9999): loss=363.3866764137376, w0=0.02975495841259827, w1=0.33131951249067504\n",
      "Gradient Descent(2470/9999): loss=363.3078610844945, w0=0.029752511478387925, w1=0.3312865955711886\n",
      "Gradient Descent(2471/9999): loss=363.22906678310153, w0=0.029750064609026128, w1=0.3312536822887107\n",
      "Gradient Descent(2472/9999): loss=363.15029349619107, w0=0.029747617804976832, w1=0.331220772641545\n",
      "Gradient Descent(2473/9999): loss=363.07154121043914, w0=0.02974517106670272, w1=0.3311878666279988\n",
      "Gradient Descent(2474/9999): loss=362.99280991256506, w0=0.029742724394665212, w1=0.33115496424638324\n",
      "Gradient Descent(2475/9999): loss=362.91409958933144, w0=0.029740277789324466, w1=0.33112206549501283\n",
      "Gradient Descent(2476/9999): loss=362.83541022754395, w0=0.029737831251139386, w1=0.3310891703722059\n",
      "Gradient Descent(2477/9999): loss=362.7567418140511, w0=0.029735384780567628, w1=0.33105627887628436\n",
      "Gradient Descent(2478/9999): loss=362.6780943357437, w0=0.029732938378065586, w1=0.3310233910055736\n",
      "Gradient Descent(2479/9999): loss=362.59946777955514, w0=0.029730492044088414, w1=0.3309905067584028\n",
      "Gradient Descent(2480/9999): loss=362.5208621324609, w0=0.02972804577909002, w1=0.33095762613310464\n",
      "Gradient Descent(2481/9999): loss=362.4422773814783, w0=0.02972559958352307, w1=0.3309247491280154\n",
      "Gradient Descent(2482/9999): loss=362.36371351366677, w0=0.029723153457838995, w1=0.3308918757414749\n",
      "Gradient Descent(2483/9999): loss=362.2851705161267, w0=0.029720707402487993, w1=0.3308590059718265\n",
      "Gradient Descent(2484/9999): loss=362.2066483760005, w0=0.029718261417919025, w1=0.3308261398174172\n",
      "Gradient Descent(2485/9999): loss=362.1281470804709, w0=0.02971581550457983, w1=0.3307932772765976\n",
      "Gradient Descent(2486/9999): loss=362.04966661676224, w0=0.029713369662916912, w1=0.33076041834772163\n",
      "Gradient Descent(2487/9999): loss=361.97120697213904, w0=0.029710923893375568, w1=0.3307275630291469\n",
      "Gradient Descent(2488/9999): loss=361.8927681339068, w0=0.02970847819639987, w1=0.3306947113192345\n",
      "Gradient Descent(2489/9999): loss=361.8143500894108, w0=0.029706032572432668, w1=0.33066186321634905\n",
      "Gradient Descent(2490/9999): loss=361.735952826037, w0=0.029703587021915608, w1=0.33062901871885864\n",
      "Gradient Descent(2491/9999): loss=361.6575763312108, w0=0.029701141545289123, w1=0.33059617782513484\n",
      "Gradient Descent(2492/9999): loss=361.5792205923976, w0=0.029698696142992444, w1=0.3305633405335528\n",
      "Gradient Descent(2493/9999): loss=361.5008855971022, w0=0.029696250815463596, w1=0.330530506842491\n",
      "Gradient Descent(2494/9999): loss=361.4225713328687, w0=0.029693805563139403, w1=0.3304976767503315\n",
      "Gradient Descent(2495/9999): loss=361.3442777872803, w0=0.029691360386455493, w1=0.33046485025545974\n",
      "Gradient Descent(2496/9999): loss=361.2660049479592, w0=0.0296889152858463, w1=0.33043202735626465\n",
      "Gradient Descent(2497/9999): loss=361.1877528025664, w0=0.029686470261745073, w1=0.3303992080511386\n",
      "Gradient Descent(2498/9999): loss=361.10952133880113, w0=0.02968402531458386, w1=0.33036639233847737\n",
      "Gradient Descent(2499/9999): loss=361.0313105444015, w0=0.029681580444793545, w1=0.3303335802166802\n",
      "Gradient Descent(2500/9999): loss=360.9531204071433, w0=0.02967913565280381, w1=0.3303007716841497\n",
      "Gradient Descent(2501/9999): loss=360.8749509148405, w0=0.029676690939043168, w1=0.3302679667392919\n",
      "Gradient Descent(2502/9999): loss=360.7968020553449, w0=0.02967424630393896, w1=0.33023516538051617\n",
      "Gradient Descent(2503/9999): loss=360.7186738165457, w0=0.029671801747917347, w1=0.3302023676062354\n",
      "Gradient Descent(2504/9999): loss=360.64056618636977, w0=0.02966935727140333, w1=0.3301695734148657\n",
      "Gradient Descent(2505/9999): loss=360.562479152781, w0=0.029666912874820733, w1=0.33013678280482667\n",
      "Gradient Descent(2506/9999): loss=360.48441270378055, w0=0.029664468558592224, w1=0.3301039957745412\n",
      "Gradient Descent(2507/9999): loss=360.4063668274062, w0=0.02966202432313931, w1=0.3300712123224356\n",
      "Gradient Descent(2508/9999): loss=360.32834151173245, w0=0.02965958016888234, w1=0.3300384324469394\n",
      "Gradient Descent(2509/9999): loss=360.25033674487037, w0=0.029657136096240506, w1=0.3300056561464857\n",
      "Gradient Descent(2510/9999): loss=360.1723525149676, w0=0.02965469210563185, w1=0.32997288341951064\n",
      "Gradient Descent(2511/9999): loss=360.09438881020736, w0=0.02965224819747327, w1=0.32994011426445385\n",
      "Gradient Descent(2512/9999): loss=360.0164456188092, w0=0.029649804372180514, w1=0.3299073486797582\n",
      "Gradient Descent(2513/9999): loss=359.93852292902847, w0=0.02964736063016819, w1=0.3298745866638699\n",
      "Gradient Descent(2514/9999): loss=359.86062072915587, w0=0.02964491697184976, w1=0.32984182821523844\n",
      "Gradient Descent(2515/9999): loss=359.78273900751765, w0=0.029642473397637564, w1=0.3298090733323166\n",
      "Gradient Descent(2516/9999): loss=359.7048777524757, w0=0.029640029907942792, w1=0.3297763220135605\n",
      "Gradient Descent(2517/9999): loss=359.6270369524264, w0=0.029637586503175512, w1=0.3297435742574294\n",
      "Gradient Descent(2518/9999): loss=359.5492165958013, w0=0.02963514318374466, w1=0.3297108300623859\n",
      "Gradient Descent(2519/9999): loss=359.4714166710666, w0=0.029632699950058055, w1=0.32967808942689575\n",
      "Gradient Descent(2520/9999): loss=359.3936371667232, w0=0.029630256802522386, w1=0.3296453523494281\n",
      "Gradient Descent(2521/9999): loss=359.31587807130643, w0=0.029627813741543226, w1=0.3296126188284552\n",
      "Gradient Descent(2522/9999): loss=359.2381393733855, w0=0.02962537076752503, w1=0.32957988886245265\n",
      "Gradient Descent(2523/9999): loss=359.16042106156425, w0=0.029622927880871147, w1=0.3295471624498992\n",
      "Gradient Descent(2524/9999): loss=359.0827231244797, w0=0.029620485081983804, w1=0.32951443958927673\n",
      "Gradient Descent(2525/9999): loss=359.00504555080323, w0=0.029618042371264128, w1=0.3294817202790704\n",
      "Gradient Descent(2526/9999): loss=358.9273883292393, w0=0.02961559974911214, w1=0.32944900451776865\n",
      "Gradient Descent(2527/9999): loss=358.84975144852615, w0=0.029613157215926757, w1=0.32941629230386293\n",
      "Gradient Descent(2528/9999): loss=358.772134897435, w0=0.029610714772105796, w1=0.329383583635848\n",
      "Gradient Descent(2529/9999): loss=358.69453866477016, w0=0.029608272418045985, w1=0.3293508785122218\n",
      "Gradient Descent(2530/9999): loss=358.61696273936883, w0=0.02960583015414295, w1=0.32931817693148524\n",
      "Gradient Descent(2531/9999): loss=358.5394071101011, w0=0.029603387980791238, w1=0.32928547889214255\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(2532/9999): loss=358.46187176586943, w0=0.029600945898384294, w1=0.3292527843927011\n",
      "Gradient Descent(2533/9999): loss=358.3843566956088, w0=0.029598503907314486, w1=0.32922009343167136\n",
      "Gradient Descent(2534/9999): loss=358.3068618882864, w0=0.029596062007973097, w1=0.3291874060075669\n",
      "Gradient Descent(2535/9999): loss=358.2293873329016, w0=0.029593620200750334, w1=0.3291547221189045\n",
      "Gradient Descent(2536/9999): loss=358.1519330184856, w0=0.029591178486035326, w1=0.32912204176420395\n",
      "Gradient Descent(2537/9999): loss=358.0744989341014, w0=0.029588736864216123, w1=0.32908936494198826\n",
      "Gradient Descent(2538/9999): loss=357.9970850688437, w0=0.029586295335679715, w1=0.3290566916507834\n",
      "Gradient Descent(2539/9999): loss=357.9196914118388, w0=0.029583853900812015, w1=0.32902402188911845\n",
      "Gradient Descent(2540/9999): loss=357.8423179522439, w0=0.029581412559997872, w1=0.32899135565552573\n",
      "Gradient Descent(2541/9999): loss=357.7649646792476, w0=0.029578971313621076, w1=0.3289586929485404\n",
      "Gradient Descent(2542/9999): loss=357.6876315820697, w0=0.02957653016206435, w1=0.32892603376670093\n",
      "Gradient Descent(2543/9999): loss=357.61031864996056, w0=0.02957408910570937, w1=0.32889337810854863\n",
      "Gradient Descent(2544/9999): loss=357.5330258722013, w0=0.029571648144936742, w1=0.32886072597262794\n",
      "Gradient Descent(2545/9999): loss=357.45575323810374, w0=0.029569207280126037, w1=0.3288280773574864\n",
      "Gradient Descent(2546/9999): loss=357.37850073701003, w0=0.029566766511655768, w1=0.3287954322616745\n",
      "Gradient Descent(2547/9999): loss=357.30126835829253, w0=0.0295643258399034, w1=0.3287627906837458\n",
      "Gradient Descent(2548/9999): loss=357.22405609135365, w0=0.029561885265245356, w1=0.3287301526222569\n",
      "Gradient Descent(2549/9999): loss=357.14686392562595, w0=0.029559444788057023, w1=0.32869751807576736\n",
      "Gradient Descent(2550/9999): loss=357.06969185057153, w0=0.029557004408712742, w1=0.32866488704283975\n",
      "Gradient Descent(2551/9999): loss=356.9925398556825, w0=0.029554564127585822, w1=0.3286322595220396\n",
      "Gradient Descent(2552/9999): loss=356.91540793048034, w0=0.029552123945048542, w1=0.3285996355119356\n",
      "Gradient Descent(2553/9999): loss=356.8382960645157, w0=0.029549683861472147, w1=0.3285670150110992\n",
      "Gradient Descent(2554/9999): loss=356.7612042473687, w0=0.029547243877226858, w1=0.3285343980181049\n",
      "Gradient Descent(2555/9999): loss=356.68413246864856, w0=0.029544803992681862, w1=0.3285017845315303\n",
      "Gradient Descent(2556/9999): loss=356.60708071799337, w0=0.02954236420820533, w1=0.32846917454995567\n",
      "Gradient Descent(2557/9999): loss=356.53004898506975, w0=0.029539924524164415, w1=0.32843656807196453\n",
      "Gradient Descent(2558/9999): loss=356.45303725957376, w0=0.02953748494092525, w1=0.32840396509614317\n",
      "Gradient Descent(2559/9999): loss=356.37604553122884, w0=0.029535045458852955, w1=0.3283713656210808\n",
      "Gradient Descent(2560/9999): loss=356.29907378978817, w0=0.029532606078311634, w1=0.32833876964536973\n",
      "Gradient Descent(2561/9999): loss=356.2221220250319, w0=0.029530166799664387, w1=0.32830617716760496\n",
      "Gradient Descent(2562/9999): loss=356.1451902267692, w0=0.029527727623273305, w1=0.3282735881863846\n",
      "Gradient Descent(2563/9999): loss=356.0682783848366, w0=0.029525288549499474, w1=0.3282410027003095\n",
      "Gradient Descent(2564/9999): loss=355.9913864890989, w0=0.02952284957870298, w1=0.3282084207079835\n",
      "Gradient Descent(2565/9999): loss=355.9145145294483, w0=0.029520410711242912, w1=0.3281758422080134\n",
      "Gradient Descent(2566/9999): loss=355.83766249580475, w0=0.029517971947477357, w1=0.3281432671990087\n",
      "Gradient Descent(2567/9999): loss=355.7608303781155, w0=0.029515533287763414, w1=0.32811069567958195\n",
      "Gradient Descent(2568/9999): loss=355.68401816635514, w0=0.029513094732457185, w1=0.3280781276483484\n",
      "Gradient Descent(2569/9999): loss=355.6072258505254, w0=0.02951065628191379, w1=0.3280455631039264\n",
      "Gradient Descent(2570/9999): loss=355.53045342065536, w0=0.02950821793648736, w1=0.3280130020449369\n",
      "Gradient Descent(2571/9999): loss=355.4537008668001, w0=0.02950577969653104, w1=0.32798044447000374\n",
      "Gradient Descent(2572/9999): loss=355.37696817904265, w0=0.029503341562396996, w1=0.3279478903777538\n",
      "Gradient Descent(2573/9999): loss=355.30025534749177, w0=0.029500903534436414, w1=0.3279153397668165\n",
      "Gradient Descent(2574/9999): loss=355.223562362283, w0=0.029498465612999507, w1=0.3278827926358244\n",
      "Gradient Descent(2575/9999): loss=355.1468892135785, w0=0.029496027798435517, w1=0.32785024898341264\n",
      "Gradient Descent(2576/9999): loss=355.07023589156637, w0=0.029493590091092706, w1=0.3278177088082192\n",
      "Gradient Descent(2577/9999): loss=354.99360238646096, w0=0.029491152491318375, w1=0.32778517210888497\n",
      "Gradient Descent(2578/9999): loss=354.9169886885024, w0=0.029488714999458855, w1=0.32775263888405354\n",
      "Gradient Descent(2579/9999): loss=354.840394787957, w0=0.029486277615859518, w1=0.3277201091323713\n",
      "Gradient Descent(2580/9999): loss=354.76382067511656, w0=0.02948384034086477, w1=0.32768758285248745\n",
      "Gradient Descent(2581/9999): loss=354.6872663402988, w0=0.029481403174818063, w1=0.32765506004305395\n",
      "Gradient Descent(2582/9999): loss=354.6107317738465, w0=0.02947896611806189, w1=0.32762254070272556\n",
      "Gradient Descent(2583/9999): loss=354.53421696612816, w0=0.02947652917093779, w1=0.32759002483015975\n",
      "Gradient Descent(2584/9999): loss=354.45772190753735, w0=0.029474092333786353, w1=0.32755751242401676\n",
      "Gradient Descent(2585/9999): loss=354.38124658849284, w0=0.02947165560694722, w1=0.3275250034829596\n",
      "Gradient Descent(2586/9999): loss=354.30479099943824, w0=0.029469218990759088, w1=0.327492498005654\n",
      "Gradient Descent(2587/9999): loss=354.2283551308423, w0=0.0294667824855597, w1=0.3274599959907684\n",
      "Gradient Descent(2588/9999): loss=354.1519389731981, w0=0.029464346091685873, w1=0.327427497436974\n",
      "Gradient Descent(2589/9999): loss=354.075542517024, w0=0.029461909809473476, w1=0.3273950023429448\n",
      "Gradient Descent(2590/9999): loss=353.9991657528622, w0=0.029459473639257442, w1=0.3273625107073572\n",
      "Gradient Descent(2591/9999): loss=353.9228086712797, w0=0.02945703758137177, w1=0.3273300225288907\n",
      "Gradient Descent(2592/9999): loss=353.8464712628677, w0=0.029454601636149526, w1=0.3272975378062272\n",
      "Gradient Descent(2593/9999): loss=353.77015351824133, w0=0.029452165803922854, w1=0.32726505653805155\n",
      "Gradient Descent(2594/9999): loss=353.69385542804, w0=0.029449730085022965, w1=0.327232578723051\n",
      "Gradient Descent(2595/9999): loss=353.61757698292723, w0=0.029447294479780145, w1=0.3272001043599157\n",
      "Gradient Descent(2596/9999): loss=353.54131817359, w0=0.029444858988523764, w1=0.32716763344733835\n",
      "Gradient Descent(2597/9999): loss=353.4650789907389, w0=0.029442423611582263, w1=0.3271351659840143\n",
      "Gradient Descent(2598/9999): loss=353.38885942510854, w0=0.029439988349283176, w1=0.3271027019686417\n",
      "Gradient Descent(2599/9999): loss=353.3126594674567, w0=0.029437553201953112, w1=0.32707024139992125\n",
      "Gradient Descent(2600/9999): loss=353.23647910856437, w0=0.029435118169917774, w1=0.3270377842765563\n",
      "Gradient Descent(2601/9999): loss=353.1603183392362, w0=0.029432683253501953, w1=0.32700533059725273\n",
      "Gradient Descent(2602/9999): loss=353.0841771502995, w0=0.029430248453029533, w1=0.32697288036071925\n",
      "Gradient Descent(2603/9999): loss=353.00805553260494, w0=0.02942781376882349, w1=0.3269404335656671\n",
      "Gradient Descent(2604/9999): loss=352.93195347702607, w0=0.029425379201205904, w1=0.3269079902108101\n",
      "Gradient Descent(2605/9999): loss=352.85587097445904, w0=0.02942294475049794, w1=0.3268755502948647\n",
      "Gradient Descent(2606/9999): loss=352.77980801582265, w0=0.02942051041701988, w1=0.32684311381655\n",
      "Gradient Descent(2607/9999): loss=352.7037645920586, w0=0.0294180762010911, w1=0.3268106807745877\n",
      "Gradient Descent(2608/9999): loss=352.62774069413064, w0=0.029415642103030083, w1=0.3267782511677019\n",
      "Gradient Descent(2609/9999): loss=352.55173631302523, w0=0.029413208123154424, w1=0.3267458249946196\n",
      "Gradient Descent(2610/9999): loss=352.47575143975087, w0=0.029410774261780827, w1=0.32671340225407014\n",
      "Gradient Descent(2611/9999): loss=352.39978606533816, w0=0.02940834051922511, w1=0.3266809829447855\n",
      "Gradient Descent(2612/9999): loss=352.3238401808399, w0=0.029405906895802202, w1=0.32664856706550016\n",
      "Gradient Descent(2613/9999): loss=352.24791377733067, w0=0.029403473391826158, w1=0.3266161546149513\n",
      "Gradient Descent(2614/9999): loss=352.1720068459071, w0=0.02940104000761014, w1=0.3265837455918785\n",
      "Gradient Descent(2615/9999): loss=352.09611937768716, w0=0.02939860674346645, w1=0.32655133999502406\n",
      "Gradient Descent(2616/9999): loss=352.0202513638106, w0=0.029396173599706494, w1=0.3265189378231326\n",
      "Gradient Descent(2617/9999): loss=351.9444027954389, w0=0.02939374057664082, w1=0.32648653907495145\n",
      "Gradient Descent(2618/9999): loss=351.86857366375455, w0=0.0293913076745791, w1=0.3264541437492303\n",
      "Gradient Descent(2619/9999): loss=351.7927639599616, w0=0.029388874893830134, w1=0.32642175184472155\n",
      "Gradient Descent(2620/9999): loss=351.71697367528526, w0=0.02938644223470186, w1=0.32638936336018\n",
      "Gradient Descent(2621/9999): loss=351.6412028009718, w0=0.02938400969750135, w1=0.3263569782943629\n",
      "Gradient Descent(2622/9999): loss=351.5654513282883, w0=0.029381577282534814, w1=0.3263245966460301\n",
      "Gradient Descent(2623/9999): loss=351.48971924852304, w0=0.0293791449901076, w1=0.3262922184139439\n",
      "Gradient Descent(2624/9999): loss=351.41400655298514, w0=0.0293767128205242, w1=0.3262598435968691\n",
      "Gradient Descent(2625/9999): loss=351.338313233004, w0=0.02937428077408825, w1=0.32622747219357295\n",
      "Gradient Descent(2626/9999): loss=351.2626392799298, w0=0.029371848851102535, w1=0.3261951042028252\n",
      "Gradient Descent(2627/9999): loss=351.1869846851333, w0=0.029369417051868987, w1=0.32616273962339815\n",
      "Gradient Descent(2628/9999): loss=351.1113494400056, w0=0.029366985376688685, w1=0.32613037845406634\n",
      "Gradient Descent(2629/9999): loss=351.0357335359581, w0=0.029364553825861874, w1=0.32609802069360694\n",
      "Gradient Descent(2630/9999): loss=350.96013696442236, w0=0.029362122399687936, w1=0.32606566634079953\n",
      "Gradient Descent(2631/9999): loss=350.88455971685016, w0=0.029359691098465427, w1=0.32603331539442615\n",
      "Gradient Descent(2632/9999): loss=350.80900178471313, w0=0.029357259922492055, w1=0.3260009678532712\n",
      "Gradient Descent(2633/9999): loss=350.73346315950283, w0=0.02935482887206469, w1=0.3259686237161216\n",
      "Gradient Descent(2634/9999): loss=350.6579438327306, w0=0.029352397947479368, w1=0.3259362829817667\n",
      "Gradient Descent(2635/9999): loss=350.5824437959278, w0=0.029349967149031292, w1=0.3259039456489981\n",
      "Gradient Descent(2636/9999): loss=350.5069630406449, w0=0.02934753647701483, w1=0.32587161171661\n",
      "Gradient Descent(2637/9999): loss=350.4315015584524, w0=0.029345105931723528, w1=0.3258392811833989\n",
      "Gradient Descent(2638/9999): loss=350.35605934093985, w0=0.029342675513450094, w1=0.32580695404816373\n",
      "Gradient Descent(2639/9999): loss=350.28063637971644, w0=0.02934024522248642, w1=0.3257746303097059\n",
      "Gradient Descent(2640/9999): loss=350.2052326664105, w0=0.029337815059123567, w1=0.325742309966829\n",
      "Gradient Descent(2641/9999): loss=350.1298481926692, w0=0.029335385023651784, w1=0.32570999301833925\n",
      "Gradient Descent(2642/9999): loss=350.05448295015947, w0=0.0293329551163605, w1=0.325677679463045\n",
      "Gradient Descent(2643/9999): loss=349.9791369305665, w0=0.029330525337538316, w1=0.3256453692997571\n",
      "Gradient Descent(2644/9999): loss=349.9038101255948, w0=0.029328095687473035, w1=0.32561306252728883\n",
      "Gradient Descent(2645/9999): loss=349.82850252696744, w0=0.029325666166451637, w1=0.32558075914445567\n",
      "Gradient Descent(2646/9999): loss=349.75321412642637, w0=0.029323236774760293, w1=0.3255484591500755\n",
      "Gradient Descent(2647/9999): loss=349.6779449157322, w0=0.029320807512684368, w1=0.32551616254296867\n",
      "Gradient Descent(2648/9999): loss=349.60269488666376, w0=0.02931837838050842, w1=0.32548386932195766\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(2649/9999): loss=349.52746403101867, w0=0.029315949378516202, w1=0.32545157948586745\n",
      "Gradient Descent(2650/9999): loss=349.45225234061246, w0=0.029313520506990667, w1=0.32541929303352524\n",
      "Gradient Descent(2651/9999): loss=349.37705980727964, w0=0.029311091766213965, w1=0.3253870099637606\n",
      "Gradient Descent(2652/9999): loss=349.301886422872, w0=0.02930866315646745, w1=0.32535473027540535\n",
      "Gradient Descent(2653/9999): loss=349.22673217926, w0=0.02930623467803168, w1=0.32532245396729376\n",
      "Gradient Descent(2654/9999): loss=349.1515970683321, w0=0.029303806331186424, w1=0.3252901810382623\n",
      "Gradient Descent(2655/9999): loss=349.0764810819945, w0=0.029301378116210648, w1=0.3252579114871497\n",
      "Gradient Descent(2656/9999): loss=349.00138421217116, w0=0.02929895003338254, w1=0.32522564531279713\n",
      "Gradient Descent(2657/9999): loss=348.92630645080396, w0=0.029296522082979497, w1=0.3251933825140479\n",
      "Gradient Descent(2658/9999): loss=348.85124778985244, w0=0.029294094265278126, w1=0.3251611230897476\n",
      "Gradient Descent(2659/9999): loss=348.7762082212936, w0=0.02929166658055425, w1=0.32512886703874416\n",
      "Gradient Descent(2660/9999): loss=348.701187737122, w0=0.02928923902908292, w1=0.3250966143598878\n",
      "Gradient Descent(2661/9999): loss=348.62618632934954, w0=0.029286811611138403, w1=0.32506436505203096\n",
      "Gradient Descent(2662/9999): loss=348.5512039900056, w0=0.029284384326994182, w1=0.3250321191140283\n",
      "Gradient Descent(2663/9999): loss=348.47624071113665, w0=0.02928195717692297, w1=0.3249998765447368\n",
      "Gradient Descent(2664/9999): loss=348.4012964848063, w0=0.029279530161196707, w1=0.3249676373430156\n",
      "Gradient Descent(2665/9999): loss=348.3263713030955, w0=0.02927710328008656, w1=0.32493540150772615\n",
      "Gradient Descent(2666/9999): loss=348.25146515810206, w0=0.02927467653386293, w1=0.32490316903773214\n",
      "Gradient Descent(2667/9999): loss=348.1765780419405, w0=0.029272249922795444, w1=0.3248709399318994\n",
      "Gradient Descent(2668/9999): loss=348.1017099467423, w0=0.02926982344715297, w1=0.32483871418909616\n",
      "Gradient Descent(2669/9999): loss=348.0268608646559, w0=0.02926739710720361, w1=0.3248064918081926\n",
      "Gradient Descent(2670/9999): loss=347.9520307878463, w0=0.029264970903214702, w1=0.32477427278806137\n",
      "Gradient Descent(2671/9999): loss=347.877219708495, w0=0.029262544835452826, w1=0.32474205712757714\n",
      "Gradient Descent(2672/9999): loss=347.8024276188001, w0=0.029260118904183806, w1=0.3247098448256169\n",
      "Gradient Descent(2673/9999): loss=347.7276545109762, w0=0.029257693109672706, w1=0.32467763588105975\n",
      "Gradient Descent(2674/9999): loss=347.6529003772542, w0=0.02925526745218384, w1=0.32464543029278703\n",
      "Gradient Descent(2675/9999): loss=347.5781652098812, w0=0.029252841931980772, w1=0.32461322805968224\n",
      "Gradient Descent(2676/9999): loss=347.50344900112077, w0=0.02925041654932631, w1=0.3245810291806311\n",
      "Gradient Descent(2677/9999): loss=347.4287517432523, w0=0.029247991304482523, w1=0.3245488336545214\n",
      "Gradient Descent(2678/9999): loss=347.3540734285715, w0=0.029245566197710727, w1=0.32451664148024323\n",
      "Gradient Descent(2679/9999): loss=347.27941404938986, w0=0.029243141229271494, w1=0.3244844526566887\n",
      "Gradient Descent(2680/9999): loss=347.2047735980351, w0=0.029240716399424656, w1=0.32445226718275216\n",
      "Gradient Descent(2681/9999): loss=347.13015206685037, w0=0.029238291708429304, w1=0.3244200850573301\n",
      "Gradient Descent(2682/9999): loss=347.05554944819477, w0=0.029235867156543794, w1=0.3243879062793212\n",
      "Gradient Descent(2683/9999): loss=346.98096573444315, w0=0.02923344274402574, w1=0.32435573084762614\n",
      "Gradient Descent(2684/9999): loss=346.90640091798576, w0=0.029231018471132023, w1=0.3243235587611479\n",
      "Gradient Descent(2685/9999): loss=346.8318549912287, w0=0.029228594338118793, w1=0.3242913900187914\n",
      "Gradient Descent(2686/9999): loss=346.75732794659325, w0=0.029226170345241474, w1=0.3242592246194639\n",
      "Gradient Descent(2687/9999): loss=346.6828197765162, w0=0.02922374649275475, w1=0.32422706256207456\n",
      "Gradient Descent(2688/9999): loss=346.60833047344966, w0=0.029221322780912582, w1=0.32419490384553484\n",
      "Gradient Descent(2689/9999): loss=346.53386002986105, w0=0.029218899209968216, w1=0.3241627484687582\n",
      "Gradient Descent(2690/9999): loss=346.4594084382327, w0=0.02921647578017416, w1=0.3241305964306602\n",
      "Gradient Descent(2691/9999): loss=346.3849756910623, w0=0.029214052491782213, w1=0.32409844773015856\n",
      "Gradient Descent(2692/9999): loss=346.3105617808626, w0=0.029211629345043445, w1=0.32406630236617295\n",
      "Gradient Descent(2693/9999): loss=346.2361667001611, w0=0.02920920634020821, w1=0.32403416033762533\n",
      "Gradient Descent(2694/9999): loss=346.16179044150044, w0=0.029206783477526154, w1=0.3240020216434396\n",
      "Gradient Descent(2695/9999): loss=346.0874329974378, w0=0.029204360757246202, w1=0.32396988628254175\n",
      "Gradient Descent(2696/9999): loss=346.0130943605453, w0=0.02920193817961657, w1=0.32393775425385984\n",
      "Gradient Descent(2697/9999): loss=345.93877452340973, w0=0.02919951574488476, w1=0.323905625556324\n",
      "Gradient Descent(2698/9999): loss=345.86447347863236, w0=0.02919709345329757, w1=0.32387350018886646\n",
      "Gradient Descent(2699/9999): loss=345.79019121882936, w0=0.02919467130510109, w1=0.32384137815042147\n",
      "Gradient Descent(2700/9999): loss=345.7159277366309, w0=0.0291922493005407, w1=0.3238092594399253\n",
      "Gradient Descent(2701/9999): loss=345.6416830246817, w0=0.029189827439861087, w1=0.32377714405631625\n",
      "Gradient Descent(2702/9999): loss=345.56745707564124, w0=0.02918740572330623, w1=0.32374503199853477\n",
      "Gradient Descent(2703/9999): loss=345.4932498821827, w0=0.029184984151119413, w1=0.32371292326552326\n",
      "Gradient Descent(2704/9999): loss=345.41906143699373, w0=0.029182562723543217, w1=0.3236808178562261\n",
      "Gradient Descent(2705/9999): loss=345.34489173277626, w0=0.029180141440819533, w1=0.3236487157695898\n",
      "Gradient Descent(2706/9999): loss=345.2707407622459, w0=0.029177720303189554, w1=0.3236166170045628\n",
      "Gradient Descent(2707/9999): loss=345.1966085181328, w0=0.02917529931089378, w1=0.3235845215600956\n",
      "Gradient Descent(2708/9999): loss=345.1224949931806, w0=0.029172878464172027, w1=0.3235524294351407\n",
      "Gradient Descent(2709/9999): loss=345.0484001801469, w0=0.029170457763263418, w1=0.3235203406286525\n",
      "Gradient Descent(2710/9999): loss=344.9743240718034, w0=0.02916803720840639, w1=0.32348825513958757\n",
      "Gradient Descent(2711/9999): loss=344.9002666609352, w0=0.029165616799838694, w1=0.3234561729669044\n",
      "Gradient Descent(2712/9999): loss=344.8262279403413, w0=0.029163196537797396, w1=0.32342409410956335\n",
      "Gradient Descent(2713/9999): loss=344.7522079028342, w0=0.029160776422518884, w1=0.323392018566527\n",
      "Gradient Descent(2714/9999): loss=344.6782065412402, w0=0.029158356454238872, w1=0.3233599463367597\n",
      "Gradient Descent(2715/9999): loss=344.60422384839876, w0=0.029155936633192384, w1=0.32332787741922786\n",
      "Gradient Descent(2716/9999): loss=344.530259817163, w0=0.029153516959613775, w1=0.32329581181289985\n",
      "Gradient Descent(2717/9999): loss=344.4563144403993, w0=0.029151097433736724, w1=0.3232637495167459\n",
      "Gradient Descent(2718/9999): loss=344.3823877109873, w0=0.029148678055794238, w1=0.3232316905297384\n",
      "Gradient Descent(2719/9999): loss=344.30847962182025, w0=0.02914625882601865, w1=0.3231996348508515\n",
      "Gradient Descent(2720/9999): loss=344.23459016580404, w0=0.02914383974464163, w1=0.3231675824790614\n",
      "Gradient Descent(2721/9999): loss=344.16071933585806, w0=0.029141420811894178, w1=0.3231355334133462\n",
      "Gradient Descent(2722/9999): loss=344.0868671249149, w0=0.029139002028006622, w1=0.323103487652686\n",
      "Gradient Descent(2723/9999): loss=344.01303352591935, w0=0.029136583393208633, w1=0.3230714451960627\n",
      "Gradient Descent(2724/9999): loss=343.93921853183036, w0=0.02913416490772922, w1=0.32303940604246023\n",
      "Gradient Descent(2725/9999): loss=343.8654221356186, w0=0.029131746571796725, w1=0.3230073701908644\n",
      "Gradient Descent(2726/9999): loss=343.79164433026835, w0=0.02912932838563884, w1=0.322975337640263\n",
      "Gradient Descent(2727/9999): loss=343.7178851087762, w0=0.029126910349482595, w1=0.32294330838964563\n",
      "Gradient Descent(2728/9999): loss=343.64414446415185, w0=0.02912449246355436, w1=0.3229112824380039\n",
      "Gradient Descent(2729/9999): loss=343.5704223894174, w0=0.029122074728079858, w1=0.32287925978433124\n",
      "Gradient Descent(2730/9999): loss=343.4967188776074, w0=0.02911965714328416, w1=0.32284724042762303\n",
      "Gradient Descent(2731/9999): loss=343.4230339217691, w0=0.02911723970939168, w1=0.3228152243668765\n",
      "Gradient Descent(2732/9999): loss=343.3493675149623, w0=0.02911482242662619, w1=0.3227832116010908\n",
      "Gradient Descent(2733/9999): loss=343.2757196502592, w0=0.02911240529521082, w1=0.32275120212926695\n",
      "Gradient Descent(2734/9999): loss=343.202090320744, w0=0.029109988315368034, w1=0.3227191959504078\n",
      "Gradient Descent(2735/9999): loss=343.1284795195138, w0=0.029107571487319673, w1=0.32268719306351823\n",
      "Gradient Descent(2736/9999): loss=343.0548872396774, w0=0.029105154811286928, w1=0.3226551934676048\n",
      "Gradient Descent(2737/9999): loss=342.9813134743561, w0=0.029102738287490347, w1=0.32262319716167603\n",
      "Gradient Descent(2738/9999): loss=342.9077582166834, w0=0.029100321916149842, w1=0.3225912041447423\n",
      "Gradient Descent(2739/9999): loss=342.83422145980444, w0=0.029097905697484692, w1=0.32255921441581575\n",
      "Gradient Descent(2740/9999): loss=342.7607031968769, w0=0.029095489631713534, w1=0.32252722797391054\n",
      "Gradient Descent(2741/9999): loss=342.68720342107, w0=0.029093073719054372, w1=0.32249524481804254\n",
      "Gradient Descent(2742/9999): loss=342.6137221255652, w0=0.029090657959724583, w1=0.3224632649472295\n",
      "Gradient Descent(2743/9999): loss=342.54025930355556, w0=0.02908824235394091, w1=0.322431288360491\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(2744/9999): loss=342.4668149482461, w0=0.02908582690191947, w1=0.3223993150568485\n",
      "Gradient Descent(2745/9999): loss=342.3933890528535, w0=0.029083411603875738, w1=0.32236734503532516\n",
      "Gradient Descent(2746/9999): loss=342.31998161060625, w0=0.029080996460024586, w1=0.32233537829494613\n",
      "Gradient Descent(2747/9999): loss=342.2465926147443, w0=0.02907858147058025, w1=0.32230341483473823\n",
      "Gradient Descent(2748/9999): loss=342.17322205851934, w0=0.029076166635756344, w1=0.3222714546537302\n",
      "Gradient Descent(2749/9999): loss=342.0998699351946, w0=0.02907375195576586, w1=0.32223949775095245\n",
      "Gradient Descent(2750/9999): loss=342.0265362380447, w0=0.029071337430821177, w1=0.32220754412543734\n",
      "Gradient Descent(2751/9999): loss=341.9532209603555, w0=0.029068923061134046, w1=0.32217559377621896\n",
      "Gradient Descent(2752/9999): loss=341.87992409542477, w0=0.029066508846915612, w1=0.32214364670233325\n",
      "Gradient Descent(2753/9999): loss=341.806645636561, w0=0.0290640947883764, w1=0.3221117029028178\n",
      "Gradient Descent(2754/9999): loss=341.73338557708433, w0=0.02906168088572633, w1=0.3220797623767122\n",
      "Gradient Descent(2755/9999): loss=341.6601439103262, w0=0.029059267139174696, w1=0.3220478251230576\n",
      "Gradient Descent(2756/9999): loss=341.5869206296289, w0=0.029056853548930194, w1=0.322015891140897\n",
      "Gradient Descent(2757/9999): loss=341.5137157283458, w0=0.02905444011520091, w1=0.3219839604292753\n",
      "Gradient Descent(2758/9999): loss=341.4405291998418, w0=0.02905202683819432, w1=0.32195203298723896\n",
      "Gradient Descent(2759/9999): loss=341.3673610374925, w0=0.0290496137181173, w1=0.32192010881383637\n",
      "Gradient Descent(2760/9999): loss=341.2942112346844, w0=0.02904720075517612, w1=0.3218881879081176\n",
      "Gradient Descent(2761/9999): loss=341.221079784815, w0=0.029044787949576444, w1=0.3218562702691344\n",
      "Gradient Descent(2762/9999): loss=341.1479666812928, w0=0.029042375301523343, w1=0.32182435589594044\n",
      "Gradient Descent(2763/9999): loss=341.07487191753694, w0=0.029039962811221287, w1=0.321792444787591\n",
      "Gradient Descent(2764/9999): loss=341.0017954869774, w0=0.029037550478874147, w1=0.32176053694314316\n",
      "Gradient Descent(2765/9999): loss=340.9287373830548, w0=0.0290351383046852, w1=0.3217286323616557\n",
      "Gradient Descent(2766/9999): loss=340.8556975992208, w0=0.029032726288857124, w1=0.3216967310421892\n",
      "Gradient Descent(2767/9999): loss=340.7826761289371, w0=0.029030314431592014, w1=0.3216648329838059\n",
      "Gradient Descent(2768/9999): loss=340.70967296567625, w0=0.029027902733091367, w1=0.32163293818556976\n",
      "Gradient Descent(2769/9999): loss=340.6366881029215, w0=0.029025491193556092, w1=0.3216010466465465\n",
      "Gradient Descent(2770/9999): loss=340.5637215341663, w0=0.029023079813186508, w1=0.32156915836580346\n",
      "Gradient Descent(2771/9999): loss=340.4907732529148, w0=0.02902066859218235, w1=0.32153727334240984\n",
      "Gradient Descent(2772/9999): loss=340.4178432526812, w0=0.02901825753074277, w1=0.32150539157543645\n",
      "Gradient Descent(2773/9999): loss=340.34493152699037, w0=0.029015846629066335, w1=0.3214735130639558\n",
      "Gradient Descent(2774/9999): loss=340.2720380693773, w0=0.029013435887351027, w1=0.3214416378070421\n",
      "Gradient Descent(2775/9999): loss=340.19916287338725, w0=0.02901102530579425, w1=0.3214097658037713\n",
      "Gradient Descent(2776/9999): loss=340.1263059325756, w0=0.02900861488459283, w1=0.32137789705322095\n",
      "Gradient Descent(2777/9999): loss=340.0534672405081, w0=0.029006204623943016, w1=0.32134603155447033\n",
      "Gradient Descent(2778/9999): loss=339.9806467907603, w0=0.029003794524040477, w1=0.32131416930660045\n",
      "Gradient Descent(2779/9999): loss=339.90784457691814, w0=0.029001384585080312, w1=0.3212823103086939\n",
      "Gradient Descent(2780/9999): loss=339.83506059257746, w0=0.028998974807257043, w1=0.32125045455983503\n",
      "Gradient Descent(2781/9999): loss=339.76229483134387, w0=0.028996565190764625, w1=0.32121860205910974\n",
      "Gradient Descent(2782/9999): loss=339.6895472868331, w0=0.02899415573579644, w1=0.3211867528056057\n",
      "Gradient Descent(2783/9999): loss=339.6168179526707, w0=0.0289917464425453, w1=0.32115490679841224\n",
      "Gradient Descent(2784/9999): loss=339.5441068224924, w0=0.02898933731120346, w1=0.3211230640366203\n",
      "Gradient Descent(2785/9999): loss=339.4714138899429, w0=0.028986928341962592, w1=0.32109122451932237\n",
      "Gradient Descent(2786/9999): loss=339.3987391486776, w0=0.02898451953501382, w1=0.3210593882456128\n",
      "Gradient Descent(2787/9999): loss=339.3260825923608, w0=0.0289821108905477, w1=0.3210275552145875\n",
      "Gradient Descent(2788/9999): loss=339.2534442146672, w0=0.02897970240875422, w1=0.3209957254253439\n",
      "Gradient Descent(2789/9999): loss=339.1808240092805, w0=0.02897729408982282, w1=0.3209638988769811\n",
      "Gradient Descent(2790/9999): loss=339.1082219698943, w0=0.02897488593394237, w1=0.32093207556860004\n",
      "Gradient Descent(2791/9999): loss=339.03563809021165, w0=0.028972477941301193, w1=0.320900255499303\n",
      "Gradient Descent(2792/9999): loss=338.963072363945, w0=0.028970070112087052, w1=0.32086843866819403\n",
      "Gradient Descent(2793/9999): loss=338.8905247848166, w0=0.028967662446487158, w1=0.32083662507437877\n",
      "Gradient Descent(2794/9999): loss=338.81799534655755, w0=0.028965254944688164, w1=0.3208048147169644\n",
      "Gradient Descent(2795/9999): loss=338.74548404290886, w0=0.028962847606876174, w1=0.32077300759505994\n",
      "Gradient Descent(2796/9999): loss=338.6729908676205, w0=0.028960440433236753, w1=0.3207412037077757\n",
      "Gradient Descent(2797/9999): loss=338.60051581445197, w0=0.0289580334239549, w1=0.3207094030542238\n",
      "Gradient Descent(2798/9999): loss=338.52805887717165, w0=0.02895562657921508, w1=0.3206776056335179\n",
      "Gradient Descent(2799/9999): loss=338.4556200495576, w0=0.02895321989920121, w1=0.3206458114447732\n",
      "Gradient Descent(2800/9999): loss=338.3831993253965, w0=0.028950813384096656, w1=0.32061402048710663\n",
      "Gradient Descent(2801/9999): loss=338.31079669848475, w0=0.02894840703408425, w1=0.32058223275963654\n",
      "Gradient Descent(2802/9999): loss=338.23841216262736, w0=0.028946000849346284, w1=0.32055044826148293\n",
      "Gradient Descent(2803/9999): loss=338.16604571163845, w0=0.0289435948300645, w1=0.3205186669917674\n",
      "Gradient Descent(2804/9999): loss=338.0936973393415, w0=0.028941188976420108, w1=0.3204868889496131\n",
      "Gradient Descent(2805/9999): loss=338.02136703956825, w0=0.028938783288593783, w1=0.3204551141341447\n",
      "Gradient Descent(2806/9999): loss=337.9490548061602, w0=0.02893637776676566, w1=0.3204233425444885\n",
      "Gradient Descent(2807/9999): loss=337.87676063296703, w0=0.028933972411115342, w1=0.3203915741797724\n",
      "Gradient Descent(2808/9999): loss=337.8044845138475, w0=0.0289315672218219, w1=0.32035980903912564\n",
      "Gradient Descent(2809/9999): loss=337.73222644266946, w0=0.02892916219906387, w1=0.3203280471216793\n",
      "Gradient Descent(2810/9999): loss=337.6599864133089, w0=0.028926757343019263, w1=0.32029628842656577\n",
      "Gradient Descent(2811/9999): loss=337.58776441965114, w0=0.028924352653865556, w1=0.3202645329529191\n",
      "Gradient Descent(2812/9999): loss=337.5155604555899, w0=0.0289219481317797, w1=0.320232780699875\n",
      "Gradient Descent(2813/9999): loss=337.4433745150275, w0=0.028919543776938124, w1=0.32020103166657043\n",
      "Gradient Descent(2814/9999): loss=337.3712065918749, w0=0.028917139589516725, w1=0.32016928585214405\n",
      "Gradient Descent(2815/9999): loss=337.2990566800519, w0=0.028914735569690885, w1=0.32013754325573607\n",
      "Gradient Descent(2816/9999): loss=337.22692477348636, w0=0.028912331717635457, w1=0.3201058038764882\n",
      "Gradient Descent(2817/9999): loss=337.154810866115, w0=0.028909928033524778, w1=0.3200740677135436\n",
      "Gradient Descent(2818/9999): loss=337.08271495188285, w0=0.028907524517532663, w1=0.320042334766047\n",
      "Gradient Descent(2819/9999): loss=337.0106370247434, w0=0.02890512116983241, w1=0.32001060503314466\n",
      "Gradient Descent(2820/9999): loss=336.9385770786585, w0=0.028902717990596804, w1=0.3199788785139844\n",
      "Gradient Descent(2821/9999): loss=336.86653510759845, w0=0.028900314979998107, w1=0.3199471552077154\n",
      "Gradient Descent(2822/9999): loss=336.79451110554146, w0=0.028897912138208075, w1=0.3199154351134884\n",
      "Gradient Descent(2823/9999): loss=336.72250506647475, w0=0.028895509465397946, w1=0.31988371823045575\n",
      "Gradient Descent(2824/9999): loss=336.650516984393, w0=0.02889310696173845, w1=0.31985200455777113\n",
      "Gradient Descent(2825/9999): loss=336.57854685329966, w0=0.02889070462739981, w1=0.3198202940945898\n",
      "Gradient Descent(2826/9999): loss=336.50659466720595, w0=0.02888830246255173, w1=0.31978858684006845\n",
      "Gradient Descent(2827/9999): loss=336.4346604201314, w0=0.028885900467363418, w1=0.31975688279336534\n",
      "Gradient Descent(2828/9999): loss=336.36274410610383, w0=0.028883498642003565, w1=0.31972518195364014\n",
      "Gradient Descent(2829/9999): loss=336.2908457191587, w0=0.02888109698664037, w1=0.319693484320054\n",
      "Gradient Descent(2830/9999): loss=336.21896525333983, w0=0.028878695501441516, w1=0.3196617898917696\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(2831/9999): loss=336.1471027026989, w0=0.028876294186574194, w1=0.3196300986679509\n",
      "Gradient Descent(2832/9999): loss=336.07525806129536, w0=0.02887389304220509, w1=0.31959841064776356\n",
      "Gradient Descent(2833/9999): loss=336.00343132319693, w0=0.02887149206850039, w1=0.31956672583037465\n",
      "Gradient Descent(2834/9999): loss=335.9316224824791, w0=0.028869091265625785, w1=0.3195350442149526\n",
      "Gradient Descent(2835/9999): loss=335.8598315332252, w0=0.028866690633746465, w1=0.31950336580066735\n",
      "Gradient Descent(2836/9999): loss=335.7880584695261, w0=0.028864290173027128, w1=0.3194716905866902\n",
      "Gradient Descent(2837/9999): loss=335.71630328548093, w0=0.02886188988363197, w1=0.3194400185721941\n",
      "Gradient Descent(2838/9999): loss=335.6445659751962, w0=0.028859489765724708, w1=0.3194083497563533\n",
      "Gradient Descent(2839/9999): loss=335.5728465327864, w0=0.028857089819468554, w1=0.3193766841383434\n",
      "Gradient Descent(2840/9999): loss=335.5011449523735, w0=0.02885469004502624, w1=0.31934502171734164\n",
      "Gradient Descent(2841/9999): loss=335.4294612280872, w0=0.028852290442559995, w1=0.31931336249252656\n",
      "Gradient Descent(2842/9999): loss=335.35779535406476, w0=0.028849891012231576, w1=0.31928170646307813\n",
      "Gradient Descent(2843/9999): loss=335.2861473244512, w0=0.028847491754202245, w1=0.31925005362817777\n",
      "Gradient Descent(2844/9999): loss=335.2145171333989, w0=0.028845092668632777, w1=0.31921840398700835\n",
      "Gradient Descent(2845/9999): loss=335.1429047750678, w0=0.028842693755683466, w1=0.31918675753875414\n",
      "Gradient Descent(2846/9999): loss=335.0713102436254, w0=0.028840295015514123, w1=0.31915511428260074\n",
      "Gradient Descent(2847/9999): loss=334.99973353324657, w0=0.02883789644828408, w1=0.3191234742177353\n",
      "Gradient Descent(2848/9999): loss=334.92817463811366, w0=0.02883549805415218, w1=0.31909183734334623\n",
      "Gradient Descent(2849/9999): loss=334.8566335524165, w0=0.028833099833276796, w1=0.3190602036586235\n",
      "Gradient Descent(2850/9999): loss=334.785110270352, w0=0.02883070178581582, w1=0.3190285731627583\n",
      "Gradient Descent(2851/9999): loss=334.71360478612445, w0=0.028828303911926662, w1=0.31899694585494337\n",
      "Gradient Descent(2852/9999): loss=334.64211709394596, w0=0.028825906211766266, w1=0.31896532173437275\n",
      "Gradient Descent(2853/9999): loss=334.57064718803514, w0=0.028823508685491096, w1=0.3189337008002419\n",
      "Gradient Descent(2854/9999): loss=334.49919506261836, w0=0.028821111333257144, w1=0.31890208305174766\n",
      "Gradient Descent(2855/9999): loss=334.42776071192895, w0=0.02881871415521993, w1=0.31887046848808825\n",
      "Gradient Descent(2856/9999): loss=334.35634413020745, w0=0.0288163171515345, w1=0.31883885710846327\n",
      "Gradient Descent(2857/9999): loss=334.2849453117016, w0=0.02881392032235544, w1=0.3188072489120737\n",
      "Gradient Descent(2858/9999): loss=334.2135642506662, w0=0.02881152366783686, w1=0.3187756438981218\n",
      "Gradient Descent(2859/9999): loss=334.1422009413633, w0=0.02880912718813241, w1=0.31874404206581136\n",
      "Gradient Descent(2860/9999): loss=334.0708553780616, w0=0.028806730883395262, w1=0.31871244341434746\n",
      "Gradient Descent(2861/9999): loss=333.99952755503733, w0=0.028804334753778138, w1=0.3186808479429365\n",
      "Gradient Descent(2862/9999): loss=333.92821746657324, w0=0.028801938799433286, w1=0.3186492556507863\n",
      "Gradient Descent(2863/9999): loss=333.8569251069595, w0=0.0287995430205125, w1=0.3186176665371059\n",
      "Gradient Descent(2864/9999): loss=333.7856504704928, w0=0.0287971474171671, w1=0.3185860806011059\n",
      "Gradient Descent(2865/9999): loss=333.71439355147703, w0=0.028794751989547968, w1=0.3185544978419981\n",
      "Gradient Descent(2866/9999): loss=333.6431543442226, w0=0.028792356737805506, w1=0.3185229182589956\n",
      "Gradient Descent(2867/9999): loss=333.57193284304714, w0=0.02878996166208967, w1=0.318491341851313\n",
      "Gradient Descent(2868/9999): loss=333.5007290422749, w0=0.02878756676254996, w1=0.31845976861816616\n",
      "Gradient Descent(2869/9999): loss=333.429542936237, w0=0.028785172039335418, w1=0.31842819855877225\n",
      "Gradient Descent(2870/9999): loss=333.35837451927125, w0=0.02878277749259463, w1=0.3183966316723498\n",
      "Gradient Descent(2871/9999): loss=333.2872237857223, w0=0.028780383122475734, w1=0.31836506795811853\n",
      "Gradient Descent(2872/9999): loss=333.2160907299412, w0=0.028777988929126412, w1=0.31833350741529975\n",
      "Gradient Descent(2873/9999): loss=333.14497534628595, w0=0.0287755949126939, w1=0.31830195004311584\n",
      "Gradient Descent(2874/9999): loss=333.07387762912134, w0=0.028773201073324987, w1=0.31827039584079064\n",
      "Gradient Descent(2875/9999): loss=333.0027975728184, w0=0.028770807411166006, w1=0.31823884480754927\n",
      "Gradient Descent(2876/9999): loss=332.931735171755, w0=0.02876841392636285, w1=0.3182072969426181\n",
      "Gradient Descent(2877/9999): loss=332.86069042031545, w0=0.028766020619060965, w1=0.31817575224522493\n",
      "Gradient Descent(2878/9999): loss=332.78966331289047, w0=0.02876362748940535, w1=0.31814421071459875\n",
      "Gradient Descent(2879/9999): loss=332.71865384387775, w0=0.02876123453754056, w1=0.31811267234996987\n",
      "Gradient Descent(2880/9999): loss=332.64766200768105, w0=0.028758841763610717, w1=0.3180811371505699\n",
      "Gradient Descent(2881/9999): loss=332.5766877987106, w0=0.02875644916775949, w1=0.3180496051156318\n",
      "Gradient Descent(2882/9999): loss=332.5057312113833, w0=0.028754056750130114, w1=0.3180180762443897\n",
      "Gradient Descent(2883/9999): loss=332.434792240122, w0=0.028751664510865388, w1=0.31798655053607916\n",
      "Gradient Descent(2884/9999): loss=332.36387087935657, w0=0.028749272450107666, w1=0.3179550279899369\n",
      "Gradient Descent(2885/9999): loss=332.29296712352266, w0=0.028746880567998873, w1=0.31792350860520097\n",
      "Gradient Descent(2886/9999): loss=332.22208096706254, w0=0.028744488864680496, w1=0.3178919923811107\n",
      "Gradient Descent(2887/9999): loss=332.1512124044246, w0=0.028742097340293583, w1=0.31786047931690675\n",
      "Gradient Descent(2888/9999): loss=332.0803614300637, w0=0.028739705994978756, w1=0.3178289694118309\n",
      "Gradient Descent(2889/9999): loss=332.00952803844064, w0=0.028737314828876204, w1=0.3177974626651264\n",
      "Gradient Descent(2890/9999): loss=331.93871222402277, w0=0.02873492384212568, w1=0.3177659590760375\n",
      "Gradient Descent(2891/9999): loss=331.8679139812835, w0=0.028732533034866514, w1=0.31773445864380995\n",
      "Gradient Descent(2892/9999): loss=331.79713330470224, w0=0.0287301424072376, w1=0.3177029613676906\n",
      "Gradient Descent(2893/9999): loss=331.72637018876475, w0=0.028727751959377414, w1=0.3176714672469277\n",
      "Gradient Descent(2894/9999): loss=331.65562462796277, w0=0.028725361691423992, w1=0.3176399762807706\n",
      "Gradient Descent(2895/9999): loss=331.58489661679425, w0=0.02872297160351496, w1=0.31760848846847006\n",
      "Gradient Descent(2896/9999): loss=331.51418614976313, w0=0.02872058169578751, w1=0.31757700380927795\n",
      "Gradient Descent(2897/9999): loss=331.4434932213794, w0=0.028718191968378416, w1=0.31754552230244737\n",
      "Gradient Descent(2898/9999): loss=331.372817826159, w0=0.028715802421424026, w1=0.31751404394723276\n",
      "Gradient Descent(2899/9999): loss=331.3021599586241, w0=0.02871341305506027, w1=0.3174825687428897\n",
      "Gradient Descent(2900/9999): loss=331.2315196133022, w0=0.028711023869422654, w1=0.31745109668867516\n",
      "Gradient Descent(2901/9999): loss=331.16089678472775, w0=0.02870863486464627, w1=0.31741962778384714\n",
      "Gradient Descent(2902/9999): loss=331.0902914674402, w0=0.028706246040865795, w1=0.31738816202766496\n",
      "Gradient Descent(2903/9999): loss=331.0197036559853, w0=0.02870385739821548, w1=0.3173566994193891\n",
      "Gradient Descent(2904/9999): loss=330.9491333449147, w0=0.02870146893682917, w1=0.3173252399582814\n",
      "Gradient Descent(2905/9999): loss=330.8785805287855, w0=0.02869908065684029, w1=0.3172937836436048\n",
      "Gradient Descent(2906/9999): loss=330.80804520216134, w0=0.02869669255838185, w1=0.31726233047462343\n",
      "Gradient Descent(2907/9999): loss=330.73752735961097, w0=0.028694304641586454, w1=0.31723088045060277\n",
      "Gradient Descent(2908/9999): loss=330.66702699570925, w0=0.028691916906586293, w1=0.31719943357080943\n",
      "Gradient Descent(2909/9999): loss=330.5965441050367, w0=0.028689529353513144, w1=0.31716798983451117\n",
      "Gradient Descent(2910/9999): loss=330.52607868217956, w0=0.02868714198249838, w1=0.31713654924097695\n",
      "Gradient Descent(2911/9999): loss=330.4556307217298, w0=0.028684754793672964, w1=0.31710511178947703\n",
      "Gradient Descent(2912/9999): loss=330.38520021828504, w0=0.028682367787167446, w1=0.31707367747928283\n",
      "Gradient Descent(2913/9999): loss=330.3147871664486, w0=0.028679980963111983, w1=0.3170422463096669\n",
      "Gradient Descent(2914/9999): loss=330.2443915608293, w0=0.02867759432163632, w1=0.31701081827990296\n",
      "Gradient Descent(2915/9999): loss=330.17401339604174, w0=0.028675207862869794, w1=0.31697939338926606\n",
      "Gradient Descent(2916/9999): loss=330.10365266670607, w0=0.028672821586941344, w1=0.31694797163703237\n",
      "Gradient Descent(2917/9999): loss=330.03330936744794, w0=0.02867043549397951, w1=0.3169165530224791\n",
      "Gradient Descent(2918/9999): loss=329.96298349289845, w0=0.028668049584112426, w1=0.3168851375448849\n",
      "Gradient Descent(2919/9999): loss=329.8926750376945, w0=0.02866566385746783, w1=0.31685372520352934\n",
      "Gradient Descent(2920/9999): loss=329.8223839964783, w0=0.02866327831417306, w1=0.31682231599769334\n",
      "Gradient Descent(2921/9999): loss=329.7521103638975, w0=0.028660892954355057, w1=0.31679090992665887\n",
      "Gradient Descent(2922/9999): loss=329.6818541346053, w0=0.028658507778140364, w1=0.3167595069897091\n",
      "Gradient Descent(2923/9999): loss=329.6116153032604, w0=0.02865612278565513, w1=0.3167281071861284\n",
      "Gradient Descent(2924/9999): loss=329.54139386452647, w0=0.028653737977025112, w1=0.3166967105152023\n",
      "Gradient Descent(2925/9999): loss=329.47118981307324, w0=0.02865135335237567, w1=0.3166653169762174\n",
      "Gradient Descent(2926/9999): loss=329.4010031435753, w0=0.028648968911831774, w1=0.31663392656846157\n",
      "Gradient Descent(2927/9999): loss=329.3308338507128, w0=0.028646584655517997, w1=0.31660253929122373\n",
      "Gradient Descent(2928/9999): loss=329.26068192917114, w0=0.028644200583558526, w1=0.31657115514379397\n",
      "Gradient Descent(2929/9999): loss=329.19054737364104, w0=0.02864181669607716, w1=0.3165397741254636\n",
      "Gradient Descent(2930/9999): loss=329.1204301788186, w0=0.028639432993197312, w1=0.31650839623552496\n",
      "Gradient Descent(2931/9999): loss=329.05033033940487, w0=0.028637049475042, w1=0.31647702147327167\n",
      "Gradient Descent(2932/9999): loss=328.9802478501066, w0=0.028634666141733857, w1=0.31644564983799833\n",
      "Gradient Descent(2933/9999): loss=328.91018270563546, w0=0.02863228299339514, w1=0.3164142813290008\n",
      "Gradient Descent(2934/9999): loss=328.84013490070816, w0=0.02862990003014771, w1=0.31638291594557594\n",
      "Gradient Descent(2935/9999): loss=328.77010443004707, w0=0.02862751725211305, w1=0.3163515536870219\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(2936/9999): loss=328.7000912883793, w0=0.028625134659412264, w1=0.31632019455263777\n",
      "Gradient Descent(2937/9999): loss=328.63009547043725, w0=0.028622752252166067, w1=0.31628883854172396\n",
      "Gradient Descent(2938/9999): loss=328.56011697095846, w0=0.028620370030494797, w1=0.3162574856535818\n",
      "Gradient Descent(2939/9999): loss=328.49015578468544, w0=0.028617987994518417, w1=0.3162261358875139\n",
      "Gradient Descent(2940/9999): loss=328.4202119063661, w0=0.028615606144356504, w1=0.3161947892428239\n",
      "Gradient Descent(2941/9999): loss=328.35028533075274, w0=0.028613224480128263, w1=0.31616344571881655\n",
      "Gradient Descent(2942/9999): loss=328.2803760526035, w0=0.028610843001952522, w1=0.31613210531479774\n",
      "Gradient Descent(2943/9999): loss=328.21048406668115, w0=0.028608461709947734, w1=0.3161007680300745\n",
      "Gradient Descent(2944/9999): loss=328.1406093677533, w0=0.028606080604231972, w1=0.3160694338639548\n",
      "Gradient Descent(2945/9999): loss=328.07075195059275, w0=0.02860369968492294, w1=0.3160381028157479\n",
      "Gradient Descent(2946/9999): loss=328.00091180997737, w0=0.028601318952137973, w1=0.31600677488476403\n",
      "Gradient Descent(2947/9999): loss=327.9310889406895, w0=0.028598938405994027, w1=0.3159754500703146\n",
      "Gradient Descent(2948/9999): loss=327.8612833375169, w0=0.028596558046607694, w1=0.31594412837171204\n",
      "Gradient Descent(2949/9999): loss=327.79149499525204, w0=0.028594177874095194, w1=0.3159128097882699\n",
      "Gradient Descent(2950/9999): loss=327.72172390869224, w0=0.028591797888572374, w1=0.31588149431930285\n",
      "Gradient Descent(2951/9999): loss=327.6519700726398, w0=0.02858941809015472, w1=0.31585018196412656\n",
      "Gradient Descent(2952/9999): loss=327.5822334819016, w0=0.02858703847895734, w1=0.3158188727220579\n",
      "Gradient Descent(2953/9999): loss=327.51251413128966, w0=0.028584659055094996, w1=0.31578756659241464\n",
      "Gradient Descent(2954/9999): loss=327.44281201562063, w0=0.02858227981868207, w1=0.3157562635745158\n",
      "Gradient Descent(2955/9999): loss=327.37312712971595, w0=0.028579900769832578, w1=0.3157249636676814\n",
      "Gradient Descent(2956/9999): loss=327.3034594684017, w0=0.028577521908660187, w1=0.3156936668712325\n",
      "Gradient Descent(2957/9999): loss=327.23380902650916, w0=0.028575143235278187, w1=0.3156623731844913\n",
      "Gradient Descent(2958/9999): loss=327.1641757988738, w0=0.028572764749799518, w1=0.31563108260678097\n",
      "Gradient Descent(2959/9999): loss=327.09455978033617, w0=0.028570386452336755, w1=0.3155997951374258\n",
      "Gradient Descent(2960/9999): loss=327.0249609657415, w0=0.02856800834300211, w1=0.31556851077575115\n",
      "Gradient Descent(2961/9999): loss=326.95537934993934, w0=0.028565630421907448, w1=0.31553722952108343\n",
      "Gradient Descent(2962/9999): loss=326.8858149277842, w0=0.02856325268916426, w1=0.31550595137275006\n",
      "Gradient Descent(2963/9999): loss=326.8162676941354, w0=0.028560875144883698, w1=0.31547467633007953\n",
      "Gradient Descent(2964/9999): loss=326.7467376438563, w0=0.028558497789176546, w1=0.31544340439240137\n",
      "Gradient Descent(2965/9999): loss=326.67722477181553, w0=0.02855612062215324, w1=0.3154121355590462\n",
      "Gradient Descent(2966/9999): loss=326.6077290728858, w0=0.028553743643923858, w1=0.31538086982934566\n",
      "Gradient Descent(2967/9999): loss=326.5382505419448, w0=0.028551366854598133, w1=0.3153496072026324\n",
      "Gradient Descent(2968/9999): loss=326.46878917387426, w0=0.02854899025428544, w1=0.31531834767824013\n",
      "Gradient Descent(2969/9999): loss=326.3993449635611, w0=0.0285466138430948, w1=0.31528709125550364\n",
      "Gradient Descent(2970/9999): loss=326.3299179058962, w0=0.02854423762113489, w1=0.31525583793375866\n",
      "Gradient Descent(2971/9999): loss=326.2605079957752, w0=0.028541861588514034, w1=0.315224587712342\n",
      "Gradient Descent(2972/9999): loss=326.1911152280981, w0=0.028539485745340213, w1=0.3151933405905915\n",
      "Gradient Descent(2973/9999): loss=326.1217395977695, w0=0.028537110091721055, w1=0.315162096567846\n",
      "Gradient Descent(2974/9999): loss=326.0523810996984, w0=0.028534734627763843, w1=0.3151308556434454\n",
      "Gradient Descent(2975/9999): loss=325.98303972879813, w0=0.028532359353575516, w1=0.31509961781673057\n",
      "Gradient Descent(2976/9999): loss=325.91371547998676, w0=0.02852998426926267, w1=0.3150683830870435\n",
      "Gradient Descent(2977/9999): loss=325.84440834818633, w0=0.028527609374931553, w1=0.315037151453727\n",
      "Gradient Descent(2978/9999): loss=325.7751183283235, w0=0.028525234670688075, w1=0.3150059229161251\n",
      "Gradient Descent(2979/9999): loss=325.7058454153293, w0=0.0285228601566378, w1=0.31497469747358275\n",
      "Gradient Descent(2980/9999): loss=325.636589604139, w0=0.028520485832885953, w1=0.31494347512544585\n",
      "Gradient Descent(2981/9999): loss=325.56735088969225, w0=0.028518111699537416, w1=0.31491225587106136\n",
      "Gradient Descent(2982/9999): loss=325.49812926693306, w0=0.02851573775669674, w1=0.31488103970977727\n",
      "Gradient Descent(2983/9999): loss=325.4289247308099, w0=0.028513364004468127, w1=0.3148498266409425\n",
      "Gradient Descent(2984/9999): loss=325.3597372762751, w0=0.028510990442955447, w1=0.3148186166639071\n",
      "Gradient Descent(2985/9999): loss=325.2905668982857, w0=0.02850861707226223, w1=0.3147874097780219\n",
      "Gradient Descent(2986/9999): loss=325.2214135918027, w0=0.02850624389249168, w1=0.31475620598263887\n",
      "Gradient Descent(2987/9999): loss=325.15227735179144, w0=0.02850387090374665, w1=0.31472500527711095\n",
      "Gradient Descent(2988/9999): loss=325.0831581732215, w0=0.028501498106129678, w1=0.314693807660792\n",
      "Gradient Descent(2989/9999): loss=325.0140560510665, w0=0.028499125499742952, w1=0.31466261313303706\n",
      "Gradient Descent(2990/9999): loss=324.9449709803047, w0=0.02849675308468834, w1=0.31463142169320185\n",
      "Gradient Descent(2991/9999): loss=324.8759029559179, w0=0.028494380861067366, w1=0.3146002333406433\n",
      "Gradient Descent(2992/9999): loss=324.80685197289273, w0=0.028492008828981237, w1=0.31456904807471925\n",
      "Gradient Descent(2993/9999): loss=324.7378180262193, w0=0.028489636988530822, w1=0.3145378658947885\n",
      "Gradient Descent(2994/9999): loss=324.6688011108924, w0=0.028487265339816662, w1=0.31450668680021077\n",
      "Gradient Descent(2995/9999): loss=324.5998012219106, w0=0.028484893882938973, w1=0.31447551079034686\n",
      "Gradient Descent(2996/9999): loss=324.5308183542767, w0=0.02848252261799764, w1=0.3144443378645585\n",
      "Gradient Descent(2997/9999): loss=324.4618525029975, w0=0.028480151545092224, w1=0.3144131680222083\n",
      "Gradient Descent(2998/9999): loss=324.392903663084, w0=0.028477780664321963, w1=0.31438200126265997\n",
      "Gradient Descent(2999/9999): loss=324.32397182955117, w0=0.02847540997578577, w1=0.31435083758527804\n",
      "Gradient Descent(3000/9999): loss=324.25505699741814, w0=0.028473039479582227, w1=0.3143196769894281\n",
      "Gradient Descent(3001/9999): loss=324.18615916170774, w0=0.0284706691758096, w1=0.31428851947447667\n",
      "Gradient Descent(3002/9999): loss=324.1172783174472, w0=0.02846829906456583, w1=0.3142573650397912\n",
      "Gradient Descent(3003/9999): loss=324.0484144596675, w0=0.028465929145948542, w1=0.3142262136847401\n",
      "Gradient Descent(3004/9999): loss=323.97956758340376, w0=0.028463559420055035, w1=0.3141950654086927\n",
      "Gradient Descent(3005/9999): loss=323.9107376836947, w0=0.028461189886982288, w1=0.3141639202110193\n",
      "Gradient Descent(3006/9999): loss=323.84192475558353, w0=0.028458820546826965, w1=0.3141327780910912\n",
      "Gradient Descent(3007/9999): loss=323.7731287941171, w0=0.02845645139968541, w1=0.31410163904828053\n",
      "Gradient Descent(3008/9999): loss=323.7043497943461, w0=0.02845408244565365, w1=0.31407050308196044\n",
      "Gradient Descent(3009/9999): loss=323.63558775132543, w0=0.0284517136848274, w1=0.31403937019150496\n",
      "Gradient Descent(3010/9999): loss=323.5668426601136, w0=0.02844934511730205, w1=0.3140082403762891\n",
      "Gradient Descent(3011/9999): loss=323.49811451577295, w0=0.028446976743172685, w1=0.31397711363568875\n",
      "Gradient Descent(3012/9999): loss=323.4294033133701, w0=0.02844460856253407, w1=0.3139459899690808\n",
      "Gradient Descent(3013/9999): loss=323.3607090479751, w0=0.02844224057548066, w1=0.313914869375843\n",
      "Gradient Descent(3014/9999): loss=323.292031714662, w0=0.0284398727821066, w1=0.31388375185535405\n",
      "Gradient Descent(3015/9999): loss=323.2233713085088, w0=0.028437505182505717, w1=0.31385263740699354\n",
      "Gradient Descent(3016/9999): loss=323.15472782459705, w0=0.02843513777677153, w1=0.3138215260301421\n",
      "Gradient Descent(3017/9999): loss=323.0861012580123, w0=0.028432770564997257, w1=0.31379041772418104\n",
      "Gradient Descent(3018/9999): loss=323.0174916038437, w0=0.028430403547275794, w1=0.31375931248849287\n",
      "Gradient Descent(3019/9999): loss=322.9488988571846, w0=0.02842803672369974, w1=0.3137282103224608\n",
      "Gradient Descent(3020/9999): loss=322.88032301313154, w0=0.028425670094361376, w1=0.313697111225469\n",
      "Gradient Descent(3021/9999): loss=322.811764066785, w0=0.028423303659352684, w1=0.3136660151969026\n",
      "Gradient Descent(3022/9999): loss=322.74322201324935, w0=0.02842093741876534, w1=0.31363492223614764\n",
      "Gradient Descent(3023/9999): loss=322.6746968476326, w0=0.02841857137269071, w1=0.313603832342591\n",
      "Gradient Descent(3024/9999): loss=322.6061885650465, w0=0.028416205521219855, w1=0.3135727455156204\n",
      "Gradient Descent(3025/9999): loss=322.5376971606064, w0=0.028413839864443544, w1=0.31354166175462467\n",
      "Gradient Descent(3026/9999): loss=322.46922262943116, w0=0.028411474402452234, w1=0.3135105810589933\n",
      "Gradient Descent(3027/9999): loss=322.4007649666439, w0=0.028409109135336084, w1=0.31347950342811687\n",
      "Gradient Descent(3028/9999): loss=322.3323241673706, w0=0.028406744063184948, w1=0.31344842886138674\n",
      "Gradient Descent(3029/9999): loss=322.2639002267415, w0=0.028404379186088385, w1=0.31341735735819515\n",
      "Gradient Descent(3030/9999): loss=322.1954931398901, w0=0.02840201450413565, w1=0.3133862889179353\n",
      "Gradient Descent(3031/9999): loss=322.12710290195383, w0=0.0283996500174157, w1=0.3133552235400012\n",
      "Gradient Descent(3032/9999): loss=322.05872950807344, w0=0.028397285726017196, w1=0.31332416122378787\n",
      "Gradient Descent(3033/9999): loss=321.9903729533935, w0=0.0283949216300285, w1=0.31329310196869103\n",
      "Gradient Descent(3034/9999): loss=321.92203323306194, w0=0.028392557729537686, w1=0.3132620457741074\n",
      "Gradient Descent(3035/9999): loss=321.85371034223033, w0=0.02839019402463252, w1=0.3132309926394345\n",
      "Gradient Descent(3036/9999): loss=321.78540427605384, w0=0.028387830515400475, w1=0.31319994256407085\n",
      "Gradient Descent(3037/9999): loss=321.71711502969106, w0=0.02838546720192874, w1=0.3131688955474157\n",
      "Gradient Descent(3038/9999): loss=321.6488425983043, w0=0.028383104084304202, w1=0.31313785158886925\n",
      "Gradient Descent(3039/9999): loss=321.5805869770595, w0=0.02838074116261346, w1=0.3131068106878325\n",
      "Gradient Descent(3040/9999): loss=321.51234816112554, w0=0.028378378436942815, w1=0.31307577284370747\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(3041/9999): loss=321.4441261456751, w0=0.02837601590737828, w1=0.31304473805589683\n",
      "Gradient Descent(3042/9999): loss=321.3759209258847, w0=0.028373653574005585, w1=0.31301370632380426\n",
      "Gradient Descent(3043/9999): loss=321.30773249693385, w0=0.02837129143691016, w1=0.31298267764683424\n",
      "Gradient Descent(3044/9999): loss=321.2395608540056, w0=0.028368929496177156, w1=0.3129516520243921\n",
      "Gradient Descent(3045/9999): loss=321.1714059922865, w0=0.028366567751891426, w1=0.3129206294558841\n",
      "Gradient Descent(3046/9999): loss=321.1032679069666, w0=0.028364206204137543, w1=0.31288960994071724\n",
      "Gradient Descent(3047/9999): loss=321.03514659323935, w0=0.02836184485299979, w1=0.3128585934782995\n",
      "Gradient Descent(3048/9999): loss=320.9670420463016, w0=0.028359483698562168, w1=0.3128275800680395\n",
      "Gradient Descent(3049/9999): loss=320.89895426135337, w0=0.02835712274090839, w1=0.31279656970934694\n",
      "Gradient Descent(3050/9999): loss=320.83088323359834, w0=0.028354761980121882, w1=0.3127655624016322\n",
      "Gradient Descent(3051/9999): loss=320.7628289582436, w0=0.028352401416285793, w1=0.3127345581443067\n",
      "Gradient Descent(3052/9999): loss=320.69479143049944, w0=0.028350041049482987, w1=0.3127035569367824\n",
      "Gradient Descent(3053/9999): loss=320.6267706455794, w0=0.02834768087979604, w1=0.3126725587784724\n",
      "Gradient Descent(3054/9999): loss=320.55876659870063, w0=0.02834532090730726, w1=0.31264156366879037\n",
      "Gradient Descent(3055/9999): loss=320.4907792850834, w0=0.02834296113209866, w1=0.312610571607151\n",
      "Gradient Descent(3056/9999): loss=320.4228086999516, w0=0.028340601554251985, w1=0.31257958259296975\n",
      "Gradient Descent(3057/9999): loss=320.35485483853176, w0=0.028338242173848692, w1=0.3125485966256629\n",
      "Gradient Descent(3058/9999): loss=320.2869176960546, w0=0.028335882990969962, w1=0.3125176137046476\n",
      "Gradient Descent(3059/9999): loss=320.2189972677535, w0=0.028333524005696702, w1=0.3124866338293417\n",
      "Gradient Descent(3060/9999): loss=320.1510935488651, w0=0.02833116521810954, w1=0.31245565699916406\n",
      "Gradient Descent(3061/9999): loss=320.0832065346298, w0=0.028328806628288825, w1=0.31242468321353417\n",
      "Gradient Descent(3062/9999): loss=320.01533622029064, w0=0.028326448236314635, w1=0.31239371247187253\n",
      "Gradient Descent(3063/9999): loss=319.94748260109446, w0=0.02832409004226677, w1=0.3123627447736003\n",
      "Gradient Descent(3064/9999): loss=319.8796456722908, w0=0.02832173204622476, w1=0.31233178011813956\n",
      "Gradient Descent(3065/9999): loss=319.8118254291328, w0=0.028319374248267852, w1=0.3123008185049131\n",
      "Gradient Descent(3066/9999): loss=319.74402186687666, w0=0.028317016648475034, w1=0.31226985993334455\n",
      "Gradient Descent(3067/9999): loss=319.6762349807819, w0=0.028314659246925014, w1=0.31223890440285845\n",
      "Gradient Descent(3068/9999): loss=319.60846476611107, w0=0.028312302043696228, w1=0.31220795191288003\n",
      "Gradient Descent(3069/9999): loss=319.5407112181298, w0=0.028309945038866847, w1=0.31217700246283536\n",
      "Gradient Descent(3070/9999): loss=319.4729743321073, w0=0.028307588232514766, w1=0.31214605605215134\n",
      "Gradient Descent(3071/9999): loss=319.40525410331554, w0=0.028305231624717615, w1=0.31211511268025566\n",
      "Gradient Descent(3072/9999): loss=319.3375505270298, w0=0.028302875215552754, w1=0.31208417234657676\n",
      "Gradient Descent(3073/9999): loss=319.26986359852845, w0=0.028300519005097276, w1=0.3120532350505439\n",
      "Gradient Descent(3074/9999): loss=319.202193313093, w0=0.028298162993428008, w1=0.31202230079158716\n",
      "Gradient Descent(3075/9999): loss=319.1345396660081, w0=0.028295807180621507, w1=0.3119913695691374\n",
      "Gradient Descent(3076/9999): loss=319.06690265256134, w0=0.028293451566754068, w1=0.3119604413826263\n",
      "Gradient Descent(3077/9999): loss=318.9992822680436, w0=0.028291096151901722, w1=0.31192951623148624\n",
      "Gradient Descent(3078/9999): loss=318.9316785077489, w0=0.028288740936140234, w1=0.3118985941151504\n",
      "Gradient Descent(3079/9999): loss=318.8640913669742, w0=0.028286385919545104, w1=0.3118676750330529\n",
      "Gradient Descent(3080/9999): loss=318.79652084101946, w0=0.02828403110219157, w1=0.3118367589846285\n",
      "Gradient Descent(3081/9999): loss=318.72896692518776, w0=0.028281676484154604, w1=0.3118058459693127\n",
      "Gradient Descent(3082/9999): loss=318.66142961478516, w0=0.028279322065508927, w1=0.31177493598654193\n",
      "Gradient Descent(3083/9999): loss=318.59390890512105, w0=0.028276967846328993, w1=0.31174402903575327\n",
      "Gradient Descent(3084/9999): loss=318.5264047915075, w0=0.028274613826688996, w1=0.3117131251163846\n",
      "Gradient Descent(3085/9999): loss=318.45891726925976, w0=0.028272260006662865, w1=0.3116822242278746\n",
      "Gradient Descent(3086/9999): loss=318.391446333696, w0=0.028269906386324282, w1=0.31165132636966275\n",
      "Gradient Descent(3087/9999): loss=318.32399198013746, w0=0.02826755296574666, w1=0.31162043154118924\n",
      "Gradient Descent(3088/9999): loss=318.2565542039082, w0=0.02826519974500316, w1=0.31158953974189507\n",
      "Gradient Descent(3089/9999): loss=318.18913300033563, w0=0.02826284672416669, w1=0.31155865097122193\n",
      "Gradient Descent(3090/9999): loss=318.12172836474974, w0=0.028260493903309893, w1=0.31152776522861236\n",
      "Gradient Descent(3091/9999): loss=318.05434029248363, w0=0.028258141282505162, w1=0.3114968825135096\n",
      "Gradient Descent(3092/9999): loss=317.98696877887346, w0=0.028255788861824636, w1=0.3114660028253577\n",
      "Gradient Descent(3093/9999): loss=317.919613819258, w0=0.028253436641340196, w1=0.3114351261636014\n",
      "Gradient Descent(3094/9999): loss=317.8522754089795, w0=0.028251084621123475, w1=0.31140425252768633\n",
      "Gradient Descent(3095/9999): loss=317.78495354338236, w0=0.028248732801245846, w1=0.3113733819170587\n",
      "Gradient Descent(3096/9999): loss=317.7176482178147, w0=0.028246381181778437, w1=0.31134251433116555\n",
      "Gradient Descent(3097/9999): loss=317.65035942762694, w0=0.02824402976279212, w1=0.3113116497694547\n",
      "Gradient Descent(3098/9999): loss=317.58308716817277, w0=0.02824167854435752, w1=0.31128078823137467\n",
      "Gradient Descent(3099/9999): loss=317.51583143480843, w0=0.028239327526545007, w1=0.3112499297163748\n",
      "Gradient Descent(3100/9999): loss=317.4485922228933, w0=0.028236976709424707, w1=0.31121907422390505\n",
      "Gradient Descent(3101/9999): loss=317.3813695277896, w0=0.02823462609306649, w1=0.3111882217534162\n",
      "Gradient Descent(3102/9999): loss=317.3141633448625, w0=0.028232275677539987, w1=0.3111573723043598\n",
      "Gradient Descent(3103/9999): loss=317.2469736694794, w0=0.028229925462914574, w1=0.3111265258761881\n",
      "Gradient Descent(3104/9999): loss=317.17980049701146, w0=0.02822757544925938, w1=0.31109568246835406\n",
      "Gradient Descent(3105/9999): loss=317.1126438228321, w0=0.028225225636643297, w1=0.31106484208031143\n",
      "Gradient Descent(3106/9999): loss=317.04550364231756, w0=0.02822287602513496, w1=0.3110340047115146\n",
      "Gradient Descent(3107/9999): loss=316.978379950847, w0=0.028220526614802767, w1=0.31100317036141883\n",
      "Gradient Descent(3108/9999): loss=316.91127274380256, w0=0.028218177405714865, w1=0.31097233902948\n",
      "Gradient Descent(3109/9999): loss=316.84418201656894, w0=0.028215828397939166, w1=0.3109415107151548\n",
      "Gradient Descent(3110/9999): loss=316.7771077645336, w0=0.02821347959154333, w1=0.3109106854179005\n",
      "Gradient Descent(3111/9999): loss=316.7100499830871, w0=0.028211130986594782, w1=0.3108798631371753\n",
      "Gradient Descent(3112/9999): loss=316.6430086676223, w0=0.0282087825831607, w1=0.31084904387243795\n",
      "Gradient Descent(3113/9999): loss=316.57598381353523, w0=0.02820643438130802, w1=0.310818227623148\n",
      "Gradient Descent(3114/9999): loss=316.5089754162245, w0=0.02820408638110344, w1=0.31078741438876567\n",
      "Gradient Descent(3115/9999): loss=316.44198347109136, w0=0.028201738582613423, w1=0.310756604168752\n",
      "Gradient Descent(3116/9999): loss=316.3750079735401, w0=0.028199390985904186, w1=0.31072579696256863\n",
      "Gradient Descent(3117/9999): loss=316.3080489189775, w0=0.028197043591041707, w1=0.310694992769678\n",
      "Gradient Descent(3118/9999): loss=316.24110630281297, w0=0.02819469639809173, w1=0.31066419158954317\n",
      "Gradient Descent(3119/9999): loss=316.1741801204589, w0=0.028192349407119763, w1=0.31063339342162793\n",
      "Gradient Descent(3120/9999): loss=316.10727036733016, w0=0.028190002618191067, w1=0.3106025982653969\n",
      "Gradient Descent(3121/9999): loss=316.0403770388446, w0=0.02818765603137068, w1=0.3105718061203152\n",
      "Gradient Descent(3122/9999): loss=315.97350013042245, w0=0.028185309646723396, w1=0.31054101698584885\n",
      "Gradient Descent(3123/9999): loss=315.9066396374867, w0=0.028182963464313773, w1=0.31051023086146445\n",
      "Gradient Descent(3124/9999): loss=315.8397955554631, w0=0.028180617484206145, w1=0.3104794477466294\n",
      "Gradient Descent(3125/9999): loss=315.7729678797801, w0=0.0281782717064646, w1=0.31044866764081164\n",
      "Gradient Descent(3126/9999): loss=315.7061566058685, w0=0.028175926131153002, w1=0.31041789054348\n",
      "Gradient Descent(3127/9999): loss=315.6393617291622, w0=0.028173580758334975, w1=0.3103871164541039\n",
      "Gradient Descent(3128/9999): loss=315.57258324509746, w0=0.02817123558807392, w1=0.3103563453721534\n",
      "Gradient Descent(3129/9999): loss=315.50582114911316, w0=0.028168890620432995, w1=0.31032557729709936\n",
      "Gradient Descent(3130/9999): loss=315.43907543665097, w0=0.028166545855475136, w1=0.31029481222841326\n",
      "Gradient Descent(3131/9999): loss=315.3723461031549, w0=0.02816420129326305, w1=0.3102640501655673\n",
      "Gradient Descent(3132/9999): loss=315.30563314407186, w0=0.02816185693385921, w1=0.3102332911080344\n",
      "Gradient Descent(3133/9999): loss=315.2389365548512, w0=0.028159512777325865, w1=0.31020253505528805\n",
      "Gradient Descent(3134/9999): loss=315.172256330945, w0=0.028157168823725025, w1=0.31017178200680257\n",
      "Gradient Descent(3135/9999): loss=315.1055924678077, w0=0.028154825073118485, w1=0.3101410319620529\n",
      "Gradient Descent(3136/9999): loss=315.0389449608965, w0=0.028152481525567805, w1=0.31011028492051457\n",
      "Gradient Descent(3137/9999): loss=314.97231380567126, w0=0.02815013818113432, w1=0.31007954088166395\n",
      "Gradient Descent(3138/9999): loss=314.9056989975939, w0=0.028147795039879145, w1=0.31004879984497796\n",
      "Gradient Descent(3139/9999): loss=314.8391005321297, w0=0.02814545210186316, w1=0.3100180618099343\n",
      "Gradient Descent(3140/9999): loss=314.77251840474577, w0=0.028143109367147028, w1=0.3099873267760112\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(3141/9999): loss=314.7059526109123, w0=0.028140766835791184, w1=0.3099565947426877\n",
      "Gradient Descent(3142/9999): loss=314.6394031461015, w0=0.02813842450785584, w1=0.30992586570944347\n",
      "Gradient Descent(3143/9999): loss=314.57287000578833, w0=0.028136082383400984, w1=0.3098951396757588\n",
      "Gradient Descent(3144/9999): loss=314.5063531854505, w0=0.028133740462486387, w1=0.30986441664111475\n",
      "Gradient Descent(3145/9999): loss=314.4398526805681, w0=0.02813139874517159, w1=0.3098336966049929\n",
      "Gradient Descent(3146/9999): loss=314.3733684866234, w0=0.028129057231515917, w1=0.3098029795668755\n",
      "Gradient Descent(3147/9999): loss=314.30690059910154, w0=0.028126715921578473, w1=0.3097722655262457\n",
      "Gradient Descent(3148/9999): loss=314.2404490134901, w0=0.028124374815418143, w1=0.309741554482587\n",
      "Gradient Descent(3149/9999): loss=314.17401372527905, w0=0.028122033913093588, w1=0.30971084643538377\n",
      "Gradient Descent(3150/9999): loss=314.1075947299608, w0=0.028119693214663254, w1=0.309680141384121\n",
      "Gradient Descent(3151/9999): loss=314.0411920230304, w0=0.028117352720185368, w1=0.30964943932828415\n",
      "Gradient Descent(3152/9999): loss=313.9748055999852, w0=0.028115012429717937, w1=0.30961874026735964\n",
      "Gradient Descent(3153/9999): loss=313.90843545632515, w0=0.028112672343318754, w1=0.3095880442008343\n",
      "Gradient Descent(3154/9999): loss=313.8420815875524, w0=0.028110332461045395, w1=0.3095573511281957\n",
      "Gradient Descent(3155/9999): loss=313.775743989172, w0=0.028107992782955213, w1=0.3095266610489321\n",
      "Gradient Descent(3156/9999): loss=313.7094226566909, w0=0.028105653309105357, w1=0.30949597396253237\n",
      "Gradient Descent(3157/9999): loss=313.6431175856187, w0=0.028103314039552754, w1=0.3094652898684859\n",
      "Gradient Descent(3158/9999): loss=313.5768287714676, w0=0.028100974974354116, w1=0.309434608766283\n",
      "Gradient Descent(3159/9999): loss=313.51055620975205, w0=0.028098636113565945, w1=0.3094039306554143\n",
      "Gradient Descent(3160/9999): loss=313.44429989598893, w0=0.028096297457244526, w1=0.30937325553537137\n",
      "Gradient Descent(3161/9999): loss=313.3780598256974, w0=0.028093959005445934, w1=0.3093425834056462\n",
      "Gradient Descent(3162/9999): loss=313.3118359943992, w0=0.028091620758226032, w1=0.3093119142657315\n",
      "Gradient Descent(3163/9999): loss=313.24562839761836, w0=0.028089282715640467, w1=0.30928124811512064\n",
      "Gradient Descent(3164/9999): loss=313.1794370308814, w0=0.02808694487774468, w1=0.3092505849533076\n",
      "Gradient Descent(3165/9999): loss=313.11326188971697, w0=0.028084607244593905, w1=0.30921992477978694\n",
      "Gradient Descent(3166/9999): loss=313.04710296965635, w0=0.028082269816243156, w1=0.30918926759405396\n",
      "Gradient Descent(3167/9999): loss=312.9809602662331, w0=0.028079932592747242, w1=0.3091586133956045\n",
      "Gradient Descent(3168/9999): loss=312.9148337749832, w0=0.02807759557416076, w1=0.30912796218393507\n",
      "Gradient Descent(3169/9999): loss=312.8487234914446, w0=0.028075258760538107, w1=0.3090973139585428\n",
      "Gradient Descent(3170/9999): loss=312.7826294111582, w0=0.028072922151933466, w1=0.30906666871892535\n",
      "Gradient Descent(3171/9999): loss=312.7165515296668, w0=0.028070585748400816, w1=0.3090360264645812\n",
      "Gradient Descent(3172/9999): loss=312.65048984251575, w0=0.028068249549993925, w1=0.3090053871950093\n",
      "Gradient Descent(3173/9999): loss=312.58444434525245, w0=0.02806591355676636, w1=0.3089747509097093\n",
      "Gradient Descent(3174/9999): loss=312.518415033427, w0=0.02806357776877148, w1=0.30894411760818136\n",
      "Gradient Descent(3175/9999): loss=312.4524019025915, w0=0.028061242186062433, w1=0.30891348728992635\n",
      "Gradient Descent(3176/9999): loss=312.38640494830054, w0=0.028058906808692174, w1=0.30888285995444575\n",
      "Gradient Descent(3177/9999): loss=312.3204241661108, w0=0.028056571636713445, w1=0.30885223560124164\n",
      "Gradient Descent(3178/9999): loss=312.2544595515816, w0=0.028054236670178787, w1=0.30882161422981674\n",
      "Gradient Descent(3179/9999): loss=312.1885111002742, w0=0.028051901909140545, w1=0.3087909958396743\n",
      "Gradient Descent(3180/9999): loss=312.1225788077523, w0=0.028049567353650846, w1=0.3087603804303182\n",
      "Gradient Descent(3181/9999): loss=312.05666266958184, w0=0.02804723300376163, w1=0.30872976800125307\n",
      "Gradient Descent(3182/9999): loss=311.99076268133115, w0=0.028044898859524634, w1=0.30869915855198393\n",
      "Gradient Descent(3183/9999): loss=311.92487883857063, w0=0.028042564920991383, w1=0.30866855208201655\n",
      "Gradient Descent(3184/9999): loss=311.8590111368731, w0=0.02804023118821321, w1=0.30863794859085725\n",
      "Gradient Descent(3185/9999): loss=311.7931595718134, w0=0.028037897661241252, w1=0.30860734807801293\n",
      "Gradient Descent(3186/9999): loss=311.72732413896887, w0=0.02803556434012644, w1=0.30857675054299116\n",
      "Gradient Descent(3187/9999): loss=311.661504833919, w0=0.028033231224919505, w1=0.3085461559853001\n",
      "Gradient Descent(3188/9999): loss=311.59570165224545, w0=0.028030898315670988, w1=0.30851556440444844\n",
      "Gradient Descent(3189/9999): loss=311.52991458953227, w0=0.028028565612431226, w1=0.3084849757999455\n",
      "Gradient Descent(3190/9999): loss=311.4641436413655, w0=0.02802623311525036, w1=0.3084543901713012\n",
      "Gradient Descent(3191/9999): loss=311.39838880333355, w0=0.02802390082417833, w1=0.3084238075180261\n",
      "Gradient Descent(3192/9999): loss=311.3326500710271, w0=0.028021568739264893, w1=0.30839322783963125\n",
      "Gradient Descent(3193/9999): loss=311.2669274400388, w0=0.028019236860559595, w1=0.30836265113562833\n",
      "Gradient Descent(3194/9999): loss=311.20122090596374, w0=0.0280169051881118, w1=0.30833207740552965\n",
      "Gradient Descent(3195/9999): loss=311.13553046439904, w0=0.028014573721970665, w1=0.3083015066488481\n",
      "Gradient Descent(3196/9999): loss=311.0698561109442, w0=0.02801224246218516, w1=0.3082709388650971\n",
      "Gradient Descent(3197/9999): loss=311.00419784120055, w0=0.028009911408804065, w1=0.3082403740537907\n",
      "Gradient Descent(3198/9999): loss=310.9385556507721, w0=0.028007580561875956, w1=0.3082098122144435\n",
      "Gradient Descent(3199/9999): loss=310.87292953526463, w0=0.028005249921449225, w1=0.3081792533465707\n",
      "Gradient Descent(3200/9999): loss=310.80731949028615, w0=0.02800291948757207, w1=0.30814869744968815\n",
      "Gradient Descent(3201/9999): loss=310.74172551144693, w0=0.0280005892602925, w1=0.30811814452331215\n",
      "Gradient Descent(3202/9999): loss=310.6761475943594, w0=0.027998259239658325, w1=0.3080875945669596\n",
      "Gradient Descent(3203/9999): loss=310.61058573463805, w0=0.027995929425717176, w1=0.30805704758014807\n",
      "Gradient Descent(3204/9999): loss=310.5450399278995, w0=0.02799359981851648, w1=0.30802650356239564\n",
      "Gradient Descent(3205/9999): loss=310.47951016976265, w0=0.02799127041810349, w1=0.3079959625132209\n",
      "Gradient Descent(3206/9999): loss=310.41399645584846, w0=0.027988941224525256, w1=0.3079654244321432\n",
      "Gradient Descent(3207/9999): loss=310.34849878178, w0=0.027986612237828647, w1=0.30793488931868224\n",
      "Gradient Descent(3208/9999): loss=310.2830171431824, w0=0.027984283458060342, w1=0.3079043571723584\n",
      "Gradient Descent(3209/9999): loss=310.21755153568296, w0=0.027981954885266832, w1=0.3078738279926927\n",
      "Gradient Descent(3210/9999): loss=310.1521019549113, w0=0.027979626519494424, w1=0.30784330177920655\n",
      "Gradient Descent(3211/9999): loss=310.0866683964988, w0=0.027977298360789233, w1=0.307812778531422\n",
      "Gradient Descent(3212/9999): loss=310.02125085607906, w0=0.027974970409197192, w1=0.30778225824886174\n",
      "Gradient Descent(3213/9999): loss=309.95584932928796, w0=0.027972642664764045, w1=0.3077517409310489\n",
      "Gradient Descent(3214/9999): loss=309.890463811763, w0=0.027970315127535356, w1=0.3077212265775073\n",
      "Gradient Descent(3215/9999): loss=309.8250942991446, w0=0.0279679877975565, w1=0.30769071518776114\n",
      "Gradient Descent(3216/9999): loss=309.7597407870743, w0=0.027965660674872668, w1=0.3076602067613354\n",
      "Gradient Descent(3217/9999): loss=309.69440327119645, w0=0.02796333375952887, w1=0.30762970129775535\n",
      "Gradient Descent(3218/9999): loss=309.62908174715693, w0=0.027961007051569932, w1=0.30759919879654707\n",
      "Gradient Descent(3219/9999): loss=309.5637762106041, w0=0.027958680551040494, w1=0.30756869925723707\n",
      "Gradient Descent(3220/9999): loss=309.4984866571883, w0=0.027956354257985018, w1=0.3075382026793524\n",
      "Gradient Descent(3221/9999): loss=309.4332130825617, w0=0.02795402817244778, w1=0.3075077090624207\n",
      "Gradient Descent(3222/9999): loss=309.3679554823788, w0=0.02795170229447288, w1=0.30747721840597014\n",
      "Gradient Descent(3223/9999): loss=309.3027138522959, w0=0.02794937662410423, w1=0.3074467307095295\n",
      "Gradient Descent(3224/9999): loss=309.2374881879714, w0=0.027947051161385572, w1=0.30741624597262796\n",
      "Gradient Descent(3225/9999): loss=309.1722784850659, w0=0.02794472590636046, w1=0.3073857641947954\n",
      "Gradient Descent(3226/9999): loss=309.107084739242, w0=0.027942400859072267, w1=0.30735528537556217\n",
      "Gradient Descent(3227/9999): loss=309.04190694616415, w0=0.027940076019564195, w1=0.3073248095144591\n",
      "Gradient Descent(3228/9999): loss=308.9767451014988, w0=0.02793775138787926, w1=0.30729433661101774\n",
      "Gradient Descent(3229/9999): loss=308.9115992009148, w0=0.027935426964060307, w1=0.30726386666476996\n",
      "Gradient Descent(3230/9999): loss=308.8464692400825, w0=0.027933102748149995, w1=0.3072333996752483\n",
      "Gradient Descent(3231/9999): loss=308.78135521467476, w0=0.027930778740190815, w1=0.30720293564198586\n",
      "Gradient Descent(3232/9999): loss=308.7162571203659, w0=0.027928454940225073, w1=0.30717247456451624\n",
      "Gradient Descent(3233/9999): loss=308.65117495283283, w0=0.027926131348294907, w1=0.3071420164423735\n",
      "Gradient Descent(3234/9999): loss=308.58610870775385, w0=0.027923807964442268, w1=0.3071115612750923\n",
      "Gradient Descent(3235/9999): loss=308.52105838080973, w0=0.027921484788708945, w1=0.3070811090622079\n",
      "Gradient Descent(3236/9999): loss=308.45602396768294, w0=0.027919161821136546, w1=0.30705065980325597\n",
      "Gradient Descent(3237/9999): loss=308.3910054640582, w0=0.027916839061766504, w1=0.30702021349777275\n",
      "Gradient Descent(3238/9999): loss=308.32600286562194, w0=0.02791451651064008, w1=0.306989770145295\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(3239/9999): loss=308.2610161680626, w0=0.02791219416779836, w1=0.30695932974536005\n",
      "Gradient Descent(3240/9999): loss=308.1960453670707, w0=0.027909872033282258, w1=0.30692889229750575\n",
      "Gradient Descent(3241/9999): loss=308.13109045833875, w0=0.02790755010713251, w1=0.3068984578012704\n",
      "Gradient Descent(3242/9999): loss=308.0661514375608, w0=0.02790522838938969, w1=0.30686802625619286\n",
      "Gradient Descent(3243/9999): loss=308.0012283004335, w0=0.027902906880094197, w1=0.30683759766181257\n",
      "Gradient Descent(3244/9999): loss=307.9363210426551, w0=0.027900585579286253, w1=0.3068071720176694\n",
      "Gradient Descent(3245/9999): loss=307.87142965992575, w0=0.027898264487005917, w1=0.3067767493233038\n",
      "Gradient Descent(3246/9999): loss=307.80655414794757, w0=0.027895943603293075, w1=0.3067463295782567\n",
      "Gradient Descent(3247/9999): loss=307.7416945024247, w0=0.02789362292818744, w1=0.3067159127820696\n",
      "Gradient Descent(3248/9999): loss=307.67685071906317, w0=0.027891302461728558, w1=0.30668549893428443\n",
      "Gradient Descent(3249/9999): loss=307.61202279357093, w0=0.02788898220395581, w1=0.3066550880344437\n",
      "Gradient Descent(3250/9999): loss=307.5472107216578, w0=0.027886662154908405, w1=0.30662468008209043\n",
      "Gradient Descent(3251/9999): loss=307.4824144990357, w0=0.02788434231462538, w1=0.3065942750767681\n",
      "Gradient Descent(3252/9999): loss=307.4176341214182, w0=0.027882022683145612, w1=0.3065638730180208\n",
      "Gradient Descent(3253/9999): loss=307.35286958452093, w0=0.027879703260507805, w1=0.306533473905393\n",
      "Gradient Descent(3254/9999): loss=307.2881208840614, w0=0.0278773840467505, w1=0.3065030777384297\n",
      "Gradient Descent(3255/9999): loss=307.2233880157591, w0=0.027875065041912072, w1=0.30647268451667653\n",
      "Gradient Descent(3256/9999): loss=307.1586709753352, w0=0.027872746246030724, w1=0.3064422942396795\n",
      "Gradient Descent(3257/9999): loss=307.093969758513, w0=0.027870427659144503, w1=0.30641190690698517\n",
      "Gradient Descent(3258/9999): loss=307.02928436101763, w0=0.02786810928129128, w1=0.30638152251814055\n",
      "Gradient Descent(3259/9999): loss=306.96461477857594, w0=0.027865791112508774, w1=0.30635114107269323\n",
      "Gradient Descent(3260/9999): loss=306.89996100691684, w0=0.027863473152834533, w1=0.30632076257019125\n",
      "Gradient Descent(3261/9999): loss=306.83532304177106, w0=0.027861155402305937, w1=0.3062903870101832\n",
      "Gradient Descent(3262/9999): loss=306.77070087887114, w0=0.027858837860960214, w1=0.30626001439221806\n",
      "Gradient Descent(3263/9999): loss=306.70609451395154, w0=0.027856520528834416, w1=0.30622964471584546\n",
      "Gradient Descent(3264/9999): loss=306.6415039427486, w0=0.027854203405965446, w1=0.3061992779806154\n",
      "Gradient Descent(3265/9999): loss=306.57692916100063, w0=0.027851886492390034, w1=0.30616891418607844\n",
      "Gradient Descent(3266/9999): loss=306.51237016444753, w0=0.027849569788144755, w1=0.3061385533317855\n",
      "Gradient Descent(3267/9999): loss=306.44782694883116, w0=0.027847253293266018, w1=0.30610819541728823\n",
      "Gradient Descent(3268/9999): loss=306.3832995098952, w0=0.027844937007790078, w1=0.30607784044213854\n",
      "Gradient Descent(3269/9999): loss=306.31878784338545, w0=0.027842620931753024, w1=0.30604748840588897\n",
      "Gradient Descent(3270/9999): loss=306.2542919450492, w0=0.02784030506519079, w1=0.3060171393080925\n",
      "Gradient Descent(3271/9999): loss=306.1898118106357, w0=0.027837989408139144, w1=0.30598679314830257\n",
      "Gradient Descent(3272/9999): loss=306.125347435896, w0=0.027835673960633703, w1=0.3059564499260732\n",
      "Gradient Descent(3273/9999): loss=306.06089881658306, w0=0.027833358722709915, w1=0.30592610964095873\n",
      "Gradient Descent(3274/9999): loss=305.9964659484517, w0=0.027831043694403083, w1=0.30589577229251413\n",
      "Gradient Descent(3275/9999): loss=305.93204882725814, w0=0.027828728875748342, w1=0.3058654378802948\n",
      "Gradient Descent(3276/9999): loss=305.86764744876115, w0=0.027826414266780675, w1=0.3058351064038566\n",
      "Gradient Descent(3277/9999): loss=305.80326180872066, w0=0.027824099867534903, w1=0.3058047778627559\n",
      "Gradient Descent(3278/9999): loss=305.7388919028987, w0=0.027821785678045694, w1=0.30577445225654953\n",
      "Gradient Descent(3279/9999): loss=305.67453772705903, w0=0.02781947169834756, w1=0.3057441295847948\n",
      "Gradient Descent(3280/9999): loss=305.61019927696725, w0=0.027817157928474855, w1=0.30571380984704954\n",
      "Gradient Descent(3281/9999): loss=305.5458765483908, w0=0.027814844368461784, w1=0.30568349304287196\n",
      "Gradient Descent(3282/9999): loss=305.48156953709884, w0=0.027812531018342388, w1=0.30565317917182083\n",
      "Gradient Descent(3283/9999): loss=305.4172782388623, w0=0.02781021787815056, w1=0.30562286823345536\n",
      "Gradient Descent(3284/9999): loss=305.353002649454, w0=0.02780790494792004, w1=0.3055925602273352\n",
      "Gradient Descent(3285/9999): loss=305.2887427646484, w0=0.027805592227684407, w1=0.30556225515302055\n",
      "Gradient Descent(3286/9999): loss=305.2244985802219, w0=0.027803279717477092, w1=0.30553195301007197\n",
      "Gradient Descent(3287/9999): loss=305.16027009195244, w0=0.027800967417331374, w1=0.30550165379805055\n",
      "Gradient Descent(3288/9999): loss=305.0960572956201, w0=0.02779865532728038, w1=0.3054713575165179\n",
      "Gradient Descent(3289/9999): loss=305.03186018700626, w0=0.02779634344735708, w1=0.3054410641650359\n",
      "Gradient Descent(3290/9999): loss=304.9676787618944, w0=0.027794031777594295, w1=0.30541077374316716\n",
      "Gradient Descent(3291/9999): loss=304.90351301606967, w0=0.027791720318024696, w1=0.3053804862504746\n",
      "Gradient Descent(3292/9999): loss=304.839362945319, w0=0.0277894090686808, w1=0.30535020168652155\n",
      "Gradient Descent(3293/9999): loss=304.77522854543105, w0=0.027787098029594982, w1=0.30531992005087194\n",
      "Gradient Descent(3294/9999): loss=304.7111098121962, w0=0.027784787200799455, w1=0.3052896413430901\n",
      "Gradient Descent(3295/9999): loss=304.6470067414066, w0=0.02778247658232629, w1=0.3052593655627408\n",
      "Gradient Descent(3296/9999): loss=304.5829193288562, w0=0.027780166174207404, w1=0.30522909270938925\n",
      "Gradient Descent(3297/9999): loss=304.51884757034054, w0=0.027777855976474572, w1=0.3051988227826012\n",
      "Gradient Descent(3298/9999): loss=304.454791461657, w0=0.027775545989159416, w1=0.30516855578194274\n",
      "Gradient Descent(3299/9999): loss=304.3907509986048, w0=0.02777323621229341, w1=0.3051382917069805\n",
      "Gradient Descent(3300/9999): loss=304.3267261769846, w0=0.027770926645907877, w1=0.3051080305572815\n",
      "Gradient Descent(3301/9999): loss=304.2627169925991, w0=0.027768617290034003, w1=0.30507777233241323\n",
      "Gradient Descent(3302/9999): loss=304.1987234412525, w0=0.027766308144702812, w1=0.30504751703194377\n",
      "Gradient Descent(3303/9999): loss=304.13474551875083, w0=0.027763999209945198, w1=0.30501726465544143\n",
      "Gradient Descent(3304/9999): loss=304.0707832209017, w0=0.027761690485791895, w1=0.30498701520247506\n",
      "Gradient Descent(3305/9999): loss=304.0068365435148, w0=0.0277593819722735, w1=0.304956768672614\n",
      "Gradient Descent(3306/9999): loss=303.942905482401, w0=0.027757073669420464, w1=0.304926525065428\n",
      "Gradient Descent(3307/9999): loss=303.87899003337316, w0=0.027754765577263085, w1=0.3048962843804872\n",
      "Gradient Descent(3308/9999): loss=303.8150901922459, w0=0.027752457695831522, w1=0.3048660466173623\n",
      "Gradient Descent(3309/9999): loss=303.7512059548355, w0=0.027750150025155795, w1=0.3048358117756243\n",
      "Gradient Descent(3310/9999): loss=303.6873373169597, w0=0.027747842565265773, w1=0.3048055798548448\n",
      "Gradient Descent(3311/9999): loss=303.62348427443834, w0=0.027745535316191183, w1=0.30477535085459573\n",
      "Gradient Descent(3312/9999): loss=303.5596468230926, w0=0.027743228277961607, w1=0.3047451247744495\n",
      "Gradient Descent(3313/9999): loss=303.4958249587456, w0=0.02774092145060649, w1=0.3047149016139789\n",
      "Gradient Descent(3314/9999): loss=303.43201867722183, w0=0.027738614834155132, w1=0.3046846813727572\n",
      "Gradient Descent(3315/9999): loss=303.3682279743478, w0=0.027736308428636686, w1=0.30465446405035823\n",
      "Gradient Descent(3316/9999): loss=303.30445284595135, w0=0.02773400223408017, w1=0.30462424964635604\n",
      "Gradient Descent(3317/9999): loss=303.2406932878624, w0=0.027731696250514457, w1=0.3045940381603252\n",
      "Gradient Descent(3318/9999): loss=303.17694929591244, w0=0.027729390477968283, w1=0.30456382959184075\n",
      "Gradient Descent(3319/9999): loss=303.11322086593424, w0=0.02772708491647024, w1=0.3045336239404781\n",
      "Gradient Descent(3320/9999): loss=303.04950799376246, w0=0.027724779566048782, w1=0.3045034212058132\n",
      "Gradient Descent(3321/9999): loss=302.9858106752338, w0=0.027722474426732223, w1=0.3044732213874223\n",
      "Gradient Descent(3322/9999): loss=302.92212890618606, w0=0.027720169498548733, w1=0.30444302448488214\n",
      "Gradient Descent(3323/9999): loss=302.8584626824589, w0=0.02771786478152635, w1=0.3044128304977699\n",
      "Gradient Descent(3324/9999): loss=302.79481199989374, w0=0.02771556027569297, w1=0.3043826394256632\n",
      "Gradient Descent(3325/9999): loss=302.73117685433346, w0=0.027713255981076348, w1=0.30435245126814\n",
      "Gradient Descent(3326/9999): loss=302.66755724162294, w0=0.027710951897704107, w1=0.30432226602477874\n",
      "Gradient Descent(3327/9999): loss=302.6039531576081, w0=0.02770864802560373, w1=0.3042920836951583\n",
      "Gradient Descent(3328/9999): loss=302.54036459813705, w0=0.02770634436480256, w1=0.30426190427885796\n",
      "Gradient Descent(3329/9999): loss=302.4767915590593, w0=0.027704040915327806, w1=0.30423172777545743\n",
      "Gradient Descent(3330/9999): loss=302.41323403622596, w0=0.02770173767720654, w1=0.3042015541845368\n",
      "Gradient Descent(3331/9999): loss=302.34969202548996, w0=0.0276994346504657, w1=0.3041713835056767\n",
      "Gradient Descent(3332/9999): loss=302.2861655227057, w0=0.027697131835132085, w1=0.304141215738458\n",
      "Gradient Descent(3333/9999): loss=302.222654523729, w0=0.027694829231232357, w1=0.30411105088246215\n",
      "Gradient Descent(3334/9999): loss=302.1591590244178, w0=0.027692526838793047, w1=0.3040808889372709\n",
      "Gradient Descent(3335/9999): loss=302.09567902063134, w0=0.027690224657840556, w1=0.3040507299024665\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(3336/9999): loss=302.0322145082305, w0=0.02768792268840114, w1=0.3040205737776316\n",
      "Gradient Descent(3337/9999): loss=301.9687654830778, w0=0.02768562093050092, w1=0.30399042056234915\n",
      "Gradient Descent(3338/9999): loss=301.90533194103733, w0=0.0276833193841659, w1=0.3039602702562027\n",
      "Gradient Descent(3339/9999): loss=301.841913877975, w0=0.027681018049421934, w1=0.3039301228587761\n",
      "Gradient Descent(3340/9999): loss=301.77851128975806, w0=0.02767871692629475, w1=0.30389997836965355\n",
      "Gradient Descent(3341/9999): loss=301.71512417225534, w0=0.02767641601480994, w1=0.3038698367884198\n",
      "Gradient Descent(3342/9999): loss=301.65175252133747, w0=0.02767411531499297, w1=0.3038396981146599\n",
      "Gradient Descent(3343/9999): loss=301.5883963328767, w0=0.027671814826869166, w1=0.30380956234795936\n",
      "Gradient Descent(3344/9999): loss=301.52505560274653, w0=0.027669514550463733, w1=0.3037794294879041\n",
      "Gradient Descent(3345/9999): loss=301.4617303268224, w0=0.027667214485801735, w1=0.3037492995340804\n",
      "Gradient Descent(3346/9999): loss=301.39842050098133, w0=0.02766491463290811, w1=0.30371917248607505\n",
      "Gradient Descent(3347/9999): loss=301.33512612110155, w0=0.027662614991807662, w1=0.3036890483434751\n",
      "Gradient Descent(3348/9999): loss=301.27184718306347, w0=0.02766031556252507, w1=0.30365892710586806\n",
      "Gradient Descent(3349/9999): loss=301.20858368274844, w0=0.027658016345084878, w1=0.3036288087728419\n",
      "Gradient Descent(3350/9999): loss=301.1453356160399, w0=0.027655717339511502, w1=0.30359869334398487\n",
      "Gradient Descent(3351/9999): loss=301.08210297882255, w0=0.027653418545829235, w1=0.30356858081888577\n",
      "Gradient Descent(3352/9999): loss=301.01888576698275, w0=0.02765111996406223, w1=0.3035384711971337\n",
      "Gradient Descent(3353/9999): loss=300.9556839764085, w0=0.027648821594234527, w1=0.30350836447831814\n",
      "Gradient Descent(3354/9999): loss=300.89249760298935, w0=0.02764652343637002, w1=0.30347826066202904\n",
      "Gradient Descent(3355/9999): loss=300.8293266426162, w0=0.027644225490492485, w1=0.30344815974785666\n",
      "Gradient Descent(3356/9999): loss=300.7661710911818, w0=0.02764192775662557, w1=0.30341806173539176\n",
      "Gradient Descent(3357/9999): loss=300.7030309445804, w0=0.027639630234792798, w1=0.3033879666242254\n",
      "Gradient Descent(3358/9999): loss=300.63990619870754, w0=0.02763733292501756, w1=0.30335787441394907\n",
      "Gradient Descent(3359/9999): loss=300.57679684946066, w0=0.027635035827323125, w1=0.3033277851041547\n",
      "Gradient Descent(3360/9999): loss=300.5137028927387, w0=0.027632738941732633, w1=0.3032976986944345\n",
      "Gradient Descent(3361/9999): loss=300.45062432444183, w0=0.0276304422682691, w1=0.30326761518438117\n",
      "Gradient Descent(3362/9999): loss=300.38756114047226, w0=0.027628145806955415, w1=0.3032375345735877\n",
      "Gradient Descent(3363/9999): loss=300.32451333673316, w0=0.027625849557814348, w1=0.3032074568616476\n",
      "Gradient Descent(3364/9999): loss=300.2614809091298, w0=0.027623553520868534, w1=0.3031773820481546\n",
      "Gradient Descent(3365/9999): loss=300.19846385356857, w0=0.02762125769614049, w1=0.30314731013270296\n",
      "Gradient Descent(3366/9999): loss=300.13546216595756, w0=0.027618962083652606, w1=0.3031172411148873\n",
      "Gradient Descent(3367/9999): loss=300.07247584220653, w0=0.027616666683427153, w1=0.3030871749943026\n",
      "Gradient Descent(3368/9999): loss=300.0095048782266, w0=0.027614371495486276, w1=0.3030571117705442\n",
      "Gradient Descent(3369/9999): loss=299.94654926993024, w0=0.027612076519851994, w1=0.3030270514432078\n",
      "Gradient Descent(3370/9999): loss=299.883609013232, w0=0.027609781756546208, w1=0.3029969940118896\n",
      "Gradient Descent(3371/9999): loss=299.82068410404736, w0=0.02760748720559069, w1=0.3029669394761861\n",
      "Gradient Descent(3372/9999): loss=299.7577745382936, w0=0.0276051928670071, w1=0.3029368878356941\n",
      "Gradient Descent(3373/9999): loss=299.69488031188956, w0=0.027602898740816964, w1=0.302906839090011\n",
      "Gradient Descent(3374/9999): loss=299.6320014207555, w0=0.027600604827041696, w1=0.3028767932387343\n",
      "Gradient Descent(3375/9999): loss=299.5691378608131, w0=0.027598311125702588, w1=0.3028467502814621\n",
      "Gradient Descent(3376/9999): loss=299.50628962798584, w0=0.027596017636820805, w1=0.3028167102177927\n",
      "Gradient Descent(3377/9999): loss=299.44345671819843, w0=0.027593724360417397, w1=0.302786673047325\n",
      "Gradient Descent(3378/9999): loss=299.380639127377, w0=0.027591431296513293, w1=0.302756638769658\n",
      "Gradient Descent(3379/9999): loss=299.31783685144967, w0=0.027589138445129297, w1=0.30272660738439133\n",
      "Gradient Descent(3380/9999): loss=299.2550498863456, w0=0.027586845806286103, w1=0.30269657889112483\n",
      "Gradient Descent(3381/9999): loss=299.1922782279956, w0=0.027584553380004276, w1=0.30266655328945874\n",
      "Gradient Descent(3382/9999): loss=299.12952187233196, w0=0.02758226116630427, w1=0.3026365305789937\n",
      "Gradient Descent(3383/9999): loss=299.0667808152886, w0=0.02757996916520641, w1=0.3026065107593307\n",
      "Gradient Descent(3384/9999): loss=299.00405505280077, w0=0.02757767737673092, w1=0.3025764938300711\n",
      "Gradient Descent(3385/9999): loss=298.9413445808051, w0=0.027575385800897887, w1=0.3025464797908167\n",
      "Gradient Descent(3386/9999): loss=298.8786493952401, w0=0.02757309443772729, w1=0.30251646864116954\n",
      "Gradient Descent(3387/9999): loss=298.81596949204527, w0=0.027570803287238994, w1=0.30248646038073207\n",
      "Gradient Descent(3388/9999): loss=298.753304867162, w0=0.027568512349452737, w1=0.3024564550091071\n",
      "Gradient Descent(3389/9999): loss=298.69065551653296, w0=0.02756622162438815, w1=0.3024264525258979\n",
      "Gradient Descent(3390/9999): loss=298.62802143610236, w0=0.027563931112064743, w1=0.30239645293070794\n",
      "Gradient Descent(3391/9999): loss=298.5654026218158, w0=0.027561640812501904, w1=0.3023664562231412\n",
      "Gradient Descent(3392/9999): loss=298.5027990696204, w0=0.027559350725718915, w1=0.30233646240280193\n",
      "Gradient Descent(3393/9999): loss=298.44021077546495, w0=0.02755706085173494, w1=0.3023064714692948\n",
      "Gradient Descent(3394/9999): loss=298.3776377352994, w0=0.027554771190569028, w1=0.30227648342222474\n",
      "Gradient Descent(3395/9999): loss=298.31507994507524, w0=0.027552481742240106, w1=0.3022464982611972\n",
      "Gradient Descent(3396/9999): loss=298.25253740074555, w0=0.027550192506766994, w1=0.30221651598581784\n",
      "Gradient Descent(3397/9999): loss=298.19001009826485, w0=0.0275479034841684, w1=0.3021865365956927\n",
      "Gradient Descent(3398/9999): loss=298.1274980335888, w0=0.027545614674462905, w1=0.3021565600904283\n",
      "Gradient Descent(3399/9999): loss=298.0650012026751, w0=0.027543326077668993, w1=0.30212658646963136\n",
      "Gradient Descent(3400/9999): loss=298.00251960148245, w0=0.02754103769380502, w1=0.302096615732909\n",
      "Gradient Descent(3401/9999): loss=297.9400532259711, w0=0.027538749522889244, w1=0.30206664787986875\n",
      "Gradient Descent(3402/9999): loss=297.8776020721028, w0=0.02753646156493979, w1=0.3020366829101184\n",
      "Gradient Descent(3403/9999): loss=297.81516613584085, w0=0.02753417381997469, w1=0.30200672082326624\n",
      "Gradient Descent(3404/9999): loss=297.7527454131498, w0=0.027531886288011856, w1=0.30197676161892073\n",
      "Gradient Descent(3405/9999): loss=297.69033989999565, w0=0.027529598969069088, w1=0.3019468052966908\n",
      "Gradient Descent(3406/9999): loss=297.62794959234617, w0=0.02752731186316407, w1=0.3019168518561857\n",
      "Gradient Descent(3407/9999): loss=297.5655744861699, w0=0.027525024970314383, w1=0.30188690129701506\n",
      "Gradient Descent(3408/9999): loss=297.5032145774377, w0=0.02752273829053749, w1=0.3018569536187888\n",
      "Gradient Descent(3409/9999): loss=297.4408698621212, w0=0.02752045182385075, w1=0.3018270088211171\n",
      "Gradient Descent(3410/9999): loss=297.3785403361936, w0=0.027518165570271404, w1=0.30179706690361074\n",
      "Gradient Descent(3411/9999): loss=297.31622599562974, w0=0.02751587952981659, w1=0.3017671278658806\n",
      "Gradient Descent(3412/9999): loss=297.2539268364057, w0=0.02751359370250333, w1=0.30173719170753804\n",
      "Gradient Descent(3413/9999): loss=297.191642854499, w0=0.027511308088348542, w1=0.30170725842819474\n",
      "Gradient Descent(3414/9999): loss=297.1293740458886, w0=0.02750902268736903, w1=0.30167732802746267\n",
      "Gradient Descent(3415/9999): loss=297.06712040655503, w0=0.027506737499581494, w1=0.3016474005049542\n",
      "Gradient Descent(3416/9999): loss=297.0048819324801, w0=0.02750445252500252, w1=0.30161747586028204\n",
      "Gradient Descent(3417/9999): loss=296.94265861964686, w0=0.027502167763648586, w1=0.3015875540930592\n",
      "Gradient Descent(3418/9999): loss=296.88045046404017, w0=0.027499883215536067, w1=0.3015576352028991\n",
      "Gradient Descent(3419/9999): loss=296.8182574616461, w0=0.027497598880681227, w1=0.30152771918941534\n",
      "Gradient Descent(3420/9999): loss=296.7560796084521, w0=0.02749531475910022, w1=0.30149780605222204\n",
      "Gradient Descent(3421/9999): loss=296.69391690044705, w0=0.0274930308508091, w1=0.3014678957909336\n",
      "Gradient Descent(3422/9999): loss=296.6317693336214, w0=0.027490747155823805, w1=0.30143798840516467\n",
      "Gradient Descent(3423/9999): loss=296.5696369039666, w0=0.027488463674160173, w1=0.3014080838945304\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(3424/9999): loss=296.50751960747607, w0=0.027486180405833935, w1=0.30137818225864604\n",
      "Gradient Descent(3425/9999): loss=296.4454174401443, w0=0.02748389735086071, w1=0.30134828349712744\n",
      "Gradient Descent(3426/9999): loss=296.3833303979671, w0=0.027481614509256014, w1=0.3013183876095906\n",
      "Gradient Descent(3427/9999): loss=296.321258476942, w0=0.02747933188103526, w1=0.30128849459565193\n",
      "Gradient Descent(3428/9999): loss=296.25920167306737, w0=0.02747704946621376, w1=0.3012586044549281\n",
      "Gradient Descent(3429/9999): loss=296.1971599823437, w0=0.027474767264806705, w1=0.30122871718703614\n",
      "Gradient Descent(3430/9999): loss=296.13513340077236, w0=0.0274724852768292, w1=0.3011988327915935\n",
      "Gradient Descent(3431/9999): loss=296.0731219243562, w0=0.027470203502296238, w1=0.30116895126821785\n",
      "Gradient Descent(3432/9999): loss=296.0111255490997, w0=0.027467921941222703, w1=0.3011390726165272\n",
      "Gradient Descent(3433/9999): loss=295.9491442710085, w0=0.027465640593623383, w1=0.30110919683613996\n",
      "Gradient Descent(3434/9999): loss=295.8871780860897, w0=0.02746335945951295, w1=0.30107932392667475\n",
      "Gradient Descent(3435/9999): loss=295.8252269903516, w0=0.027461078538905993, w1=0.3010494538877506\n",
      "Gradient Descent(3436/9999): loss=295.7632909798042, w0=0.02745879783181698, w1=0.30101958671898693\n",
      "Gradient Descent(3437/9999): loss=295.70137005045865, w0=0.02745651733826028, w1=0.3009897224200033\n",
      "Gradient Descent(3438/9999): loss=295.63946419832763, w0=0.027454237058250168, w1=0.30095986099041977\n",
      "Gradient Descent(3439/9999): loss=295.5775734194253, w0=0.027451956991800808, w1=0.30093000242985657\n",
      "Gradient Descent(3440/9999): loss=295.5156977097666, w0=0.02744967713892626, w1=0.30090014673793436\n",
      "Gradient Descent(3441/9999): loss=295.45383706536865, w0=0.027447397499640494, w1=0.30087029391427406\n",
      "Gradient Descent(3442/9999): loss=295.39199148224924, w0=0.02744511807395737, w1=0.30084044395849696\n",
      "Gradient Descent(3443/9999): loss=295.33016095642813, w0=0.027442838861890646, w1=0.30081059687022466\n",
      "Gradient Descent(3444/9999): loss=295.26834548392594, w0=0.02744055986345398, w1=0.30078075264907905\n",
      "Gradient Descent(3445/9999): loss=295.2065450607651, w0=0.027438281078660937, w1=0.30075091129468234\n",
      "Gradient Descent(3446/9999): loss=295.14475968296904, w0=0.02743600250752497, w1=0.3007210728066571\n",
      "Gradient Descent(3447/9999): loss=295.0829893465627, w0=0.02743372415005944, w1=0.30069123718462615\n",
      "Gradient Descent(3448/9999): loss=295.02123404757236, w0=0.027431446006277605, w1=0.30066140442821265\n",
      "Gradient Descent(3449/9999): loss=294.95949378202585, w0=0.027429168076192625, w1=0.30063157453704015\n",
      "Gradient Descent(3450/9999): loss=294.897768545952, w0=0.02742689035981756, w1=0.30060174751073243\n",
      "Gradient Descent(3451/9999): loss=294.83605833538127, w0=0.02742461285716537, w1=0.3005719233489136\n",
      "Gradient Descent(3452/9999): loss=294.7743631463455, w0=0.027422335568248916, w1=0.300542102051208\n",
      "Gradient Descent(3453/9999): loss=294.7126829748775, w0=0.02742005849308096, w1=0.30051228361724047\n",
      "Gradient Descent(3454/9999): loss=294.65101781701196, w0=0.02741778163167417, w1=0.30048246804663603\n",
      "Gradient Descent(3455/9999): loss=294.58936766878463, w0=0.027415504984041113, w1=0.30045265533902005\n",
      "Gradient Descent(3456/9999): loss=294.52773252623257, w0=0.027413228550194256, w1=0.30042284549401815\n",
      "Gradient Descent(3457/9999): loss=294.4661123853942, w0=0.027410952330145972, w1=0.30039303851125637\n",
      "Gradient Descent(3458/9999): loss=294.4045072423094, w0=0.02740867632390854, w1=0.30036323439036094\n",
      "Gradient Descent(3459/9999): loss=294.34291709301937, w0=0.02740640053149413, w1=0.30033343313095845\n",
      "Gradient Descent(3460/9999): loss=294.2813419335666, w0=0.027404124952914832, w1=0.30030363473267585\n",
      "Gradient Descent(3461/9999): loss=294.2197817599949, w0=0.027401849588182627, w1=0.3002738391951403\n",
      "Gradient Descent(3462/9999): loss=294.1582365683494, w0=0.027399574437309403, w1=0.3002440465179793\n",
      "Gradient Descent(3463/9999): loss=294.0967063546768, w0=0.027397299500306952, w1=0.3002142567008207\n",
      "Gradient Descent(3464/9999): loss=294.0351911150247, w0=0.02739502477718697, w1=0.30018446974329255\n",
      "Gradient Descent(3465/9999): loss=293.9736908454425, w0=0.027392750267961063, w1=0.30015468564502334\n",
      "Gradient Descent(3466/9999): loss=293.91220554198077, w0=0.027390475972640736, w1=0.30012490440564177\n",
      "Gradient Descent(3467/9999): loss=293.85073520069096, w0=0.0273882018912374, w1=0.30009512602477684\n",
      "Gradient Descent(3468/9999): loss=293.7892798176266, w0=0.02738592802376237, w1=0.3000653505020579\n",
      "Gradient Descent(3469/9999): loss=293.727839388842, w0=0.027383654370226874, w1=0.30003557783711454\n",
      "Gradient Descent(3470/9999): loss=293.66641391039315, w0=0.027381380930642032, w1=0.3000058080295767\n",
      "Gradient Descent(3471/9999): loss=293.60500337833713, w0=0.027379107705018883, w1=0.2999760410790746\n",
      "Gradient Descent(3472/9999): loss=293.5436077887323, w0=0.02737683469336837, w1=0.29994627698523874\n",
      "Gradient Descent(3473/9999): loss=293.4822271376386, w0=0.027374561895701338, w1=0.29991651574769995\n",
      "Gradient Descent(3474/9999): loss=293.4208614211169, w0=0.027372289312028543, w1=0.29988675736608933\n",
      "Gradient Descent(3475/9999): loss=293.3595106352299, w0=0.027370016942360645, w1=0.2998570018400383\n",
      "Gradient Descent(3476/9999): loss=293.2981747760412, w0=0.027367744786708212, w1=0.2998272491691785\n",
      "Gradient Descent(3477/9999): loss=293.23685383961583, w0=0.02736547284508172, w1=0.29979749935314204\n",
      "Gradient Descent(3478/9999): loss=293.17554782202023, w0=0.02736320111749156, w1=0.2997677523915611\n",
      "Gradient Descent(3479/9999): loss=293.11425671932193, w0=0.027360929603948012, w1=0.2997380082840683\n",
      "Gradient Descent(3480/9999): loss=293.05298052759, w0=0.027358658304461287, w1=0.2997082670302965\n",
      "Gradient Descent(3481/9999): loss=292.99171924289476, w0=0.027356387219041493, w1=0.2996785286298789\n",
      "Gradient Descent(3482/9999): loss=292.9304728613077, w0=0.027354116347698645, w1=0.29964879308244896\n",
      "Gradient Descent(3483/9999): loss=292.86924137890185, w0=0.02735184569044267, w1=0.29961906038764036\n",
      "Gradient Descent(3484/9999): loss=292.8080247917513, w0=0.02734957524728341, w1=0.2995893305450872\n",
      "Gradient Descent(3485/9999): loss=292.7468230959315, w0=0.027347305018230603, w1=0.29955960355442374\n",
      "Gradient Descent(3486/9999): loss=292.68563628751946, w0=0.02734503500329391, w1=0.2995298794152846\n",
      "Gradient Descent(3487/9999): loss=292.62446436259313, w0=0.027342765202482897, w1=0.2995001581273047\n",
      "Gradient Descent(3488/9999): loss=292.5633073172318, w0=0.027340495615807037, w1=0.2994704396901192\n",
      "Gradient Descent(3489/9999): loss=292.50216514751634, w0=0.027338226243275716, w1=0.29944072410336364\n",
      "Gradient Descent(3490/9999): loss=292.4410378495286, w0=0.027335957084898235, w1=0.2994110113666737\n",
      "Gradient Descent(3491/9999): loss=292.37992541935193, w0=0.0273336881406838, w1=0.29938130147968545\n",
      "Gradient Descent(3492/9999): loss=292.31882785307107, w0=0.027331419410641534, w1=0.2993515944420352\n",
      "Gradient Descent(3493/9999): loss=292.2577451467715, w0=0.027329150894780465, w1=0.29932189025335953\n",
      "Gradient Descent(3494/9999): loss=292.1966772965406, w0=0.027326882593109537, w1=0.2992921889132954\n",
      "Gradient Descent(3495/9999): loss=292.13562429846684, w0=0.027324614505637602, w1=0.2992624904214799\n",
      "Gradient Descent(3496/9999): loss=292.0745861486398, w0=0.02732234663237343, w1=0.2992327947775505\n",
      "Gradient Descent(3497/9999): loss=292.01356284315045, w0=0.0273200789733257, w1=0.29920310198114497\n",
      "Gradient Descent(3498/9999): loss=291.9525543780912, w0=0.027317811528503003, w1=0.2991734120319013\n",
      "Gradient Descent(3499/9999): loss=291.89156074955565, w0=0.027315544297913844, w1=0.29914372492945773\n",
      "Gradient Descent(3500/9999): loss=291.8305819536385, w0=0.02731327728156664, w1=0.29911404067345293\n",
      "Gradient Descent(3501/9999): loss=291.76961798643595, w0=0.02731101047946972, w1=0.29908435926352567\n",
      "Gradient Descent(3502/9999): loss=291.70866884404523, w0=0.027308743891631335, w1=0.29905468069931507\n",
      "Gradient Descent(3503/9999): loss=291.64773452256526, w0=0.02730647751805964, w1=0.29902500498046053\n",
      "Gradient Descent(3504/9999): loss=291.5868150180958, w0=0.02730421135876271, w1=0.2989953321066018\n",
      "Gradient Descent(3505/9999): loss=291.5259103267381, w0=0.027301945413748526, w1=0.29896566207737874\n",
      "Gradient Descent(3506/9999): loss=291.4650204445948, w0=0.027299679683024998, w1=0.2989359948924316\n",
      "Gradient Descent(3507/9999): loss=291.4041453677695, w0=0.027297414166599938, w1=0.29890633055140087\n",
      "Gradient Descent(3508/9999): loss=291.34328509236724, w0=0.027295148864481075, w1=0.29887666905392735\n",
      "Gradient Descent(3509/9999): loss=291.2824396144943, w0=0.027292883776676057, w1=0.29884701039965206\n",
      "Gradient Descent(3510/9999): loss=291.2216089302583, w0=0.027290618903192447, w1=0.2988173545882163\n",
      "Gradient Descent(3511/9999): loss=291.1607930357679, w0=0.027288354244037718, w1=0.29878770161926166\n",
      "Gradient Descent(3512/9999): loss=291.0999919271333, w0=0.027286089799219267, w1=0.29875805149243007\n",
      "Gradient Descent(3513/9999): loss=291.039205600466, w0=0.027283825568744403, w1=0.2987284042073636\n",
      "Gradient Descent(3514/9999): loss=290.9784340518785, w0=0.027281561552620348, w1=0.2986987597637046\n",
      "Gradient Descent(3515/9999): loss=290.9176772774844, w0=0.027279297750854248, w1=0.29866911816109576\n",
      "Gradient Descent(3516/9999): loss=290.85693527339913, w0=0.02727703416345316, w1=0.29863947939918006\n",
      "Gradient Descent(3517/9999): loss=290.796208035739, w0=0.02727477079042406, w1=0.29860984347760067\n",
      "Gradient Descent(3518/9999): loss=290.7354955606216, w0=0.027272507631773843, w1=0.298580210396001\n",
      "Gradient Descent(3519/9999): loss=290.67479784416577, w0=0.027270244687509316, w1=0.2985505801540249\n",
      "Gradient Descent(3520/9999): loss=290.6141148824918, w0=0.027267981957637206, w1=0.2985209527513163\n",
      "Gradient Descent(3521/9999): loss=290.55344667172096, w0=0.027265719442164164, w1=0.2984913281875194\n",
      "Gradient Descent(3522/9999): loss=290.4927932079759, w0=0.02726345714109675, w1=0.2984617064622788\n",
      "Gradient Descent(3523/9999): loss=290.43215448738056, w0=0.027261195054441446, w1=0.2984320875752393\n",
      "Gradient Descent(3524/9999): loss=290.37153050605997, w0=0.027258933182204657, w1=0.29840247152604593\n",
      "Gradient Descent(3525/9999): loss=290.31092126014056, w0=0.027256671524392696, w1=0.29837285831434396\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(3526/9999): loss=290.25032674575, w0=0.027254410081011805, w1=0.298343247939779\n",
      "Gradient Descent(3527/9999): loss=290.18974695901716, w0=0.027252148852068142, w1=0.2983136404019969\n",
      "Gradient Descent(3528/9999): loss=290.1291818960721, w0=0.027249887837567783, w1=0.29828403570064377\n",
      "Gradient Descent(3529/9999): loss=290.06863155304603, w0=0.027247627037516726, w1=0.29825443383536593\n",
      "Gradient Descent(3530/9999): loss=290.00809592607186, w0=0.027245366451920886, w1=0.29822483480581\n",
      "Gradient Descent(3531/9999): loss=289.9475750112832, w0=0.0272431060807861, w1=0.2981952386116229\n",
      "Gradient Descent(3532/9999): loss=289.88706880481516, w0=0.027240845924118125, w1=0.2981656452524517\n",
      "Gradient Descent(3533/9999): loss=289.82657730280397, w0=0.027238585981922636, w1=0.2981360547279438\n",
      "Gradient Descent(3534/9999): loss=289.7661005013873, w0=0.02723632625420523, w1=0.29810646703774685\n",
      "Gradient Descent(3535/9999): loss=289.7056383967038, w0=0.02723406674097143, w1=0.2980768821815088\n",
      "Gradient Descent(3536/9999): loss=289.64519098489353, w0=0.027231807442226676, w1=0.29804730015887776\n",
      "Gradient Descent(3537/9999): loss=289.58475826209764, w0=0.027229548357976325, w1=0.29801772096950213\n",
      "Gradient Descent(3538/9999): loss=289.5243402244587, w0=0.02722728948822566, w1=0.2979881446130306\n",
      "Gradient Descent(3539/9999): loss=289.4639368681203, w0=0.027225030832979887, w1=0.29795857108911206\n",
      "Gradient Descent(3540/9999): loss=289.4035481892275, w0=0.027222772392244132, w1=0.2979290003973957\n",
      "Gradient Descent(3541/9999): loss=289.3431741839262, w0=0.027220514166023443, w1=0.29789943253753104\n",
      "Gradient Descent(3542/9999): loss=289.28281484836407, w0=0.02721825615432279, w1=0.2978698675091676\n",
      "Gradient Descent(3543/9999): loss=289.2224701786894, w0=0.027215998357147067, w1=0.2978403053119554\n",
      "Gradient Descent(3544/9999): loss=289.1621401710523, w0=0.027213740774501092, w1=0.2978107459455446\n",
      "Gradient Descent(3545/9999): loss=289.1018248216033, w0=0.0272114834063896, w1=0.2977811894095856\n",
      "Gradient Descent(3546/9999): loss=289.0415241264953, w0=0.027209226252817256, w1=0.2977516357037291\n",
      "Gradient Descent(3547/9999): loss=288.9812380818813, w0=0.027206969313788645, w1=0.2977220848276261\n",
      "Gradient Descent(3548/9999): loss=288.9209666839162, w0=0.027204712589308278, w1=0.29769253678092766\n",
      "Gradient Descent(3549/9999): loss=288.86070992875585, w0=0.027202456079380585, w1=0.29766299156328524\n",
      "Gradient Descent(3550/9999): loss=288.80046781255726, w0=0.027200199784009924, w1=0.29763344917435053\n",
      "Gradient Descent(3551/9999): loss=288.740240331479, w0=0.027197943703200576, w1=0.2976039096137755\n",
      "Gradient Descent(3552/9999): loss=288.6800274816804, w0=0.02719568783695675, w1=0.2975743728812122\n",
      "Gradient Descent(3553/9999): loss=288.6198292593224, w0=0.02719343218528257, w1=0.29754483897631306\n",
      "Gradient Descent(3554/9999): loss=288.5596456605668, w0=0.027191176748182097, w1=0.2975153078987308\n",
      "Gradient Descent(3555/9999): loss=288.4994766815768, w0=0.02718892152565931, w1=0.2974857796481183\n",
      "Gradient Descent(3556/9999): loss=288.439322318517, w0=0.02718666651771811, w1=0.29745625422412864\n",
      "Gradient Descent(3557/9999): loss=288.3791825675527, w0=0.027184411724362333, w1=0.29742673162641525\n",
      "Gradient Descent(3558/9999): loss=288.3190574248509, w0=0.027182157145595732, w1=0.29739721185463175\n",
      "Gradient Descent(3559/9999): loss=288.25894688657945, w0=0.027179902781421992, w1=0.297367694908432\n",
      "Gradient Descent(3560/9999): loss=288.1988509489077, w0=0.027177648631844718, w1=0.29733818078747015\n",
      "Gradient Descent(3561/9999): loss=288.138769608006, w0=0.027175394696867448, w1=0.2973086694914005\n",
      "Gradient Descent(3562/9999): loss=288.07870286004606, w0=0.02717314097649364, w1=0.29727916101987767\n",
      "Gradient Descent(3563/9999): loss=288.0186507012005, w0=0.027170887470726685, w1=0.29724965537255643\n",
      "Gradient Descent(3564/9999): loss=287.95861312764345, w0=0.02716863417956989, w1=0.2972201525490919\n",
      "Gradient Descent(3565/9999): loss=287.8985901355501, w0=0.027166381103026504, w1=0.2971906525491394\n",
      "Gradient Descent(3566/9999): loss=287.838581721097, w0=0.02716412824109969, w1=0.29716115537235444\n",
      "Gradient Descent(3567/9999): loss=287.77858788046154, w0=0.02716187559379255, w1=0.29713166101839283\n",
      "Gradient Descent(3568/9999): loss=287.7186086098226, w0=0.0271596231611081, w1=0.2971021694869106\n",
      "Gradient Descent(3569/9999): loss=287.6586439053602, w0=0.0271573709430493, w1=0.29707268077756394\n",
      "Gradient Descent(3570/9999): loss=287.59869376325554, w0=0.027155118939619024, w1=0.29704319489000935\n",
      "Gradient Descent(3571/9999): loss=287.538758179691, w0=0.027152867150820083, w1=0.2970137118239036\n",
      "Gradient Descent(3572/9999): loss=287.47883715085004, w0=0.02715061557665521, w1=0.2969842315789037\n",
      "Gradient Descent(3573/9999): loss=287.4189306729176, w0=0.027148364217127068, w1=0.2969547541546667\n",
      "Gradient Descent(3574/9999): loss=287.3590387420795, w0=0.02714611307223825, w1=0.2969252795508502\n",
      "Gradient Descent(3575/9999): loss=287.2991613545229, w0=0.02714386214199128, w1=0.2968958077671117\n",
      "Gradient Descent(3576/9999): loss=287.2392985064362, w0=0.02714161142638861, w1=0.2968663388031092\n",
      "Gradient Descent(3577/9999): loss=287.17945019400884, w0=0.027139360925432624, w1=0.2968368726585008\n",
      "Gradient Descent(3578/9999): loss=287.1196164134315, w0=0.027137110639125623, w1=0.2968074093329448\n",
      "Gradient Descent(3579/9999): loss=287.0597971608961, w0=0.027134860567469856, w1=0.29677794882609987\n",
      "Gradient Descent(3580/9999): loss=286.9999924325956, w0=0.02713261071046749, w1=0.2967484911376248\n",
      "Gradient Descent(3581/9999): loss=286.9402022247245, w0=0.02713036106812062, w1=0.2967190362671786\n",
      "Gradient Descent(3582/9999): loss=286.88042653347793, w0=0.02712811164043128, w1=0.2966895842144206\n",
      "Gradient Descent(3583/9999): loss=286.8206653550527, w0=0.02712586242740143, w1=0.2966601349790103\n",
      "Gradient Descent(3584/9999): loss=286.7609186856464, w0=0.027123613429032963, w1=0.29663068856060737\n",
      "Gradient Descent(3585/9999): loss=286.70118652145817, w0=0.0271213646453277, w1=0.2966012449588718\n",
      "Gradient Descent(3586/9999): loss=286.6414688586881, w0=0.027119116076287395, w1=0.2965718041734638\n",
      "Gradient Descent(3587/9999): loss=286.5817656935374, w0=0.02711686772191373, w1=0.2965423662040438\n",
      "Gradient Descent(3588/9999): loss=286.5220770222086, w0=0.027114619582208323, w1=0.2965129310502724\n",
      "Gradient Descent(3589/9999): loss=286.46240284090527, w0=0.027112371657172722, w1=0.2964834987118104\n",
      "Gradient Descent(3590/9999): loss=286.40274314583246, w0=0.027110123946808402, w1=0.296454069188319\n",
      "Gradient Descent(3591/9999): loss=286.34309793319596, w0=0.02710787645111678, w1=0.2964246424794595\n",
      "Gradient Descent(3592/9999): loss=286.28346719920313, w0=0.027105629170099194, w1=0.29639521858489337\n",
      "Gradient Descent(3593/9999): loss=286.22385094006205, w0=0.027103382103756922, w1=0.29636579750428244\n",
      "Gradient Descent(3594/9999): loss=286.1642491519825, w0=0.027101135252091173, w1=0.29633637923728867\n",
      "Gradient Descent(3595/9999): loss=286.10466183117506, w0=0.027098888615103085, w1=0.29630696378357424\n",
      "Gradient Descent(3596/9999): loss=286.04508897385153, w0=0.027096642192793733, w1=0.29627755114280163\n",
      "Gradient Descent(3597/9999): loss=285.9855305762249, w0=0.027094395985164124, w1=0.29624814131463345\n",
      "Gradient Descent(3598/9999): loss=285.92598663450946, w0=0.0270921499922152, w1=0.2962187342987326\n",
      "Gradient Descent(3599/9999): loss=285.86645714492045, w0=0.027089904213947826, w1=0.2961893300947621\n",
      "Gradient Descent(3600/9999): loss=285.8069421036743, w0=0.027087658650362816, w1=0.29615992870238533\n",
      "Gradient Descent(3601/9999): loss=285.7474415069888, w0=0.02708541330146091, w1=0.2961305301212658\n",
      "Gradient Descent(3602/9999): loss=285.6879553510828, w0=0.027083168167242783, w1=0.2961011343510672\n",
      "Gradient Descent(3603/9999): loss=285.6284836321761, w0=0.027080923247709044, w1=0.2960717413914536\n",
      "Gradient Descent(3604/9999): loss=285.56902634649, w0=0.027078678542860234, w1=0.2960423512420891\n",
      "Gradient Descent(3605/9999): loss=285.5095834902467, w0=0.02707643405269683, w1=0.29601296390263815\n",
      "Gradient Descent(3606/9999): loss=285.4501550596697, w0=0.027074189777219246, w1=0.2959835793727653\n",
      "Gradient Descent(3607/9999): loss=285.3907410509836, w0=0.027071945716427827, w1=0.2959541976521355\n",
      "Gradient Descent(3608/9999): loss=285.3313414604142, w0=0.02706970187032286, w1=0.29592481874041365\n",
      "Gradient Descent(3609/9999): loss=285.2719562841884, w0=0.027067458238904555, w1=0.2958954426372651\n",
      "Gradient Descent(3610/9999): loss=285.2125855185341, w0=0.02706521482217307, w1=0.2958660693423553\n",
      "Gradient Descent(3611/9999): loss=285.15322915968085, w0=0.027062971620128492, w1=0.29583669885534997\n",
      "Gradient Descent(3612/9999): loss=285.0938872038588, w0=0.027060728632770842, w1=0.29580733117591496\n",
      "Gradient Descent(3613/9999): loss=285.03455964729955, w0=0.02705848586010008, w1=0.29577796630371644\n",
      "Gradient Descent(3614/9999): loss=284.9752464862357, w0=0.027056243302116107, w1=0.29574860423842075\n",
      "Gradient Descent(3615/9999): loss=284.9159477169011, w0=0.027054000958818752, w1=0.2957192449796944\n",
      "Gradient Descent(3616/9999): loss=284.8566633355308, w0=0.02705175883020778, w1=0.29568988852720407\n",
      "Gradient Descent(3617/9999): loss=284.79739333836085, w0=0.0270495169162829, w1=0.29566053488061683\n",
      "Gradient Descent(3618/9999): loss=284.7381377216285, w0=0.02704727521704375, w1=0.2956311840395998\n",
      "Gradient Descent(3619/9999): loss=284.6788964815722, w0=0.027045033732489916, w1=0.29560183600382045\n",
      "Gradient Descent(3620/9999): loss=284.61966961443153, w0=0.027042792462620906, w1=0.2955724907729463\n",
      "Gradient Descent(3621/9999): loss=284.56045711644714, w0=0.027040551407436175, w1=0.29554314834664513\n",
      "Gradient Descent(3622/9999): loss=284.50125898386085, w0=0.027038310566935113, w1=0.29551380872458505\n",
      "Gradient Descent(3623/9999): loss=284.44207521291565, w0=0.02703606994111705, w1=0.2954844719064342\n",
      "Gradient Descent(3624/9999): loss=284.3829057998556, w0=0.027033829529981256, w1=0.295455137891861\n",
      "Gradient Descent(3625/9999): loss=284.3237507409261, w0=0.027031589333526927, w1=0.29542580668053414\n",
      "Gradient Descent(3626/9999): loss=284.2646100323734, w0=0.027029349351753208, w1=0.2953964782721224\n",
      "Gradient Descent(3627/9999): loss=284.2054836704452, w0=0.027027109584659176, w1=0.2953671526662949\n",
      "Gradient Descent(3628/9999): loss=284.14637165139004, w0=0.027024870032243853, w1=0.2953378298627208\n",
      "Gradient Descent(3629/9999): loss=284.0872739714578, w0=0.027022630694506194, w1=0.2953085098610696\n",
      "Gradient Descent(3630/9999): loss=284.0281906268994, w0=0.027020391571445097, w1=0.29527919266101105\n",
      "Gradient Descent(3631/9999): loss=283.9691216139669, w0=0.027018152663059396, w1=0.29524987826221494\n",
      "Gradient Descent(3632/9999): loss=283.91006692891347, w0=0.027015913969347865, w1=0.2952205666643513\n",
      "Gradient Descent(3633/9999): loss=283.85102656799353, w0=0.027013675490309218, w1=0.2951912578670905\n",
      "Gradient Descent(3634/9999): loss=283.7920005274625, w0=0.027011437225942105, w1=0.2951619518701029\n",
      "Gradient Descent(3635/9999): loss=283.7329888035771, w0=0.027009199176245117, w1=0.2951326486730593\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(3636/9999): loss=283.673991392595, w0=0.02700696134121679, w1=0.2951033482756305\n",
      "Gradient Descent(3637/9999): loss=283.61500829077494, w0=0.027004723720855595, w1=0.29507405067748765\n",
      "Gradient Descent(3638/9999): loss=283.55603949437716, w0=0.027002486315159946, w1=0.295044755878302\n",
      "Gradient Descent(3639/9999): loss=283.49708499966255, w0=0.02700024912412819, w1=0.29501546387774497\n",
      "Gradient Descent(3640/9999): loss=283.43814480289353, w0=0.026998012147758627, w1=0.2949861746754883\n",
      "Gradient Descent(3641/9999): loss=283.37921890033334, w0=0.026995775386049484, w1=0.294956888271204\n",
      "Gradient Descent(3642/9999): loss=283.3203072882465, w0=0.026993538838998937, w1=0.2949276046645639\n",
      "Gradient Descent(3643/9999): loss=283.2614099628988, w0=0.0269913025066051, w1=0.2948983238552405\n",
      "Gradient Descent(3644/9999): loss=283.20252692055686, w0=0.02698906638886603, w1=0.2948690458429062\n",
      "Gradient Descent(3645/9999): loss=283.14365815748846, w0=0.026986830485779722, w1=0.29483977062723365\n",
      "Gradient Descent(3646/9999): loss=283.08480366996275, w0=0.02698459479734412, w1=0.29481049820789573\n",
      "Gradient Descent(3647/9999): loss=283.0259634542498, w0=0.0269823593235571, w1=0.29478122858456557\n",
      "Gradient Descent(3648/9999): loss=282.9671375066209, w0=0.02698012406441648, w1=0.29475196175691637\n",
      "Gradient Descent(3649/9999): loss=282.90832582334815, w0=0.026977889019920032, w1=0.2947226977246216\n",
      "Gradient Descent(3650/9999): loss=282.84952840070525, w0=0.026975654190065455, w1=0.294693436487355\n",
      "Gradient Descent(3651/9999): loss=282.7907452349668, w0=0.026973419574850402, w1=0.2946641780447904\n",
      "Gradient Descent(3652/9999): loss=282.73197632240846, w0=0.02697118517427246, w1=0.2946349223966018\n",
      "Gradient Descent(3653/9999): loss=282.673221659307, w0=0.026968950988329164, w1=0.2946056695424634\n",
      "Gradient Descent(3654/9999): loss=282.61448124194044, w0=0.026966717017017985, w1=0.29457641948204977\n",
      "Gradient Descent(3655/9999): loss=282.5557550665877, w0=0.026964483260336344, w1=0.29454717221503546\n",
      "Gradient Descent(3656/9999): loss=282.49704312952906, w0=0.0269622497182816, w1=0.2945179277410953\n",
      "Gradient Descent(3657/9999): loss=282.4383454270458, w0=0.02696001639085106, w1=0.2944886860599043\n",
      "Gradient Descent(3658/9999): loss=282.37966195542043, w0=0.026957783278041967, w1=0.29445944717113764\n",
      "Gradient Descent(3659/9999): loss=282.3209927109361, w0=0.026955550379851517, w1=0.29443021107447076\n",
      "Gradient Descent(3660/9999): loss=282.2623376898777, w0=0.026953317696276843, w1=0.29440097776957924\n",
      "Gradient Descent(3661/9999): loss=282.2036968885309, w0=0.02695108522731502, w1=0.2943717472561389\n",
      "Gradient Descent(3662/9999): loss=282.1450703031825, w0=0.026948852972963074, w1=0.2943425195338257\n",
      "Gradient Descent(3663/9999): loss=282.0864579301204, w0=0.02694662093321797, w1=0.2943132946023157\n",
      "Gradient Descent(3664/9999): loss=282.02785976563376, w0=0.026944389108076613, w1=0.29428407246128535\n",
      "Gradient Descent(3665/9999): loss=281.96927580601266, w0=0.026942157497535865, w1=0.29425485311041116\n",
      "Gradient Descent(3666/9999): loss=281.9107060475484, w0=0.02693992610159252, w1=0.2942256365493699\n",
      "Gradient Descent(3667/9999): loss=281.85215048653333, w0=0.026937694920243324, w1=0.2941964227778384\n",
      "Gradient Descent(3668/9999): loss=281.7936091192609, w0=0.026935463953484964, w1=0.2941672117954938\n",
      "Gradient Descent(3669/9999): loss=281.73508194202566, w0=0.026933233201314074, w1=0.2941380036020134\n",
      "Gradient Descent(3670/9999): loss=281.6765689511234, w0=0.026931002663727232, w1=0.2941087981970747\n",
      "Gradient Descent(3671/9999): loss=281.61807014285074, w0=0.026928772340720962, w1=0.2940795955803553\n",
      "Gradient Descent(3672/9999): loss=281.5595855135057, w0=0.026926542232291732, w1=0.2940503957515331\n",
      "Gradient Descent(3673/9999): loss=281.50111505938725, w0=0.02692431233843596, w1=0.29402119871028604\n",
      "Gradient Descent(3674/9999): loss=281.44265877679544, w0=0.026922082659150002, w1=0.2939920044562924\n",
      "Gradient Descent(3675/9999): loss=281.38421666203135, w0=0.026919853194430167, w1=0.29396281298923066\n",
      "Gradient Descent(3676/9999): loss=281.3257887113973, w0=0.026917623944272707, w1=0.2939336243087793\n",
      "Gradient Descent(3677/9999): loss=281.2673749211968, w0=0.02691539490867382, w1=0.2939044384146171\n",
      "Gradient Descent(3678/9999): loss=281.20897528773423, w0=0.02691316608762965, w1=0.29387525530642306\n",
      "Gradient Descent(3679/9999): loss=281.15058980731516, w0=0.026910937481136286, w1=0.29384607498387627\n",
      "Gradient Descent(3680/9999): loss=281.09221847624633, w0=0.02690870908918977, w1=0.29381689744665607\n",
      "Gradient Descent(3681/9999): loss=281.03386129083543, w0=0.026906480911786084, w1=0.293787722694442\n",
      "Gradient Descent(3682/9999): loss=280.9755182473912, w0=0.026904252948921157, w1=0.29375855072691365\n",
      "Gradient Descent(3683/9999): loss=280.9171893422239, w0=0.02690202520059087, w1=0.29372938154375094\n",
      "Gradient Descent(3684/9999): loss=280.85887457164415, w0=0.026899797666791046, w1=0.2937002151446339\n",
      "Gradient Descent(3685/9999): loss=280.80057393196444, w0=0.02689757034751746, w1=0.2936710515292428\n",
      "Gradient Descent(3686/9999): loss=280.7422874194978, w0=0.026895343242765827, w1=0.293641890697258\n",
      "Gradient Descent(3687/9999): loss=280.68401503055867, w0=0.02689311635253182, w1=0.2936127326483601\n",
      "Gradient Descent(3688/9999): loss=280.62575676146247, w0=0.026890889676811053, w1=0.2935835773822298\n",
      "Gradient Descent(3689/9999): loss=280.5675126085255, w0=0.026888663215599087, w1=0.2935544248985481\n",
      "Gradient Descent(3690/9999): loss=280.5092825680655, w0=0.026886436968891436, w1=0.2935252751969961\n",
      "Gradient Descent(3691/9999): loss=280.45106663640115, w0=0.026884210936683556, w1=0.29349612827725513\n",
      "Gradient Descent(3692/9999): loss=280.39286480985203, w0=0.026881985118970855, w1=0.2934669841390066\n",
      "Gradient Descent(3693/9999): loss=280.33467708473916, w0=0.02687975951574869, w1=0.29343784278193225\n",
      "Gradient Descent(3694/9999): loss=280.27650345738436, w0=0.026877534127012367, w1=0.29340870420571386\n",
      "Gradient Descent(3695/9999): loss=280.21834392411074, w0=0.026875308952757138, w1=0.2933795684100334\n",
      "Gradient Descent(3696/9999): loss=280.16019848124245, w0=0.026873083992978206, w1=0.29335043539457306\n",
      "Gradient Descent(3697/9999): loss=280.1020671251045, w0=0.02687085924767072, w1=0.2933213051590152\n",
      "Gradient Descent(3698/9999): loss=280.04394985202316, w0=0.026868634716829782, w1=0.29329217770304244\n",
      "Gradient Descent(3699/9999): loss=279.98584665832584, w0=0.026866410400450444, w1=0.2932630530263374\n",
      "Gradient Descent(3700/9999): loss=279.927757540341, w0=0.0268641862985277, w1=0.2932339311285829\n",
      "Gradient Descent(3701/9999): loss=279.86968249439815, w0=0.0268619624110565, w1=0.29320481200946213\n",
      "Gradient Descent(3702/9999): loss=279.8116215168278, w0=0.026859738738031744, w1=0.29317569566865825\n",
      "Gradient Descent(3703/9999): loss=279.7535746039617, w0=0.02685751527944828, w1=0.29314658210585465\n",
      "Gradient Descent(3704/9999): loss=279.6955417521325, w0=0.0268552920353009, w1=0.2931174713207349\n",
      "Gradient Descent(3705/9999): loss=279.6375229576741, w0=0.02685306900558436, w1=0.29308836331298277\n",
      "Gradient Descent(3706/9999): loss=279.5795182169213, w0=0.026850846190293355, w1=0.2930592580822821\n",
      "Gradient Descent(3707/9999): loss=279.52152752621026, w0=0.026848623589422534, w1=0.2930301556283171\n",
      "Gradient Descent(3708/9999): loss=279.46355088187795, w0=0.026846401202966497, w1=0.2930010559507719\n",
      "Gradient Descent(3709/9999): loss=279.40558828026235, w0=0.026844179030919794, w1=0.29297195904933104\n",
      "Gradient Descent(3710/9999): loss=279.3476397177028, w0=0.026841957073276926, w1=0.29294286492367905\n",
      "Gradient Descent(3711/9999): loss=279.2897051905397, w0=0.02683973533003234, w1=0.29291377357350074\n",
      "Gradient Descent(3712/9999): loss=279.2317846951142, w0=0.026837513801180445, w1=0.292884684998481\n",
      "Gradient Descent(3713/9999): loss=279.1738782277687, w0=0.02683529248671559, w1=0.292855599198305\n",
      "Gradient Descent(3714/9999): loss=279.1159857848469, w0=0.026833071386632084, w1=0.292826516172658\n",
      "Gradient Descent(3715/9999): loss=279.0581073626932, w0=0.02683085050092418, w1=0.2927974359212254\n",
      "Gradient Descent(3716/9999): loss=279.0002429576533, w0=0.02682862982958609, w1=0.29276835844369287\n",
      "Gradient Descent(3717/9999): loss=278.9423925660738, w0=0.026826409372611975, w1=0.29273928373974617\n",
      "Gradient Descent(3718/9999): loss=278.88455618430265, w0=0.02682418912999594, w1=0.29271021180907125\n",
      "Gradient Descent(3719/9999): loss=278.8267338086886, w0=0.026821969101732054, w1=0.2926811426513542\n",
      "Gradient Descent(3720/9999): loss=278.76892543558154, w0=0.02681974928781433, w1=0.2926520762662813\n",
      "Gradient Descent(3721/9999): loss=278.71113106133254, w0=0.026817529688236738, w1=0.292623012653539\n",
      "Gradient Descent(3722/9999): loss=278.65335068229354, w0=0.0268153103029932, w1=0.29259395181281395\n",
      "Gradient Descent(3723/9999): loss=278.5955842948177, w0=0.026813091132077582, w1=0.2925648937437929\n",
      "Gradient Descent(3724/9999): loss=278.53783189525933, w0=0.026810872175483717, w1=0.2925358384461628\n",
      "Gradient Descent(3725/9999): loss=278.48009347997345, w0=0.026808653433205383, w1=0.29250678591961077\n",
      "Gradient Descent(3726/9999): loss=278.42236904531654, w0=0.02680643490523631, w1=0.29247773616382405\n",
      "Gradient Descent(3727/9999): loss=278.36465858764575, w0=0.02680421659157018, w1=0.2924486891784901\n",
      "Gradient Descent(3728/9999): loss=278.30696210331973, w0=0.026801998492200635, w1=0.2924196449632965\n",
      "Gradient Descent(3729/9999): loss=278.24927958869796, w0=0.02679978060712126, w1=0.292390603517931\n",
      "Gradient Descent(3730/9999): loss=278.19161104014086, w0=0.026797562936325608, w1=0.2923615648420816\n",
      "Gradient Descent(3731/9999): loss=278.13395645401016, w0=0.026795345479807168, w1=0.2923325289354363\n",
      "Gradient Descent(3732/9999): loss=278.0763158266686, w0=0.026793128237559395, w1=0.2923034957976834\n",
      "Gradient Descent(3733/9999): loss=278.01868915447983, w0=0.026790911209575694, w1=0.29227446542851127\n",
      "Gradient Descent(3734/9999): loss=277.9610764338086, w0=0.026788694395849424, w1=0.29224543782760853\n",
      "Gradient Descent(3735/9999): loss=277.9034776610209, w0=0.0267864777963739, w1=0.2922164129946639\n",
      "Gradient Descent(3736/9999): loss=277.8458928324835, w0=0.026784261411142388, w1=0.29218739092936624\n",
      "Gradient Descent(3737/9999): loss=277.7883219445645, w0=0.02678204524014811, w1=0.2921583716314046\n",
      "Gradient Descent(3738/9999): loss=277.73076499363276, w0=0.026779829283384245, w1=0.2921293551004683\n",
      "Gradient Descent(3739/9999): loss=277.6732219760586, w0=0.026777613540843917, w1=0.29210034133624657\n",
      "Gradient Descent(3740/9999): loss=277.61569288821306, w0=0.026775398012520216, w1=0.292071330338429\n",
      "Gradient Descent(3741/9999): loss=277.5581777264683, w0=0.026773182698406183, w1=0.29204232210670533\n",
      "Gradient Descent(3742/9999): loss=277.50067648719755, w0=0.02677096759849481, w1=0.29201331664076535\n",
      "Gradient Descent(3743/9999): loss=277.44318916677526, w0=0.026768752712779047, w1=0.29198431394029906\n",
      "Gradient Descent(3744/9999): loss=277.38571576157653, w0=0.026766538041251803, w1=0.29195531400499664\n",
      "Gradient Descent(3745/9999): loss=277.328256267978, w0=0.02676432358390594, w1=0.2919263168345484\n",
      "Gradient Descent(3746/9999): loss=277.27081068235714, w0=0.02676210934073427, w1=0.29189732242864486\n",
      "Gradient Descent(3747/9999): loss=277.2133790010923, w0=0.026759895311729568, w1=0.2918683307869766\n",
      "Gradient Descent(3748/9999): loss=277.1559612205631, w0=0.026757681496884556, w1=0.2918393419092345\n",
      "Gradient Descent(3749/9999): loss=277.09855733715017, w0=0.026755467896191924, w1=0.2918103557951094\n",
      "Gradient Descent(3750/9999): loss=277.0411673472353, w0=0.026753254509644307, w1=0.2917813724442925\n",
      "Gradient Descent(3751/9999): loss=276.98379124720094, w0=0.0267510413372343, w1=0.29175239185647495\n",
      "Gradient Descent(3752/9999): loss=276.9264290334311, w0=0.026748828378954457, w1=0.2917234140313482\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(3753/9999): loss=276.86908070231044, w0=0.026746615634797282, w1=0.2916944389686039\n",
      "Gradient Descent(3754/9999): loss=276.81174625022487, w0=0.026744403104755243, w1=0.2916654666679337\n",
      "Gradient Descent(3755/9999): loss=276.75442567356134, w0=0.026742190788820758, w1=0.29163649712902945\n",
      "Gradient Descent(3756/9999): loss=276.6971189687077, w0=0.026739978686986205, w1=0.29160753035158327\n",
      "Gradient Descent(3757/9999): loss=276.63982613205303, w0=0.026737766799243917, w1=0.2915785663352873\n",
      "Gradient Descent(3758/9999): loss=276.5825471599873, w0=0.026735555125586185, w1=0.2915496050798338\n",
      "Gradient Descent(3759/9999): loss=276.52528204890166, w0=0.026733343666005258, w1=0.29152064658491533\n",
      "Gradient Descent(3760/9999): loss=276.46803079518816, w0=0.02673113242049334, w1=0.2914916908502245\n",
      "Gradient Descent(3761/9999): loss=276.41079339524003, w0=0.026728921389042593, w1=0.2914627378754542\n",
      "Gradient Descent(3762/9999): loss=276.35356984545143, w0=0.026726710571645135, w1=0.2914337876602972\n",
      "Gradient Descent(3763/9999): loss=276.2963601422177, w0=0.026724499968293045, w1=0.29140484020444674\n",
      "Gradient Descent(3764/9999): loss=276.2391642819351, w0=0.02672228957897836, w1=0.291375895507596\n",
      "Gradient Descent(3765/9999): loss=276.1819822610009, w0=0.026720079403693073, w1=0.2913469535694384\n",
      "Gradient Descent(3766/9999): loss=276.12481407581356, w0=0.026717869442429126, w1=0.2913180143896675\n",
      "Gradient Descent(3767/9999): loss=276.0676597227724, w0=0.026715659695178435, w1=0.2912890779679769\n",
      "Gradient Descent(3768/9999): loss=276.010519198278, w0=0.026713450161932866, w1=0.2912601443040605\n",
      "Gradient Descent(3769/9999): loss=275.95339249873183, w0=0.02671124084268424, w1=0.2912312133976123\n",
      "Gradient Descent(3770/9999): loss=275.89627962053635, w0=0.02670903173742434, w1=0.2912022852483265\n",
      "Gradient Descent(3771/9999): loss=275.83918056009514, w0=0.02670682284614491, w1=0.29117335985589726\n",
      "Gradient Descent(3772/9999): loss=275.78209531381293, w0=0.026704614168837647, w1=0.2911444372200191\n",
      "Gradient Descent(3773/9999): loss=275.72502387809516, w0=0.02670240570549421, w1=0.2911155173403866\n",
      "Gradient Descent(3774/9999): loss=275.6679662493487, w0=0.026700197456106217, w1=0.29108660021669447\n",
      "Gradient Descent(3775/9999): loss=275.61092242398115, w0=0.02669798942066524, w1=0.2910576858486376\n",
      "Gradient Descent(3776/9999): loss=275.55389239840133, w0=0.02669578159916282, w1=0.29102877423591106\n",
      "Gradient Descent(3777/9999): loss=275.49687616901895, w0=0.026693573991590443, w1=0.29099986537821\n",
      "Gradient Descent(3778/9999): loss=275.4398737322449, w0=0.02669136659793957, w1=0.29097095927522965\n",
      "Gradient Descent(3779/9999): loss=275.38288508449097, w0=0.026689159418201613, w1=0.29094205592666555\n",
      "Gradient Descent(3780/9999): loss=275.32591022217, w0=0.02668695245236794, w1=0.29091315533221335\n",
      "Gradient Descent(3781/9999): loss=275.268949141696, w0=0.026684745700429887, w1=0.29088425749156877\n",
      "Gradient Descent(3782/9999): loss=275.212001839484, w0=0.02668253916237874, w1=0.29085536240442766\n",
      "Gradient Descent(3783/9999): loss=275.1550683119497, w0=0.026680332838205752, w1=0.2908264700704861\n",
      "Gradient Descent(3784/9999): loss=275.0981485555104, w0=0.026678126727902134, w1=0.2907975804894403\n",
      "Gradient Descent(3785/9999): loss=275.04124256658406, w0=0.02667592083145906, w1=0.2907686936609866\n",
      "Gradient Descent(3786/9999): loss=274.98435034158956, w0=0.02667371514886766, w1=0.2907398095848215\n",
      "Gradient Descent(3787/9999): loss=274.9274718769472, w0=0.02667150968011902, w1=0.2907109282606415\n",
      "Gradient Descent(3788/9999): loss=274.870607169078, w0=0.026669304425204196, w1=0.29068204968814343\n",
      "Gradient Descent(3789/9999): loss=274.81375621440424, w0=0.0266670993841142, w1=0.2906531738670242\n",
      "Gradient Descent(3790/9999): loss=274.7569190093488, w0=0.02666489455684, w1=0.29062430079698093\n",
      "Gradient Descent(3791/9999): loss=274.70009555033624, w0=0.026662689943372533, w1=0.2905954304777107\n",
      "Gradient Descent(3792/9999): loss=274.6432858337916, w0=0.02666048554370269, w1=0.29056656290891086\n",
      "Gradient Descent(3793/9999): loss=274.58648985614116, w0=0.026658281357821334, w1=0.2905376980902789\n",
      "Gradient Descent(3794/9999): loss=274.5297076138122, w0=0.026656077385719273, w1=0.29050883602151245\n",
      "Gradient Descent(3795/9999): loss=274.4729391032331, w0=0.026653873627387286, w1=0.29047997670230924\n",
      "Gradient Descent(3796/9999): loss=274.41618432083305, w0=0.026651670082816114, w1=0.2904511201323672\n",
      "Gradient Descent(3797/9999): loss=274.3594432630425, w0=0.02664946675199645, w1=0.2904222663113843\n",
      "Gradient Descent(3798/9999): loss=274.3027159262928, w0=0.026647263634918962, w1=0.2903934152390587\n",
      "Gradient Descent(3799/9999): loss=274.2460023070164, w0=0.026645060731574268, w1=0.2903645669150888\n",
      "Gradient Descent(3800/9999): loss=274.1893024016466, w0=0.026642858041952953, w1=0.29033572133917296\n",
      "Gradient Descent(3801/9999): loss=274.13261620661797, w0=0.026640655566045564, w1=0.29030687851100984\n",
      "Gradient Descent(3802/9999): loss=274.0759437183658, w0=0.026638453303842607, w1=0.29027803843029815\n",
      "Gradient Descent(3803/9999): loss=274.0192849333268, w0=0.026636251255334555, w1=0.2902492010967368\n",
      "Gradient Descent(3804/9999): loss=273.96263984793825, w0=0.026634049420511837, w1=0.2902203665100247\n",
      "Gradient Descent(3805/9999): loss=273.90600845863895, w0=0.026631847799364845, w1=0.2901915346698611\n",
      "Gradient Descent(3806/9999): loss=273.8493907618681, w0=0.02662964639188394, w1=0.2901627055759452\n",
      "Gradient Descent(3807/9999): loss=273.7927867540665, w0=0.026627445198059436, w1=0.29013387922797645\n",
      "Gradient Descent(3808/9999): loss=273.73619643167564, w0=0.02662524421788162, w1=0.29010505562565436\n",
      "Gradient Descent(3809/9999): loss=273.6796197911382, w0=0.026623043451340724, w1=0.29007623476867866\n",
      "Gradient Descent(3810/9999): loss=273.6230568288977, w0=0.026620842898426963, w1=0.2900474166567492\n",
      "Gradient Descent(3811/9999): loss=273.5665075413986, w0=0.026618642559130507, w1=0.2900186012895659\n",
      "Gradient Descent(3812/9999): loss=273.509971925087, w0=0.026616442433441485, w1=0.2899897886668289\n",
      "Gradient Descent(3813/9999): loss=273.4534499764091, w0=0.02661424252134999, w1=0.28996097878823834\n",
      "Gradient Descent(3814/9999): loss=273.39694169181274, w0=0.02661204282284609, w1=0.28993217165349466\n",
      "Gradient Descent(3815/9999): loss=273.34044706774665, w0=0.026609843337919794, w1=0.2899033672622984\n",
      "Gradient Descent(3816/9999): loss=273.28396610066045, w0=0.026607644066561093, w1=0.2898745656143501\n",
      "Gradient Descent(3817/9999): loss=273.22749878700506, w0=0.026605445008759936, w1=0.2898457667093506\n",
      "Gradient Descent(3818/9999): loss=273.17104512323186, w0=0.026603246164506232, w1=0.2898169705470008\n",
      "Gradient Descent(3819/9999): loss=273.1146051057939, w0=0.02660104753378986, w1=0.28978817712700167\n",
      "Gradient Descent(3820/9999): loss=273.0581787311447, w0=0.02659884911660065, w1=0.28975938644905447\n",
      "Gradient Descent(3821/9999): loss=273.0017659957392, w0=0.026596650912928413, w1=0.2897305985128604\n",
      "Gradient Descent(3822/9999): loss=272.94536689603314, w0=0.026594452922762912, w1=0.28970181331812106\n",
      "Gradient Descent(3823/9999): loss=272.88898142848313, w0=0.026592255146093882, w1=0.2896730308645379\n",
      "Gradient Descent(3824/9999): loss=272.8326095895472, w0=0.026590057582911015, w1=0.2896442511518126\n",
      "Gradient Descent(3825/9999): loss=272.77625137568396, w0=0.02658786023320397, w1=0.289615474179647\n",
      "Gradient Descent(3826/9999): loss=272.7199067833534, w0=0.02658566309696237, w1=0.2895866999477431\n",
      "Gradient Descent(3827/9999): loss=272.6635758090162, w0=0.026583466174175806, w1=0.289557928455803\n",
      "Gradient Descent(3828/9999): loss=272.6072584491342, w0=0.02658126946483383, w1=0.28952915970352894\n",
      "Gradient Descent(3829/9999): loss=272.5509547001703, w0=0.026579072968925952, w1=0.28950039369062325\n",
      "Gradient Descent(3830/9999): loss=272.49466455858817, w0=0.026576876686441663, w1=0.2894716304167884\n",
      "Gradient Descent(3831/9999): loss=272.43838802085287, w0=0.026574680617370405, w1=0.289442869881727\n",
      "Gradient Descent(3832/9999): loss=272.3821250834302, w0=0.02657248476170159, w1=0.28941411208514184\n",
      "Gradient Descent(3833/9999): loss=272.32587574278693, w0=0.026570289119424596, w1=0.28938535702673573\n",
      "Gradient Descent(3834/9999): loss=272.26963999539106, w0=0.02656809369052876, w1=0.2893566047062117\n",
      "Gradient Descent(3835/9999): loss=272.21341783771135, w0=0.026565898475003392, w1=0.28932785512327297\n",
      "Gradient Descent(3836/9999): loss=272.1572092662176, w0=0.026563703472837766, w1=0.2892991082776227\n",
      "Gradient Descent(3837/9999): loss=272.1010142773808, w0=0.02656150868402112, w1=0.2892703641689643\n",
      "Gradient Descent(3838/9999): loss=272.0448328676729, w0=0.026559314108542655, w1=0.2892416227970013\n",
      "Gradient Descent(3839/9999): loss=271.98866503356663, w0=0.02655711974639154, w1=0.2892128841614373\n",
      "Gradient Descent(3840/9999): loss=271.93251077153593, w0=0.02655492559755691, w1=0.28918414826197614\n",
      "Gradient Descent(3841/9999): loss=271.8763700780557, w0=0.026552731662027866, w1=0.28915541509832166\n",
      "Gradient Descent(3842/9999): loss=271.8202429496017, w0=0.02655053793979347, w1=0.28912668467017794\n",
      "Gradient Descent(3843/9999): loss=271.7641293826511, w0=0.02654834443084276, w1=0.2890979569772491\n",
      "Gradient Descent(3844/9999): loss=271.7080293736815, w0=0.026546151135164727, w1=0.28906923201923945\n",
      "Gradient Descent(3845/9999): loss=271.6519429191719, w0=0.026543958052748343, w1=0.2890405097958534\n",
      "Gradient Descent(3846/9999): loss=271.5958700156022, w0=0.026541765183582536, w1=0.2890117903067954\n",
      "Gradient Descent(3847/9999): loss=271.53981065945317, w0=0.026539572527656204, w1=0.28898307355177016\n",
      "Gradient Descent(3848/9999): loss=271.4837648472069, w0=0.02653738008495821, w1=0.2889543595304825\n",
      "Gradient Descent(3849/9999): loss=271.42773257534617, w0=0.026535187855477385, w1=0.2889256482426372\n",
      "Gradient Descent(3850/9999): loss=271.3717138403548, w0=0.026532995839202524, w1=0.28889693968793945\n",
      "Gradient Descent(3851/9999): loss=271.31570863871764, w0=0.02653080403612239, w1=0.28886823386609434\n",
      "Gradient Descent(3852/9999): loss=271.25971696692073, w0=0.02652861244622572, w1=0.28883953077680713\n",
      "Gradient Descent(3853/9999): loss=271.2037388214508, w0=0.026526421069501205, w1=0.2888108304197832\n",
      "Gradient Descent(3854/9999): loss=271.1477741987958, w0=0.026524229905937512, w1=0.2887821327947282\n",
      "Gradient Descent(3855/9999): loss=271.0918230954444, w0=0.026522038955523273, w1=0.28875343790134766\n",
      "Gradient Descent(3856/9999): loss=271.0358855078867, w0=0.026519848218247082, w1=0.2887247457393474\n",
      "Gradient Descent(3857/9999): loss=270.9799614326133, w0=0.02651765769409751, w1=0.2886960563084333\n",
      "Gradient Descent(3858/9999): loss=270.92405086611626, w0=0.02651546738306309, w1=0.2886673696083114\n",
      "Gradient Descent(3859/9999): loss=270.8681538048884, w0=0.026513277285132322, w1=0.28863868563868783\n",
      "Gradient Descent(3860/9999): loss=270.81227024542335, w0=0.026511087400293676, w1=0.2886100043992688\n",
      "Gradient Descent(3861/9999): loss=270.7564001842162, w0=0.026508897728535586, w1=0.2885813258897608\n",
      "Gradient Descent(3862/9999): loss=270.7005436177625, w0=0.026506708269846458, w1=0.2885526501098703\n",
      "Gradient Descent(3863/9999): loss=270.64470054255924, w0=0.026504519024214666, w1=0.2885239770593039\n",
      "Gradient Descent(3864/9999): loss=270.5888709551041, w0=0.026502329991628545, w1=0.28849530673776835\n",
      "Gradient Descent(3865/9999): loss=270.5330548518959, w0=0.026500141172076407, w1=0.2884666391449705\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(3866/9999): loss=270.4772522294344, w0=0.026497952565546525, w1=0.2884379742806174\n",
      "Gradient Descent(3867/9999): loss=270.42146308422036, w0=0.026495764172027144, w1=0.2884093121444161\n",
      "Gradient Descent(3868/9999): loss=270.36568741275556, w0=0.02649357599150648, w1=0.2883806527360739\n",
      "Gradient Descent(3869/9999): loss=270.30992521154286, w0=0.02649138802397271, w1=0.2883519960552981\n",
      "Gradient Descent(3870/9999): loss=270.2541764770857, w0=0.02648920026941399, w1=0.28832334210179617\n",
      "Gradient Descent(3871/9999): loss=270.198441205889, w0=0.026487012727818428, w1=0.2882946908752757\n",
      "Gradient Descent(3872/9999): loss=270.14271939445854, w0=0.02648482539917412, w1=0.2882660423754444\n",
      "Gradient Descent(3873/9999): loss=270.08701103930076, w0=0.02648263828346912, w1=0.2882373966020101\n",
      "Gradient Descent(3874/9999): loss=270.0313161369237, w0=0.02648045138069145, w1=0.28820875355468073\n",
      "Gradient Descent(3875/9999): loss=269.9756346838356, w0=0.0264782646908291, w1=0.2881801132331644\n",
      "Gradient Descent(3876/9999): loss=269.91996667654644, w0=0.02647607821387004, w1=0.28815147563716925\n",
      "Gradient Descent(3877/9999): loss=269.8643121115667, w0=0.026473891949802198, w1=0.2881228407664036\n",
      "Gradient Descent(3878/9999): loss=269.80867098540807, w0=0.026471705898613473, w1=0.28809420862057583\n",
      "Gradient Descent(3879/9999): loss=269.75304329458305, w0=0.02646952006029174, w1=0.28806557919939446\n",
      "Gradient Descent(3880/9999): loss=269.6974290356053, w0=0.026467334434824834, w1=0.2880369525025682\n",
      "Gradient Descent(3881/9999): loss=269.6418282049895, w0=0.026465149022200568, w1=0.2880083285298058\n",
      "Gradient Descent(3882/9999): loss=269.5862407992511, w0=0.026462963822406715, w1=0.2879797072808161\n",
      "Gradient Descent(3883/9999): loss=269.5306668149066, w0=0.026460778835431027, w1=0.28795108875530817\n",
      "Gradient Descent(3884/9999): loss=269.47510624847365, w0=0.02645859406126122, w1=0.28792247295299106\n",
      "Gradient Descent(3885/9999): loss=269.41955909647066, w0=0.026456409499884984, w1=0.287893859873574\n",
      "Gradient Descent(3886/9999): loss=269.3640253554171, w0=0.02645422515128998, w1=0.2878652495167664\n",
      "Gradient Descent(3887/9999): loss=269.30850502183347, w0=0.026452041015463824, w1=0.28783664188227764\n",
      "Gradient Descent(3888/9999): loss=269.2529980922412, w0=0.026449857092394124, w1=0.2878080369698173\n",
      "Gradient Descent(3889/9999): loss=269.19750456316285, w0=0.026447673382068444, w1=0.2877794347790951\n",
      "Gradient Descent(3890/9999): loss=269.1420244311216, w0=0.026445489884474323, w1=0.2877508353098209\n",
      "Gradient Descent(3891/9999): loss=269.08655769264203, w0=0.02644330659959927, w1=0.2877222385617045\n",
      "Gradient Descent(3892/9999): loss=269.03110434424934, w0=0.02644112352743076, w1=0.287693644534456\n",
      "Gradient Descent(3893/9999): loss=268.97566438247003, w0=0.026438940667956243, w1=0.2876650532277855\n",
      "Gradient Descent(3894/9999): loss=268.92023780383136, w0=0.026436758021163142, w1=0.28763646464140336\n",
      "Gradient Descent(3895/9999): loss=268.8648246048616, w0=0.026434575587038846, w1=0.28760787877501987\n",
      "Gradient Descent(3896/9999): loss=268.80942478209, w0=0.026432393365570718, w1=0.2875792956283455\n",
      "Gradient Descent(3897/9999): loss=268.7540383320469, w0=0.026430211356746086, w1=0.2875507152010909\n",
      "Gradient Descent(3898/9999): loss=268.69866525126355, w0=0.026428029560552256, w1=0.28752213749296673\n",
      "Gradient Descent(3899/9999): loss=268.6433055362721, w0=0.026425847976976502, w1=0.2874935625036839\n",
      "Gradient Descent(3900/9999): loss=268.5879591836057, w0=0.02642366660600607, w1=0.2874649902329533\n",
      "Gradient Descent(3901/9999): loss=268.5326261897985, w0=0.026421485447628176, w1=0.2874364206804859\n",
      "Gradient Descent(3902/9999): loss=268.4773065513857, w0=0.026419304501830006, w1=0.287407853845993\n",
      "Gradient Descent(3903/9999): loss=268.42200026490343, w0=0.02641712376859872, w1=0.28737928972918575\n",
      "Gradient Descent(3904/9999): loss=268.3667073268887, w0=0.026414943247921447, w1=0.2873507283297756\n",
      "Gradient Descent(3905/9999): loss=268.31142773387944, w0=0.026412762939785287, w1=0.287322169647474\n",
      "Gradient Descent(3906/9999): loss=268.256161482415, w0=0.026410582844177317, w1=0.2872936136819925\n",
      "Gradient Descent(3907/9999): loss=268.20090856903505, w0=0.02640840296108458, w1=0.28726506043304295\n",
      "Gradient Descent(3908/9999): loss=268.14566899028085, w0=0.026406223290494094, w1=0.28723650990033706\n",
      "Gradient Descent(3909/9999): loss=268.09044274269417, w0=0.026404043832392844, w1=0.2872079620835868\n",
      "Gradient Descent(3910/9999): loss=268.03522982281794, w0=0.026401864586767795, w1=0.2871794169825042\n",
      "Gradient Descent(3911/9999): loss=267.98003022719604, w0=0.026399685553605876, w1=0.28715087459680144\n",
      "Gradient Descent(3912/9999): loss=267.9248439523734, w0=0.026397506732893993, w1=0.28712233492619077\n",
      "Gradient Descent(3913/9999): loss=267.8696709948957, w0=0.026395328124619023, w1=0.28709379797038453\n",
      "Gradient Descent(3914/9999): loss=267.81451135131, w0=0.026393149728767815, w1=0.28706526372909524\n",
      "Gradient Descent(3915/9999): loss=267.75936501816375, w0=0.02639097154532719, w1=0.28703673220203546\n",
      "Gradient Descent(3916/9999): loss=267.70423199200593, w0=0.026388793574283937, w1=0.28700820338891786\n",
      "Gradient Descent(3917/9999): loss=267.64911226938614, w0=0.026386615815624827, w1=0.2869796772894553\n",
      "Gradient Descent(3918/9999): loss=267.59400584685505, w0=0.026384438269336598, w1=0.2869511539033606\n",
      "Gradient Descent(3919/9999): loss=267.53891272096433, w0=0.02638226093540596, w1=0.2869226332303469\n",
      "Gradient Descent(3920/9999): loss=267.48383288826665, w0=0.026380083813819597, w1=0.28689411527012726\n",
      "Gradient Descent(3921/9999): loss=267.42876634531535, w0=0.026377906904564163, w1=0.2868656000224149\n",
      "Gradient Descent(3922/9999): loss=267.3737130886652, w0=0.02637573020762629, w1=0.28683708748692316\n",
      "Gradient Descent(3923/9999): loss=267.3186731148716, w0=0.026373553722992583, w1=0.2868085776633655\n",
      "Gradient Descent(3924/9999): loss=267.2636464204911, w0=0.026371377450649613, w1=0.2867800705514555\n",
      "Gradient Descent(3925/9999): loss=267.20863300208117, w0=0.02636920139058393, w1=0.28675156615090674\n",
      "Gradient Descent(3926/9999): loss=267.1536328562, w0=0.026367025542782058, w1=0.2867230644614331\n",
      "Gradient Descent(3927/9999): loss=267.09864597940725, w0=0.026364849907230488, w1=0.28669456548274835\n",
      "Gradient Descent(3928/9999): loss=267.0436723682629, w0=0.026362674483915688, w1=0.2866660692145665\n",
      "Gradient Descent(3929/9999): loss=266.9887120193286, w0=0.026360499272824103, w1=0.28663757565660164\n",
      "Gradient Descent(3930/9999): loss=266.93376492916633, w0=0.026358324273942142, w1=0.28660908480856795\n",
      "Gradient Descent(3931/9999): loss=266.87883109433955, w0=0.0263561494872562, w1=0.2865805966701797\n",
      "Gradient Descent(3932/9999): loss=266.82391051141235, w0=0.026353974912752636, w1=0.2865521112411513\n",
      "Gradient Descent(3933/9999): loss=266.7690031769498, w0=0.026351800550417786, w1=0.2865236285211973\n",
      "Gradient Descent(3934/9999): loss=266.71410908751807, w0=0.02634962640023796, w1=0.2864951485100322\n",
      "Gradient Descent(3935/9999): loss=266.6592282396843, w0=0.02634745246219944, w1=0.2864666712073708\n",
      "Gradient Descent(3936/9999): loss=266.6043606300164, w0=0.026345278736288485, w1=0.28643819661292785\n",
      "Gradient Descent(3937/9999): loss=266.5495062550834, w0=0.026343105222491326, w1=0.2864097247264183\n",
      "Gradient Descent(3938/9999): loss=266.49466511145533, w0=0.026340931920794168, w1=0.28638125554755717\n",
      "Gradient Descent(3939/9999): loss=266.439837195703, w0=0.026338758831183192, w1=0.28635278907605954\n",
      "Gradient Descent(3940/9999): loss=266.38502250439836, w0=0.02633658595364455, w1=0.2863243253116407\n",
      "Gradient Descent(3941/9999): loss=266.3302210341143, w0=0.026334413288164372, w1=0.28629586425401593\n",
      "Gradient Descent(3942/9999): loss=266.2754327814244, w0=0.026332240834728758, w1=0.2862674059029007\n",
      "Gradient Descent(3943/9999): loss=266.22065774290354, w0=0.026330068593323788, w1=0.2862389502580105\n",
      "Gradient Descent(3944/9999): loss=266.16589591512746, w0=0.02632789656393551, w1=0.286210497319061\n",
      "Gradient Descent(3945/9999): loss=266.1111472946728, w0=0.026325724746549952, w1=0.28618204708576794\n",
      "Gradient Descent(3946/9999): loss=266.05641187811716, w0=0.026323553141153117, w1=0.28615359955784714\n",
      "Gradient Descent(3947/9999): loss=266.00168966203915, w0=0.026321381747730976, w1=0.28612515473501454\n",
      "Gradient Descent(3948/9999): loss=265.94698064301826, w0=0.026319210566269482, w1=0.28609671261698616\n",
      "Gradient Descent(3949/9999): loss=265.892284817635, w0=0.02631703959675456, w1=0.28606827320347816\n",
      "Gradient Descent(3950/9999): loss=265.8376021824708, w0=0.026314868839172112, w1=0.2860398364942068\n",
      "Gradient Descent(3951/9999): loss=265.7829327341081, w0=0.02631269829350801, w1=0.2860114024888884\n",
      "Gradient Descent(3952/9999): loss=265.7282764691302, w0=0.0263105279597481, w1=0.2859829711872394\n",
      "Gradient Descent(3953/9999): loss=265.6736333841216, w0=0.026308357837878216, w1=0.2859545425889764\n",
      "Gradient Descent(3954/9999): loss=265.61900347566734, w0=0.026306187927884157, w1=0.28592611669381596\n",
      "Gradient Descent(3955/9999): loss=265.5643867403538, w0=0.026304018229751695, w1=0.28589769350147487\n",
      "Gradient Descent(3956/9999): loss=265.5097831747679, w0=0.026301848743466583, w1=0.28586927301167\n",
      "Gradient Descent(3957/9999): loss=265.4551927754981, w0=0.02629967946901455, w1=0.28584085522411823\n",
      "Gradient Descent(3958/9999): loss=265.4006155391333, w0=0.026297510406381293, w1=0.2858124401385366\n",
      "Gradient Descent(3959/9999): loss=265.3460514622636, w0=0.026295341555552496, w1=0.28578402775464234\n",
      "Gradient Descent(3960/9999): loss=265.2915005414799, w0=0.02629317291651381, w1=0.2857556180721526\n",
      "Gradient Descent(3961/9999): loss=265.2369627733741, w0=0.026291004489250862, w1=0.2857272110907848\n",
      "Gradient Descent(3962/9999): loss=265.1824381545392, w0=0.026288836273749262, w1=0.28569880681025633\n",
      "Gradient Descent(3963/9999): loss=265.1279266815691, w0=0.02628666826999459, w1=0.2856704052302847\n",
      "Gradient Descent(3964/9999): loss=265.07342835105845, w0=0.0262845004779724, w1=0.28564200635058756\n",
      "Gradient Descent(3965/9999): loss=265.01894315960305, w0=0.026282332897668226, w1=0.28561361017088266\n",
      "Gradient Descent(3966/9999): loss=264.96447110379955, w0=0.026280165529067583, w1=0.2855852166908878\n",
      "Gradient Descent(3967/9999): loss=264.9100121802456, w0=0.02627799837215595, w1=0.28555682591032094\n",
      "Gradient Descent(3968/9999): loss=264.8555663855398, w0=0.026275831426918788, w1=0.2855284378289001\n",
      "Gradient Descent(3969/9999): loss=264.8011337162818, w0=0.02627366469334154, w1=0.2855000524463433\n",
      "Gradient Descent(3970/9999): loss=264.7467141690718, w0=0.026271498171409616, w1=0.2854716697623689\n",
      "Gradient Descent(3971/9999): loss=264.69230774051147, w0=0.02626933186110841, w1=0.2854432897766951\n",
      "Gradient Descent(3972/9999): loss=264.6379144272032, w0=0.026267165762423286, w1=0.2854149124890404\n",
      "Gradient Descent(3973/9999): loss=264.58353422575016, w0=0.02626499987533959, w1=0.2853865378991232\n",
      "Gradient Descent(3974/9999): loss=264.52916713275675, w0=0.026262834199842645, w1=0.28535816600666225\n",
      "Gradient Descent(3975/9999): loss=264.47481314482803, w0=0.026260668735917744, w1=0.2853297968113761\n",
      "Gradient Descent(3976/9999): loss=264.42047225857044, w0=0.02625850348355016, w1=0.2853014303129836\n",
      "Gradient Descent(3977/9999): loss=264.3661444705907, w0=0.02625633844272515, w1=0.28527306651120365\n",
      "Gradient Descent(3978/9999): loss=264.31182977749734, w0=0.026254173613427936, w1=0.28524470540575525\n",
      "Gradient Descent(3979/9999): loss=264.2575281758989, w0=0.026252008995643724, w1=0.28521634699635745\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(3980/9999): loss=264.2032396624058, w0=0.026249844589357696, w1=0.2851879912827294\n",
      "Gradient Descent(3981/9999): loss=264.1489642336286, w0=0.026247680394555013, w1=0.28515963826459045\n",
      "Gradient Descent(3982/9999): loss=264.0947018861791, w0=0.02624551641122081, w1=0.28513128794165987\n",
      "Gradient Descent(3983/9999): loss=264.0404526166703, w0=0.0262433526393402, w1=0.2851029403136571\n",
      "Gradient Descent(3984/9999): loss=263.9862164217157, w0=0.02624118907889827, w1=0.2850745953803018\n",
      "Gradient Descent(3985/9999): loss=263.93199329793015, w0=0.026239025729880096, w1=0.2850462531413135\n",
      "Gradient Descent(3986/9999): loss=263.8777832419292, w0=0.026236862592270718, w1=0.285017913596412\n",
      "Gradient Descent(3987/9999): loss=263.8235862503293, w0=0.026234699666055163, w1=0.2849895767453171\n",
      "Gradient Descent(3988/9999): loss=263.7694023197481, w0=0.026232536951218428, w1=0.2849612425877488\n",
      "Gradient Descent(3989/9999): loss=263.715231446804, w0=0.02623037444774549, w1=0.28493291112342706\n",
      "Gradient Descent(3990/9999): loss=263.6610736281163, w0=0.02622821215562131, w1=0.28490458235207194\n",
      "Gradient Descent(3991/9999): loss=263.60692886030535, w0=0.026226050074830823, w1=0.28487625627340374\n",
      "Gradient Descent(3992/9999): loss=263.5527971399923, w0=0.026223888205358935, w1=0.28484793288714266\n",
      "Gradient Descent(3993/9999): loss=263.4986784637996, w0=0.026221726547190537, w1=0.28481961219300916\n",
      "Gradient Descent(3994/9999): loss=263.44457282835026, w0=0.026219565100310496, w1=0.2847912941907237\n",
      "Gradient Descent(3995/9999): loss=263.3904802302682, w0=0.02621740386470366, w1=0.28476297888000685\n",
      "Gradient Descent(3996/9999): loss=263.33640066617863, w0=0.026215242840354855, w1=0.28473466626057925\n",
      "Gradient Descent(3997/9999): loss=263.28233413270743, w0=0.026213082027248875, w1=0.2847063563321617\n",
      "Gradient Descent(3998/9999): loss=263.2282806264815, w0=0.026210921425370508, w1=0.28467804909447497\n",
      "Gradient Descent(3999/9999): loss=263.1742401441286, w0=0.026208761034704507, w1=0.28464974454724007\n",
      "Gradient Descent(4000/9999): loss=263.12021268227755, w0=0.02620660085523561, w1=0.28462144269017803\n",
      "Gradient Descent(4001/9999): loss=263.06619823755807, w0=0.026204440886948538, w1=0.28459314352300996\n",
      "Gradient Descent(4002/9999): loss=263.01219680660085, w0=0.02620228112982798, w1=0.28456484704545704\n",
      "Gradient Descent(4003/9999): loss=262.9582083860373, w0=0.026200121583858607, w1=0.2845365532572406\n",
      "Gradient Descent(4004/9999): loss=262.9042329725001, w0=0.026197962249025073, w1=0.284508262158082\n",
      "Gradient Descent(4005/9999): loss=262.85027056262277, w0=0.026195803125312004, w1=0.28447997374770273\n",
      "Gradient Descent(4006/9999): loss=262.79632115303934, w0=0.026193644212704013, w1=0.2844516880258244\n",
      "Gradient Descent(4007/9999): loss=262.74238474038543, w0=0.026191485511185685, w1=0.28442340499216856\n",
      "Gradient Descent(4008/9999): loss=262.68846132129727, w0=0.026189327020741584, w1=0.28439512464645705\n",
      "Gradient Descent(4009/9999): loss=262.6345508924118, w0=0.026187168741356258, w1=0.2843668469884117\n",
      "Gradient Descent(4010/9999): loss=262.5806534503675, w0=0.02618501067301423, w1=0.2843385720177544\n",
      "Gradient Descent(4011/9999): loss=262.5267689918032, w0=0.026182852815700004, w1=0.28431029973420724\n",
      "Gradient Descent(4012/9999): loss=262.472897513359, w0=0.02618069516939806, w1=0.2842820301374923\n",
      "Gradient Descent(4013/9999): loss=262.41903901167564, w0=0.026178537734092858, w1=0.28425376322733176\n",
      "Gradient Descent(4014/9999): loss=262.36519348339516, w0=0.026176380509768844, w1=0.2842254990034479\n",
      "Gradient Descent(4015/9999): loss=262.3113609251603, w0=0.026174223496410437, w1=0.2841972374655631\n",
      "Gradient Descent(4016/9999): loss=262.2575413336147, w0=0.026172066694002034, w1=0.2841689786133998\n",
      "Gradient Descent(4017/9999): loss=262.2037347054031, w0=0.02616991010252801, w1=0.28414072244668054\n",
      "Gradient Descent(4018/9999): loss=262.14994103717095, w0=0.02616775372197273, w1=0.284112468965128\n",
      "Gradient Descent(4019/9999): loss=262.096160325565, w0=0.026165597552320532, w1=0.28408421816846485\n",
      "Gradient Descent(4020/9999): loss=262.0423925672326, w0=0.026163441593555728, w1=0.284055970056414\n",
      "Gradient Descent(4021/9999): loss=261.988637758822, w0=0.02616128584566262, w1=0.28402772462869824\n",
      "Gradient Descent(4022/9999): loss=261.93489589698265, w0=0.02615913030862548, w1=0.2839994818850406\n",
      "Gradient Descent(4023/9999): loss=261.88116697836466, w0=0.026156974982428565, w1=0.28397124182516414\n",
      "Gradient Descent(4024/9999): loss=261.8274509996192, w0=0.026154819867056117, w1=0.283943004448792\n",
      "Gradient Descent(4025/9999): loss=261.7737479573985, w0=0.02615266496249235, w1=0.2839147697556475\n",
      "Gradient Descent(4026/9999): loss=261.72005784835545, w0=0.026150510268721457, w1=0.2838865377454539\n",
      "Gradient Descent(4027/9999): loss=261.6663806691441, w0=0.026148355785727617, w1=0.28385830841793463\n",
      "Gradient Descent(4028/9999): loss=261.61271641641923, w0=0.026146201513494984, w1=0.2838300817728132\n",
      "Gradient Descent(4029/9999): loss=261.55906508683665, w0=0.0261440474520077, w1=0.2838018578098132\n",
      "Gradient Descent(4030/9999): loss=261.50542667705326, w0=0.026141893601249876, w1=0.28377363652865834\n",
      "Gradient Descent(4031/9999): loss=261.45180118372645, w0=0.026139739961205614, w1=0.2837454179290723\n",
      "Gradient Descent(4032/9999): loss=261.39818860351494, w0=0.026137586531858988, w1=0.283717202010779\n",
      "Gradient Descent(4033/9999): loss=261.34458893307817, w0=0.026135433313194057, w1=0.2836889887735023\n",
      "Gradient Descent(4034/9999): loss=261.29100216907676, w0=0.026133280305194857, w1=0.2836607782169663\n",
      "Gradient Descent(4035/9999): loss=261.23742830817196, w0=0.02613112750784541, w1=0.283632570340895\n",
      "Gradient Descent(4036/9999): loss=261.18386734702597, w0=0.026128974921129713, w1=0.2836043651450127\n",
      "Gradient Descent(4037/9999): loss=261.1303192823022, w0=0.02612682254503175, w1=0.28357616262904356\n",
      "Gradient Descent(4038/9999): loss=261.07678411066456, w0=0.026124670379535475, w1=0.28354796279271205\n",
      "Gradient Descent(4039/9999): loss=261.02326182877835, w0=0.026122518424624836, w1=0.2835197656357425\n",
      "Gradient Descent(4040/9999): loss=260.96975243330934, w0=0.026120366680283752, w1=0.2834915711578595\n",
      "Gradient Descent(4041/9999): loss=260.9162559209245, w0=0.026118215146496124, w1=0.2834633793587876\n",
      "Gradient Descent(4042/9999): loss=260.8627722882918, w0=0.026116063823245838, w1=0.2834351902382516\n",
      "Gradient Descent(4043/9999): loss=260.80930153207976, w0=0.02611391271051676, w1=0.28340700379597616\n",
      "Gradient Descent(4044/9999): loss=260.75584364895826, w0=0.026111761808292736, w1=0.2833788200316862\n",
      "Gradient Descent(4045/9999): loss=260.7023986355977, w0=0.02610961111655759, w1=0.2833506389451066\n",
      "Gradient Descent(4046/9999): loss=260.64896648866994, w0=0.02610746063529513, w1=0.28332246053596244\n",
      "Gradient Descent(4047/9999): loss=260.5955472048471, w0=0.02610531036448915, w1=0.28329428480397884\n",
      "Gradient Descent(4048/9999): loss=260.54214078080275, w0=0.026103160304123418, w1=0.28326611174888094\n",
      "Gradient Descent(4049/9999): loss=260.4887472132109, w0=0.026101010454181686, w1=0.28323794137039404\n",
      "Gradient Descent(4050/9999): loss=260.435366498747, w0=0.026098860814647684, w1=0.28320977366824346\n",
      "Gradient Descent(4051/9999): loss=260.38199863408715, w0=0.02609671138550513, w1=0.28318160864215464\n",
      "Gradient Descent(4052/9999): loss=260.3286436159084, w0=0.02609456216673772, w1=0.28315344629185313\n",
      "Gradient Descent(4053/9999): loss=260.27530144088854, w0=0.026092413158329133, w1=0.2831252866170645\n",
      "Gradient Descent(4054/9999): loss=260.22197210570675, w0=0.026090264360263023, w1=0.28309712961751443\n",
      "Gradient Descent(4055/9999): loss=260.1686556070426, w0=0.026088115772523037, w1=0.28306897529292874\n",
      "Gradient Descent(4056/9999): loss=260.1153519415769, w0=0.026085967395092797, w1=0.28304082364303323\n",
      "Gradient Descent(4057/9999): loss=260.0620611059913, w0=0.026083819227955902, w1=0.2830126746675538\n",
      "Gradient Descent(4058/9999): loss=260.00878309696833, w0=0.02608167127109594, w1=0.2829845283662165\n",
      "Gradient Descent(4059/9999): loss=259.95551791119146, w0=0.026079523524496485, w1=0.28295638473874746\n",
      "Gradient Descent(4060/9999): loss=259.90226554534513, w0=0.02607737598814108, w1=0.28292824378487275\n",
      "Gradient Descent(4061/9999): loss=259.8490259961146, w0=0.02607522866201326, w1=0.28290010550431866\n",
      "Gradient Descent(4062/9999): loss=259.795799260186, w0=0.026073081546096537, w1=0.2828719698968115\n",
      "Gradient Descent(4063/9999): loss=259.7425853342468, w0=0.026070934640374407, w1=0.28284383696207777\n",
      "Gradient Descent(4064/9999): loss=259.68938421498484, w0=0.02606878794483035, w1=0.28281570669984385\n",
      "Gradient Descent(4065/9999): loss=259.63619589908905, w0=0.026066641459447825, w1=0.2827875791098364\n",
      "Gradient Descent(4066/9999): loss=259.58302038324945, w0=0.026064495184210276, w1=0.28275945419178194\n",
      "Gradient Descent(4067/9999): loss=259.52985766415674, w0=0.026062349119101123, w1=0.28273133194540734\n",
      "Gradient Descent(4068/9999): loss=259.4767077385028, w0=0.02606020326410378, w1=0.2827032123704393\n",
      "Gradient Descent(4069/9999): loss=259.42357060298, w0=0.026058057619201633, w1=0.28267509546660485\n",
      "Gradient Descent(4070/9999): loss=259.3704462542822, w0=0.026055912184378055, w1=0.28264698123363086\n",
      "Gradient Descent(4071/9999): loss=259.3173346891035, w0=0.0260537669596164, w1=0.2826188696712444\n",
      "Gradient Descent(4072/9999): loss=259.26423590413964, w0=0.026051621944900006, w1=0.28259076077917256\n",
      "Gradient Descent(4073/9999): loss=259.21114989608674, w0=0.02604947714021219, w1=0.2825626545571426\n",
      "Gradient Descent(4074/9999): loss=259.15807666164216, w0=0.026047332545536258, w1=0.28253455100488173\n",
      "Gradient Descent(4075/9999): loss=259.1050161975038, w0=0.026045188160855492, w1=0.28250645012211734\n",
      "Gradient Descent(4076/9999): loss=259.05196850037066, w0=0.02604304398615316, w1=0.2824783519085769\n",
      "Gradient Descent(4077/9999): loss=258.998933566943, w0=0.026040900021412516, w1=0.2824502563639879\n",
      "Gradient Descent(4078/9999): loss=258.94591139392134, w0=0.02603875626661679, w1=0.28242216348807797\n",
      "Gradient Descent(4079/9999): loss=258.89290197800756, w0=0.026036612721749203, w1=0.28239407328057475\n",
      "Gradient Descent(4080/9999): loss=258.83990531590445, w0=0.02603446938679295, w1=0.282365985741206\n",
      "Gradient Descent(4081/9999): loss=258.7869214043154, w0=0.026032326261731215, w1=0.28233790086969957\n",
      "Gradient Descent(4082/9999): loss=258.73395023994505, w0=0.026030183346547164, w1=0.2823098186657833\n",
      "Gradient Descent(4083/9999): loss=258.6809918194987, w0=0.026028040641223948, w1=0.28228173912918525\n",
      "Gradient Descent(4084/9999): loss=258.62804613968274, w0=0.026025898145744696, w1=0.28225366225963344\n",
      "Gradient Descent(4085/9999): loss=258.5751131972044, w0=0.026023755860092524, w1=0.28222558805685605\n",
      "Gradient Descent(4086/9999): loss=258.52219298877174, w0=0.026021613784250528, w1=0.2821975165205813\n",
      "Gradient Descent(4087/9999): loss=258.4692855110939, w0=0.026019471918201795, w1=0.2821694476505374\n",
      "Gradient Descent(4088/9999): loss=258.41639076088086, w0=0.026017330261929386, w1=0.2821413814464528\n",
      "Gradient Descent(4089/9999): loss=258.3635087348433, w0=0.02601518881541635, w1=0.2821133179080559\n",
      "Gradient Descent(4090/9999): loss=258.3106394296932, w0=0.02601304757864572, w1=0.2820852570350752\n",
      "Gradient Descent(4091/9999): loss=258.25778284214306, w0=0.02601090655160051, w1=0.28205719882723934\n",
      "Gradient Descent(4092/9999): loss=258.20493896890656, w0=0.026008765734263722, w1=0.282029143284277\n",
      "Gradient Descent(4093/9999): loss=258.15210780669815, w0=0.02600662512661834, w1=0.28200109040591687\n",
      "Gradient Descent(4094/9999): loss=258.09928935223326, w0=0.026004484728647328, w1=0.28197304019188785\n",
      "Gradient Descent(4095/9999): loss=258.04648360222814, w0=0.026002344540333636, w1=0.2819449926419188\n",
      "Gradient Descent(4096/9999): loss=257.99369055340014, w0=0.026000204561660197, w1=0.2819169477557387\n",
      "Gradient Descent(4097/9999): loss=257.9409102024672, w0=0.02599806479260993, w1=0.2818889055330766\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(4098/9999): loss=257.8881425461484, w0=0.02599592523316574, w1=0.2818608659736616\n",
      "Gradient Descent(4099/9999): loss=257.83538758116373, w0=0.02599378588331051, w1=0.28183282907722296\n",
      "Gradient Descent(4100/9999): loss=257.7826453042338, w0=0.02599164674302711, w1=0.2818047948434899\n",
      "Gradient Descent(4101/9999): loss=257.7299157120807, w0=0.025989507812298394, w1=0.2817767632721918\n",
      "Gradient Descent(4102/9999): loss=257.6771988014268, w0=0.0259873690911072, w1=0.2817487343630581\n",
      "Gradient Descent(4103/9999): loss=257.6244945689957, w0=0.025985230579436355, w1=0.28172070811581823\n",
      "Gradient Descent(4104/9999): loss=257.57180301151186, w0=0.025983092277268655, w1=0.28169268453020185\n",
      "Gradient Descent(4105/9999): loss=257.51912412570067, w0=0.0259809541845869, w1=0.28166466360593856\n",
      "Gradient Descent(4106/9999): loss=257.4664579082883, w0=0.025978816301373858, w1=0.28163664534275806\n",
      "Gradient Descent(4107/9999): loss=257.413804356002, w0=0.02597667862761229, w1=0.2816086297403902\n",
      "Gradient Descent(4108/9999): loss=257.36116346556975, w0=0.025974541163284945, w1=0.2815806167985648\n",
      "Gradient Descent(4109/9999): loss=257.3085352337205, w0=0.025972403908374544, w1=0.2815526065170118\n",
      "Gradient Descent(4110/9999): loss=257.2559196571842, w0=0.025970266862863804, w1=0.2815245988954612\n",
      "Gradient Descent(4111/9999): loss=257.2033167326915, w0=0.025968130026735415, w1=0.2814965939336432\n",
      "Gradient Descent(4112/9999): loss=257.15072645697416, w0=0.025965993399972066, w1=0.28146859163128785\n",
      "Gradient Descent(4113/9999): loss=257.09814882676477, w0=0.025963856982556422, w1=0.28144059198812543\n",
      "Gradient Descent(4114/9999): loss=257.0455838387966, w0=0.02596172077447113, w1=0.2814125950038862\n",
      "Gradient Descent(4115/9999): loss=256.9930314898042, w0=0.02595958477569883, w1=0.2813846006783006\n",
      "Gradient Descent(4116/9999): loss=256.94049177652283, w0=0.02595744898622214, w1=0.2813566090110991\n",
      "Gradient Descent(4117/9999): loss=256.8879646956884, w0=0.025955313406023663, w1=0.28132862000201214\n",
      "Gradient Descent(4118/9999): loss=256.8354502440384, w0=0.025953178035085993, w1=0.28130063365077035\n",
      "Gradient Descent(4119/9999): loss=256.78294841831047, w0=0.025951042873391703, w1=0.28127264995710444\n",
      "Gradient Descent(4120/9999): loss=256.73045921524357, w0=0.02594890792092335, w1=0.28124466892074507\n",
      "Gradient Descent(4121/9999): loss=256.67798263157744, w0=0.025946773177663487, w1=0.2812166905414231\n",
      "Gradient Descent(4122/9999): loss=256.6255186640527, w0=0.025944638643594635, w1=0.2811887148188695\n",
      "Gradient Descent(4123/9999): loss=256.57306730941104, w0=0.025942504318699315, w1=0.2811607417528151\n",
      "Gradient Descent(4124/9999): loss=256.5206285643948, w0=0.025940370202960025, w1=0.281132771342991\n",
      "Gradient Descent(4125/9999): loss=256.4682024257473, w0=0.02593823629635925, w1=0.2811048035891282\n",
      "Gradient Descent(4126/9999): loss=256.415788890213, w0=0.02593610259887946, w1=0.281076838490958\n",
      "Gradient Descent(4127/9999): loss=256.36338795453673, w0=0.025933969110503108, w1=0.28104887604821155\n",
      "Gradient Descent(4128/9999): loss=256.31099961546477, w0=0.025931835831212637, w1=0.2810209162606202\n",
      "Gradient Descent(4129/9999): loss=256.25862386974393, w0=0.025929702760990478, w1=0.28099295912791533\n",
      "Gradient Descent(4130/9999): loss=256.20626071412204, w0=0.02592756989981904, w1=0.28096500464982843\n",
      "Gradient Descent(4131/9999): loss=256.15391014534794, w0=0.02592543724768072, w1=0.280937052826091\n",
      "Gradient Descent(4132/9999): loss=256.10157216017114, w0=0.0259233048045579, w1=0.2809091036564346\n",
      "Gradient Descent(4133/9999): loss=256.0492467553422, w0=0.02592117257043295, w1=0.280881157140591\n",
      "Gradient Descent(4134/9999): loss=255.99693392761253, w0=0.02591904054528822, w1=0.28085321327829177\n",
      "Gradient Descent(4135/9999): loss=255.94463367373436, w0=0.02591690872910606, w1=0.2808252720692688\n",
      "Gradient Descent(4136/9999): loss=255.89234599046102, w0=0.025914777121868782, w1=0.28079733351325403\n",
      "Gradient Descent(4137/9999): loss=255.84007087454648, w0=0.025912645723558708, w1=0.2807693976099793\n",
      "Gradient Descent(4138/9999): loss=255.78780832274595, w0=0.025910514534158127, w1=0.2807414643591767\n",
      "Gradient Descent(4139/9999): loss=255.73555833181499, w0=0.025908383553649324, w1=0.2807135337605783\n",
      "Gradient Descent(4140/9999): loss=255.68332089851057, w0=0.025906252782014572, w1=0.2806856058139162\n",
      "Gradient Descent(4141/9999): loss=255.63109601959027, w0=0.02590412221923612, w1=0.2806576805189227\n",
      "Gradient Descent(4142/9999): loss=255.57888369181268, w0=0.025901991865296212, w1=0.28062975787533\n",
      "Gradient Descent(4143/9999): loss=255.5266839119373, w0=0.02589986172017707, w1=0.2806018378828706\n",
      "Gradient Descent(4144/9999): loss=255.47449667672433, w0=0.02589773178386091, w1=0.2805739205412768\n",
      "Gradient Descent(4145/9999): loss=255.4223219829352, w0=0.02589560205632993, w1=0.2805460058502811\n",
      "Gradient Descent(4146/9999): loss=255.3701598273318, w0=0.025893472537566315, w1=0.2805180938096162\n",
      "Gradient Descent(4147/9999): loss=255.31801020667731, w0=0.025891343227552235, w1=0.2804901844190146\n",
      "Gradient Descent(4148/9999): loss=255.2658731177357, w0=0.025889214126269846, w1=0.280462277678209\n",
      "Gradient Descent(4149/9999): loss=255.2137485572715, w0=0.02588708523370129, w1=0.2804343735869323\n",
      "Gradient Descent(4150/9999): loss=255.16163652205054, w0=0.025884956549828704, w1=0.28040647214491726\n",
      "Gradient Descent(4151/9999): loss=255.1095370088394, w0=0.025882828074634195, w1=0.28037857335189675\n",
      "Gradient Descent(4152/9999): loss=255.05745001440553, w0=0.02588069980809987, w1=0.2803506772076038\n",
      "Gradient Descent(4153/9999): loss=255.0053755355172, w0=0.025878571750207812, w1=0.2803227837117715\n",
      "Gradient Descent(4154/9999): loss=254.95331356894394, w0=0.025876443900940103, w1=0.2802948928641329\n",
      "Gradient Descent(4155/9999): loss=254.90126411145545, w0=0.025874316260278803, w1=0.28026700466442117\n",
      "Gradient Descent(4156/9999): loss=254.84922715982313, w0=0.02587218882820596, w1=0.2802391191123696\n",
      "Gradient Descent(4157/9999): loss=254.7972027108186, w0=0.025870061604703606, w1=0.28021123620771143\n",
      "Gradient Descent(4158/9999): loss=254.7451907612147, w0=0.025867934589753765, w1=0.2801833559501801\n",
      "Gradient Descent(4159/9999): loss=254.69319130778524, w0=0.025865807783338447, w1=0.2801554783395091\n",
      "Gradient Descent(4160/9999): loss=254.6412043473047, w0=0.025863681185439646, w1=0.28012760337543186\n",
      "Gradient Descent(4161/9999): loss=254.58922987654847, w0=0.025861554796039344, w1=0.280099731057682\n",
      "Gradient Descent(4162/9999): loss=254.53726789229304, w0=0.025859428615119508, w1=0.28007186138599316\n",
      "Gradient Descent(4163/9999): loss=254.48531839131547, w0=0.025857302642662092, w1=0.280043994360099\n",
      "Gradient Descent(4164/9999): loss=254.43338137039393, w0=0.025855176878649043, w1=0.28001612997973346\n",
      "Gradient Descent(4165/9999): loss=254.38145682630736, w0=0.025853051323062284, w1=0.27998826824463025\n",
      "Gradient Descent(4166/9999): loss=254.32954475583585, w0=0.025850925975883735, w1=0.2799604091545233\n",
      "Gradient Descent(4167/9999): loss=254.27764515575987, w0=0.0258488008370953, w1=0.27993255270914663\n",
      "Gradient Descent(4168/9999): loss=254.2257580228613, w0=0.02584667590667887, w1=0.2799046989082343\n",
      "Gradient Descent(4169/9999): loss=254.17388335392243, w0=0.02584455118461632, w1=0.27987684775152033\n",
      "Gradient Descent(4170/9999): loss=254.12202114572702, w0=0.025842426670889516, w1=0.27984899923873896\n",
      "Gradient Descent(4171/9999): loss=254.07017139505913, w0=0.025840302365480305, w1=0.27982115336962443\n",
      "Gradient Descent(4172/9999): loss=254.0183340987039, w0=0.02583817826837053, w1=0.27979331014391107\n",
      "Gradient Descent(4173/9999): loss=253.9665092534474, w0=0.02583605437954202, w1=0.27976546956133325\n",
      "Gradient Descent(4174/9999): loss=253.91469685607683, w0=0.025833930698976583, w1=0.27973763162162535\n",
      "Gradient Descent(4175/9999): loss=253.86289690337983, w0=0.025831807226656022, w1=0.2797097963245219\n",
      "Gradient Descent(4176/9999): loss=253.811109392145, w0=0.025829683962562126, w1=0.27968196366975756\n",
      "Gradient Descent(4177/9999): loss=253.7593343191622, w0=0.02582756090667667, w1=0.2796541336570669\n",
      "Gradient Descent(4178/9999): loss=253.70757168122168, w0=0.02582543805898142, w1=0.2796263062861845\n",
      "Gradient Descent(4179/9999): loss=253.65582147511498, w0=0.02582331541945812, w1=0.2795984815568453\n",
      "Gradient Descent(4180/9999): loss=253.6040836976341, w0=0.025821192988088514, w1=0.27957065946878407\n",
      "Gradient Descent(4181/9999): loss=253.55235834557243, w0=0.025819070764854325, w1=0.2795428400217357\n",
      "Gradient Descent(4182/9999): loss=253.5006454157238, w0=0.025816948749737268, w1=0.2795150232154351\n",
      "Gradient Descent(4183/9999): loss=253.4489449048831, w0=0.025814826942719044, w1=0.2794872090496173\n",
      "Gradient Descent(4184/9999): loss=253.39725680984623, w0=0.025812705343781342, w1=0.2794593975240175\n",
      "Gradient Descent(4185/9999): loss=253.34558112740964, w0=0.025810583952905836, w1=0.2794315886383707\n",
      "Gradient Descent(4186/9999): loss=253.293917854371, w0=0.025808462770074193, w1=0.27940378239241215\n",
      "Gradient Descent(4187/9999): loss=253.24226698752867, w0=0.025806341795268063, w1=0.27937597878587717\n",
      "Gradient Descent(4188/9999): loss=253.1906285236818, w0=0.025804221028469087, w1=0.2793481778185011\n",
      "Gradient Descent(4189/9999): loss=253.13900245963075, w0=0.025802100469658893, w1=0.27932037949001925\n",
      "Gradient Descent(4190/9999): loss=253.08738879217637, w0=0.025799980118819097, w1=0.2792925838001672\n",
      "Gradient Descent(4191/9999): loss=253.0357875181207, w0=0.025797859975931304, w1=0.27926479074868044\n",
      "Gradient Descent(4192/9999): loss=252.9841986342665, w0=0.025795740040977105, w1=0.2792370003352946\n",
      "Gradient Descent(4193/9999): loss=252.93262213741738, w0=0.02579362031393808, w1=0.27920921255974523\n",
      "Gradient Descent(4194/9999): loss=252.8810580243779, w0=0.025791500794795794, w1=0.27918142742176816\n",
      "Gradient Descent(4195/9999): loss=252.8295062919535, w0=0.02578938148353181, w1=0.2791536449210991\n",
      "Gradient Descent(4196/9999): loss=252.77796693695055, w0=0.025787262380127667, w1=0.27912586505747394\n",
      "Gradient Descent(4197/9999): loss=252.7264399561761, w0=0.0257851434845649, w1=0.27909808783062856\n",
      "Gradient Descent(4198/9999): loss=252.67492534643827, w0=0.025783024796825028, w1=0.27907031324029896\n",
      "Gradient Descent(4199/9999): loss=252.62342310454605, w0=0.025780906316889562, w1=0.27904254128622114\n",
      "Gradient Descent(4200/9999): loss=252.57193322730905, w0=0.025778788044739997, w1=0.2790147719681312\n",
      "Gradient Descent(4201/9999): loss=252.52045571153818, w0=0.02577666998035782, w1=0.27898700528576525\n",
      "Gradient Descent(4202/9999): loss=252.4689905540447, w0=0.025774552123724506, w1=0.27895924123885957\n",
      "Gradient Descent(4203/9999): loss=252.41753775164122, w0=0.025772434474821516, w1=0.27893147982715044\n",
      "Gradient Descent(4204/9999): loss=252.36609730114108, w0=0.025770317033630304, w1=0.2789037210503742\n",
      "Gradient Descent(4205/9999): loss=252.3146691993585, w0=0.025768199800132306, w1=0.2788759649082672\n",
      "Gradient Descent(4206/9999): loss=252.26325344310837, w0=0.025766082774308953, w1=0.278848211400566\n",
      "Gradient Descent(4207/9999): loss=252.21185002920657, w0=0.02576396595614166, w1=0.27882046052700704\n",
      "Gradient Descent(4208/9999): loss=252.16045895446996, w0=0.025761849345611834, w1=0.27879271228732694\n",
      "Gradient Descent(4209/9999): loss=252.10908021571638, w0=0.02575973294270087, w1=0.2787649666812624\n",
      "Gradient Descent(4210/9999): loss=252.05771380976415, w0=0.025757616747390147, w1=0.27873722370855003\n",
      "Gradient Descent(4211/9999): loss=252.0063597334328, w0=0.02575550075966104, w1=0.27870948336892665\n",
      "Gradient Descent(4212/9999): loss=251.9550179835425, w0=0.02575338497949491, w1=0.2786817456621291\n",
      "Gradient Descent(4213/9999): loss=251.90368855691446, w0=0.025751269406873105, w1=0.2786540105878943\n",
      "Gradient Descent(4214/9999): loss=251.85237145037078, w0=0.02574915404177696, w1=0.27862627814595914\n",
      "Gradient Descent(4215/9999): loss=251.80106666073428, w0=0.025747038884187806, w1=0.2785985483360607\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(4216/9999): loss=251.74977418482874, w0=0.025744923934086957, w1=0.27857082115793597\n",
      "Gradient Descent(4217/9999): loss=251.69849401947886, w0=0.025742809191455718, w1=0.27854309661132215\n",
      "Gradient Descent(4218/9999): loss=251.64722616151005, w0=0.02574069465627538, w1=0.27851537469595644\n",
      "Gradient Descent(4219/9999): loss=251.59597060774874, w0=0.025738580328527227, w1=0.2784876554115761\n",
      "Gradient Descent(4220/9999): loss=251.54472735502227, w0=0.025736466208192536, w1=0.2784599387579184\n",
      "Gradient Descent(4221/9999): loss=251.4934964001586, w0=0.025734352295252564, w1=0.2784322247347208\n",
      "Gradient Descent(4222/9999): loss=251.4422777399868, w0=0.025732238589688563, w1=0.2784045133417206\n",
      "Gradient Descent(4223/9999): loss=251.3910713713366, w0=0.02573012509148177, w1=0.27837680457865543\n",
      "Gradient Descent(4224/9999): loss=251.33987729103902, w0=0.02572801180061341, w1=0.2783490984452628\n",
      "Gradient Descent(4225/9999): loss=251.2886954959254, w0=0.02572589871706471, w1=0.2783213949412803\n",
      "Gradient Descent(4226/9999): loss=251.23752598282834, w0=0.02572378584081687, w1=0.2782936940664456\n",
      "Gradient Descent(4227/9999): loss=251.1863687485811, w0=0.02572167317185109, w1=0.2782659958204965\n",
      "Gradient Descent(4228/9999): loss=251.13522379001785, w0=0.025719560710148553, w1=0.2782383002031707\n",
      "Gradient Descent(4229/9999): loss=251.08409110397372, w0=0.025717448455690434, w1=0.2782106072142061\n",
      "Gradient Descent(4230/9999): loss=251.03297068728466, w0=0.0257153364084579, w1=0.2781829168533406\n",
      "Gradient Descent(4231/9999): loss=250.98186253678745, w0=0.025713224568432103, w1=0.27815522912031226\n",
      "Gradient Descent(4232/9999): loss=250.9307666493197, w0=0.025711112935594187, w1=0.278127544014859\n",
      "Gradient Descent(4233/9999): loss=250.87968302172004, w0=0.025709001509925285, w1=0.27809986153671895\n",
      "Gradient Descent(4234/9999): loss=250.8286116508278, w0=0.02570689029140652, w1=0.27807218168563025\n",
      "Gradient Descent(4235/9999): loss=250.7775525334833, w0=0.025704779280019005, w1=0.2780445044613311\n",
      "Gradient Descent(4236/9999): loss=250.72650566652757, w0=0.02570266847574384, w1=0.27801682986355974\n",
      "Gradient Descent(4237/9999): loss=250.6754710468027, w0=0.02570055787856212, w1=0.27798915789205453\n",
      "Gradient Descent(4238/9999): loss=250.62444867115153, w0=0.025698447488454925, w1=0.27796148854655384\n",
      "Gradient Descent(4239/9999): loss=250.57343853641777, w0=0.025696337305403324, w1=0.2779338218267961\n",
      "Gradient Descent(4240/9999): loss=250.52244063944605, w0=0.025694227329388374, w1=0.2779061577325198\n",
      "Gradient Descent(4241/9999): loss=250.47145497708172, w0=0.025692117560391133, w1=0.27787849626346356\n",
      "Gradient Descent(4242/9999): loss=250.4204815461712, w0=0.025690007998392637, w1=0.27785083741936595\n",
      "Gradient Descent(4243/9999): loss=250.36952034356162, w0=0.025687898643373917, w1=0.27782318119996563\n",
      "Gradient Descent(4244/9999): loss=250.31857136610108, w0=0.025685789495315994, w1=0.2777955276050013\n",
      "Gradient Descent(4245/9999): loss=250.2676346106384, w0=0.025683680554199877, w1=0.2777678766342118\n",
      "Gradient Descent(4246/9999): loss=250.21671007402344, w0=0.025681571820006566, w1=0.27774022828733597\n",
      "Gradient Descent(4247/9999): loss=250.1657977531068, w0=0.025679463292717055, w1=0.2777125825641127\n",
      "Gradient Descent(4248/9999): loss=250.11489764473998, w0=0.02567735497231232, w1=0.2776849394642809\n",
      "Gradient Descent(4249/9999): loss=250.06400974577537, w0=0.02567524685877333, w1=0.2776572989875796\n",
      "Gradient Descent(4250/9999): loss=250.013134053066, w0=0.025673138952081048, w1=0.277629661133748\n",
      "Gradient Descent(4251/9999): loss=249.9622705634662, w0=0.025671031252216422, w1=0.27760202590252503\n",
      "Gradient Descent(4252/9999): loss=249.91141927383086, w0=0.025668923759160394, w1=0.27757439329365\n",
      "Gradient Descent(4253/9999): loss=249.8605801810157, w0=0.025666816472893896, w1=0.2775467633068622\n",
      "Gradient Descent(4254/9999): loss=249.80975328187745, w0=0.025664709393397846, w1=0.27751913594190075\n",
      "Gradient Descent(4255/9999): loss=249.75893857327353, w0=0.025662602520653163, w1=0.2774915111985052\n",
      "Gradient Descent(4256/9999): loss=249.70813605206237, w0=0.02566049585464074, w1=0.27746388907641484\n",
      "Gradient Descent(4257/9999): loss=249.6573457151033, w0=0.02565838939534147, w1=0.27743626957536915\n",
      "Gradient Descent(4258/9999): loss=249.60656755925632, w0=0.02565628314273624, w1=0.27740865269510767\n",
      "Gradient Descent(4259/9999): loss=249.55580158138244, w0=0.02565417709680592, w1=0.27738103843537004\n",
      "Gradient Descent(4260/9999): loss=249.50504777834348, w0=0.025652071257531373, w1=0.2773534267958958\n",
      "Gradient Descent(4261/9999): loss=249.45430614700206, w0=0.025649965624893453, w1=0.27732581777642473\n",
      "Gradient Descent(4262/9999): loss=249.40357668422183, w0=0.025647860198873006, w1=0.2772982113766965\n",
      "Gradient Descent(4263/9999): loss=249.352859386867, w0=0.025645754979450864, w1=0.277270607596451\n",
      "Gradient Descent(4264/9999): loss=249.30215425180313, w0=0.025643649966607854, w1=0.27724300643542804\n",
      "Gradient Descent(4265/9999): loss=249.25146127589608, w0=0.02564154516032479, w1=0.27721540789336757\n",
      "Gradient Descent(4266/9999): loss=249.20078045601278, w0=0.02563944056058248, w1=0.2771878119700095\n",
      "Gradient Descent(4267/9999): loss=249.15011178902128, w0=0.025637336167361722, w1=0.27716021866509394\n",
      "Gradient Descent(4268/9999): loss=249.0994552717901, w0=0.0256352319806433, w1=0.2771326279783609\n",
      "Gradient Descent(4269/9999): loss=249.04881090118886, w0=0.025633128000407998, w1=0.27710503990955054\n",
      "Gradient Descent(4270/9999): loss=248.99817867408788, w0=0.025631024226636583, w1=0.2770774544584031\n",
      "Gradient Descent(4271/9999): loss=248.9475585873585, w0=0.02562892065930981, w1=0.2770498716246588\n",
      "Gradient Descent(4272/9999): loss=248.89695063787272, w0=0.02562681729840844, w1=0.2770222914080579\n",
      "Gradient Descent(4273/9999): loss=248.84635482250366, w0=0.025624714143913208, w1=0.2769947138083408\n",
      "Gradient Descent(4274/9999): loss=248.79577113812493, w0=0.025622611195804848, w1=0.2769671388252479\n",
      "Gradient Descent(4275/9999): loss=248.7451995816113, w0=0.025620508454064085, w1=0.27693956645851975\n",
      "Gradient Descent(4276/9999): loss=248.6946401498384, w0=0.025618405918671632, w1=0.27691199670789673\n",
      "Gradient Descent(4277/9999): loss=248.6440928396825, w0=0.025616303589608196, w1=0.27688442957311954\n",
      "Gradient Descent(4278/9999): loss=248.5935576480209, w0=0.02561420146685447, w1=0.27685686505392876\n",
      "Gradient Descent(4279/9999): loss=248.54303457173157, w0=0.025612099550391145, w1=0.27682930315006504\n",
      "Gradient Descent(4280/9999): loss=248.49252360769353, w0=0.0256099978401989, w1=0.2768017438612692\n",
      "Gradient Descent(4281/9999): loss=248.44202475278672, w0=0.0256078963362584, w1=0.276774187187282\n",
      "Gradient Descent(4282/9999): loss=248.39153800389153, w0=0.025605795038550316, w1=0.27674663312784425\n",
      "Gradient Descent(4283/9999): loss=248.34106335788965, w0=0.025603693947055292, w1=0.27671908168269693\n",
      "Gradient Descent(4284/9999): loss=248.29060081166338, w0=0.025601593061753972, w1=0.27669153285158093\n",
      "Gradient Descent(4285/9999): loss=248.24015036209585, w0=0.025599492382626994, w1=0.2766639866342373\n",
      "Gradient Descent(4286/9999): loss=248.18971200607123, w0=0.025597391909654977, w1=0.2766364430304071\n",
      "Gradient Descent(4287/9999): loss=248.13928574047443, w0=0.025595291642818544, w1=0.2766089020398314\n",
      "Gradient Descent(4288/9999): loss=248.08887156219114, w0=0.0255931915820983, w1=0.27658136366225144\n",
      "Gradient Descent(4289/9999): loss=248.03846946810808, w0=0.025591091727474848, w1=0.27655382789740846\n",
      "Gradient Descent(4290/9999): loss=247.98807945511263, w0=0.025588992078928778, w1=0.2765262947450437\n",
      "Gradient Descent(4291/9999): loss=247.93770152009313, w0=0.02558689263644067, w1=0.2764987642048985\n",
      "Gradient Descent(4292/9999): loss=247.88733565993877, w0=0.0255847933999911, w1=0.2764712362767142\n",
      "Gradient Descent(4293/9999): loss=247.8369818715395, w0=0.025582694369560635, w1=0.2764437109602323\n",
      "Gradient Descent(4294/9999): loss=247.78664015178634, w0=0.025580595545129828, w1=0.27641618825519426\n",
      "Gradient Descent(4295/9999): loss=247.73631049757074, w0=0.02557849692667923, w1=0.27638866816134167\n",
      "Gradient Descent(4296/9999): loss=247.68599290578558, w0=0.025576398514189382, w1=0.27636115067841605\n",
      "Gradient Descent(4297/9999): loss=247.63568737332412, w0=0.025574300307640813, w1=0.2763336358061591\n",
      "Gradient Descent(4298/9999): loss=247.58539389708054, w0=0.02557220230701405, w1=0.27630612354431255\n",
      "Gradient Descent(4299/9999): loss=247.53511247395016, w0=0.025570104512289603, w1=0.2762786138926181\n",
      "Gradient Descent(4300/9999): loss=247.48484310082878, w0=0.02556800692344798, w1=0.2762511068508176\n",
      "Gradient Descent(4301/9999): loss=247.4345857746133, w0=0.025565909540469677, w1=0.2762236024186529\n",
      "Gradient Descent(4302/9999): loss=247.38434049220123, w0=0.02556381236333519, w1=0.2761961005958659\n",
      "Gradient Descent(4303/9999): loss=247.33410725049126, w0=0.025561715392024995, w1=0.27616860138219856\n",
      "Gradient Descent(4304/9999): loss=247.2838860463827, w0=0.025559618626519568, w1=0.2761411047773929\n",
      "Gradient Descent(4305/9999): loss=247.2336768767757, w0=0.025557522066799374, w1=0.276113610781191\n",
      "Gradient Descent(4306/9999): loss=247.18347973857126, w0=0.02555542571284487, w1=0.276086119393335\n",
      "Gradient Descent(4307/9999): loss=247.1332946286714, w0=0.025553329564636504, w1=0.27605863061356706\n",
      "Gradient Descent(4308/9999): loss=247.08312154397882, w0=0.025551233622154716, w1=0.2760311444416294\n",
      "Gradient Descent(4309/9999): loss=247.03296048139708, w0=0.025549137885379943, w1=0.2760036608772643\n",
      "Gradient Descent(4310/9999): loss=246.98281143783052, w0=0.025547042354292608, w1=0.2759761799202141\n",
      "Gradient Descent(4311/9999): loss=246.93267441018466, w0=0.025544947028873128, w1=0.27594870157022117\n",
      "Gradient Descent(4312/9999): loss=246.88254939536537, w0=0.02554285190910191, w1=0.27592122582702794\n",
      "Gradient Descent(4313/9999): loss=246.8324363902798, w0=0.025540756994959356, w1=0.2758937526903769\n",
      "Gradient Descent(4314/9999): loss=246.7823353918357, w0=0.025538662286425855, w1=0.27586628216001063\n",
      "Gradient Descent(4315/9999): loss=246.7322463969417, w0=0.0255365677834818, w1=0.27583881423567164\n",
      "Gradient Descent(4316/9999): loss=246.68216940250724, w0=0.025534473486107562, w1=0.2758113489171026\n",
      "Gradient Descent(4317/9999): loss=246.63210440544282, w0=0.025532379394283513, w1=0.2757838862040462\n",
      "Gradient Descent(4318/9999): loss=246.58205140265957, w0=0.025530285507990014, w1=0.2757564260962452\n",
      "Gradient Descent(4319/9999): loss=246.5320103910695, w0=0.02552819182720742, w1=0.2757289685934423\n",
      "Gradient Descent(4320/9999): loss=246.48198136758555, w0=0.025526098351916073, w1=0.2757015136953805\n",
      "Gradient Descent(4321/9999): loss=246.43196432912129, w0=0.025524005082096315, w1=0.27567406140180256\n",
      "Gradient Descent(4322/9999): loss=246.38195927259136, w0=0.025521912017728472, w1=0.2756466117124515\n",
      "Gradient Descent(4323/9999): loss=246.3319661949113, w0=0.025519819158792874, w1=0.27561916462707026\n",
      "Gradient Descent(4324/9999): loss=246.28198509299713, w0=0.02551772650526983, w1=0.27559172014540195\n",
      "Gradient Descent(4325/9999): loss=246.23201596376612, w0=0.025515634057139653, w1=0.2755642782671896\n",
      "Gradient Descent(4326/9999): loss=246.18205880413615, w0=0.025513541814382635, w1=0.2755368389921764\n",
      "Gradient Descent(4327/9999): loss=246.1321136110258, w0=0.025511449776979073, w1=0.2755094023201055\n",
      "Gradient Descent(4328/9999): loss=246.08218038135502, w0=0.025509357944909255, w1=0.2754819682507202\n",
      "Gradient Descent(4329/9999): loss=246.03225911204407, w0=0.025507266318153452, w1=0.27545453678376375\n",
      "Gradient Descent(4330/9999): loss=245.98234980001422, w0=0.025505174896691938, w1=0.27542710791897956\n",
      "Gradient Descent(4331/9999): loss=245.93245244218772, w0=0.025503083680504975, w1=0.27539968165611095\n",
      "Gradient Descent(4332/9999): loss=245.88256703548745, w0=0.025500992669572817, w1=0.27537225799490145\n",
      "Gradient Descent(4333/9999): loss=245.83269357683724, w0=0.025498901863875715, w1=0.2753448369350945\n",
      "Gradient Descent(4334/9999): loss=245.78283206316172, w0=0.025496811263393905, w1=0.27531741847643365\n",
      "Gradient Descent(4335/9999): loss=245.7329824913865, w0=0.025494720868107622, w1=0.2752900026186625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(4336/9999): loss=245.68314485843774, w0=0.02549263067799709, w1=0.2752625893615247\n",
      "Gradient Descent(4337/9999): loss=245.6333191612428, w0=0.02549054069304253, w1=0.27523517870476394\n",
      "Gradient Descent(4338/9999): loss=245.58350539672963, w0=0.02548845091322415, w1=0.27520777064812396\n",
      "Gradient Descent(4339/9999): loss=245.533703561827, w0=0.02548636133852215, w1=0.2751803651913486\n",
      "Gradient Descent(4340/9999): loss=245.48391365346464, w0=0.025484271968916738, w1=0.2751529623341816\n",
      "Gradient Descent(4341/9999): loss=245.43413566857308, w0=0.025482182804388093, w1=0.27512556207636696\n",
      "Gradient Descent(4342/9999): loss=245.38436960408362, w0=0.025480093844916403, w1=0.27509816441764856\n",
      "Gradient Descent(4343/9999): loss=245.33461545692862, w0=0.025478005090481842, w1=0.2750707693577704\n",
      "Gradient Descent(4344/9999): loss=245.28487322404092, w0=0.025475916541064573, w1=0.27504337689647657\n",
      "Gradient Descent(4345/9999): loss=245.23514290235454, w0=0.025473828196644763, w1=0.2750159870335111\n",
      "Gradient Descent(4346/9999): loss=245.1854244888042, w0=0.025471740057202562, w1=0.27498859976861817\n",
      "Gradient Descent(4347/9999): loss=245.13571798032527, w0=0.02546965212271812, w1=0.2749612151015419\n",
      "Gradient Descent(4348/9999): loss=245.08602337385432, w0=0.025467564393171573, w1=0.2749338330320266\n",
      "Gradient Descent(4349/9999): loss=245.03634066632856, w0=0.025465476868543058, w1=0.2749064535598166\n",
      "Gradient Descent(4350/9999): loss=244.98666985468594, w0=0.0254633895488127, w1=0.27487907668465605\n",
      "Gradient Descent(4351/9999): loss=244.93701093586535, w0=0.025461302433960612, w1=0.2748517024062895\n",
      "Gradient Descent(4352/9999): loss=244.88736390680657, w0=0.025459215523966912, w1=0.2748243307244613\n",
      "Gradient Descent(4353/9999): loss=244.8377287644502, w0=0.0254571288188117, w1=0.2747969616389159\n",
      "Gradient Descent(4354/9999): loss=244.7881055057375, w0=0.025455042318475084, w1=0.27476959514939797\n",
      "Gradient Descent(4355/9999): loss=244.7384941276109, w0=0.025452956022937148, w1=0.27474223125565195\n",
      "Gradient Descent(4356/9999): loss=244.68889462701327, w0=0.025450869932177975, w1=0.2747148699574225\n",
      "Gradient Descent(4357/9999): loss=244.6393070008886, w0=0.025448784046177647, w1=0.2746875112544543\n",
      "Gradient Descent(4358/9999): loss=244.58973124618154, w0=0.025446698364916233, w1=0.27466015514649206\n",
      "Gradient Descent(4359/9999): loss=244.5401673598378, w0=0.025444612888373797, w1=0.2746328016332805\n",
      "Gradient Descent(4360/9999): loss=244.4906153388037, w0=0.025442527616530396, w1=0.27460545071456455\n",
      "Gradient Descent(4361/9999): loss=244.44107518002644, w0=0.02544044254936608, w1=0.27457810239008895\n",
      "Gradient Descent(4362/9999): loss=244.39154688045403, w0=0.0254383576868609, w1=0.27455075665959866\n",
      "Gradient Descent(4363/9999): loss=244.3420304370355, w0=0.02543627302899489, w1=0.27452341352283866\n",
      "Gradient Descent(4364/9999): loss=244.2925258467204, w0=0.025434188575748078, w1=0.2744960729795539\n",
      "Gradient Descent(4365/9999): loss=244.2430331064595, w0=0.02543210432710049, w1=0.27446873502948954\n",
      "Gradient Descent(4366/9999): loss=244.1935522132041, w0=0.025430020283032144, w1=0.27444139967239056\n",
      "Gradient Descent(4367/9999): loss=244.14408316390637, w0=0.025427936443523053, w1=0.2744140669080022\n",
      "Gradient Descent(4368/9999): loss=244.09462595551938, w0=0.025425852808553223, w1=0.27438673673606956\n",
      "Gradient Descent(4369/9999): loss=244.04518058499696, w0=0.02542376937810265, w1=0.27435940915633794\n",
      "Gradient Descent(4370/9999): loss=243.995747049294, w0=0.025421686152151325, w1=0.2743320841685526\n",
      "Gradient Descent(4371/9999): loss=243.9463253453659, w0=0.025419603130679234, w1=0.2743047617724589\n",
      "Gradient Descent(4372/9999): loss=243.89691547016912, w0=0.02541752031366636, w1=0.2742774419678022\n",
      "Gradient Descent(4373/9999): loss=243.84751742066072, w0=0.02541543770109267, w1=0.2742501247543279\n",
      "Gradient Descent(4374/9999): loss=243.7981311937989, w0=0.025413355292938134, w1=0.2742228101317816\n",
      "Gradient Descent(4375/9999): loss=243.7487567865424, w0=0.02541127308918271, w1=0.27419549809990873\n",
      "Gradient Descent(4376/9999): loss=243.699394195851, w0=0.025409191089806354, w1=0.2741681886584549\n",
      "Gradient Descent(4377/9999): loss=243.65004341868507, w0=0.025407109294789013, w1=0.27414088180716567\n",
      "Gradient Descent(4378/9999): loss=243.6007044520062, w0=0.025405027704110628, w1=0.27411357754578675\n",
      "Gradient Descent(4379/9999): loss=243.5513772927764, w0=0.02540294631775113, w1=0.2740862758740638\n",
      "Gradient Descent(4380/9999): loss=243.50206193795867, w0=0.025400865135690454, w1=0.2740589767917426\n",
      "Gradient Descent(4381/9999): loss=243.45275838451684, w0=0.025398784157908517, w1=0.274031680298569\n",
      "Gradient Descent(4382/9999): loss=243.4034666294157, w0=0.02539670338438524, w1=0.2740043863942888\n",
      "Gradient Descent(4383/9999): loss=243.35418666962062, w0=0.025394622815100527, w1=0.2739770950786479\n",
      "Gradient Descent(4384/9999): loss=243.30491850209796, w0=0.02539254245003429, w1=0.27394980635139227\n",
      "Gradient Descent(4385/9999): loss=243.25566212381472, w0=0.025390462289166426, w1=0.2739225202122678\n",
      "Gradient Descent(4386/9999): loss=243.2064175317391, w0=0.02538838233247682, w1=0.2738952366610206\n",
      "Gradient Descent(4387/9999): loss=243.15718472283973, w0=0.025386302579945362, w1=0.2738679556973968\n",
      "Gradient Descent(4388/9999): loss=243.1079636940862, w0=0.025384223031551932, w1=0.2738406773211424\n",
      "Gradient Descent(4389/9999): loss=243.05875444244916, w0=0.025382143687276405, w1=0.2738134015320037\n",
      "Gradient Descent(4390/9999): loss=243.00955696489976, w0=0.02538006454709865, w1=0.27378612832972676\n",
      "Gradient Descent(4391/9999): loss=242.96037125841002, w0=0.02537798561099852, w1=0.273758857714058\n",
      "Gradient Descent(4392/9999): loss=242.91119731995298, w0=0.025375906878955884, w1=0.27373158968474365\n",
      "Gradient Descent(4393/9999): loss=242.86203514650228, w0=0.025373828350950582, w1=0.2737043242415301\n",
      "Gradient Descent(4394/9999): loss=242.81288473503264, w0=0.025371750026962462, w1=0.27367706138416364\n",
      "Gradient Descent(4395/9999): loss=242.7637460825194, w0=0.025369671906971362, w1=0.27364980111239084\n",
      "Gradient Descent(4396/9999): loss=242.7146191859387, w0=0.025367593990957116, w1=0.27362254342595815\n",
      "Gradient Descent(4397/9999): loss=242.66550404226757, w0=0.025365516278899547, w1=0.2735952883246121\n",
      "Gradient Descent(4398/9999): loss=242.61640064848402, w0=0.025363438770778478, w1=0.2735680358080993\n",
      "Gradient Descent(4399/9999): loss=242.56730900156657, w0=0.025361361466573723, w1=0.2735407858761663\n",
      "Gradient Descent(4400/9999): loss=242.5182290984949, w0=0.025359284366265094, w1=0.27351353852855986\n",
      "Gradient Descent(4401/9999): loss=242.46916093624924, w0=0.02535720746983239, w1=0.2734862937650266\n",
      "Gradient Descent(4402/9999): loss=242.42010451181085, w0=0.025355130777255416, w1=0.2734590515853133\n",
      "Gradient Descent(4403/9999): loss=242.3710598221616, w0=0.025353054288513956, w1=0.27343181198916683\n",
      "Gradient Descent(4404/9999): loss=242.32202686428434, w0=0.0253509780035878, w1=0.27340457497633397\n",
      "Gradient Descent(4405/9999): loss=242.27300563516263, w0=0.02534890192245673, w1=0.27337734054656165\n",
      "Gradient Descent(4406/9999): loss=242.22399613178104, w0=0.025346826045100516, w1=0.2733501086995968\n",
      "Gradient Descent(4407/9999): loss=242.17499835112477, w0=0.025344750371498935, w1=0.2733228794351864\n",
      "Gradient Descent(4408/9999): loss=242.1260122901799, w0=0.025342674901631747, w1=0.2732956527530775\n",
      "Gradient Descent(4409/9999): loss=242.07703794593328, w0=0.025340599635478713, w1=0.27326842865301715\n",
      "Gradient Descent(4410/9999): loss=242.02807531537275, w0=0.025338524573019584, w1=0.2732412071347525\n",
      "Gradient Descent(4411/9999): loss=241.9791243954869, w0=0.02533644971423411, w1=0.2732139881980306\n",
      "Gradient Descent(4412/9999): loss=241.93018518326497, w0=0.025334375059102028, w1=0.2731867718425988\n",
      "Gradient Descent(4413/9999): loss=241.88125767569716, w0=0.025332300607603078, w1=0.2731595580682043\n",
      "Gradient Descent(4414/9999): loss=241.83234186977467, w0=0.02533022635971699, w1=0.27313234687459437\n",
      "Gradient Descent(4415/9999): loss=241.78343776248903, w0=0.02532815231542349, w1=0.27310513826151633\n",
      "Gradient Descent(4416/9999): loss=241.7345453508332, w0=0.025326078474702297, w1=0.2730779322287176\n",
      "Gradient Descent(4417/9999): loss=241.68566463180045, w0=0.025324004837533127, w1=0.2730507287759456\n",
      "Gradient Descent(4418/9999): loss=241.63679560238506, w0=0.02532193140389569, w1=0.2730235279029478\n",
      "Gradient Descent(4419/9999): loss=241.58793825958227, w0=0.02531985817376969, w1=0.27299632960947173\n",
      "Gradient Descent(4420/9999): loss=241.53909260038782, w0=0.025317785147134828, w1=0.2729691338952649\n",
      "Gradient Descent(4421/9999): loss=241.49025862179866, w0=0.025315712323970792, w1=0.27294194076007494\n",
      "Gradient Descent(4422/9999): loss=241.4414363208122, w0=0.025313639704257276, w1=0.27291475020364947\n",
      "Gradient Descent(4423/9999): loss=241.3926256944268, w0=0.02531156728797396, w1=0.27288756222573624\n",
      "Gradient Descent(4424/9999): loss=241.34382673964177, w0=0.025309495075100524, w1=0.2728603768260829\n",
      "Gradient Descent(4425/9999): loss=241.29503945345712, w0=0.02530742306561664, w1=0.2728331940044373\n",
      "Gradient Descent(4426/9999): loss=241.24626383287358, w0=0.025305351259501972, w1=0.2728060137605472\n",
      "Gradient Descent(4427/9999): loss=241.19749987489286, w0=0.025303279656736184, w1=0.2727788360941605\n",
      "Gradient Descent(4428/9999): loss=241.14874757651734, w0=0.025301208257298934, w1=0.27275166100502507\n",
      "Gradient Descent(4429/9999): loss=241.1000069347504, w0=0.025299137061169874, w1=0.2727244884928889\n",
      "Gradient Descent(4430/9999): loss=241.0512779465961, w0=0.025297066068328652, w1=0.2726973185574999\n",
      "Gradient Descent(4431/9999): loss=241.0025606090593, w0=0.025294995278754905, w1=0.2726701511986062\n",
      "Gradient Descent(4432/9999): loss=240.95385491914573, w0=0.025292924692428276, w1=0.27264298641595586\n",
      "Gradient Descent(4433/9999): loss=240.905160873862, w0=0.025290854309328394, w1=0.272615824209297\n",
      "Gradient Descent(4434/9999): loss=240.8564784702154, w0=0.025288784129434885, w1=0.2725886645783777\n",
      "Gradient Descent(4435/9999): loss=240.80780770521426, w0=0.025286714152727367, w1=0.2725615075229463\n",
      "Gradient Descent(4436/9999): loss=240.75914857586736, w0=0.025284644379185466, w1=0.2725343530427509\n",
      "Gradient Descent(4437/9999): loss=240.71050107918447, w0=0.025282574808788788, w1=0.27250720113753996\n",
      "Gradient Descent(4438/9999): loss=240.66186521217637, w0=0.02528050544151694, w1=0.2724800518070617\n",
      "Gradient Descent(4439/9999): loss=240.61324097185434, w0=0.025278436277349523, w1=0.2724529050510645\n",
      "Gradient Descent(4440/9999): loss=240.56462835523078, w0=0.025276367316266136, w1=0.2724257608692968\n",
      "Gradient Descent(4441/9999): loss=240.5160273593185, w0=0.02527429855824637, w1=0.2723986192615071\n",
      "Gradient Descent(4442/9999): loss=240.46743798113158, w0=0.02527223000326981, w1=0.27237148022744384\n",
      "Gradient Descent(4443/9999): loss=240.41886021768462, w0=0.025270161651316042, w1=0.2723443437668556\n",
      "Gradient Descent(4444/9999): loss=240.37029406599316, w0=0.02526809350236464, w1=0.272317209879491\n",
      "Gradient Descent(4445/9999): loss=240.3217395230733, w0=0.02526602555639518, w1=0.2722900785650986\n",
      "Gradient Descent(4446/9999): loss=240.2731965859424, w0=0.025263957813387233, w1=0.27226294982342714\n",
      "Gradient Descent(4447/9999): loss=240.22466525161826, w0=0.025261890273320358, w1=0.2722358236542253\n",
      "Gradient Descent(4448/9999): loss=240.17614551711964, w0=0.02525982293617411, w1=0.27220870005724185\n",
      "Gradient Descent(4449/9999): loss=240.12763737946605, w0=0.02525775580192805, w1=0.27218157903222556\n",
      "Gradient Descent(4450/9999): loss=240.0791408356779, w0=0.025255688870561725, w1=0.27215446057892534\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(4451/9999): loss=240.03065588277636, w0=0.025253622142054673, w1=0.27212734469709\n",
      "Gradient Descent(4452/9999): loss=239.9821825177834, w0=0.02525155561638644, w1=0.2721002313864685\n",
      "Gradient Descent(4453/9999): loss=239.9337207377217, w0=0.02524948929353656, w1=0.27207312064680983\n",
      "Gradient Descent(4454/9999): loss=239.88527053961508, w0=0.025247423173484565, w1=0.272046012477863\n",
      "Gradient Descent(4455/9999): loss=239.83683192048767, w0=0.02524535725620998, w1=0.27201890687937696\n",
      "Gradient Descent(4456/9999): loss=239.78840487736497, w0=0.025243291541692324, w1=0.27199180385110094\n",
      "Gradient Descent(4457/9999): loss=239.73998940727284, w0=0.025241226029911117, w1=0.271964703392784\n",
      "Gradient Descent(4458/9999): loss=239.6915855072381, w0=0.025239160720845866, w1=0.27193760550417534\n",
      "Gradient Descent(4459/9999): loss=239.64319317428843, w0=0.025237095614476082, w1=0.27191051018502416\n",
      "Gradient Descent(4460/9999): loss=239.59481240545242, w0=0.02523503071078127, w1=0.2718834174350797\n",
      "Gradient Descent(4461/9999): loss=239.54644319775915, w0=0.025232966009740926, w1=0.2718563272540913\n",
      "Gradient Descent(4462/9999): loss=239.49808554823875, w0=0.025230901511334548, w1=0.2718292396418083\n",
      "Gradient Descent(4463/9999): loss=239.44973945392204, w0=0.025228837215541623, w1=0.27180215459798\n",
      "Gradient Descent(4464/9999): loss=239.4014049118408, w0=0.025226773122341634, w1=0.27177507212235597\n",
      "Gradient Descent(4465/9999): loss=239.35308191902752, w0=0.025224709231714063, w1=0.27174799221468554\n",
      "Gradient Descent(4466/9999): loss=239.30477047251537, w0=0.02522264554363839, w1=0.2717209148747183\n",
      "Gradient Descent(4467/9999): loss=239.25647056933857, w0=0.025220582058094083, w1=0.27169384010220377\n",
      "Gradient Descent(4468/9999): loss=239.20818220653206, w0=0.02521851877506061, w1=0.27166676789689154\n",
      "Gradient Descent(4469/9999): loss=239.15990538113155, w0=0.02521645569451744, w1=0.27163969825853124\n",
      "Gradient Descent(4470/9999): loss=239.1116400901734, w0=0.025214392816444026, w1=0.2716126311868725\n",
      "Gradient Descent(4471/9999): loss=239.06338633069518, w0=0.025212330140819826, w1=0.2715855666816651\n",
      "Gradient Descent(4472/9999): loss=239.01514409973493, w0=0.02521026766762429, w1=0.27155850474265875\n",
      "Gradient Descent(4473/9999): loss=238.96691339433153, w0=0.025208205396836863, w1=0.2715314453696032\n",
      "Gradient Descent(4474/9999): loss=238.91869421152484, w0=0.02520614332843699, w1=0.2715043885622484\n",
      "Gradient Descent(4475/9999): loss=238.87048654835525, w0=0.025204081462404104, w1=0.2714773343203441\n",
      "Gradient Descent(4476/9999): loss=238.82229040186425, w0=0.025202019798717643, w1=0.2714502826436403\n",
      "Gradient Descent(4477/9999): loss=238.774105769094, w0=0.025199958337357036, w1=0.2714232335318869\n",
      "Gradient Descent(4478/9999): loss=238.72593264708738, w0=0.025197897078301706, w1=0.2713961869848339\n",
      "Gradient Descent(4479/9999): loss=238.67777103288822, w0=0.025195836021531078, w1=0.27136914300223136\n",
      "Gradient Descent(4480/9999): loss=238.6296209235412, w0=0.025193775167024566, w1=0.2713421015838293\n",
      "Gradient Descent(4481/9999): loss=238.5814823160915, w0=0.025191714514761585, w1=0.27131506272937794\n",
      "Gradient Descent(4482/9999): loss=238.53335520758532, w0=0.025189654064721543, w1=0.27128802643862737\n",
      "Gradient Descent(4483/9999): loss=238.48523959506983, w0=0.025187593816883843, w1=0.27126099271132775\n",
      "Gradient Descent(4484/9999): loss=238.43713547559264, w0=0.025185533771227887, w1=0.2712339615472294\n",
      "Gradient Descent(4485/9999): loss=238.3890428462025, w0=0.02518347392773307, w1=0.2712069329460825\n",
      "Gradient Descent(4486/9999): loss=238.3409617039487, w0=0.025181414286378788, w1=0.27117990690763744\n",
      "Gradient Descent(4487/9999): loss=238.29289204588127, w0=0.02517935484714443, w1=0.27115288343164456\n",
      "Gradient Descent(4488/9999): loss=238.24483386905138, w0=0.025177295610009375, w1=0.27112586251785425\n",
      "Gradient Descent(4489/9999): loss=238.19678717051104, w0=0.02517523657495301, w1=0.27109884416601693\n",
      "Gradient Descent(4490/9999): loss=238.14875194731243, w0=0.025173177741954704, w1=0.27107182837588306\n",
      "Gradient Descent(4491/9999): loss=238.10072819650915, w0=0.025171119110993838, w1=0.2710448151472032\n",
      "Gradient Descent(4492/9999): loss=238.05271591515543, w0=0.025169060682049778, w1=0.27101780447972784\n",
      "Gradient Descent(4493/9999): loss=238.00471510030624, w0=0.025167002455101888, w1=0.27099079637320767\n",
      "Gradient Descent(4494/9999): loss=237.9567257490174, w0=0.02516494443012953, w1=0.27096379082739325\n",
      "Gradient Descent(4495/9999): loss=237.90874785834552, w0=0.02516288660711206, w1=0.2709367878420353\n",
      "Gradient Descent(4496/9999): loss=237.86078142534788, w0=0.025160828986028835, w1=0.2709097874168845\n",
      "Gradient Descent(4497/9999): loss=237.81282644708293, w0=0.025158771566859202, w1=0.2708827895516916\n",
      "Gradient Descent(4498/9999): loss=237.76488292060944, w0=0.025156714349582505, w1=0.27085579424620737\n",
      "Gradient Descent(4499/9999): loss=237.7169508429872, w0=0.02515465733417809, w1=0.2708288015001827\n",
      "Gradient Descent(4500/9999): loss=237.669030211277, w0=0.025152600520625294, w1=0.2708018113133684\n",
      "Gradient Descent(4501/9999): loss=237.6211210225401, w0=0.02515054390890345, w1=0.2707748236855154\n",
      "Gradient Descent(4502/9999): loss=237.57322327383878, w0=0.025148487498991887, w1=0.2707478386163747\n",
      "Gradient Descent(4503/9999): loss=237.52533696223597, w0=0.025146431290869936, w1=0.27072085610569724\n",
      "Gradient Descent(4504/9999): loss=237.4774620847955, w0=0.025144375284516917, w1=0.27069387615323404\n",
      "Gradient Descent(4505/9999): loss=237.42959863858187, w0=0.02514231947991215, w1=0.2706668987587362\n",
      "Gradient Descent(4506/9999): loss=237.38174662066066, w0=0.025140263877034955, w1=0.27063992392195474\n",
      "Gradient Descent(4507/9999): loss=237.33390602809789, w0=0.02513820847586464, w1=0.27061295164264093\n",
      "Gradient Descent(4508/9999): loss=237.28607685796067, w0=0.025136153276380517, w1=0.2705859819205459\n",
      "Gradient Descent(4509/9999): loss=237.23825910731668, w0=0.025134098278561886, w1=0.2705590147554208\n",
      "Gradient Descent(4510/9999): loss=237.19045277323445, w0=0.025132043482388053, w1=0.270532050147017\n",
      "Gradient Descent(4511/9999): loss=237.14265785278354, w0=0.025129988887838312, w1=0.27050508809508567\n",
      "Gradient Descent(4512/9999): loss=237.0948743430338, w0=0.025127934494891962, w1=0.27047812859937825\n",
      "Gradient Descent(4513/9999): loss=237.04710224105665, w0=0.02512588030352829, w1=0.2704511716596461\n",
      "Gradient Descent(4514/9999): loss=236.99934154392352, w0=0.025123826313726585, w1=0.27042421727564064\n",
      "Gradient Descent(4515/9999): loss=236.9515922487071, w0=0.02512177252546613, w1=0.2703972654471133\n",
      "Gradient Descent(4516/9999): loss=236.90385435248066, w0=0.025119718938726204, w1=0.2703703161738155\n",
      "Gradient Descent(4517/9999): loss=236.85612785231845, w0=0.025117665553486084, w1=0.2703433694554989\n",
      "Gradient Descent(4518/9999): loss=236.8084127452953, w0=0.025115612369725048, w1=0.270316425291915\n",
      "Gradient Descent(4519/9999): loss=236.76070902848696, w0=0.025113559387422362, w1=0.27028948368281547\n",
      "Gradient Descent(4520/9999): loss=236.71301669897005, w0=0.02511150660655729, w1=0.27026254462795185\n",
      "Gradient Descent(4521/9999): loss=236.66533575382186, w0=0.025109454027109096, w1=0.27023560812707587\n",
      "Gradient Descent(4522/9999): loss=236.6176661901205, w0=0.025107401649057042, w1=0.27020867417993927\n",
      "Gradient Descent(4523/9999): loss=236.57000800494487, w0=0.025105349472380384, w1=0.2701817427862938\n",
      "Gradient Descent(4524/9999): loss=236.52236119537477, w0=0.02510329749705837, w1=0.27015481394589125\n",
      "Gradient Descent(4525/9999): loss=236.47472575849054, w0=0.025101245723070254, w1=0.27012788765848345\n",
      "Gradient Descent(4526/9999): loss=236.42710169137362, w0=0.02509919415039528, w1=0.27010096392382227\n",
      "Gradient Descent(4527/9999): loss=236.37948899110603, w0=0.025097142779012694, w1=0.27007404274165964\n",
      "Gradient Descent(4528/9999): loss=236.33188765477058, w0=0.02509509160890173, w1=0.27004712411174747\n",
      "Gradient Descent(4529/9999): loss=236.28429767945102, w0=0.025093040640041628, w1=0.2700202080338378\n",
      "Gradient Descent(4530/9999): loss=236.23671906223186, w0=0.02509098987241162, w1=0.2699932945076826\n",
      "Gradient Descent(4531/9999): loss=236.18915180019832, w0=0.025088939305990935, w1=0.269966383533034\n",
      "Gradient Descent(4532/9999): loss=236.14159589043632, w0=0.0250868889407588, w1=0.269939475109644\n",
      "Gradient Descent(4533/9999): loss=236.09405133003295, w0=0.025084838776694436, w1=0.2699125692372648\n",
      "Gradient Descent(4534/9999): loss=236.04651811607565, w0=0.025082788813777065, w1=0.2698856659156486\n",
      "Gradient Descent(4535/9999): loss=235.99899624565282, w0=0.025080739051985903, w1=0.2698587651445476\n",
      "Gradient Descent(4536/9999): loss=235.9514857158538, w0=0.02507868949130016, w1=0.269831866923714\n",
      "Gradient Descent(4537/9999): loss=235.9039865237685, w0=0.025076640131699055, w1=0.26980497125290015\n",
      "Gradient Descent(4538/9999): loss=235.85649866648774, w0=0.025074590973161788, w1=0.26977807813185833\n",
      "Gradient Descent(4539/9999): loss=235.80902214110313, w0=0.025072542015667568, w1=0.26975118756034094\n",
      "Gradient Descent(4540/9999): loss=235.761556944707, w0=0.02507049325919559, w1=0.26972429953810034\n",
      "Gradient Descent(4541/9999): loss=235.7141030743925, w0=0.02506844470372506, w1=0.269697414064889\n",
      "Gradient Descent(4542/9999): loss=235.66666052725364, w0=0.025066396349235166, w1=0.26967053114045936\n",
      "Gradient Descent(4543/9999): loss=235.61922930038511, w0=0.0250643481957051, w1=0.269643650764564\n",
      "Gradient Descent(4544/9999): loss=235.57180939088246, w0=0.025062300243114052, w1=0.26961677293695535\n",
      "Gradient Descent(4545/9999): loss=235.52440079584198, w0=0.02506025249144121, w1=0.2695898976573861\n",
      "Gradient Descent(4546/9999): loss=235.47700351236082, w0=0.02505820494066575, w1=0.26956302492560885\n",
      "Gradient Descent(4547/9999): loss=235.42961753753676, w0=0.02505615759076686, w1=0.2695361547413762\n",
      "Gradient Descent(4548/9999): loss=235.3822428684686, w0=0.02505411044172371, w1=0.26950928710444094\n",
      "Gradient Descent(4549/9999): loss=235.33487950225586, w0=0.025052063493515478, w1=0.26948242201455574\n",
      "Gradient Descent(4550/9999): loss=235.28752743599858, w0=0.025050016746121333, w1=0.26945555947147337\n",
      "Gradient Descent(4551/9999): loss=235.24018666679794, w0=0.02504797019952044, w1=0.26942869947494663\n",
      "Gradient Descent(4552/9999): loss=235.19285719175588, w0=0.025045923853691966, w1=0.2694018420247284\n",
      "Gradient Descent(4553/9999): loss=235.14553900797483, w0=0.025043877708615072, w1=0.26937498712057156\n",
      "Gradient Descent(4554/9999): loss=235.0982321125582, w0=0.02504183176426892, w1=0.269348134762229\n",
      "Gradient Descent(4555/9999): loss=235.05093650261034, w0=0.025039786020632664, w1=0.26932128494945373\n",
      "Gradient Descent(4556/9999): loss=235.00365217523603, w0=0.025037740477685456, w1=0.2692944376819987\n",
      "Gradient Descent(4557/9999): loss=234.9563791275412, w0=0.025035695135406447, w1=0.2692675929596169\n",
      "Gradient Descent(4558/9999): loss=234.9091173566323, w0=0.025033649993774786, w1=0.26924075078206144\n",
      "Gradient Descent(4559/9999): loss=234.86186685961673, w0=0.025031605052769614, w1=0.2692139111490854\n",
      "Gradient Descent(4560/9999): loss=234.81462763360256, w0=0.025029560312370077, w1=0.2691870740604419\n",
      "Gradient Descent(4561/9999): loss=234.76739967569864, w0=0.02502751577255531, w1=0.2691602395158842\n",
      "Gradient Descent(4562/9999): loss=234.72018298301484, w0=0.025025471433304452, w1=0.2691334075151654\n",
      "Gradient Descent(4563/9999): loss=234.6729775526614, w0=0.025023427294596636, w1=0.2691065780580388\n",
      "Gradient Descent(4564/9999): loss=234.6257833817497, w0=0.025021383356410992, w1=0.26907975114425764\n",
      "Gradient Descent(4565/9999): loss=234.5786004673918, w0=0.02501933961872665, w1=0.2690529267735753\n",
      "Gradient Descent(4566/9999): loss=234.53142880670043, w0=0.025017296081522734, w1=0.2690261049457451\n",
      "Gradient Descent(4567/9999): loss=234.4842683967893, w0=0.025015252744778366, w1=0.26899928566052045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(4568/9999): loss=234.43711923477278, w0=0.025013209608472666, w1=0.26897246891765475\n",
      "Gradient Descent(4569/9999): loss=234.38998131776592, w0=0.02501116667258475, w1=0.2689456547169015\n",
      "Gradient Descent(4570/9999): loss=234.34285464288487, w0=0.025009123937093734, w1=0.26891884305801417\n",
      "Gradient Descent(4571/9999): loss=234.2957392072462, w0=0.02500708140197873, w1=0.2688920339407463\n",
      "Gradient Descent(4572/9999): loss=234.24863500796758, w0=0.025005039067218845, w1=0.2688652273648514\n",
      "Gradient Descent(4573/9999): loss=234.2015420421672, w0=0.025002996932793186, w1=0.2688384233300832\n",
      "Gradient Descent(4574/9999): loss=234.15446030696418, w0=0.02500095499868086, w1=0.2688116218361952\n",
      "Gradient Descent(4575/9999): loss=234.10738979947848, w0=0.024998913264860964, w1=0.26878482288294125\n",
      "Gradient Descent(4576/9999): loss=234.06033051683056, w0=0.0249968717313126, w1=0.2687580264700749\n",
      "Gradient Descent(4577/9999): loss=234.0132824561421, w0=0.024994830398014863, w1=0.26873123259735\n",
      "Gradient Descent(4578/9999): loss=233.966245614535, w0=0.02499278926494685, w1=0.26870444126452026\n",
      "Gradient Descent(4579/9999): loss=233.91921998913253, w0=0.02499074833208765, w1=0.26867765247133957\n",
      "Gradient Descent(4580/9999): loss=233.87220557705825, w0=0.024988707599416345, w1=0.26865086621756173\n",
      "Gradient Descent(4581/9999): loss=233.82520237543687, w0=0.024986667066912032, w1=0.2686240825029407\n",
      "Gradient Descent(4582/9999): loss=233.7782103813938, w0=0.02498462673455379, w1=0.2685973013272303\n",
      "Gradient Descent(4583/9999): loss=233.7312295920548, w0=0.024982586602320694, w1=0.2685705226901846\n",
      "Gradient Descent(4584/9999): loss=233.68426000454707, w0=0.024980546670191832, w1=0.26854374659155755\n",
      "Gradient Descent(4585/9999): loss=233.63730161599824, w0=0.024978506938146273, w1=0.26851697303110317\n",
      "Gradient Descent(4586/9999): loss=233.5903544235367, w0=0.024976467406163095, w1=0.26849020200857554\n",
      "Gradient Descent(4587/9999): loss=233.5434184242917, w0=0.02497442807422137, w1=0.26846343352372876\n",
      "Gradient Descent(4588/9999): loss=233.49649361539326, w0=0.02497238894230016, w1=0.268436667576317\n",
      "Gradient Descent(4589/9999): loss=233.4495799939721, w0=0.02497035001037854, w1=0.2684099041660944\n",
      "Gradient Descent(4590/9999): loss=233.40267755715988, w0=0.02496831127843557, w1=0.2683831432928152\n",
      "Gradient Descent(4591/9999): loss=233.35578630208892, w0=0.024966272746450312, w1=0.2683563849562336\n",
      "Gradient Descent(4592/9999): loss=233.30890622589231, w0=0.024964234414401824, w1=0.2683296291561039\n",
      "Gradient Descent(4593/9999): loss=233.26203732570391, w0=0.024962196282269165, w1=0.26830287589218044\n",
      "Gradient Descent(4594/9999): loss=233.21517959865858, w0=0.02496015835003139, w1=0.26827612516421756\n",
      "Gradient Descent(4595/9999): loss=233.16833304189157, w0=0.02495812061766755, w1=0.2682493769719696\n",
      "Gradient Descent(4596/9999): loss=233.1214976525392, w0=0.024956083085156695, w1=0.26822263131519103\n",
      "Gradient Descent(4597/9999): loss=233.0746734277384, w0=0.024954045752477873, w1=0.26819588819363627\n",
      "Gradient Descent(4598/9999): loss=233.02786036462712, w0=0.02495200861961013, w1=0.26816914760705984\n",
      "Gradient Descent(4599/9999): loss=232.98105846034383, w0=0.024949971686532506, w1=0.26814240955521623\n",
      "Gradient Descent(4600/9999): loss=232.93426771202786, w0=0.024947934953224046, w1=0.26811567403786\n",
      "Gradient Descent(4601/9999): loss=232.88748811681924, w0=0.024945898419663787, w1=0.2680889410547458\n",
      "Gradient Descent(4602/9999): loss=232.84071967185903, w0=0.024943862085830767, w1=0.2680622106056282\n",
      "Gradient Descent(4603/9999): loss=232.7939623742887, w0=0.02494182595170402, w1=0.2680354826902619\n",
      "Gradient Descent(4604/9999): loss=232.7472162212508, w0=0.024939790017262577, w1=0.26800875730840157\n",
      "Gradient Descent(4605/9999): loss=232.7004812098886, w0=0.02493775428248547, w1=0.26798203445980195\n",
      "Gradient Descent(4606/9999): loss=232.65375733734612, w0=0.02493571874735172, w1=0.2679553141442178\n",
      "Gradient Descent(4607/9999): loss=232.60704460076792, w0=0.02493368341184036, w1=0.2679285963614039\n",
      "Gradient Descent(4608/9999): loss=232.56034299729953, w0=0.024931648275930408, w1=0.2679018811111152\n",
      "Gradient Descent(4609/9999): loss=232.5136525240875, w0=0.02492961333960089, w1=0.26787516839310643\n",
      "Gradient Descent(4610/9999): loss=232.46697317827875, w0=0.024927578602830822, w1=0.26784845820713254\n",
      "Gradient Descent(4611/9999): loss=232.42030495702133, w0=0.024925544065599225, w1=0.2678217505529485\n",
      "Gradient Descent(4612/9999): loss=232.37364785746365, w0=0.024923509727885106, w1=0.26779504543030924\n",
      "Gradient Descent(4613/9999): loss=232.32700187675516, w0=0.024921475589667483, w1=0.2677683428389698\n",
      "Gradient Descent(4614/9999): loss=232.280367012046, w0=0.024919441650925366, w1=0.2677416427786852\n",
      "Gradient Descent(4615/9999): loss=232.23374326048727, w0=0.024917407911637768, w1=0.26771494524921047\n",
      "Gradient Descent(4616/9999): loss=232.1871306192306, w0=0.024915374371783688, w1=0.2676882502503008\n",
      "Gradient Descent(4617/9999): loss=232.14052908542845, w0=0.024913341031342136, w1=0.2676615577817113\n",
      "Gradient Descent(4618/9999): loss=232.09393865623414, w0=0.02491130789029211, w1=0.2676348678431972\n",
      "Gradient Descent(4619/9999): loss=232.04735932880186, w0=0.024909274948612618, w1=0.2676081804345136\n",
      "Gradient Descent(4620/9999): loss=232.00079110028614, w0=0.02490724220628265, w1=0.2675814955554159\n",
      "Gradient Descent(4621/9999): loss=231.95423396784273, w0=0.024905209663281205, w1=0.2675548132056592\n",
      "Gradient Descent(4622/9999): loss=231.90768792862792, w0=0.024903177319587282, w1=0.2675281333849989\n",
      "Gradient Descent(4623/9999): loss=231.86115297979882, w0=0.02490114517517987, w1=0.2675014560931904\n",
      "Gradient Descent(4624/9999): loss=231.81462911851332, w0=0.02489911323003796, w1=0.26747478132998903\n",
      "Gradient Descent(4625/9999): loss=231.76811634193027, w0=0.024897081484140545, w1=0.2674481090951502\n",
      "Gradient Descent(4626/9999): loss=231.72161464720878, w0=0.024895049937466607, w1=0.2674214393884294\n",
      "Gradient Descent(4627/9999): loss=231.67512403150937, w0=0.024893018589995132, w1=0.267394772209582\n",
      "Gradient Descent(4628/9999): loss=231.6286444919929, w0=0.024890987441705103, w1=0.26736810755836365\n",
      "Gradient Descent(4629/9999): loss=231.582176025821, w0=0.0248889564925755, w1=0.2673414454345298\n",
      "Gradient Descent(4630/9999): loss=231.5357186301563, w0=0.024886925742585306, w1=0.2673147858378361\n",
      "Gradient Descent(4631/9999): loss=231.4892723021621, w0=0.024884895191713496, w1=0.2672881287680381\n",
      "Gradient Descent(4632/9999): loss=231.44283703900237, w0=0.024882864839939046, w1=0.26726147422489155\n",
      "Gradient Descent(4633/9999): loss=231.39641283784206, w0=0.02488083468724093, w1=0.26723482220815203\n",
      "Gradient Descent(4634/9999): loss=231.34999969584663, w0=0.024878804733598117, w1=0.26720817271757535\n",
      "Gradient Descent(4635/9999): loss=231.3035976101824, w0=0.02487677497898958, w1=0.2671815257529172\n",
      "Gradient Descent(4636/9999): loss=231.25720657801668, w0=0.02487474542339429, w1=0.2671548813139334\n",
      "Gradient Descent(4637/9999): loss=231.21082659651722, w0=0.024872716066791208, w1=0.2671282394003797\n",
      "Gradient Descent(4638/9999): loss=231.16445766285278, w0=0.024870686909159303, w1=0.26710160001201205\n",
      "Gradient Descent(4639/9999): loss=231.11809977419264, w0=0.024868657950477536, w1=0.26707496314858625\n",
      "Gradient Descent(4640/9999): loss=231.07175292770714, w0=0.024866629190724868, w1=0.2670483288098583\n",
      "Gradient Descent(4641/9999): loss=231.02541712056725, w0=0.02486460062988026, w1=0.2670216969955841\n",
      "Gradient Descent(4642/9999): loss=230.9790923499445, w0=0.02486257226792267, w1=0.2669950677055196\n",
      "Gradient Descent(4643/9999): loss=230.9327786130117, w0=0.024860544104831052, w1=0.2669684409394209\n",
      "Gradient Descent(4644/9999): loss=230.88647590694183, w0=0.02485851614058436, w1=0.266941816697044\n",
      "Gradient Descent(4645/9999): loss=230.84018422890898, w0=0.02485648837516155, w1=0.266915194978145\n",
      "Gradient Descent(4646/9999): loss=230.79390357608807, w0=0.024854460808541572, w1=0.26688857578248\n",
      "Gradient Descent(4647/9999): loss=230.74763394565454, w0=0.024852433440703373, w1=0.26686195910980515\n",
      "Gradient Descent(4648/9999): loss=230.70137533478467, w0=0.024850406271625904, w1=0.26683534495987665\n",
      "Gradient Descent(4649/9999): loss=230.65512774065576, w0=0.02484837930128811, w1=0.2668087333324507\n",
      "Gradient Descent(4650/9999): loss=230.6088911604455, w0=0.024846352529668932, w1=0.26678212422728353\n",
      "Gradient Descent(4651/9999): loss=230.56266559133246, w0=0.024844325956747317, w1=0.26675551764413147\n",
      "Gradient Descent(4652/9999): loss=230.51645103049614, w0=0.024842299582502204, w1=0.2667289135827508\n",
      "Gradient Descent(4653/9999): loss=230.4702474751167, w0=0.024840273406912534, w1=0.26670231204289785\n",
      "Gradient Descent(4654/9999): loss=230.424054922375, w0=0.024838247429957244, w1=0.2666757130243291\n",
      "Gradient Descent(4655/9999): loss=230.37787336945266, w0=0.02483622165161527, w1=0.2666491165268008\n",
      "Gradient Descent(4656/9999): loss=230.33170281353222, w0=0.02483419607186555, w1=0.2666225225500695\n",
      "Gradient Descent(4657/9999): loss=230.28554325179692, w0=0.024832170690687015, w1=0.2665959310938917\n",
      "Gradient Descent(4658/9999): loss=230.23939468143044, w0=0.024830145508058593, w1=0.26656934215802386\n",
      "Gradient Descent(4659/9999): loss=230.19325709961785, w0=0.02482812052395922, w1=0.2665427557422225\n",
      "Gradient Descent(4660/9999): loss=230.14713050354442, w0=0.02482609573836782, w1=0.26651617184624427\n",
      "Gradient Descent(4661/9999): loss=230.10101489039656, w0=0.024824071151263328, w1=0.2664895904698457\n",
      "Gradient Descent(4662/9999): loss=230.0549102573612, w0=0.02482204676262466, w1=0.2664630116127835\n",
      "Gradient Descent(4663/9999): loss=230.00881660162622, w0=0.02482002257243075, w1=0.2664364352748143\n",
      "Gradient Descent(4664/9999): loss=229.96273392037992, w0=0.024817998580660514, w1=0.26640986145569484\n",
      "Gradient Descent(4665/9999): loss=229.91666221081186, w0=0.024815974787292874, w1=0.2663832901551818\n",
      "Gradient Descent(4666/9999): loss=229.87060147011198, w0=0.02481395119230675, w1=0.266356721373032\n",
      "Gradient Descent(4667/9999): loss=229.82455169547103, w0=0.024811927795681067, w1=0.26633015510900226\n",
      "Gradient Descent(4668/9999): loss=229.77851288408078, w0=0.02480990459739473, w1=0.26630359136284937\n",
      "Gradient Descent(4669/9999): loss=229.73248503313334, w0=0.024807881597426664, w1=0.2662770301343302\n",
      "Gradient Descent(4670/9999): loss=229.6864681398221, w0=0.02480585879575578, w1=0.26625047142320163\n",
      "Gradient Descent(4671/9999): loss=229.6404622013408, w0=0.02480383619236099, w1=0.2662239152292207\n",
      "Gradient Descent(4672/9999): loss=229.59446721488393, w0=0.024801813787221207, w1=0.26619736155214424\n",
      "Gradient Descent(4673/9999): loss=229.54848317764709, w0=0.02479979158031534, w1=0.2661708103917293\n",
      "Gradient Descent(4674/9999): loss=229.5025100868263, w0=0.0247977695716223, w1=0.2661442617477329\n",
      "Gradient Descent(4675/9999): loss=229.45654793961845, w0=0.02479574776112099, w1=0.26611771561991215\n",
      "Gradient Descent(4676/9999): loss=229.41059673322133, w0=0.024793726148790315, w1=0.26609117200802407\n",
      "Gradient Descent(4677/9999): loss=229.36465646483322, w0=0.024791704734609187, w1=0.26606463091182586\n",
      "Gradient Descent(4678/9999): loss=229.31872713165336, w0=0.024789683518556503, w1=0.2660380923310746\n",
      "Gradient Descent(4679/9999): loss=229.27280873088168, w0=0.02478766250061117, w1=0.2660115562655275\n",
      "Gradient Descent(4680/9999): loss=229.2269012597189, w0=0.024785641680752084, w1=0.26598502271494184\n",
      "Gradient Descent(4681/9999): loss=229.1810047153665, w0=0.024783621058958146, w1=0.2659584916790748\n",
      "Gradient Descent(4682/9999): loss=229.13511909502662, w0=0.024781600635208255, w1=0.2659319631576837\n",
      "Gradient Descent(4683/9999): loss=229.08924439590226, w0=0.024779580409481305, w1=0.26590543715052584\n",
      "Gradient Descent(4684/9999): loss=229.04338061519712, w0=0.024777560381756197, w1=0.26587891365735855\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(4685/9999): loss=228.99752775011586, w0=0.02477554055201182, w1=0.26585239267793925\n",
      "Gradient Descent(4686/9999): loss=228.95168579786346, w0=0.024773520920227072, w1=0.2658258742120253\n",
      "Gradient Descent(4687/9999): loss=228.90585475564615, w0=0.02477150148638084, w1=0.2657993582593742\n",
      "Gradient Descent(4688/9999): loss=228.86003462067052, w0=0.024769482250452016, w1=0.26577284481974334\n",
      "Gradient Descent(4689/9999): loss=228.8142253901442, w0=0.02476746321241949, w1=0.2657463338928903\n",
      "Gradient Descent(4690/9999): loss=228.76842706127536, w0=0.024765444372262156, w1=0.26571982547857265\n",
      "Gradient Descent(4691/9999): loss=228.72263963127315, w0=0.024763425729958894, w1=0.26569331957654785\n",
      "Gradient Descent(4692/9999): loss=228.6768630973472, w0=0.024761407285488594, w1=0.26566681618657356\n",
      "Gradient Descent(4693/9999): loss=228.63109745670818, w0=0.024759389038830137, w1=0.2656403153084074\n",
      "Gradient Descent(4694/9999): loss=228.58534270656725, w0=0.024757370989962408, w1=0.2656138169418071\n",
      "Gradient Descent(4695/9999): loss=228.53959884413658, w0=0.024755353138864292, w1=0.26558732108653027\n",
      "Gradient Descent(4696/9999): loss=228.49386586662897, w0=0.024753335485514668, w1=0.2655608277423347\n",
      "Gradient Descent(4697/9999): loss=228.44814377125792, w0=0.024751318029892416, w1=0.2655343369089781\n",
      "Gradient Descent(4698/9999): loss=228.4024325552377, w0=0.02474930077197642, w1=0.26550784858621823\n",
      "Gradient Descent(4699/9999): loss=228.35673221578347, w0=0.024747283711745546, w1=0.26548136277381296\n",
      "Gradient Descent(4700/9999): loss=228.31104275011094, w0=0.024745266849178684, w1=0.26545487947152013\n",
      "Gradient Descent(4701/9999): loss=228.26536415543677, w0=0.024743250184254703, w1=0.2654283986790976\n",
      "Gradient Descent(4702/9999): loss=228.2196964289782, w0=0.024741233716952478, w1=0.2654019203963034\n",
      "Gradient Descent(4703/9999): loss=228.17403956795346, w0=0.024739217447250886, w1=0.2653754446228953\n",
      "Gradient Descent(4704/9999): loss=228.12839356958128, w0=0.024737201375128796, w1=0.2653489713586314\n",
      "Gradient Descent(4705/9999): loss=228.08275843108126, w0=0.024735185500565082, w1=0.2653225006032697\n",
      "Gradient Descent(4706/9999): loss=228.03713414967368, w0=0.024733169823538615, w1=0.2652960323565681\n",
      "Gradient Descent(4707/9999): loss=227.99152072257968, w0=0.024731154344028266, w1=0.2652695666182849\n",
      "Gradient Descent(4708/9999): loss=227.94591814702116, w0=0.0247291390620129, w1=0.265243103388178\n",
      "Gradient Descent(4709/9999): loss=227.90032642022064, w0=0.024727123977471385, w1=0.26521664266600564\n",
      "Gradient Descent(4710/9999): loss=227.85474553940153, w0=0.024725109090382592, w1=0.2651901844515259\n",
      "Gradient Descent(4711/9999): loss=227.80917550178782, w0=0.02472309440072538, w1=0.2651637287444971\n",
      "Gradient Descent(4712/9999): loss=227.76361630460445, w0=0.024721079908478622, w1=0.26513727554467736\n",
      "Gradient Descent(4713/9999): loss=227.7180679450769, w0=0.024719065613621173, w1=0.26511082485182497\n",
      "Gradient Descent(4714/9999): loss=227.67253042043168, w0=0.0247170515161319, w1=0.2650843766656982\n",
      "Gradient Descent(4715/9999): loss=227.62700372789587, w0=0.024715037615989668, w1=0.26505793098605535\n",
      "Gradient Descent(4716/9999): loss=227.58148786469727, w0=0.024713023913173333, w1=0.26503148781265484\n",
      "Gradient Descent(4717/9999): loss=227.53598282806442, w0=0.024711010407661757, w1=0.26500504714525497\n",
      "Gradient Descent(4718/9999): loss=227.49048861522672, w0=0.024708997099433798, w1=0.2649786089836142\n",
      "Gradient Descent(4719/9999): loss=227.4450052234144, w0=0.024706983988468316, w1=0.26495217332749094\n",
      "Gradient Descent(4720/9999): loss=227.3995326498581, w0=0.024704971074744165, w1=0.26492574017664366\n",
      "Gradient Descent(4721/9999): loss=227.35407089178955, w0=0.024702958358240205, w1=0.2648993095308309\n",
      "Gradient Descent(4722/9999): loss=227.30861994644107, w0=0.02470094583893529, w1=0.2648728813898112\n",
      "Gradient Descent(4723/9999): loss=227.26317981104572, w0=0.024698933516808276, w1=0.26484645575334304\n",
      "Gradient Descent(4724/9999): loss=227.2177504828375, w0=0.024696921391838014, w1=0.2648200326211851\n",
      "Gradient Descent(4725/9999): loss=227.17233195905075, w0=0.024694909464003358, w1=0.2647936119930959\n",
      "Gradient Descent(4726/9999): loss=227.12692423692113, w0=0.02469289773328316, w1=0.26476719386883424\n",
      "Gradient Descent(4727/9999): loss=227.08152731368446, w0=0.024690886199656276, w1=0.2647407782481587\n",
      "Gradient Descent(4728/9999): loss=227.03614118657765, w0=0.02468887486310155, w1=0.2647143651308281\n",
      "Gradient Descent(4729/9999): loss=226.9907658528384, w0=0.024686863723597838, w1=0.26468795451660104\n",
      "Gradient Descent(4730/9999): loss=226.94540130970492, w0=0.02468485278112398, w1=0.26466154640523637\n",
      "Gradient Descent(4731/9999): loss=226.90004755441632, w0=0.024682842035658834, w1=0.26463514079649286\n",
      "Gradient Descent(4732/9999): loss=226.85470458421253, w0=0.02468083148718124, w1=0.2646087376901294\n",
      "Gradient Descent(4733/9999): loss=226.80937239633394, w0=0.024678821135670048, w1=0.26458233708590484\n",
      "Gradient Descent(4734/9999): loss=226.76405098802206, w0=0.0246768109811041, w1=0.26455593898357804\n",
      "Gradient Descent(4735/9999): loss=226.7187403565188, w0=0.024674801023462246, w1=0.264529543382908\n",
      "Gradient Descent(4736/9999): loss=226.67344049906723, w0=0.02467279126272333, w1=0.2645031502836536\n",
      "Gradient Descent(4737/9999): loss=226.6281514129105, w0=0.024670781698866193, w1=0.26447675968557394\n",
      "Gradient Descent(4738/9999): loss=226.58287309529322, w0=0.02466877233186968, w1=0.26445037158842793\n",
      "Gradient Descent(4739/9999): loss=226.53760554346047, w0=0.02466676316171263, w1=0.26442398599197464\n",
      "Gradient Descent(4740/9999): loss=226.49234875465785, w0=0.024664754188373886, w1=0.26439760289597314\n",
      "Gradient Descent(4741/9999): loss=226.44710272613202, w0=0.02466274541183229, w1=0.26437122230018256\n",
      "Gradient Descent(4742/9999): loss=226.40186745513026, w0=0.024660736832066682, w1=0.26434484420436205\n",
      "Gradient Descent(4743/9999): loss=226.35664293890065, w0=0.0246587284490559, w1=0.26431846860827074\n",
      "Gradient Descent(4744/9999): loss=226.31142917469182, w0=0.02465672026277879, w1=0.2642920955116679\n",
      "Gradient Descent(4745/9999): loss=226.26622615975347, w0=0.02465471227321418, w1=0.26426572491431266\n",
      "Gradient Descent(4746/9999): loss=226.2210338913358, w0=0.02465270448034091, w1=0.2642393568159644\n",
      "Gradient Descent(4747/9999): loss=226.17585236668975, w0=0.024650696884137818, w1=0.2642129912163823\n",
      "Gradient Descent(4748/9999): loss=226.13068158306703, w0=0.024648689484583743, w1=0.26418662811532573\n",
      "Gradient Descent(4749/9999): loss=226.0855215377204, w0=0.024646682281657513, w1=0.264160267512554\n",
      "Gradient Descent(4750/9999): loss=226.04037222790285, w0=0.02464467527533797, w1=0.26413390940782655\n",
      "Gradient Descent(4751/9999): loss=225.99523365086839, w0=0.024642668465603944, w1=0.26410755380090273\n",
      "Gradient Descent(4752/9999): loss=225.95010580387182, w0=0.024640661852434274, w1=0.264081200691542\n",
      "Gradient Descent(4753/9999): loss=225.90498868416864, w0=0.024638655435807788, w1=0.26405485007950386\n",
      "Gradient Descent(4754/9999): loss=225.85988228901496, w0=0.024636649215703323, w1=0.26402850196454775\n",
      "Gradient Descent(4755/9999): loss=225.81478661566774, w0=0.02463464319209971, w1=0.2640021563464332\n",
      "Gradient Descent(4756/9999): loss=225.76970166138477, w0=0.024632637364975774, w1=0.2639758132249198\n",
      "Gradient Descent(4757/9999): loss=225.72462742342438, w0=0.024630631734310352, w1=0.26394947259976714\n",
      "Gradient Descent(4758/9999): loss=225.67956389904583, w0=0.024628626300082272, w1=0.26392313447073484\n",
      "Gradient Descent(4759/9999): loss=225.63451108550902, w0=0.024626621062270367, w1=0.2638967988375825\n",
      "Gradient Descent(4760/9999): loss=225.58946898007449, w0=0.024624616020853463, w1=0.26387046570006983\n",
      "Gradient Descent(4761/9999): loss=225.54443758000383, w0=0.02462261117581039, w1=0.2638441350579565\n",
      "Gradient Descent(4762/9999): loss=225.49941688255902, w0=0.024620606527119976, w1=0.2638178069110023\n",
      "Gradient Descent(4763/9999): loss=225.45440688500304, w0=0.02461860207476105, w1=0.26379148125896695\n",
      "Gradient Descent(4764/9999): loss=225.4094075845995, w0=0.02461659781871244, w1=0.2637651581016103\n",
      "Gradient Descent(4765/9999): loss=225.36441897861266, w0=0.02461459375895297, w1=0.2637388374386921\n",
      "Gradient Descent(4766/9999): loss=225.3194410643077, w0=0.02461258989546147, w1=0.2637125192699722\n",
      "Gradient Descent(4767/9999): loss=225.2744738389506, w0=0.024610586228216758, w1=0.2636862035952105\n",
      "Gradient Descent(4768/9999): loss=225.22951729980772, w0=0.024608582757197666, w1=0.263659890414167\n",
      "Gradient Descent(4769/9999): loss=225.18457144414648, w0=0.02460657948238302, w1=0.2636335797266015\n",
      "Gradient Descent(4770/9999): loss=225.13963626923487, w0=0.02460457640375164, w1=0.2636072715322741\n",
      "Gradient Descent(4771/9999): loss=225.09471177234175, w0=0.024602573521282353, w1=0.2635809658309447\n",
      "Gradient Descent(4772/9999): loss=225.04979795073672, w0=0.024600570834953983, w1=0.26355466262237337\n",
      "Gradient Descent(4773/9999): loss=225.0048948016899, w0=0.02459856834474535, w1=0.2635283619063201\n",
      "Gradient Descent(4774/9999): loss=224.9600023224724, w0=0.02459656605063528, w1=0.2635020636825451\n",
      "Gradient Descent(4775/9999): loss=224.9151205103559, w0=0.024594563952602594, w1=0.26347576795080835\n",
      "Gradient Descent(4776/9999): loss=224.87024936261298, w0=0.024592562050626116, w1=0.2634494747108701\n",
      "Gradient Descent(4777/9999): loss=224.8253888765168, w0=0.024590560344684664, w1=0.2634231839624905\n",
      "Gradient Descent(4778/9999): loss=224.78053904934134, w0=0.024588558834757062, w1=0.26339689570542973\n",
      "Gradient Descent(4779/9999): loss=224.73569987836126, w0=0.02458655752082213, w1=0.26337060993944805\n",
      "Gradient Descent(4780/9999): loss=224.69087136085201, w0=0.02458455640285869, w1=0.2633443266643057\n",
      "Gradient Descent(4781/9999): loss=224.64605349408976, w0=0.02458255548084556, w1=0.26331804587976293\n",
      "Gradient Descent(4782/9999): loss=224.60124627535149, w0=0.024580554754761563, w1=0.26329176758558015\n",
      "Gradient Descent(4783/9999): loss=224.5564497019148, w0=0.024578554224585516, w1=0.26326549178151765\n",
      "Gradient Descent(4784/9999): loss=224.511663771058, w0=0.024576553890296237, w1=0.26323921846733583\n",
      "Gradient Descent(4785/9999): loss=224.46688848006028, w0=0.02457455375187255, w1=0.26321294764279507\n",
      "Gradient Descent(4786/9999): loss=224.42212382620158, w0=0.02457255380929327, w1=0.2631866793076558\n",
      "Gradient Descent(4787/9999): loss=224.37736980676232, w0=0.024570554062537213, w1=0.26316041346167857\n",
      "Gradient Descent(4788/9999): loss=224.33262641902385, w0=0.024568554511583202, w1=0.26313415010462377\n",
      "Gradient Descent(4789/9999): loss=224.28789366026828, w0=0.024566555156410052, w1=0.26310788923625195\n",
      "Gradient Descent(4790/9999): loss=224.24317152777837, w0=0.024564555996996577, w1=0.2630816308563237\n",
      "Gradient Descent(4791/9999): loss=224.19846001883775, w0=0.0245625570333216, w1=0.2630553749645996\n",
      "Gradient Descent(4792/9999): loss=224.15375913073055, w0=0.024560558265363935, w1=0.2630291215608402\n",
      "Gradient Descent(4793/9999): loss=224.10906886074173, w0=0.0245585596931024, w1=0.26300287064480615\n",
      "Gradient Descent(4794/9999): loss=224.06438920615713, w0=0.02455656131651581, w1=0.2629766222162581\n",
      "Gradient Descent(4795/9999): loss=224.01972016426325, w0=0.024554563135582984, w1=0.2629503762749568\n",
      "Gradient Descent(4796/9999): loss=223.9750617323471, w0=0.024552565150282733, w1=0.26292413282066296\n",
      "Gradient Descent(4797/9999): loss=223.9304139076968, w0=0.024550567360593877, w1=0.26289789185313733\n",
      "Gradient Descent(4798/9999): loss=223.88577668760084, w0=0.024548569766495228, w1=0.2628716533721407\n",
      "Gradient Descent(4799/9999): loss=223.8411500693488, w0=0.024546572367965605, w1=0.26284541737743383\n",
      "Gradient Descent(4800/9999): loss=223.7965340502306, w0=0.02454457516498382, w1=0.26281918386877756\n",
      "Gradient Descent(4801/9999): loss=223.7519286275372, w0=0.024542578157528694, w1=0.2627929528459328\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(4802/9999): loss=223.70733379856028, w0=0.024540581345579033, w1=0.26276672430866044\n",
      "Gradient Descent(4803/9999): loss=223.66274956059206, w0=0.024538584729113657, w1=0.26274049825672136\n",
      "Gradient Descent(4804/9999): loss=223.61817591092563, w0=0.02453658830811138, w1=0.26271427468987657\n",
      "Gradient Descent(4805/9999): loss=223.5736128468547, w0=0.024534592082551016, w1=0.262688053607887\n",
      "Gradient Descent(4806/9999): loss=223.5290603656738, w0=0.024532596052411378, w1=0.2626618350105137\n",
      "Gradient Descent(4807/9999): loss=223.4845184646784, w0=0.02453060021767128, w1=0.2626356188975177\n",
      "Gradient Descent(4808/9999): loss=223.43998714116418, w0=0.024528604578309538, w1=0.26260940526866\n",
      "Gradient Descent(4809/9999): loss=223.39546639242798, w0=0.024526609134304964, w1=0.2625831941237018\n",
      "Gradient Descent(4810/9999): loss=223.35095621576724, w0=0.02452461388563637, w1=0.26255698546240414\n",
      "Gradient Descent(4811/9999): loss=223.30645660848023, w0=0.024522618832282572, w1=0.26253077928452817\n",
      "Gradient Descent(4812/9999): loss=223.26196756786555, w0=0.02452062397422238, w1=0.26250457558983514\n",
      "Gradient Descent(4813/9999): loss=223.21748909122311, w0=0.024518629311434614, w1=0.2624783743780862\n",
      "Gradient Descent(4814/9999): loss=223.17302117585328, w0=0.02451663484389808, w1=0.26245217564904255\n",
      "Gradient Descent(4815/9999): loss=223.1285638190569, w0=0.02451464057159159, w1=0.26242597940246554\n",
      "Gradient Descent(4816/9999): loss=223.08411701813597, w0=0.024512646494493966, w1=0.26239978563811645\n",
      "Gradient Descent(4817/9999): loss=223.0396807703931, w0=0.024510652612584012, w1=0.2623735943557565\n",
      "Gradient Descent(4818/9999): loss=222.99525507313135, w0=0.024508658925840546, w1=0.2623474055551472\n",
      "Gradient Descent(4819/9999): loss=222.95083992365497, w0=0.02450666543424238, w1=0.2623212192360498\n",
      "Gradient Descent(4820/9999): loss=222.90643531926852, w0=0.024504672137768323, w1=0.26229503539822574\n",
      "Gradient Descent(4821/9999): loss=222.8620412572776, w0=0.024502679036397194, w1=0.26226885404143646\n",
      "Gradient Descent(4822/9999): loss=222.8176577349883, w0=0.024500686130107804, w1=0.2622426751654434\n",
      "Gradient Descent(4823/9999): loss=222.77328474970756, w0=0.02449869341887896, w1=0.2622164987700081\n",
      "Gradient Descent(4824/9999): loss=222.728922298743, w0=0.024496700902689482, w1=0.26219032485489197\n",
      "Gradient Descent(4825/9999): loss=222.68457037940297, w0=0.024494708581518178, w1=0.26216415341985666\n",
      "Gradient Descent(4826/9999): loss=222.64022898899668, w0=0.024492716455343864, w1=0.2621379844646637\n",
      "Gradient Descent(4827/9999): loss=222.59589812483384, w0=0.02449072452414535, w1=0.2621118179890747\n",
      "Gradient Descent(4828/9999): loss=222.5515777842251, w0=0.02448873278790145, w1=0.2620856539928513\n",
      "Gradient Descent(4829/9999): loss=222.50726796448166, w0=0.024486741246590976, w1=0.2620594924757551\n",
      "Gradient Descent(4830/9999): loss=222.46296866291559, w0=0.02448474990019274, w1=0.2620333334375478\n",
      "Gradient Descent(4831/9999): loss=222.41867987683946, w0=0.024482758748685555, w1=0.2620071768779912\n",
      "Gradient Descent(4832/9999): loss=222.37440160356695, w0=0.024480767792048233, w1=0.26198102279684693\n",
      "Gradient Descent(4833/9999): loss=222.33013384041223, w0=0.02447877703025959, w1=0.26195487119387684\n",
      "Gradient Descent(4834/9999): loss=222.28587658469002, w0=0.024476786463298438, w1=0.26192872206884266\n",
      "Gradient Descent(4835/9999): loss=222.24162983371608, w0=0.02447479609114359, w1=0.26190257542150625\n",
      "Gradient Descent(4836/9999): loss=222.19739358480686, w0=0.024472805913773855, w1=0.26187643125162946\n",
      "Gradient Descent(4837/9999): loss=222.15316783527925, w0=0.02447081593116805, w1=0.26185028955897416\n",
      "Gradient Descent(4838/9999): loss=222.10895258245117, w0=0.024468826143304985, w1=0.26182415034330225\n",
      "Gradient Descent(4839/9999): loss=222.06474782364108, w0=0.024466836550163476, w1=0.26179801360437566\n",
      "Gradient Descent(4840/9999): loss=222.02055355616838, w0=0.024464847151722337, w1=0.26177187934195634\n",
      "Gradient Descent(4841/9999): loss=221.97636977735294, w0=0.024462857947960378, w1=0.2617457475558063\n",
      "Gradient Descent(4842/9999): loss=221.93219648451546, w0=0.024460868938856416, w1=0.2617196182456876\n",
      "Gradient Descent(4843/9999): loss=221.88803367497735, w0=0.024458880124389265, w1=0.2616934914113623\n",
      "Gradient Descent(4844/9999): loss=221.8438813460608, w0=0.024456891504537736, w1=0.2616673670525923\n",
      "Gradient Descent(4845/9999): loss=221.79973949508883, w0=0.024454903079280645, w1=0.2616412451691399\n",
      "Gradient Descent(4846/9999): loss=221.75560811938487, w0=0.024452914848596803, w1=0.2616151257607671\n",
      "Gradient Descent(4847/9999): loss=221.71148721627316, w0=0.024450926812465027, w1=0.2615890088272361\n",
      "Gradient Descent(4848/9999): loss=221.6673767830789, w0=0.024448938970864132, w1=0.2615628943683091\n",
      "Gradient Descent(4849/9999): loss=221.6232768171278, w0=0.02444695132377293, w1=0.2615367823837483\n",
      "Gradient Descent(4850/9999): loss=221.5791873157464, w0=0.024444963871170237, w1=0.26151067287331586\n",
      "Gradient Descent(4851/9999): loss=221.53510827626187, w0=0.024442976613034868, w1=0.26148456583677415\n",
      "Gradient Descent(4852/9999): loss=221.49103969600216, w0=0.024440989549345635, w1=0.26145846127388545\n",
      "Gradient Descent(4853/9999): loss=221.44698157229584, w0=0.024439002680081357, w1=0.26143235918441204\n",
      "Gradient Descent(4854/9999): loss=221.4029339024724, w0=0.02443701600522085, w1=0.2614062595681163\n",
      "Gradient Descent(4855/9999): loss=221.35889668386184, w0=0.024435029524742927, w1=0.2613801624247606\n",
      "Gradient Descent(4856/9999): loss=221.3148699137951, w0=0.024433043238626405, w1=0.2613540677541073\n",
      "Gradient Descent(4857/9999): loss=221.2708535896036, w0=0.0244310571468501, w1=0.26132797555591886\n",
      "Gradient Descent(4858/9999): loss=221.2268477086197, w0=0.02442907124939283, w1=0.2613018858299577\n",
      "Gradient Descent(4859/9999): loss=221.18285226817616, w0=0.02442708554623341, w1=0.2612757985759864\n",
      "Gradient Descent(4860/9999): loss=221.13886726560702, w0=0.024425100037350655, w1=0.2612497137937674\n",
      "Gradient Descent(4861/9999): loss=221.09489269824644, w0=0.024423114722723387, w1=0.2612236314830632\n",
      "Gradient Descent(4862/9999): loss=221.0509285634296, w0=0.024421129602330418, w1=0.26119755164363645\n",
      "Gradient Descent(4863/9999): loss=221.00697485849244, w0=0.024419144676150566, w1=0.26117147427524967\n",
      "Gradient Descent(4864/9999): loss=220.96303158077143, w0=0.024417159944162648, w1=0.26114539937766557\n",
      "Gradient Descent(4865/9999): loss=220.9190987276039, w0=0.024415175406345482, w1=0.2611193269506467\n",
      "Gradient Descent(4866/9999): loss=220.87517629632796, w0=0.02441319106267789, w1=0.2610932569939558\n",
      "Gradient Descent(4867/9999): loss=220.83126428428227, w0=0.024411206913138687, w1=0.2610671895073555\n",
      "Gradient Descent(4868/9999): loss=220.78736268880624, w0=0.024409222957706696, w1=0.2610411244906086\n",
      "Gradient Descent(4869/9999): loss=220.74347150724014, w0=0.02440723919636073, w1=0.26101506194347784\n",
      "Gradient Descent(4870/9999): loss=220.69959073692476, w0=0.024405255629079615, w1=0.260989001865726\n",
      "Gradient Descent(4871/9999): loss=220.65572037520178, w0=0.024403272255842167, w1=0.26096294425711586\n",
      "Gradient Descent(4872/9999): loss=220.61186041941352, w0=0.024401289076627208, w1=0.2609368891174103\n",
      "Gradient Descent(4873/9999): loss=220.56801086690288, w0=0.024399306091413553, w1=0.26091083644637214\n",
      "Gradient Descent(4874/9999): loss=220.52417171501378, w0=0.02439732330018003, w1=0.2608847862437643\n",
      "Gradient Descent(4875/9999): loss=220.4803429610907, w0=0.02439534070290545, w1=0.2608587385093497\n",
      "Gradient Descent(4876/9999): loss=220.4365246024787, w0=0.024393358299568645, w1=0.26083269324289127\n",
      "Gradient Descent(4877/9999): loss=220.39271663652386, w0=0.02439137609014843, w1=0.26080665044415197\n",
      "Gradient Descent(4878/9999): loss=220.34891906057263, w0=0.02438939407462363, w1=0.26078061011289483\n",
      "Gradient Descent(4879/9999): loss=220.30513187197243, w0=0.024387412252973067, w1=0.2607545722488829\n",
      "Gradient Descent(4880/9999): loss=220.2613550680714, w0=0.024385430625175564, w1=0.26072853685187913\n",
      "Gradient Descent(4881/9999): loss=220.21758864621813, w0=0.02438344919120994, w1=0.2607025039216467\n",
      "Gradient Descent(4882/9999): loss=220.17383260376226, w0=0.024381467951055027, w1=0.26067647345794864\n",
      "Gradient Descent(4883/9999): loss=220.1300869380539, w0=0.02437948690468964, w1=0.26065044546054816\n",
      "Gradient Descent(4884/9999): loss=220.0863516464441, w0=0.02437750605209261, w1=0.2606244199292084\n",
      "Gradient Descent(4885/9999): loss=220.04262672628437, w0=0.024375525393242756, w1=0.26059839686369246\n",
      "Gradient Descent(4886/9999): loss=219.998912174927, w0=0.024373544928118905, w1=0.2605723762637636\n",
      "Gradient Descent(4887/9999): loss=219.9552079897253, w0=0.024371564656699884, w1=0.26054635812918514\n",
      "Gradient Descent(4888/9999): loss=219.91151416803282, w0=0.024369584578964515, w1=0.2605203424597202\n",
      "Gradient Descent(4889/9999): loss=219.86783070720406, w0=0.024367604694891628, w1=0.2604943292551322\n",
      "Gradient Descent(4890/9999): loss=219.82415760459443, w0=0.02436562500446005, w1=0.2604683185151844\n",
      "Gradient Descent(4891/9999): loss=219.78049485755963, w0=0.024363645507648603, w1=0.2604423102396402\n",
      "Gradient Descent(4892/9999): loss=219.73684246345644, w0=0.02436166620443612, w1=0.2604163044282629\n",
      "Gradient Descent(4893/9999): loss=219.6932004196421, w0=0.02435968709480143, w1=0.26039030108081596\n",
      "Gradient Descent(4894/9999): loss=219.64956872347474, w0=0.024357708178723354, w1=0.26036430019706275\n",
      "Gradient Descent(4895/9999): loss=219.6059473723131, w0=0.024355729456180728, w1=0.26033830177676676\n",
      "Gradient Descent(4896/9999): loss=219.56233636351666, w0=0.02435375092715238, w1=0.26031230581969145\n",
      "Gradient Descent(4897/9999): loss=219.51873569444555, w0=0.024351772591617138, w1=0.26028631232560034\n",
      "Gradient Descent(4898/9999): loss=219.47514536246086, w0=0.024349794449553833, w1=0.26026032129425697\n",
      "Gradient Descent(4899/9999): loss=219.4315653649241, w0=0.024347816500941293, w1=0.26023433272542484\n",
      "Gradient Descent(4900/9999): loss=219.38799569919752, w0=0.024345838745758352, w1=0.2602083466188676\n",
      "Gradient Descent(4901/9999): loss=219.3444363626444, w0=0.02434386118398384, w1=0.2601823629743488\n",
      "Gradient Descent(4902/9999): loss=219.30088735262828, w0=0.024341883815596592, w1=0.2601563817916322\n",
      "Gradient Descent(4903/9999): loss=219.25734866651368, w0=0.024339906640575438, w1=0.26013040307048135\n",
      "Gradient Descent(4904/9999): loss=219.21382030166575, w0=0.02433792965889921, w1=0.26010442681066\n",
      "Gradient Descent(4905/9999): loss=219.17030225545057, w0=0.024335952870546747, w1=0.2600784530119318\n",
      "Gradient Descent(4906/9999): loss=219.12679452523454, w0=0.02433397627549688, w1=0.26005248167406053\n",
      "Gradient Descent(4907/9999): loss=219.08329710838507, w0=0.024331999873728444, w1=0.26002651279681\n",
      "Gradient Descent(4908/9999): loss=219.03981000227006, w0=0.024330023665220274, w1=0.26000054637994396\n",
      "Gradient Descent(4909/9999): loss=218.99633320425843, w0=0.024328047649951206, w1=0.25997458242322624\n",
      "Gradient Descent(4910/9999): loss=218.95286671171954, w0=0.024326071827900076, w1=0.2599486209264207\n",
      "Gradient Descent(4911/9999): loss=218.90941052202342, w0=0.02432409619904572, w1=0.2599226618892912\n",
      "Gradient Descent(4912/9999): loss=218.86596463254105, w0=0.024322120763366976, w1=0.25989670531160164\n",
      "Gradient Descent(4913/9999): loss=218.82252904064404, w0=0.024320145520842683, w1=0.259870751193116\n",
      "Gradient Descent(4914/9999): loss=218.7791037437046, w0=0.02431817047145168, w1=0.2598447995335981\n",
      "Gradient Descent(4915/9999): loss=218.73568873909565, w0=0.024316195615172804, w1=0.2598188503328121\n",
      "Gradient Descent(4916/9999): loss=218.69228402419105, w0=0.024314220951984895, w1=0.25979290359052193\n",
      "Gradient Descent(4917/9999): loss=218.64888959636514, w0=0.024312246481866796, w1=0.2597669593064916\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(4918/9999): loss=218.60550545299301, w0=0.024310272204797343, w1=0.2597410174804851\n",
      "Gradient Descent(4919/9999): loss=218.56213159145048, w0=0.024308298120755384, w1=0.2597150781122667\n",
      "Gradient Descent(4920/9999): loss=218.51876800911404, w0=0.024306324229719758, w1=0.25968914120160036\n",
      "Gradient Descent(4921/9999): loss=218.47541470336105, w0=0.024304350531669305, w1=0.2596632067482503\n",
      "Gradient Descent(4922/9999): loss=218.43207167156942, w0=0.024302377026582868, w1=0.25963727475198056\n",
      "Gradient Descent(4923/9999): loss=218.38873891111763, w0=0.024300403714439294, w1=0.25961134521255547\n",
      "Gradient Descent(4924/9999): loss=218.34541641938523, w0=0.024298430595217427, w1=0.2595854181297392\n",
      "Gradient Descent(4925/9999): loss=218.30210419375226, w0=0.024296457668896113, w1=0.25955949350329593\n",
      "Gradient Descent(4926/9999): loss=218.2588022315994, w0=0.024294484935454195, w1=0.25953357133299004\n",
      "Gradient Descent(4927/9999): loss=218.21551053030822, w0=0.024292512394870524, w1=0.25950765161858574\n",
      "Gradient Descent(4928/9999): loss=218.17222908726083, w0=0.024290540047123943, w1=0.2594817343598474\n",
      "Gradient Descent(4929/9999): loss=218.12895789984017, w0=0.0242885678921933, w1=0.2594558195565393\n",
      "Gradient Descent(4930/9999): loss=218.08569696542995, w0=0.024286595930057442, w1=0.25942990720842585\n",
      "Gradient Descent(4931/9999): loss=218.0424462814143, w0=0.024284624160695222, w1=0.2594039973152715\n",
      "Gradient Descent(4932/9999): loss=217.9992058451782, w0=0.024282652584085487, w1=0.2593780898768405\n",
      "Gradient Descent(4933/9999): loss=217.95597565410753, w0=0.024280681200207085, w1=0.2593521848928975\n",
      "Gradient Descent(4934/9999): loss=217.9127557055886, w0=0.024278710009038872, w1=0.25932628236320693\n",
      "Gradient Descent(4935/9999): loss=217.86954599700854, w0=0.024276739010559698, w1=0.2593003822875332\n",
      "Gradient Descent(4936/9999): loss=217.82634652575527, w0=0.02427476820474841, w1=0.2592744846656409\n",
      "Gradient Descent(4937/9999): loss=217.7831572892172, w0=0.02427279759158387, w1=0.25924858949729457\n",
      "Gradient Descent(4938/9999): loss=217.73997828478352, w0=0.024270827171044927, w1=0.2592226967822588\n",
      "Gradient Descent(4939/9999): loss=217.69680950984434, w0=0.024268856943110435, w1=0.25919680652029814\n",
      "Gradient Descent(4940/9999): loss=217.6536509617903, w0=0.02426688690775925, w1=0.2591709187111773\n",
      "Gradient Descent(4941/9999): loss=217.6105026380126, w0=0.024264917064970228, w1=0.25914503335466094\n",
      "Gradient Descent(4942/9999): loss=217.56736453590338, w0=0.024262947414722225, w1=0.25911915045051365\n",
      "Gradient Descent(4943/9999): loss=217.52423665285536, w0=0.024260977956994098, w1=0.2590932699985002\n",
      "Gradient Descent(4944/9999): loss=217.48111898626212, w0=0.024259008691764707, w1=0.2590673919983853\n",
      "Gradient Descent(4945/9999): loss=217.4380115335176, w0=0.024257039619012908, w1=0.25904151644993373\n",
      "Gradient Descent(4946/9999): loss=217.39491429201684, w0=0.024255070738717563, w1=0.2590156433529103\n",
      "Gradient Descent(4947/9999): loss=217.35182725915527, w0=0.02425310205085753, w1=0.25898977270707974\n",
      "Gradient Descent(4948/9999): loss=217.3087504323293, w0=0.02425113355541167, w1=0.2589639045122069\n",
      "Gradient Descent(4949/9999): loss=217.26568380893582, w0=0.024249165252358845, w1=0.2589380387680567\n",
      "Gradient Descent(4950/9999): loss=217.2226273863725, w0=0.024247197141677918, w1=0.258912175474394\n",
      "Gradient Descent(4951/9999): loss=217.17958116203772, w0=0.02424522922334775, w1=0.2588863146309837\n",
      "Gradient Descent(4952/9999): loss=217.13654513333063, w0=0.02424326149734721, w1=0.2588604562375908\n",
      "Gradient Descent(4953/9999): loss=217.0935192976509, w0=0.02424129396365516, w1=0.25883460029398014\n",
      "Gradient Descent(4954/9999): loss=217.05050365239904, w0=0.024239326622250465, w1=0.25880874679991683\n",
      "Gradient Descent(4955/9999): loss=217.00749819497625, w0=0.02423735947311199, w1=0.25878289575516583\n",
      "Gradient Descent(4956/9999): loss=216.96450292278436, w0=0.024235392516218607, w1=0.25875704715949216\n",
      "Gradient Descent(4957/9999): loss=216.921517833226, w0=0.02423342575154918, w1=0.2587312010126609\n",
      "Gradient Descent(4958/9999): loss=216.87854292370437, w0=0.02423145917908258, w1=0.25870535731443717\n",
      "Gradient Descent(4959/9999): loss=216.83557819162357, w0=0.024229492798797672, w1=0.25867951606458606\n",
      "Gradient Descent(4960/9999): loss=216.7926236343882, w0=0.02422752661067333, w1=0.2586536772628727\n",
      "Gradient Descent(4961/9999): loss=216.74967924940364, w0=0.024225560614688425, w1=0.25862784090906227\n",
      "Gradient Descent(4962/9999): loss=216.706745034076, w0=0.024223594810821827, w1=0.25860200700292\n",
      "Gradient Descent(4963/9999): loss=216.66382098581207, w0=0.02422162919905241, w1=0.2585761755442111\n",
      "Gradient Descent(4964/9999): loss=216.62090710201926, w0=0.02421966377935905, w1=0.25855034653270076\n",
      "Gradient Descent(4965/9999): loss=216.57800338010574, w0=0.024217698551720616, w1=0.2585245199681543\n",
      "Gradient Descent(4966/9999): loss=216.5351098174805, w0=0.02421573351611599, w1=0.25849869585033697\n",
      "Gradient Descent(4967/9999): loss=216.49222641155296, w0=0.024213768672524043, w1=0.2584728741790141\n",
      "Gradient Descent(4968/9999): loss=216.44935315973353, w0=0.024211804020923655, w1=0.2584470549539511\n",
      "Gradient Descent(4969/9999): loss=216.40649005943314, w0=0.0242098395612937, w1=0.25842123817491325\n",
      "Gradient Descent(4970/9999): loss=216.36363710806333, w0=0.024207875293613058, w1=0.258395423841666\n",
      "Gradient Descent(4971/9999): loss=216.32079430303668, w0=0.024205911217860608, w1=0.2583696119539748\n",
      "Gradient Descent(4972/9999): loss=216.27796164176604, w0=0.024203947334015236, w1=0.258343802511605\n",
      "Gradient Descent(4973/9999): loss=216.23513912166527, w0=0.024201983642055818, w1=0.2583179955143221\n",
      "Gradient Descent(4974/9999): loss=216.1923267401489, w0=0.024200020141961237, w1=0.25829219096189165\n",
      "Gradient Descent(4975/9999): loss=216.14952449463192, w0=0.024198056833710374, w1=0.25826638885407915\n",
      "Gradient Descent(4976/9999): loss=216.1067323825303, w0=0.024196093717282117, w1=0.2582405891906501\n",
      "Gradient Descent(4977/9999): loss=216.06395040126054, w0=0.02419413079265535, w1=0.2582147919713702\n",
      "Gradient Descent(4978/9999): loss=216.02117854823985, w0=0.024192168059808953, w1=0.2581889971960049\n",
      "Gradient Descent(4979/9999): loss=215.9784168208862, w0=0.02419020551872182, w1=0.25816320486431993\n",
      "Gradient Descent(4980/9999): loss=215.93566521661833, w0=0.024188243169372833, w1=0.25813741497608084\n",
      "Gradient Descent(4981/9999): loss=215.89292373285528, w0=0.024186281011740886, w1=0.2581116275310534\n",
      "Gradient Descent(4982/9999): loss=215.85019236701737, w0=0.024184319045804866, w1=0.2580858425290032\n",
      "Gradient Descent(4983/9999): loss=215.80747111652516, w0=0.02418235727154366, w1=0.25806005996969605\n",
      "Gradient Descent(4984/9999): loss=215.76475997880019, w0=0.024180395688936165, w1=0.2580342798528977\n",
      "Gradient Descent(4985/9999): loss=215.72205895126444, w0=0.02417843429796127, w1=0.25800850217837384\n",
      "Gradient Descent(4986/9999): loss=215.6793680313408, w0=0.024176473098597866, w1=0.25798272694589036\n",
      "Gradient Descent(4987/9999): loss=215.6366872164527, w0=0.02417451209082485, w1=0.257956954155213\n",
      "Gradient Descent(4988/9999): loss=215.5940165040244, w0=0.024172551274621114, w1=0.25793118380610774\n",
      "Gradient Descent(4989/9999): loss=215.55135589148074, w0=0.02417059064996556, w1=0.2579054158983403\n",
      "Gradient Descent(4990/9999): loss=215.50870537624732, w0=0.024168630216837076, w1=0.2578796504316767\n",
      "Gradient Descent(4991/9999): loss=215.46606495575048, w0=0.024166669975214564, w1=0.2578538874058828\n",
      "Gradient Descent(4992/9999): loss=215.42343462741692, w0=0.024164709925076924, w1=0.25782812682072453\n",
      "Gradient Descent(4993/9999): loss=215.38081438867474, w0=0.024162750066403053, w1=0.2578023686759679\n",
      "Gradient Descent(4994/9999): loss=215.33820423695195, w0=0.024160790399171853, w1=0.2577766129713789\n",
      "Gradient Descent(4995/9999): loss=215.2956041696777, w0=0.024158830923362226, w1=0.25775085970672357\n",
      "Gradient Descent(4996/9999): loss=215.2530141842817, w0=0.024156871638953078, w1=0.2577251088817679\n",
      "Gradient Descent(4997/9999): loss=215.21043427819453, w0=0.024154912545923307, w1=0.25769936049627806\n",
      "Gradient Descent(4998/9999): loss=215.16786444884707, w0=0.02415295364425182, w1=0.2576736145500201\n",
      "Gradient Descent(4999/9999): loss=215.12530469367138, w0=0.02415099493391752, w1=0.2576478710427601\n",
      "Gradient Descent(5000/9999): loss=215.08275501009982, w0=0.02414903641489932, w1=0.25762212997426426\n",
      "Gradient Descent(5001/9999): loss=215.04021539556553, w0=0.024147078087176116, w1=0.25759639134429874\n",
      "Gradient Descent(5002/9999): loss=214.99768584750262, w0=0.024145119950726827, w1=0.2575706551526297\n",
      "Gradient Descent(5003/9999): loss=214.9551663633455, w0=0.02414316200553036, w1=0.25754492139902346\n",
      "Gradient Descent(5004/9999): loss=214.91265694052956, w0=0.024141204251565625, w1=0.25751919008324614\n",
      "Gradient Descent(5005/9999): loss=214.8701575764906, w0=0.024139246688811535, w1=0.2574934612050641\n",
      "Gradient Descent(5006/9999): loss=214.8276682686654, w0=0.024137289317247, w1=0.2574677347642436\n",
      "Gradient Descent(5007/9999): loss=214.78518901449135, w0=0.024135332136850932, w1=0.257442010760551\n",
      "Gradient Descent(5008/9999): loss=214.74271981140646, w0=0.024133375147602252, w1=0.2574162891937526\n",
      "Gradient Descent(5009/9999): loss=214.7002606568494, w0=0.02413141834947987, w1=0.25739057006361477\n",
      "Gradient Descent(5010/9999): loss=214.6578115482595, w0=0.024129461742462707, w1=0.25736485336990395\n",
      "Gradient Descent(5011/9999): loss=214.61537248307704, w0=0.02412750532652968, w1=0.25733913911238654\n",
      "Gradient Descent(5012/9999): loss=214.57294345874274, w0=0.024125549101659705, w1=0.25731342729082896\n",
      "Gradient Descent(5013/9999): loss=214.53052447269815, w0=0.024123593067831704, w1=0.2572877179049977\n",
      "Gradient Descent(5014/9999): loss=214.48811552238536, w0=0.024121637225024598, w1=0.2572620109546593\n",
      "Gradient Descent(5015/9999): loss=214.44571660524718, w0=0.024119681573217307, w1=0.2572363064395802\n",
      "Gradient Descent(5016/9999): loss=214.4033277187273, w0=0.024117726112388757, w1=0.2572106043595269\n",
      "Gradient Descent(5017/9999): loss=214.36094886026993, w0=0.024115770842517873, w1=0.2571849047142661\n",
      "Gradient Descent(5018/9999): loss=214.31858002731997, w0=0.024113815763583575, w1=0.2571592075035643\n",
      "Gradient Descent(5019/9999): loss=214.276221217323, w0=0.024111860875564793, w1=0.25713351272718815\n",
      "Gradient Descent(5020/9999): loss=214.23387242772537, w0=0.024109906178440457, w1=0.2571078203849043\n",
      "Gradient Descent(5021/9999): loss=214.1915336559741, w0=0.02410795167218949, w1=0.25708213047647943\n",
      "Gradient Descent(5022/9999): loss=214.14920489951686, w0=0.024105997356790827, w1=0.2570564430016802\n",
      "Gradient Descent(5023/9999): loss=214.10688615580202, w0=0.024104043232223392, w1=0.2570307579602733\n",
      "Gradient Descent(5024/9999): loss=214.06457742227857, w0=0.02410208929846612, w1=0.25700507535202555\n",
      "Gradient Descent(5025/9999): loss=214.0222786963963, w0=0.024100135555497944, w1=0.25697939517670365\n",
      "Gradient Descent(5026/9999): loss=213.9799899756057, w0=0.0240981820032978, w1=0.2569537174340744\n",
      "Gradient Descent(5027/9999): loss=213.93771125735773, w0=0.02409622864184462, w1=0.25692804212390463\n",
      "Gradient Descent(5028/9999): loss=213.89544253910435, w0=0.024094275471117345, w1=0.25690236924596116\n",
      "Gradient Descent(5029/9999): loss=213.85318381829794, w0=0.02409232249109491, w1=0.25687669880001085\n",
      "Gradient Descent(5030/9999): loss=213.81093509239173, w0=0.024090369701756254, w1=0.2568510307858206\n",
      "Gradient Descent(5031/9999): loss=213.76869635883958, w0=0.024088417103080313, w1=0.25682536520315735\n",
      "Gradient Descent(5032/9999): loss=213.726467615096, w0=0.02408646469504603, w1=0.256799702051788\n",
      "Gradient Descent(5033/9999): loss=213.68424885861631, w0=0.024084512477632346, w1=0.25677404133147946\n",
      "Gradient Descent(5034/9999): loss=213.64204008685635, w0=0.024082560450818205, w1=0.2567483830419988\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(5035/9999): loss=213.5998412972727, w0=0.02408060861458255, w1=0.256722727183113\n",
      "Gradient Descent(5036/9999): loss=213.55765248732266, w0=0.024078656968904332, w1=0.2566970737545891\n",
      "Gradient Descent(5037/9999): loss=213.51547365446427, w0=0.02407670551376249, w1=0.25667142275619415\n",
      "Gradient Descent(5038/9999): loss=213.47330479615616, w0=0.024074754249135976, w1=0.25664577418769524\n",
      "Gradient Descent(5039/9999): loss=213.43114590985758, w0=0.024072803175003737, w1=0.2566201280488595\n",
      "Gradient Descent(5040/9999): loss=213.38899699302863, w0=0.024070852291344723, w1=0.2565944843394539\n",
      "Gradient Descent(5041/9999): loss=213.34685804313003, w0=0.024068901598137887, w1=0.2565688430592458\n",
      "Gradient Descent(5042/9999): loss=213.30472905762318, w0=0.02406695109536218, w1=0.2565432042080023\n",
      "Gradient Descent(5043/9999): loss=213.26261003397016, w0=0.024065000782996553, w1=0.2565175677854906\n",
      "Gradient Descent(5044/9999): loss=213.2205009696336, w0=0.024063050661019963, w1=0.25649193379147794\n",
      "Gradient Descent(5045/9999): loss=213.17840186207707, w0=0.024061100729411367, w1=0.2564663022257315\n",
      "Gradient Descent(5046/9999): loss=213.1363127087647, w0=0.02405915098814972, w1=0.25644067308801866\n",
      "Gradient Descent(5047/9999): loss=213.09423350716122, w0=0.024057201437213984, w1=0.25641504637810664\n",
      "Gradient Descent(5048/9999): loss=213.05216425473216, w0=0.024055252076583114, w1=0.2563894220957628\n",
      "Gradient Descent(5049/9999): loss=213.01010494894368, w0=0.024053302906236074, w1=0.25636380024075456\n",
      "Gradient Descent(5050/9999): loss=212.96805558726268, w0=0.024051353926151823, w1=0.25633818081284915\n",
      "Gradient Descent(5051/9999): loss=212.92601616715663, w0=0.024049405136309324, w1=0.25631256381181405\n",
      "Gradient Descent(5052/9999): loss=212.88398668609383, w0=0.024047456536687543, w1=0.2562869492374167\n",
      "Gradient Descent(5053/9999): loss=212.84196714154305, w0=0.024045508127265445, w1=0.25626133708942445\n",
      "Gradient Descent(5054/9999): loss=212.79995753097396, w0=0.024043559908021997, w1=0.25623572736760486\n",
      "Gradient Descent(5055/9999): loss=212.75795785185676, w0=0.024041611878936168, w1=0.2562101200717254\n",
      "Gradient Descent(5056/9999): loss=212.71596810166258, w0=0.024039664039986926, w1=0.25618451520155355\n",
      "Gradient Descent(5057/9999): loss=212.67398827786278, w0=0.02403771639115324, w1=0.2561589127568569\n",
      "Gradient Descent(5058/9999): loss=212.63201837792985, w0=0.02403576893241408, w1=0.25613331273740303\n",
      "Gradient Descent(5059/9999): loss=212.5900583993366, w0=0.024033821663748425, w1=0.2561077151429595\n",
      "Gradient Descent(5060/9999): loss=212.54810833955688, w0=0.024031874585135244, w1=0.2560821199732939\n",
      "Gradient Descent(5061/9999): loss=212.50616819606498, w0=0.024029927696553516, w1=0.2560565272281739\n",
      "Gradient Descent(5062/9999): loss=212.46423796633596, w0=0.024027980997982215, w1=0.25603093690736717\n",
      "Gradient Descent(5063/9999): loss=212.4223176478454, w0=0.02402603448940032, w1=0.25600534901064137\n",
      "Gradient Descent(5064/9999): loss=212.38040723806975, w0=0.02402408817078681, w1=0.2559797635377642\n",
      "Gradient Descent(5065/9999): loss=212.33850673448606, w0=0.024022142042120664, w1=0.2559541804885034\n",
      "Gradient Descent(5066/9999): loss=212.29661613457208, w0=0.024020196103380864, w1=0.2559285998626267\n",
      "Gradient Descent(5067/9999): loss=212.25473543580628, w0=0.024018250354546394, w1=0.255903021659902\n",
      "Gradient Descent(5068/9999): loss=212.21286463566767, w0=0.024016304795596237, w1=0.255877445880097\n",
      "Gradient Descent(5069/9999): loss=212.17100373163612, w0=0.02401435942650938, w1=0.25585187252297953\n",
      "Gradient Descent(5070/9999): loss=212.12915272119196, w0=0.02401241424726481, w1=0.2558263015883175\n",
      "Gradient Descent(5071/9999): loss=212.0873116018164, w0=0.02401046925784151, w1=0.25580073307587875\n",
      "Gradient Descent(5072/9999): loss=212.04548037099133, w0=0.024008524458218477, w1=0.2557751669854312\n",
      "Gradient Descent(5073/9999): loss=212.00365902619913, w0=0.024006579848374697, w1=0.25574960331674274\n",
      "Gradient Descent(5074/9999): loss=211.96184756492303, w0=0.024004635428289164, w1=0.2557240420695813\n",
      "Gradient Descent(5075/9999): loss=211.92004598464683, w0=0.024002691197940868, w1=0.25569848324371497\n",
      "Gradient Descent(5076/9999): loss=211.87825428285498, w0=0.024000747157308808, w1=0.25567292683891163\n",
      "Gradient Descent(5077/9999): loss=211.83647245703284, w0=0.023998803306371976, w1=0.2556473728549393\n",
      "Gradient Descent(5078/9999): loss=211.7947005046662, w0=0.02399685964510937, w1=0.25562182129156613\n",
      "Gradient Descent(5079/9999): loss=211.7529384232417, w0=0.02399491617349999, w1=0.2555962721485601\n",
      "Gradient Descent(5080/9999): loss=211.71118621024635, w0=0.023992972891522833, w1=0.25557072542568937\n",
      "Gradient Descent(5081/9999): loss=211.66944386316823, w0=0.0239910297991569, w1=0.25554518112272195\n",
      "Gradient Descent(5082/9999): loss=211.62771137949593, w0=0.023989086896381198, w1=0.25551963923942606\n",
      "Gradient Descent(5083/9999): loss=211.5859887567186, w0=0.023987144183174727, w1=0.25549409977556986\n",
      "Gradient Descent(5084/9999): loss=211.5442759923263, w0=0.023985201659516495, w1=0.2554685627309215\n",
      "Gradient Descent(5085/9999): loss=211.5025730838095, w0=0.023983259325385503, w1=0.2554430281052492\n",
      "Gradient Descent(5086/9999): loss=211.46088002865957, w0=0.023981317180760763, w1=0.25541749589832125\n",
      "Gradient Descent(5087/9999): loss=211.41919682436847, w0=0.023979375225621283, w1=0.25539196610990583\n",
      "Gradient Descent(5088/9999): loss=211.37752346842876, w0=0.023977433459946074, w1=0.2553664387397713\n",
      "Gradient Descent(5089/9999): loss=211.3358599583338, w0=0.023975491883714145, w1=0.25534091378768586\n",
      "Gradient Descent(5090/9999): loss=211.2942062915776, w0=0.023973550496904514, w1=0.25531539125341796\n",
      "Gradient Descent(5091/9999): loss=211.2525624656548, w0=0.023971609299496193, w1=0.25528987113673585\n",
      "Gradient Descent(5092/9999): loss=211.21092847806057, w0=0.023969668291468196, w1=0.25526435343740794\n",
      "Gradient Descent(5093/9999): loss=211.1693043262912, w0=0.023967727472799544, w1=0.25523883815520265\n",
      "Gradient Descent(5094/9999): loss=211.12769000784317, w0=0.023965786843469252, w1=0.25521332528988844\n",
      "Gradient Descent(5095/9999): loss=211.0860855202138, w0=0.02396384640345634, w1=0.2551878148412337\n",
      "Gradient Descent(5096/9999): loss=211.0444908609013, w0=0.023961906152739833, w1=0.2551623068090069\n",
      "Gradient Descent(5097/9999): loss=211.00290602740412, w0=0.023959966091298752, w1=0.2551368011929766\n",
      "Gradient Descent(5098/9999): loss=210.96133101722188, w0=0.02395802621911212, w1=0.2551112979929112\n",
      "Gradient Descent(5099/9999): loss=210.91976582785446, w0=0.023956086536158962, w1=0.2550857972085794\n",
      "Gradient Descent(5100/9999): loss=210.87821045680266, w0=0.023954147042418307, w1=0.2550602988397496\n",
      "Gradient Descent(5101/9999): loss=210.8366649015678, w0=0.02395220773786918, w1=0.2550348028861905\n",
      "Gradient Descent(5102/9999): loss=210.79512915965194, w0=0.02395026862249061, w1=0.2550093093476707\n",
      "Gradient Descent(5103/9999): loss=210.75360322855786, w0=0.023948329696261636, w1=0.25498381822395877\n",
      "Gradient Descent(5104/9999): loss=210.7120871057889, w0=0.023946390959161283, w1=0.25495832951482345\n",
      "Gradient Descent(5105/9999): loss=210.67058078884926, w0=0.023944452411168583, w1=0.25493284322003334\n",
      "Gradient Descent(5106/9999): loss=210.62908427524349, w0=0.023942514052262578, w1=0.2549073593393572\n",
      "Gradient Descent(5107/9999): loss=210.58759756247719, w0=0.0239405758824223, w1=0.2548818778725638\n",
      "Gradient Descent(5108/9999): loss=210.54612064805627, w0=0.02393863790162679, w1=0.2548563988194218\n",
      "Gradient Descent(5109/9999): loss=210.50465352948765, w0=0.023936700109855085, w1=0.25483092217970005\n",
      "Gradient Descent(5110/9999): loss=210.46319620427863, w0=0.023934762507086225, w1=0.25480544795316734\n",
      "Gradient Descent(5111/9999): loss=210.42174866993753, w0=0.023932825093299255, w1=0.25477997613959247\n",
      "Gradient Descent(5112/9999): loss=210.38031092397287, w0=0.02393088786847322, w1=0.2547545067387443\n",
      "Gradient Descent(5113/9999): loss=210.33888296389412, w0=0.02392895083258716, w1=0.2547290397503917\n",
      "Gradient Descent(5114/9999): loss=210.2974647872116, w0=0.023927013985620124, w1=0.25470357517430353\n",
      "Gradient Descent(5115/9999): loss=210.25605639143586, w0=0.02392507732755116, w1=0.2546781130102488\n",
      "Gradient Descent(5116/9999): loss=210.2146577740784, w0=0.023923140858359317, w1=0.25465265325799635\n",
      "Gradient Descent(5117/9999): loss=210.17326893265147, w0=0.023921204578023648, w1=0.25462719591731525\n",
      "Gradient Descent(5118/9999): loss=210.13188986466764, w0=0.023919268486523204, w1=0.2546017409879744\n",
      "Gradient Descent(5119/9999): loss=210.09052056764057, w0=0.02391733258383704, w1=0.25457628846974284\n",
      "Gradient Descent(5120/9999): loss=210.0491610390843, w0=0.02391539686994421, w1=0.2545508383623896\n",
      "Gradient Descent(5121/9999): loss=210.00781127651354, w0=0.02391346134482377, w1=0.25452539066568375\n",
      "Gradient Descent(5122/9999): loss=209.9664712774439, w0=0.02391152600845478, w1=0.25449994537939435\n",
      "Gradient Descent(5123/9999): loss=209.92514103939143, w0=0.023909590860816298, w1=0.25447450250329057\n",
      "Gradient Descent(5124/9999): loss=209.88382055987293, w0=0.023907655901887384, w1=0.25444906203714146\n",
      "Gradient Descent(5125/9999): loss=209.8425098364059, w0=0.023905721131647104, w1=0.2544236239807162\n",
      "Gradient Descent(5126/9999): loss=209.8012088665084, w0=0.02390378655007452, w1=0.25439818833378397\n",
      "Gradient Descent(5127/9999): loss=209.75991764769933, w0=0.023901852157148698, w1=0.25437275509611396\n",
      "Gradient Descent(5128/9999): loss=209.718636177498, w0=0.023899917952848702, w1=0.25434732426747536\n",
      "Gradient Descent(5129/9999): loss=209.67736445342467, w0=0.023897983937153602, w1=0.2543218958476375\n",
      "Gradient Descent(5130/9999): loss=209.63610247300022, w0=0.02389605011004247, w1=0.25429646983636955\n",
      "Gradient Descent(5131/9999): loss=209.59485023374592, w0=0.023894116471494375, w1=0.2542710462334409\n",
      "Gradient Descent(5132/9999): loss=209.55360773318398, w0=0.023892183021488392, w1=0.25424562503862075\n",
      "Gradient Descent(5133/9999): loss=209.5123749688373, w0=0.023890249760003594, w1=0.2542202062516785\n",
      "Gradient Descent(5134/9999): loss=209.47115193822918, w0=0.02388831668701906, w1=0.2541947898723836\n",
      "Gradient Descent(5135/9999): loss=209.42993863888393, w0=0.02388638380251386, w1=0.25416937590050526\n",
      "Gradient Descent(5136/9999): loss=209.38873506832618, w0=0.023884451106467083, w1=0.254143964335813\n",
      "Gradient Descent(5137/9999): loss=209.3475412240815, w0=0.023882518598857802, w1=0.25411855517807624\n",
      "Gradient Descent(5138/9999): loss=209.30635710367594, w0=0.023880586279665102, w1=0.25409314842706443\n",
      "Gradient Descent(5139/9999): loss=209.26518270463643, w0=0.023878654148868063, w1=0.254067744082547\n",
      "Gradient Descent(5140/9999): loss=209.22401802449028, w0=0.023876722206445773, w1=0.2540423421442935\n",
      "Gradient Descent(5141/9999): loss=209.18286306076564, w0=0.023874790452377316, w1=0.2540169426120734\n",
      "Gradient Descent(5142/9999): loss=209.14171781099134, w0=0.023872858886641785, w1=0.25399154548565633\n",
      "Gradient Descent(5143/9999): loss=209.10058227269684, w0=0.023870927509218265, w1=0.25396615076481177\n",
      "Gradient Descent(5144/9999): loss=209.05945644341222, w0=0.023868996320085847, w1=0.25394075844930936\n",
      "Gradient Descent(5145/9999): loss=209.01834032066824, w0=0.023867065319223623, w1=0.2539153685389187\n",
      "Gradient Descent(5146/9999): loss=208.97723390199644, w0=0.02386513450661069, w1=0.2538899810334095\n",
      "Gradient Descent(5147/9999): loss=208.93613718492884, w0=0.023863203882226142, w1=0.2538645959325513\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(5148/9999): loss=208.89505016699826, w0=0.023861273446049075, w1=0.25383921323611386\n",
      "Gradient Descent(5149/9999): loss=208.8539728457381, w0=0.02385934319805859, w1=0.2538138329438669\n",
      "Gradient Descent(5150/9999): loss=208.8129052186825, w0=0.023857413138233786, w1=0.25378845505558006\n",
      "Gradient Descent(5151/9999): loss=208.77184728336613, w0=0.023855483266553765, w1=0.2537630795710232\n",
      "Gradient Descent(5152/9999): loss=208.7307990373245, w0=0.02385355358299763, w1=0.253737706489966\n",
      "Gradient Descent(5153/9999): loss=208.6897604780937, w0=0.023851624087544484, w1=0.2537123358121784\n",
      "Gradient Descent(5154/9999): loss=208.64873160321042, w0=0.023849694780173437, w1=0.2536869675374301\n",
      "Gradient Descent(5155/9999): loss=208.60771241021214, w0=0.023847765660863596, w1=0.25366160166549095\n",
      "Gradient Descent(5156/9999): loss=208.56670289663685, w0=0.023845836729594068, w1=0.2536362381961309\n",
      "Gradient Descent(5157/9999): loss=208.52570306002335, w0=0.023843907986343966, w1=0.2536108771291198\n",
      "Gradient Descent(5158/9999): loss=208.48471289791098, w0=0.0238419794310924, w1=0.2535855184642275\n",
      "Gradient Descent(5159/9999): loss=208.4437324078398, w0=0.02384005106381849, w1=0.253560162201224\n",
      "Gradient Descent(5160/9999): loss=208.4027615873505, w0=0.023838122884501347, w1=0.2535348083398793\n",
      "Gradient Descent(5161/9999): loss=208.36180043398466, w0=0.02383619489312009, w1=0.2535094568799633\n",
      "Gradient Descent(5162/9999): loss=208.32084894528404, w0=0.02383426708965384, w1=0.2534841078212461\n",
      "Gradient Descent(5163/9999): loss=208.27990711879153, w0=0.023832339474081712, w1=0.25345876116349764\n",
      "Gradient Descent(5164/9999): loss=208.2389749520504, w0=0.023830412046382834, w1=0.253433416906488\n",
      "Gradient Descent(5165/9999): loss=208.19805244260473, w0=0.023828484806536324, w1=0.2534080750499873\n",
      "Gradient Descent(5166/9999): loss=208.1571395879991, w0=0.02382655775452131, w1=0.25338273559376556\n",
      "Gradient Descent(5167/9999): loss=208.116236385779, w0=0.023824630890316924, w1=0.253357398537593\n",
      "Gradient Descent(5168/9999): loss=208.0753428334903, w0=0.023822704213902285, w1=0.25333206388123963\n",
      "Gradient Descent(5169/9999): loss=208.03445892867973, w0=0.023820777725256526, w1=0.25330673162447576\n",
      "Gradient Descent(5170/9999): loss=207.99358466889453, w0=0.023818851424358783, w1=0.2532814017670715\n",
      "Gradient Descent(5171/9999): loss=207.95272005168275, w0=0.023816925311188186, w1=0.25325607430879704\n",
      "Gradient Descent(5172/9999): loss=207.91186507459307, w0=0.02381499938572387, w1=0.2532307492494227\n",
      "Gradient Descent(5173/9999): loss=207.87101973517466, w0=0.02381307364794497, w1=0.25320542658871864\n",
      "Gradient Descent(5174/9999): loss=207.83018403097756, w0=0.023811148097830626, w1=0.2531801063264552\n",
      "Gradient Descent(5175/9999): loss=207.78935795955232, w0=0.02380922273535998, w1=0.2531547884624027\n",
      "Gradient Descent(5176/9999): loss=207.74854151845025, w0=0.023807297560512166, w1=0.25312947299633143\n",
      "Gradient Descent(5177/9999): loss=207.70773470522334, w0=0.023805372573266335, w1=0.2531041599280118\n",
      "Gradient Descent(5178/9999): loss=207.6669375174241, w0=0.02380344777360163, w1=0.2530788492572141\n",
      "Gradient Descent(5179/9999): loss=207.6261499526057, w0=0.023801523161497192, w1=0.25305354098370875\n",
      "Gradient Descent(5180/9999): loss=207.58537200832222, w0=0.023799598736932173, w1=0.2530282351072662\n",
      "Gradient Descent(5181/9999): loss=207.54460368212807, w0=0.02379767449988572, w1=0.2530029316276568\n",
      "Gradient Descent(5182/9999): loss=207.50384497157862, w0=0.023795750450336987, w1=0.2529776305446511\n",
      "Gradient Descent(5183/9999): loss=207.46309587422954, w0=0.023793826588265125, w1=0.2529523318580196\n",
      "Gradient Descent(5184/9999): loss=207.42235638763754, w0=0.02379190291364929, w1=0.2529270355675327\n",
      "Gradient Descent(5185/9999): loss=207.38162650935976, w0=0.023789979426468634, w1=0.2529017416729611\n",
      "Gradient Descent(5186/9999): loss=207.3409062369539, w0=0.023788056126702317, w1=0.25287645017407523\n",
      "Gradient Descent(5187/9999): loss=207.30019556797865, w0=0.0237861330143295, w1=0.2528511610706457\n",
      "Gradient Descent(5188/9999): loss=207.25949449999305, w0=0.023784210089329343, w1=0.2528258743624431\n",
      "Gradient Descent(5189/9999): loss=207.21880303055693, w0=0.023782287351681006, w1=0.25280059004923805\n",
      "Gradient Descent(5190/9999): loss=207.17812115723078, w0=0.023780364801363654, w1=0.2527753081308012\n",
      "Gradient Descent(5191/9999): loss=207.13744887757565, w0=0.023778442438356456, w1=0.2527500286069032\n",
      "Gradient Descent(5192/9999): loss=207.09678618915345, w0=0.023776520262638573, w1=0.25272475147731477\n",
      "Gradient Descent(5193/9999): loss=207.05613308952655, w0=0.02377459827418918, w1=0.2526994767418066\n",
      "Gradient Descent(5194/9999): loss=207.01548957625792, w0=0.023772676472987444, w1=0.2526742044001494\n",
      "Gradient Descent(5195/9999): loss=206.97485564691144, w0=0.02377075485901254, w1=0.25264893445211406\n",
      "Gradient Descent(5196/9999): loss=206.93423129905145, w0=0.023768833432243643, w1=0.25262366689747123\n",
      "Gradient Descent(5197/9999): loss=206.8936165302431, w0=0.023766912192659927, w1=0.25259840173599174\n",
      "Gradient Descent(5198/9999): loss=206.85301133805189, w0=0.02376499114024057, w1=0.25257313896744643\n",
      "Gradient Descent(5199/9999): loss=206.81241572004433, w0=0.02376307027496475, w1=0.25254787859160616\n",
      "Gradient Descent(5200/9999): loss=206.77182967378735, w0=0.023761149596811647, w1=0.25252262060824177\n",
      "Gradient Descent(5201/9999): loss=206.7312531968487, w0=0.02375922910576045, w1=0.25249736501712416\n",
      "Gradient Descent(5202/9999): loss=206.69068628679668, w0=0.023757308801790337, w1=0.25247211181802426\n",
      "Gradient Descent(5203/9999): loss=206.6501289412001, w0=0.023755388684880495, w1=0.252446861010713\n",
      "Gradient Descent(5204/9999): loss=206.60958115762884, w0=0.02375346875501011, w1=0.2524216125949614\n",
      "Gradient Descent(5205/9999): loss=206.56904293365298, w0=0.023751549012158375, w1=0.2523963665705403\n",
      "Gradient Descent(5206/9999): loss=206.52851426684353, w0=0.02374962945630448, w1=0.25237112293722086\n",
      "Gradient Descent(5207/9999): loss=206.4879951547721, w0=0.02374771008742762, w1=0.252345881694774\n",
      "Gradient Descent(5208/9999): loss=206.44748559501085, w0=0.02374579090550698, w1=0.25232064284297084\n",
      "Gradient Descent(5209/9999): loss=206.4069855851327, w0=0.023743871910521765, w1=0.25229540638158243\n",
      "Gradient Descent(5210/9999): loss=206.3664951227112, w0=0.02374195310245117, w1=0.25227017231037985\n",
      "Gradient Descent(5211/9999): loss=206.3260142053205, w0=0.023740034481274392, w1=0.2522449406291342\n",
      "Gradient Descent(5212/9999): loss=206.28554283053543, w0=0.023738116046970636, w1=0.2522197113376167\n",
      "Gradient Descent(5213/9999): loss=206.24508099593154, w0=0.023736197799519104, w1=0.2521944844355985\n",
      "Gradient Descent(5214/9999): loss=206.20462869908496, w0=0.023734279738898998, w1=0.2521692599228507\n",
      "Gradient Descent(5215/9999): loss=206.16418593757246, w0=0.023732361865089528, w1=0.2521440377991446\n",
      "Gradient Descent(5216/9999): loss=206.12375270897152, w0=0.023730444178069902, w1=0.2521188180642514\n",
      "Gradient Descent(5217/9999): loss=206.08332901086018, w0=0.023728526677819328, w1=0.25209360071794235\n",
      "Gradient Descent(5218/9999): loss=206.04291484081722, w0=0.023726609364317017, w1=0.2520683857599887\n",
      "Gradient Descent(5219/9999): loss=206.00251019642212, w0=0.023724692237542182, w1=0.2520431731901618\n",
      "Gradient Descent(5220/9999): loss=205.96211507525476, w0=0.02372277529747404, w1=0.2520179630082329\n",
      "Gradient Descent(5221/9999): loss=205.92172947489587, w0=0.023720858544091804, w1=0.2519927552139734\n",
      "Gradient Descent(5222/9999): loss=205.8813533929269, w0=0.023718941977374695, w1=0.2519675498071547\n",
      "Gradient Descent(5223/9999): loss=205.84098682692974, w0=0.023717025597301933, w1=0.25194234678754807\n",
      "Gradient Descent(5224/9999): loss=205.80062977448713, w0=0.023715109403852738, w1=0.251917146154925\n",
      "Gradient Descent(5225/9999): loss=205.7602822331822, w0=0.023713193397006335, w1=0.25189194790905695\n",
      "Gradient Descent(5226/9999): loss=205.71994420059906, w0=0.02371127757674195, w1=0.2518667520497153\n",
      "Gradient Descent(5227/9999): loss=205.6796156743223, w0=0.02370936194303881, w1=0.2518415585766716\n",
      "Gradient Descent(5228/9999): loss=205.63929665193706, w0=0.023707446495876142, w1=0.2518163674896973\n",
      "Gradient Descent(5229/9999): loss=205.59898713102922, w0=0.02370553123523318, w1=0.25179117878856394\n",
      "Gradient Descent(5230/9999): loss=205.5586871091854, w0=0.02370361616108915, w1=0.25176599247304304\n",
      "Gradient Descent(5231/9999): loss=205.5183965839928, w0=0.02370170127342329, w1=0.2517408085429062\n",
      "Gradient Descent(5232/9999): loss=205.47811555303915, w0=0.023699786572214835, w1=0.251715626997925\n",
      "Gradient Descent(5233/9999): loss=205.4378440139129, w0=0.023697872057443028, w1=0.25169044783787103\n",
      "Gradient Descent(5234/9999): loss=205.39758196420323, w0=0.0236959577290871, w1=0.25166527106251596\n",
      "Gradient Descent(5235/9999): loss=205.35732940150004, w0=0.023694043587126297, w1=0.25164009667163145\n",
      "Gradient Descent(5236/9999): loss=205.31708632339362, w0=0.02369212963153986, w1=0.25161492466498914\n",
      "Gradient Descent(5237/9999): loss=205.27685272747507, w0=0.023690215862307035, w1=0.25158975504236075\n",
      "Gradient Descent(5238/9999): loss=205.23662861133616, w0=0.02368830227940707, w1=0.251564587803518\n",
      "Gradient Descent(5239/9999): loss=205.19641397256905, w0=0.023686388882819206, w1=0.2515394229482326\n",
      "Gradient Descent(5240/9999): loss=205.15620880876693, w0=0.023684475672522697, w1=0.2515142604762764\n",
      "Gradient Descent(5241/9999): loss=205.1160131175234, w0=0.023682562648496795, w1=0.2514891003874211\n",
      "Gradient Descent(5242/9999): loss=205.07582689643283, w0=0.023680649810720756, w1=0.25146394268143857\n",
      "Gradient Descent(5243/9999): loss=205.03565014308998, w0=0.023678737159173832, w1=0.2514387873581006\n",
      "Gradient Descent(5244/9999): loss=204.99548285509064, w0=0.02367682469383528, w1=0.2514136344171791\n",
      "Gradient Descent(5245/9999): loss=204.95532503003096, w0=0.02367491241468436, w1=0.2513884838584459\n",
      "Gradient Descent(5246/9999): loss=204.91517666550783, w0=0.02367300032170033, w1=0.25136333568167296\n",
      "Gradient Descent(5247/9999): loss=204.8750377591188, w0=0.023671088414862453, w1=0.25133818988663215\n",
      "Gradient Descent(5248/9999): loss=204.83490830846202, w0=0.02366917669415, w1=0.25131304647309544\n",
      "Gradient Descent(5249/9999): loss=204.7947883111363, w0=0.02366726515954223, w1=0.25128790544083474\n",
      "Gradient Descent(5250/9999): loss=204.75467776474102, w0=0.023665353811018412, w1=0.2512627667896221\n",
      "Gradient Descent(5251/9999): loss=204.7145766668765, w0=0.023663442648557815, w1=0.25123763051922954\n",
      "Gradient Descent(5252/9999): loss=204.67448501514318, w0=0.02366153167213971, w1=0.25121249662942907\n",
      "Gradient Descent(5253/9999): loss=204.63440280714275, w0=0.023659620881743375, w1=0.25118736511999273\n",
      "Gradient Descent(5254/9999): loss=204.59433004047708, w0=0.02365771027734808, w1=0.2511622359906926\n",
      "Gradient Descent(5255/9999): loss=204.55426671274896, w0=0.0236557998589331, w1=0.2511371092413008\n",
      "Gradient Descent(5256/9999): loss=204.51421282156153, w0=0.023653889626477718, w1=0.2511119848715894\n",
      "Gradient Descent(5257/9999): loss=204.47416836451896, w0=0.02365197957996121, w1=0.25108686288133064\n",
      "Gradient Descent(5258/9999): loss=204.43413333922572, w0=0.023650069719362864, w1=0.2510617432702966\n",
      "Gradient Descent(5259/9999): loss=204.39410774328718, w0=0.02364816004466196, w1=0.25103662603825955\n",
      "Gradient Descent(5260/9999): loss=204.35409157430914, w0=0.023646250555837786, w1=0.2510115111849916\n",
      "Gradient Descent(5261/9999): loss=204.31408482989815, w0=0.023644341252869627, w1=0.25098639871026507\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(5262/9999): loss=204.27408750766145, w0=0.023642432135736774, w1=0.2509612886138522\n",
      "Gradient Descent(5263/9999): loss=204.2340996052068, w0=0.02364052320441852, w1=0.2509361808955252\n",
      "Gradient Descent(5264/9999): loss=204.19412112014268, w0=0.023638614458894154, w1=0.25091107555505643\n",
      "Gradient Descent(5265/9999): loss=204.1541520500782, w0=0.023636705899142973, w1=0.2508859725922182\n",
      "Gradient Descent(5266/9999): loss=204.1141923926231, w0=0.023634797525144274, w1=0.2508608720067828\n",
      "Gradient Descent(5267/9999): loss=204.0742421453878, w0=0.023632889336877357, w1=0.2508357737985227\n",
      "Gradient Descent(5268/9999): loss=204.03430130598332, w0=0.023630981334321517, w1=0.2508106779672102\n",
      "Gradient Descent(5269/9999): loss=203.9943698720213, w0=0.02362907351745606, w1=0.25078558451261773\n",
      "Gradient Descent(5270/9999): loss=203.95444784111416, w0=0.023627165886260293, w1=0.2507604934345178\n",
      "Gradient Descent(5271/9999): loss=203.9145352108747, w0=0.023625258440713517, w1=0.25073540473268274\n",
      "Gradient Descent(5272/9999): loss=203.87463197891657, w0=0.023623351180795043, w1=0.25071031840688507\n",
      "Gradient Descent(5273/9999): loss=203.83473814285406, w0=0.023621444106484177, w1=0.25068523445689733\n",
      "Gradient Descent(5274/9999): loss=203.79485370030199, w0=0.023619537217760233, w1=0.250660152882492\n",
      "Gradient Descent(5275/9999): loss=203.7549786488758, w0=0.023617630514602524, w1=0.2506350736834416\n",
      "Gradient Descent(5276/9999): loss=203.71511298619177, w0=0.023615723996990363, w1=0.25060999685951874\n",
      "Gradient Descent(5277/9999): loss=203.6752567098667, w0=0.02361381766490307, w1=0.25058492241049596\n",
      "Gradient Descent(5278/9999): loss=203.63540981751788, w0=0.02361191151831996, w1=0.25055985033614586\n",
      "Gradient Descent(5279/9999): loss=203.59557230676347, w0=0.023610005557220357, w1=0.25053478063624113\n",
      "Gradient Descent(5280/9999): loss=203.5557441752223, w0=0.023608099781583583, w1=0.2505097133105544\n",
      "Gradient Descent(5281/9999): loss=203.5159254205135, w0=0.023606194191388964, w1=0.2504846483588583\n",
      "Gradient Descent(5282/9999): loss=203.47611604025724, w0=0.023604288786615822, w1=0.2504595857809256\n",
      "Gradient Descent(5283/9999): loss=203.43631603207407, w0=0.02360238356724349, w1=0.2504345255765289\n",
      "Gradient Descent(5284/9999): loss=203.3965253935852, w0=0.023600478533251295, w1=0.25040946774544104\n",
      "Gradient Descent(5285/9999): loss=203.35674412241264, w0=0.02359857368461857, w1=0.25038441228743474\n",
      "Gradient Descent(5286/9999): loss=203.31697221617878, w0=0.023596669021324648, w1=0.2503593592022828\n",
      "Gradient Descent(5287/9999): loss=203.27720967250696, w0=0.023594764543348863, w1=0.250334308489758\n",
      "Gradient Descent(5288/9999): loss=203.23745648902093, w0=0.023592860250670554, w1=0.2503092601496332\n",
      "Gradient Descent(5289/9999): loss=203.19771266334521, w0=0.02359095614326906, w1=0.2502842141816812\n",
      "Gradient Descent(5290/9999): loss=203.15797819310475, w0=0.023589052221123723, w1=0.2502591705856749\n",
      "Gradient Descent(5291/9999): loss=203.1182530759254, w0=0.023587148484213886, w1=0.25023412936138717\n",
      "Gradient Descent(5292/9999): loss=203.07853730943347, w0=0.023585244932518894, w1=0.25020909050859097\n",
      "Gradient Descent(5293/9999): loss=203.038830891256, w0=0.02358334156601809, w1=0.2501840540270592\n",
      "Gradient Descent(5294/9999): loss=202.99913381902059, w0=0.023581438384690828, w1=0.2501590199165648\n",
      "Gradient Descent(5295/9999): loss=202.95944609035558, w0=0.023579535388516457, w1=0.25013398817688076\n",
      "Gradient Descent(5296/9999): loss=202.91976770288974, w0=0.023577632577474326, w1=0.2501089588077801\n",
      "Gradient Descent(5297/9999): loss=202.88009865425278, w0=0.023575729951543795, w1=0.2500839318090359\n",
      "Gradient Descent(5298/9999): loss=202.84043894207483, w0=0.023573827510704214, w1=0.25005890718042106\n",
      "Gradient Descent(5299/9999): loss=202.80078856398674, w0=0.02357192525493495, w1=0.25003388492170875\n",
      "Gradient Descent(5300/9999): loss=202.7611475176198, w0=0.023570023184215354, w1=0.250008865032672\n",
      "Gradient Descent(5301/9999): loss=202.72151580060628, w0=0.02356812129852479, w1=0.24998384751308397\n",
      "Gradient Descent(5302/9999): loss=202.68189341057885, w0=0.023566219597842622, w1=0.24995883236271776\n",
      "Gradient Descent(5303/9999): loss=202.6422803451708, w0=0.02356431808214822, w1=0.24993381958134656\n",
      "Gradient Descent(5304/9999): loss=202.60267660201626, w0=0.023562416751420946, w1=0.2499088091687435\n",
      "Gradient Descent(5305/9999): loss=202.5630821787497, w0=0.023560515605640174, w1=0.2498838011246818\n",
      "Gradient Descent(5306/9999): loss=202.52349707300658, w0=0.023558614644785272, w1=0.2498587954489347\n",
      "Gradient Descent(5307/9999): loss=202.48392128242264, w0=0.023556713868835615, w1=0.24983379214127538\n",
      "Gradient Descent(5308/9999): loss=202.44435480463443, w0=0.023554813277770578, w1=0.24980879120147714\n",
      "Gradient Descent(5309/9999): loss=202.40479763727916, w0=0.02355291287156954, w1=0.24978379262931327\n",
      "Gradient Descent(5310/9999): loss=202.3652497779946, w0=0.023551012650211877, w1=0.24975879642455706\n",
      "Gradient Descent(5311/9999): loss=202.32571122441914, w0=0.02354911261367697, w1=0.24973380258698183\n",
      "Gradient Descent(5312/9999): loss=202.28618197419203, w0=0.023547212761944207, w1=0.24970881111636095\n",
      "Gradient Descent(5313/9999): loss=202.24666202495277, w0=0.023545313094992967, w1=0.24968382201246778\n",
      "Gradient Descent(5314/9999): loss=202.20715137434178, w0=0.02354341361280264, w1=0.2496588352750757\n",
      "Gradient Descent(5315/9999): loss=202.16765002, w0=0.023541514315352615, w1=0.24963385090395815\n",
      "Gradient Descent(5316/9999): loss=202.12815795956902, w0=0.023539615202622276, w1=0.24960886889888856\n",
      "Gradient Descent(5317/9999): loss=202.08867519069108, w0=0.023537716274591024, w1=0.24958388925964037\n",
      "Gradient Descent(5318/9999): loss=202.049201711009, w0=0.02353581753123825, w1=0.24955891198598706\n",
      "Gradient Descent(5319/9999): loss=202.0097375181664, w0=0.02353391897254335, w1=0.24953393707770216\n",
      "Gradient Descent(5320/9999): loss=201.97028260980733, w0=0.02353202059848572, w1=0.24950896453455915\n",
      "Gradient Descent(5321/9999): loss=201.93083698357648, w0=0.023530122409044764, w1=0.2494839943563316\n",
      "Gradient Descent(5322/9999): loss=201.89140063711932, w0=0.023528224404199882, w1=0.24945902654279306\n",
      "Gradient Descent(5323/9999): loss=201.85197356808186, w0=0.02352632658393048, w1=0.24943406109371713\n",
      "Gradient Descent(5324/9999): loss=201.81255577411076, w0=0.023524428948215964, w1=0.24940909800887742\n",
      "Gradient Descent(5325/9999): loss=201.77314725285322, w0=0.023522531497035738, w1=0.24938413728804756\n",
      "Gradient Descent(5326/9999): loss=201.7337480019573, w0=0.023520634230369215, w1=0.2493591789310012\n",
      "Gradient Descent(5327/9999): loss=201.69435801907144, w0=0.023518737148195808, w1=0.249334222937512\n",
      "Gradient Descent(5328/9999): loss=201.65497730184484, w0=0.023516840250494928, w1=0.24930926930735367\n",
      "Gradient Descent(5329/9999): loss=201.61560584792724, w0=0.02351494353724599, w1=0.24928431804029993\n",
      "Gradient Descent(5330/9999): loss=201.5762436549692, w0=0.023513047008428416, w1=0.2492593691361245\n",
      "Gradient Descent(5331/9999): loss=201.5368907206217, w0=0.023511150664021624, w1=0.24923442259460116\n",
      "Gradient Descent(5332/9999): loss=201.4975470425364, w0=0.023509254504005036, w1=0.24920947841550367\n",
      "Gradient Descent(5333/9999): loss=201.45821261836576, w0=0.02350735852835807, w1=0.24918453659860584\n",
      "Gradient Descent(5334/9999): loss=201.4188874457626, w0=0.023505462737060157, w1=0.24915959714368152\n",
      "Gradient Descent(5335/9999): loss=201.37957152238056, w0=0.023503567130090722, w1=0.24913466005050453\n",
      "Gradient Descent(5336/9999): loss=201.34026484587386, w0=0.023501671707429197, w1=0.24910972531884873\n",
      "Gradient Descent(5337/9999): loss=201.30096741389735, w0=0.023499776469055014, w1=0.24908479294848804\n",
      "Gradient Descent(5338/9999): loss=201.26167922410644, w0=0.023497881414947602, w1=0.24905986293919635\n",
      "Gradient Descent(5339/9999): loss=201.2224002741573, w0=0.0234959865450864, w1=0.2490349352907476\n",
      "Gradient Descent(5340/9999): loss=201.18313056170666, w0=0.023494091859450842, w1=0.24901001000291573\n",
      "Gradient Descent(5341/9999): loss=201.1438700844117, w0=0.023492197358020366, w1=0.24898508707547473\n",
      "Gradient Descent(5342/9999): loss=201.10461883993062, w0=0.02349030304077442, w1=0.2489601665081986\n",
      "Gradient Descent(5343/9999): loss=201.06537682592196, w0=0.02348840890769244, w1=0.24893524830086136\n",
      "Gradient Descent(5344/9999): loss=201.0261440400449, w0=0.023486514958753875, w1=0.24891033245323702\n",
      "Gradient Descent(5345/9999): loss=200.9869204799594, w0=0.023484621193938172, w1=0.24888541896509966\n",
      "Gradient Descent(5346/9999): loss=200.9477061433258, w0=0.02348272761322478, w1=0.24886050783622338\n",
      "Gradient Descent(5347/9999): loss=200.90850102780536, w0=0.023480834216593145, w1=0.24883559906638225\n",
      "Gradient Descent(5348/9999): loss=200.86930513105986, w0=0.023478941004022726, w1=0.24881069265535044\n",
      "Gradient Descent(5349/9999): loss=200.83011845075148, w0=0.023477047975492975, w1=0.24878578860290207\n",
      "Gradient Descent(5350/9999): loss=200.79094098454345, w0=0.02347515513098335, w1=0.2487608869088113\n",
      "Gradient Descent(5351/9999): loss=200.75177273009913, w0=0.02347326247047331, w1=0.2487359875728523\n",
      "Gradient Descent(5352/9999): loss=200.71261368508303, w0=0.02347136999394232, w1=0.24871109059479937\n",
      "Gradient Descent(5353/9999): loss=200.6734638471599, w0=0.023469477701369836, w1=0.24868619597442668\n",
      "Gradient Descent(5354/9999): loss=200.63432321399532, w0=0.023467585592735325, w1=0.24866130371150846\n",
      "Gradient Descent(5355/9999): loss=200.59519178325525, w0=0.023465693668018255, w1=0.24863641380581902\n",
      "Gradient Descent(5356/9999): loss=200.55606955260663, w0=0.023463801927198095, w1=0.24861152625713268\n",
      "Gradient Descent(5357/9999): loss=200.5169565197167, w0=0.023461910370254316, w1=0.2485866410652237\n",
      "Gradient Descent(5358/9999): loss=200.47785268225357, w0=0.023460018997166387, w1=0.24856175822986648\n",
      "Gradient Descent(5359/9999): loss=200.43875803788592, w0=0.023458127807913787, w1=0.24853687775083536\n",
      "Gradient Descent(5360/9999): loss=200.39967258428285, w0=0.02345623680247599, w1=0.2485119996279047\n",
      "Gradient Descent(5361/9999): loss=200.36059631911425, w0=0.02345434598083248, w1=0.24848712386084895\n",
      "Gradient Descent(5362/9999): loss=200.32152924005072, w0=0.02345245534296273, w1=0.2484622504494425\n",
      "Gradient Descent(5363/9999): loss=200.28247134476337, w0=0.02345056488884623, w1=0.2484373793934598\n",
      "Gradient Descent(5364/9999): loss=200.24342263092385, w0=0.023448674618462458, w1=0.24841251069267534\n",
      "Gradient Descent(5365/9999): loss=200.20438309620477, w0=0.023446784531790905, w1=0.24838764434686358\n",
      "Gradient Descent(5366/9999): loss=200.1653527382789, w0=0.02344489462881106, w1=0.24836278035579906\n",
      "Gradient Descent(5367/9999): loss=200.12633155481996, w0=0.02344300490950241, w1=0.24833791871925628\n",
      "Gradient Descent(5368/9999): loss=200.08731954350213, w0=0.023441115373844454, w1=0.24831305943700982\n",
      "Gradient Descent(5369/9999): loss=200.0483167020003, w0=0.02343922602181668, w1=0.24828820250883424\n",
      "Gradient Descent(5370/9999): loss=200.0093230279901, w0=0.02343733685339859, w1=0.24826334793450416\n",
      "Gradient Descent(5371/9999): loss=199.9703385191475, w0=0.023435447868569683, w1=0.2482384957137942\n",
      "Gradient Descent(5372/9999): loss=199.93136317314924, w0=0.023433559067309456, w1=0.24821364584647895\n",
      "Gradient Descent(5373/9999): loss=199.89239698767267, w0=0.023431670449597417, w1=0.24818879833233312\n",
      "Gradient Descent(5374/9999): loss=199.8534399603959, w0=0.023429782015413066, w1=0.24816395317113138\n",
      "Gradient Descent(5375/9999): loss=199.8144920889974, w0=0.023427893764735915, w1=0.2481391103626484\n",
      "Gradient Descent(5376/9999): loss=199.77555337115646, w0=0.02342600569754547, w1=0.24811426990665897\n",
      "Gradient Descent(5377/9999): loss=199.736623804553, w0=0.023424117813821237, w1=0.24808943180293777\n",
      "Gradient Descent(5378/9999): loss=199.69770338686735, w0=0.023422230113542736, w1=0.2480645960512596\n",
      "Gradient Descent(5379/9999): loss=199.6587921157806, w0=0.02342034259668948, w1=0.24803976265139926\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(5380/9999): loss=199.61988998897462, w0=0.023418455263240986, w1=0.24801493160313154\n",
      "Gradient Descent(5381/9999): loss=199.58099700413163, w0=0.023416568113176772, w1=0.24799010290623127\n",
      "Gradient Descent(5382/9999): loss=199.54211315893457, w0=0.02341468114647636, w1=0.24796527656047332\n",
      "Gradient Descent(5383/9999): loss=199.50323845106703, w0=0.02341279436311927, w1=0.24794045256563255\n",
      "Gradient Descent(5384/9999): loss=199.46437287821334, w0=0.02341090776308503, w1=0.24791563092148386\n",
      "Gradient Descent(5385/9999): loss=199.4255164380581, w0=0.023409021346353167, w1=0.24789081162780216\n",
      "Gradient Descent(5386/9999): loss=199.38666912828685, w0=0.02340713511290321, w1=0.2478659946843624\n",
      "Gradient Descent(5387/9999): loss=199.34783094658567, w0=0.02340524906271469, w1=0.24784118009093956\n",
      "Gradient Descent(5388/9999): loss=199.30900189064127, w0=0.023403363195767142, w1=0.24781636784730857\n",
      "Gradient Descent(5389/9999): loss=199.2701819581408, w0=0.023401477512040098, w1=0.24779155795324445\n",
      "Gradient Descent(5390/9999): loss=199.23137114677232, w0=0.0233995920115131, w1=0.24776675040852222\n",
      "Gradient Descent(5391/9999): loss=199.19256945422433, w0=0.02339770669416568, w1=0.24774194521291695\n",
      "Gradient Descent(5392/9999): loss=199.15377687818597, w0=0.023395821559977388, w1=0.24771714236620368\n",
      "Gradient Descent(5393/9999): loss=199.114993416347, w0=0.023393936608927763, w1=0.2476923418681575\n",
      "Gradient Descent(5394/9999): loss=199.07621906639775, w0=0.02339205184099635, w1=0.2476675437185535\n",
      "Gradient Descent(5395/9999): loss=199.03745382602938, w0=0.0233901672561627, w1=0.24764274791716684\n",
      "Gradient Descent(5396/9999): loss=198.9986976929334, w0=0.023388282854406355, w1=0.24761795446377266\n",
      "Gradient Descent(5397/9999): loss=198.95995066480222, w0=0.023386398635706872, w1=0.2475931633581461\n",
      "Gradient Descent(5398/9999): loss=198.92121273932844, w0=0.023384514600043805, w1=0.2475683746000624\n",
      "Gradient Descent(5399/9999): loss=198.88248391420575, w0=0.02338263074739671, w1=0.24754358818929675\n",
      "Gradient Descent(5400/9999): loss=198.8437641871282, w0=0.02338074707774514, w1=0.24751880412562438\n",
      "Gradient Descent(5401/9999): loss=198.80505355579052, w0=0.023378863591068662, w1=0.24749402240882054\n",
      "Gradient Descent(5402/9999): loss=198.76635201788798, w0=0.023376980287346832, w1=0.24746924303866052\n",
      "Gradient Descent(5403/9999): loss=198.72765957111667, w0=0.02337509716655922, w1=0.24744446601491962\n",
      "Gradient Descent(5404/9999): loss=198.68897621317302, w0=0.023373214228685383, w1=0.24741969133737315\n",
      "Gradient Descent(5405/9999): loss=198.65030194175426, w0=0.023371331473704897, w1=0.24739491900579647\n",
      "Gradient Descent(5406/9999): loss=198.61163675455825, w0=0.02336944890159733, w1=0.24737014901996493\n",
      "Gradient Descent(5407/9999): loss=198.57298064928335, w0=0.02336756651234225, w1=0.2473453813796539\n",
      "Gradient Descent(5408/9999): loss=198.5343336236286, w0=0.02336568430591924, w1=0.2473206160846388\n",
      "Gradient Descent(5409/9999): loss=198.4956956752937, w0=0.023363802282307868, w1=0.24729585313469504\n",
      "Gradient Descent(5410/9999): loss=198.45706680197884, w0=0.023361920441487716, w1=0.24727109252959809\n",
      "Gradient Descent(5411/9999): loss=198.41844700138503, w0=0.023360038783438362, w1=0.24724633426912337\n",
      "Gradient Descent(5412/9999): loss=198.3798362712136, w0=0.02335815730813939, w1=0.24722157835304642\n",
      "Gradient Descent(5413/9999): loss=198.34123460916683, w0=0.023356276015570387, w1=0.24719682478114272\n",
      "Gradient Descent(5414/9999): loss=198.30264201294733, w0=0.023354394905710937, w1=0.2471720735531878\n",
      "Gradient Descent(5415/9999): loss=198.26405848025857, w0=0.023352513978540632, w1=0.24714732466895722\n",
      "Gradient Descent(5416/9999): loss=198.22548400880441, w0=0.023350633234039058, w1=0.24712257812822658\n",
      "Gradient Descent(5417/9999): loss=198.18691859628942, w0=0.02334875267218581, w1=0.24709783393077142\n",
      "Gradient Descent(5418/9999): loss=198.1483622404189, w0=0.023346872292960483, w1=0.24707309207636738\n",
      "Gradient Descent(5419/9999): loss=198.10981493889852, w0=0.023344992096342672, w1=0.2470483525647901\n",
      "Gradient Descent(5420/9999): loss=198.0712766894348, w0=0.02334311208231198, w1=0.24702361539581524\n",
      "Gradient Descent(5421/9999): loss=198.03274748973476, w0=0.023341232250848002, w1=0.24699888056921845\n",
      "Gradient Descent(5422/9999): loss=197.99422733750598, w0=0.023339352601930347, w1=0.24697414808477547\n",
      "Gradient Descent(5423/9999): loss=197.95571623045683, w0=0.02333747313553862, w1=0.246949417942262\n",
      "Gradient Descent(5424/9999): loss=197.9172141662961, w0=0.023335593851652424, w1=0.24692469014145377\n",
      "Gradient Descent(5425/9999): loss=197.8787211427333, w0=0.023333714750251373, w1=0.24689996468212655\n",
      "Gradient Descent(5426/9999): loss=197.8402371574785, w0=0.023331835831315078, w1=0.24687524156405613\n",
      "Gradient Descent(5427/9999): loss=197.80176220824254, w0=0.02332995709482315, w1=0.2468505207870183\n",
      "Gradient Descent(5428/9999): loss=197.7632962927366, w0=0.023328078540755207, w1=0.24682580235078888\n",
      "Gradient Descent(5429/9999): loss=197.7248394086727, w0=0.023326200169090868, w1=0.24680108625514374\n",
      "Gradient Descent(5430/9999): loss=197.68639155376334, w0=0.02332432197980975, w1=0.24677637249985873\n",
      "Gradient Descent(5431/9999): loss=197.64795272572178, w0=0.023322443972891474, w1=0.24675166108470975\n",
      "Gradient Descent(5432/9999): loss=197.60952292226168, w0=0.023320566148315666, w1=0.2467269520094727\n",
      "Gradient Descent(5433/9999): loss=197.57110214109753, w0=0.023318688506061953, w1=0.2467022452739235\n",
      "Gradient Descent(5434/9999): loss=197.5326903799443, w0=0.023316811046109966, w1=0.24667754087783814\n",
      "Gradient Descent(5435/9999): loss=197.49428763651756, w0=0.023314933768439332, w1=0.24665283882099256\n",
      "Gradient Descent(5436/9999): loss=197.4558939085336, w0=0.023313056673029683, w1=0.24662813910316275\n",
      "Gradient Descent(5437/9999): loss=197.41750919370918, w0=0.02331117975986065, w1=0.24660344172412474\n",
      "Gradient Descent(5438/9999): loss=197.37913348976187, w0=0.023309303028911876, w1=0.24657874668365456\n",
      "Gradient Descent(5439/9999): loss=197.34076679440957, w0=0.023307426480163, w1=0.24655405398152827\n",
      "Gradient Descent(5440/9999): loss=197.30240910537114, w0=0.023305550113593657, w1=0.24652936361752192\n",
      "Gradient Descent(5441/9999): loss=197.26406042036564, w0=0.023303673929183494, w1=0.24650467559141165\n",
      "Gradient Descent(5442/9999): loss=197.22572073711316, w0=0.023301797926912157, w1=0.24647998990297354\n",
      "Gradient Descent(5443/9999): loss=197.187390053334, w0=0.023299922106759293, w1=0.24645530655198375\n",
      "Gradient Descent(5444/9999): loss=197.14906836674945, w0=0.02329804646870455, w1=0.24643062553821843\n",
      "Gradient Descent(5445/9999): loss=197.11075567508118, w0=0.023296171012727577, w1=0.24640594686145378\n",
      "Gradient Descent(5446/9999): loss=197.07245197605147, w0=0.02329429573880803, w1=0.246381270521466\n",
      "Gradient Descent(5447/9999): loss=197.0341572673834, w0=0.023292420646925566, w1=0.2463565965180313\n",
      "Gradient Descent(5448/9999): loss=196.99587154680026, w0=0.02329054573705984, w1=0.24633192485092592\n",
      "Gradient Descent(5449/9999): loss=196.95759481202643, w0=0.023288671009190513, w1=0.24630725551992613\n",
      "Gradient Descent(5450/9999): loss=196.91932706078666, w0=0.023286796463297246, w1=0.24628258852480825\n",
      "Gradient Descent(5451/9999): loss=196.8810682908062, w0=0.023284922099359704, w1=0.24625792386534853\n",
      "Gradient Descent(5452/9999): loss=196.84281849981116, w0=0.023283047917357552, w1=0.24623326154132333\n",
      "Gradient Descent(5453/9999): loss=196.80457768552805, w0=0.023281173917270463, w1=0.246208601552509\n",
      "Gradient Descent(5454/9999): loss=196.7663458456842, w0=0.023279300099078103, w1=0.24618394389868192\n",
      "Gradient Descent(5455/9999): loss=196.72812297800726, w0=0.023277426462760147, w1=0.24615928857961847\n",
      "Gradient Descent(5456/9999): loss=196.6899090802257, w0=0.023275553008296267, w1=0.24613463559509505\n",
      "Gradient Descent(5457/9999): loss=196.65170415006867, w0=0.02327367973566614, w1=0.2461099849448881\n",
      "Gradient Descent(5458/9999): loss=196.6135081852657, w0=0.023271806644849447, w1=0.24608533662877405\n",
      "Gradient Descent(5459/9999): loss=196.57532118354698, w0=0.02326993373582587, w1=0.2460606906465294\n",
      "Gradient Descent(5460/9999): loss=196.5371431426434, w0=0.023268061008575085, w1=0.24603604699793066\n",
      "Gradient Descent(5461/9999): loss=196.49897406028649, w0=0.023266188463076785, w1=0.2460114056827543\n",
      "Gradient Descent(5462/9999): loss=196.46081393420818, w0=0.023264316099310654, w1=0.2459867667007769\n",
      "Gradient Descent(5463/9999): loss=196.42266276214127, w0=0.023262443917256383, w1=0.245962130051775\n",
      "Gradient Descent(5464/9999): loss=196.38452054181897, w0=0.023260571916893662, w1=0.24593749573552517\n",
      "Gradient Descent(5465/9999): loss=196.34638727097519, w0=0.023258700098202186, w1=0.24591286375180404\n",
      "Gradient Descent(5466/9999): loss=196.30826294734442, w0=0.02325682846116165, w1=0.2458882341003882\n",
      "Gradient Descent(5467/9999): loss=196.27014756866174, w0=0.023254957005751756, w1=0.24586360678105432\n",
      "Gradient Descent(5468/9999): loss=196.23204113266289, w0=0.0232530857319522, w1=0.24583898179357902\n",
      "Gradient Descent(5469/9999): loss=196.19394363708406, w0=0.023251214639742685, w1=0.245814359137739\n",
      "Gradient Descent(5470/9999): loss=196.1558550796623, w0=0.023249343729102912, w1=0.24578973881331098\n",
      "Gradient Descent(5471/9999): loss=196.1177754581351, w0=0.023247473000012592, w1=0.24576512082007168\n",
      "Gradient Descent(5472/9999): loss=196.0797047702405, w0=0.02324560245245143, w1=0.24574050515779783\n",
      "Gradient Descent(5473/9999): loss=196.04164301371742, w0=0.023243732086399148, w1=0.2457158918262662\n",
      "Gradient Descent(5474/9999): loss=196.00359018630496, w0=0.023241861901835446, w1=0.24569128082525357\n",
      "Gradient Descent(5475/9999): loss=195.96554628574324, w0=0.023239991898740042, w1=0.24566667215453677\n",
      "Gradient Descent(5476/9999): loss=195.92751130977268, w0=0.02323812207709266, w1=0.2456420658138926\n",
      "Gradient Descent(5477/9999): loss=195.88948525613455, w0=0.02323625243687301, w1=0.24561746180309793\n",
      "Gradient Descent(5478/9999): loss=195.85146812257048, w0=0.02323438297806082, w1=0.24559286012192963\n",
      "Gradient Descent(5479/9999): loss=195.8134599068229, w0=0.023232513700635813, w1=0.24556826077016458\n",
      "Gradient Descent(5480/9999): loss=195.77546060663477, w0=0.023230644604577712, w1=0.2455436637475797\n",
      "Gradient Descent(5481/9999): loss=195.73747021974964, w0=0.023228775689866248, w1=0.2455190690539519\n",
      "Gradient Descent(5482/9999): loss=195.6994887439117, w0=0.02322690695648115, w1=0.24549447668905816\n",
      "Gradient Descent(5483/9999): loss=195.6615161768657, w0=0.02322503840440215, w1=0.24546988665267544\n",
      "Gradient Descent(5484/9999): loss=195.62355251635705, w0=0.023223170033608985, w1=0.24544529894458073\n",
      "Gradient Descent(5485/9999): loss=195.58559776013163, w0=0.023221301844081384, w1=0.24542071356455106\n",
      "Gradient Descent(5486/9999): loss=195.5476519059361, w0=0.023219433835799094, w1=0.24539613051236345\n",
      "Gradient Descent(5487/9999): loss=195.5097149515177, w0=0.023217566008741853, w1=0.24537154978779496\n",
      "Gradient Descent(5488/9999): loss=195.47178689462407, w0=0.023215698362889406, w1=0.24534697139062267\n",
      "Gradient Descent(5489/9999): loss=195.43386773300378, w0=0.02321383089822149, w1=0.2453223953206237\n",
      "Gradient Descent(5490/9999): loss=195.39595746440568, w0=0.023211963614717862, w1=0.24529782157757513\n",
      "Gradient Descent(5491/9999): loss=195.35805608657938, w0=0.023210096512358266, w1=0.24527325016125412\n",
      "Gradient Descent(5492/9999): loss=195.32016359727515, w0=0.023208229591122457, w1=0.24524868107143782\n",
      "Gradient Descent(5493/9999): loss=195.28227999424374, w0=0.02320636285099019, w1=0.24522411430790342\n",
      "Gradient Descent(5494/9999): loss=195.2444052752365, w0=0.023204496291941216, w1=0.24519954987042814\n",
      "Gradient Descent(5495/9999): loss=195.2065394380055, w0=0.023202629913955296, w1=0.24517498775878915\n",
      "Gradient Descent(5496/9999): loss=195.16868248030335, w0=0.02320076371701219, w1=0.24515042797276373\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(5497/9999): loss=195.1308343998832, w0=0.02319889770109166, w1=0.24512587051212914\n",
      "Gradient Descent(5498/9999): loss=195.09299519449897, w0=0.02319703186617347, w1=0.24510131537666266\n",
      "Gradient Descent(5499/9999): loss=195.0551648619049, w0=0.02319516621223739, w1=0.2450767625661416\n",
      "Gradient Descent(5500/9999): loss=195.01734339985606, w0=0.023193300739263185, w1=0.2450522120803433\n",
      "Gradient Descent(5501/9999): loss=194.9795308061081, w0=0.02319143544723063, w1=0.24502766391904507\n",
      "Gradient Descent(5502/9999): loss=194.94172707841713, w0=0.023189570336119496, w1=0.2450031180820243\n",
      "Gradient Descent(5503/9999): loss=194.90393221454013, w0=0.02318770540590956, w1=0.24497857456905836\n",
      "Gradient Descent(5504/9999): loss=194.8661462122343, w0=0.023185840656580595, w1=0.2449540333799247\n",
      "Gradient Descent(5505/9999): loss=194.8283690692578, w0=0.023183976088112387, w1=0.2449294945144007\n",
      "Gradient Descent(5506/9999): loss=194.79060078336914, w0=0.023182111700484714, w1=0.24490495797226383\n",
      "Gradient Descent(5507/9999): loss=194.7528413523276, w0=0.02318024749367736, w1=0.24488042375329155\n",
      "Gradient Descent(5508/9999): loss=194.7150907738929, w0=0.023178383467670115, w1=0.24485589185726137\n",
      "Gradient Descent(5509/9999): loss=194.67734904582554, w0=0.023176519622442764, w1=0.24483136228395078\n",
      "Gradient Descent(5510/9999): loss=194.63961616588645, w0=0.023174655957975097, w1=0.24480683503313733\n",
      "Gradient Descent(5511/9999): loss=194.6018921318372, w0=0.02317279247424691, w1=0.24478231010459855\n",
      "Gradient Descent(5512/9999): loss=194.56417694144008, w0=0.023170929171237996, w1=0.24475778749811206\n",
      "Gradient Descent(5513/9999): loss=194.5264705924578, w0=0.023169066048928152, w1=0.24473326721345542\n",
      "Gradient Descent(5514/9999): loss=194.4887730826538, w0=0.02316720310729718, w1=0.24470874925040625\n",
      "Gradient Descent(5515/9999): loss=194.45108440979212, w0=0.023165340346324878, w1=0.24468423360874217\n",
      "Gradient Descent(5516/9999): loss=194.41340457163733, w0=0.023163477765991054, w1=0.24465972028824085\n",
      "Gradient Descent(5517/9999): loss=194.37573356595453, w0=0.02316161536627551, w1=0.24463520928867996\n",
      "Gradient Descent(5518/9999): loss=194.33807139050964, w0=0.023159753147158055, w1=0.24461070060983722\n",
      "Gradient Descent(5519/9999): loss=194.30041804306896, w0=0.0231578911086185, w1=0.24458619425149034\n",
      "Gradient Descent(5520/9999): loss=194.26277352139945, w0=0.02315602925063666, w1=0.24456169021341703\n",
      "Gradient Descent(5521/9999): loss=194.2251378232688, w0=0.023154167573192344, w1=0.24453718849539507\n",
      "Gradient Descent(5522/9999): loss=194.18751094644503, w0=0.02315230607626537, w1=0.24451268909720222\n",
      "Gradient Descent(5523/9999): loss=194.14989288869702, w0=0.02315044475983556, w1=0.2444881920186163\n",
      "Gradient Descent(5524/9999): loss=194.11228364779416, w0=0.023148583623882734, w1=0.24446369725941514\n",
      "Gradient Descent(5525/9999): loss=194.07468322150635, w0=0.023146722668386716, w1=0.24443920481937656\n",
      "Gradient Descent(5526/9999): loss=194.03709160760408, w0=0.02314486189332733, w1=0.2444147146982784\n",
      "Gradient Descent(5527/9999): loss=193.99950880385867, w0=0.023143001298684407, w1=0.2443902268958986\n",
      "Gradient Descent(5528/9999): loss=193.9619348080418, w0=0.02314114088443777, w1=0.24436574141201503\n",
      "Gradient Descent(5529/9999): loss=193.92436961792583, w0=0.023139280650567258, w1=0.24434125824640562\n",
      "Gradient Descent(5530/9999): loss=193.88681323128364, w0=0.0231374205970527, w1=0.2443167773988483\n",
      "Gradient Descent(5531/9999): loss=193.8492656458889, w0=0.02313556072387394, w1=0.24429229886912104\n",
      "Gradient Descent(5532/9999): loss=193.8117268595156, w0=0.023133701031010808, w1=0.24426782265700184\n",
      "Gradient Descent(5533/9999): loss=193.77419686993855, w0=0.023131841518443153, w1=0.2442433487622687\n",
      "Gradient Descent(5534/9999): loss=193.73667567493302, w0=0.023129982186150814, w1=0.24421887718469962\n",
      "Gradient Descent(5535/9999): loss=193.69916327227503, w0=0.023128123034113636, w1=0.24419440792407268\n",
      "Gradient Descent(5536/9999): loss=193.66165965974102, w0=0.02312626406231147, w1=0.24416994098016592\n",
      "Gradient Descent(5537/9999): loss=193.6241648351081, w0=0.02312440527072416, w1=0.24414547635275746\n",
      "Gradient Descent(5538/9999): loss=193.58667879615393, w0=0.023122546659331562, w1=0.24412101404162537\n",
      "Gradient Descent(5539/9999): loss=193.5492015406569, w0=0.02312068822811353, w1=0.2440965540465478\n",
      "Gradient Descent(5540/9999): loss=193.51173306639592, w0=0.023118829977049914, w1=0.24407209636730287\n",
      "Gradient Descent(5541/9999): loss=193.4742733711504, w0=0.02311697190612058, w1=0.2440476410036688\n",
      "Gradient Descent(5542/9999): loss=193.43682245270037, w0=0.023115114015305388, w1=0.24402318795542374\n",
      "Gradient Descent(5543/9999): loss=193.3993803088266, w0=0.023113256304584197, w1=0.24399873722234594\n",
      "Gradient Descent(5544/9999): loss=193.36194693731025, w0=0.023111398773936875, w1=0.2439742888042136\n",
      "Gradient Descent(5545/9999): loss=193.3245223359333, w0=0.02310954142334329, w1=0.24394984270080497\n",
      "Gradient Descent(5546/9999): loss=193.28710650247808, w0=0.023107684252783308, w1=0.24392539891189835\n",
      "Gradient Descent(5547/9999): loss=193.2496994347278, w0=0.023105827262236804, w1=0.243900957437272\n",
      "Gradient Descent(5548/9999): loss=193.2123011304659, w0=0.02310397045168365, w1=0.24387651827670426\n",
      "Gradient Descent(5549/9999): loss=193.17491158747674, w0=0.023102113821103724, w1=0.24385208142997344\n",
      "Gradient Descent(5550/9999): loss=193.13753080354505, w0=0.0231002573704769, w1=0.2438276468968579\n",
      "Gradient Descent(5551/9999): loss=193.1001587764563, w0=0.02309840109978307, w1=0.243803214677136\n",
      "Gradient Descent(5552/9999): loss=193.06279550399657, w0=0.0230965450090021, w1=0.24377878477058618\n",
      "Gradient Descent(5553/9999): loss=193.02544098395222, w0=0.023094689098113887, w1=0.24375435717698682\n",
      "Gradient Descent(5554/9999): loss=192.9880952141107, w0=0.023092833367098314, w1=0.24372993189611636\n",
      "Gradient Descent(5555/9999): loss=192.95075819225963, w0=0.023090977815935273, w1=0.24370550892775325\n",
      "Gradient Descent(5556/9999): loss=192.9134299161874, w0=0.023089122444604652, w1=0.24368108827167598\n",
      "Gradient Descent(5557/9999): loss=192.876110383683, w0=0.023087267253086345, w1=0.24365666992766302\n",
      "Gradient Descent(5558/9999): loss=192.83879959253596, w0=0.02308541224136025, w1=0.24363225389549292\n",
      "Gradient Descent(5559/9999): loss=192.80149754053647, w0=0.023083557409406267, w1=0.24360784017494422\n",
      "Gradient Descent(5560/9999): loss=192.76420422547514, w0=0.023081702757204295, w1=0.24358342876579545\n",
      "Gradient Descent(5561/9999): loss=192.72691964514343, w0=0.023079848284734233, w1=0.2435590196678252\n",
      "Gradient Descent(5562/9999): loss=192.68964379733316, w0=0.023077993991975992, w1=0.24353461288081207\n",
      "Gradient Descent(5563/9999): loss=192.65237667983692, w0=0.023076139878909475, w1=0.2435102084045347\n",
      "Gradient Descent(5564/9999): loss=192.61511829044773, w0=0.023074285945514594, w1=0.2434858062387717\n",
      "Gradient Descent(5565/9999): loss=192.57786862695932, w0=0.023072432191771255, w1=0.24346140638330174\n",
      "Gradient Descent(5566/9999): loss=192.5406276871659, w0=0.02307057861765938, w1=0.2434370088379035\n",
      "Gradient Descent(5567/9999): loss=192.50339546886235, w0=0.02306872522315888, w1=0.2434126136023557\n",
      "Gradient Descent(5568/9999): loss=192.46617196984423, w0=0.023066872008249674, w1=0.24338822067643703\n",
      "Gradient Descent(5569/9999): loss=192.4289571879074, w0=0.02306501897291168, w1=0.24336383005992626\n",
      "Gradient Descent(5570/9999): loss=192.3917511208486, w0=0.023063166117124827, w1=0.24333944175260214\n",
      "Gradient Descent(5571/9999): loss=192.35455376646502, w0=0.023061313440869032, w1=0.24331505575424345\n",
      "Gradient Descent(5572/9999): loss=192.3173651225544, w0=0.023059460944124227, w1=0.24329067206462898\n",
      "Gradient Descent(5573/9999): loss=192.28018518691533, w0=0.02305760862687034, w1=0.24326629068353758\n",
      "Gradient Descent(5574/9999): loss=192.24301395734662, w0=0.023055756489087306, w1=0.24324191161074807\n",
      "Gradient Descent(5575/9999): loss=192.2058514316478, w0=0.02305390453075505, w1=0.24321753484603933\n",
      "Gradient Descent(5576/9999): loss=192.16869760761918, w0=0.023052052751853517, w1=0.24319316038919025\n",
      "Gradient Descent(5577/9999): loss=192.13155248306143, w0=0.023050201152362643, w1=0.24316878823997973\n",
      "Gradient Descent(5578/9999): loss=192.0944160557759, w0=0.023048349732262368, w1=0.24314441839818668\n",
      "Gradient Descent(5579/9999): loss=192.05728832356442, w0=0.023046498491532633, w1=0.24312005086359006\n",
      "Gradient Descent(5580/9999): loss=192.02016928422958, w0=0.023044647430153383, w1=0.24309568563596884\n",
      "Gradient Descent(5581/9999): loss=191.98305893557455, w0=0.023042796548104566, w1=0.24307132271510198\n",
      "Gradient Descent(5582/9999): loss=191.94595727540292, w0=0.02304094584536613, w1=0.2430469621007685\n",
      "Gradient Descent(5583/9999): loss=191.90886430151897, w0=0.023039095321918033, w1=0.24302260379274745\n",
      "Gradient Descent(5584/9999): loss=191.87178001172754, w0=0.02303724497774022, w1=0.24299824779081786\n",
      "Gradient Descent(5585/9999): loss=191.83470440383405, w0=0.023035394812812655, w1=0.24297389409475878\n",
      "Gradient Descent(5586/9999): loss=191.79763747564456, w0=0.023033544827115292, w1=0.24294954270434932\n",
      "Gradient Descent(5587/9999): loss=191.76057922496577, w0=0.023031695020628094, w1=0.24292519361936857\n",
      "Gradient Descent(5588/9999): loss=191.72352964960479, w0=0.02302984539333102, w1=0.24290084683959567\n",
      "Gradient Descent(5589/9999): loss=191.6864887473693, w0=0.023027995945204036, w1=0.2428765023648098\n",
      "Gradient Descent(5590/9999): loss=191.64945651606783, w0=0.02302614667622711, w1=0.24285216019479006\n",
      "Gradient Descent(5591/9999): loss=191.6124329535093, w0=0.023024297586380214, w1=0.24282782032931569\n",
      "Gradient Descent(5592/9999): loss=191.57541805750327, w0=0.023022448675643317, w1=0.24280348276816588\n",
      "Gradient Descent(5593/9999): loss=191.53841182585975, w0=0.023020599943996394, w1=0.24277914751111987\n",
      "Gradient Descent(5594/9999): loss=191.50141425638955, w0=0.02301875139141942, w1=0.24275481455795692\n",
      "Gradient Descent(5595/9999): loss=191.464425346904, w0=0.023016903017892377, w1=0.24273048390845628\n",
      "Gradient Descent(5596/9999): loss=191.42744509521492, w0=0.02301505482339524, w1=0.24270615556239725\n",
      "Gradient Descent(5597/9999): loss=191.39047349913474, w0=0.023013206807908, w1=0.24268182951955913\n",
      "Gradient Descent(5598/9999): loss=191.35351055647658, w0=0.02301135897141063, w1=0.24265750577972128\n",
      "Gradient Descent(5599/9999): loss=191.316556265054, w0=0.023009511313883128, w1=0.24263318434266304\n",
      "Gradient Descent(5600/9999): loss=191.27961062268133, w0=0.02300766383530548, w1=0.24260886520816374\n",
      "Gradient Descent(5601/9999): loss=191.2426736271733, w0=0.023005816535657676, w1=0.24258454837600282\n",
      "Gradient Descent(5602/9999): loss=191.20574527634525, w0=0.023003969414919712, w1=0.24256023384595968\n",
      "Gradient Descent(5603/9999): loss=191.16882556801318, w0=0.023002122473071585, w1=0.24253592161781376\n",
      "Gradient Descent(5604/9999): loss=191.13191449999374, w0=0.023000275710093294, w1=0.2425116116913445\n",
      "Gradient Descent(5605/9999): loss=191.09501207010393, w0=0.02299842912596484, w1=0.24248730406633137\n",
      "Gradient Descent(5606/9999): loss=191.05811827616148, w0=0.022996582720666226, w1=0.2424629987425539\n",
      "Gradient Descent(5607/9999): loss=191.02123311598478, w0=0.022994736494177458, w1=0.24243869571979154\n",
      "Gradient Descent(5608/9999): loss=190.98435658739263, w0=0.02299289044647854, w1=0.24241439499782388\n",
      "Gradient Descent(5609/9999): loss=190.94748868820457, w0=0.022991044577549486, w1=0.24239009657643046\n",
      "Gradient Descent(5610/9999): loss=190.91062941624054, w0=0.022989198887370305, w1=0.24236580045539086\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(5611/9999): loss=190.87377876932123, w0=0.022987353375921015, w1=0.24234150663448464\n",
      "Gradient Descent(5612/9999): loss=190.83693674526788, w0=0.02298550804318163, w1=0.24231721511349147\n",
      "Gradient Descent(5613/9999): loss=190.80010334190223, w0=0.02298366288913217, w1=0.24229292589219092\n",
      "Gradient Descent(5614/9999): loss=190.76327855704665, w0=0.022981817913752655, w1=0.2422686389703627\n",
      "Gradient Descent(5615/9999): loss=190.72646238852417, w0=0.02297997311702311, w1=0.24224435434778646\n",
      "Gradient Descent(5616/9999): loss=190.6896548341582, w0=0.02297812849892356, w1=0.2422200720242419\n",
      "Gradient Descent(5617/9999): loss=190.65285589177307, w0=0.02297628405943403, w1=0.24219579199950875\n",
      "Gradient Descent(5618/9999): loss=190.61606555919326, w0=0.022974439798534554, w1=0.24217151427336672\n",
      "Gradient Descent(5619/9999): loss=190.5792838342441, w0=0.022972595716205163, w1=0.24214723884559558\n",
      "Gradient Descent(5620/9999): loss=190.54251071475153, w0=0.022970751812425895, w1=0.2421229657159751\n",
      "Gradient Descent(5621/9999): loss=190.50574619854197, w0=0.022968908087176783, w1=0.2420986948842851\n",
      "Gradient Descent(5622/9999): loss=190.46899028344234, w0=0.022967064540437867, w1=0.24207442635030538\n",
      "Gradient Descent(5623/9999): loss=190.4322429672803, w0=0.022965221172189187, w1=0.24205016011381578\n",
      "Gradient Descent(5624/9999): loss=190.39550424788413, w0=0.02296337798241079, w1=0.24202589617459616\n",
      "Gradient Descent(5625/9999): loss=190.35877412308253, w0=0.022961534971082723, w1=0.2420016345324264\n",
      "Gradient Descent(5626/9999): loss=190.32205259070477, w0=0.02295969213818503, w1=0.24197737518708637\n",
      "Gradient Descent(5627/9999): loss=190.28533964858084, w0=0.02295784948369776, w1=0.24195311813835602\n",
      "Gradient Descent(5628/9999): loss=190.2486352945412, w0=0.022956007007600972, w1=0.24192886338601527\n",
      "Gradient Descent(5629/9999): loss=190.211939526417, w0=0.022954164709874717, w1=0.24190461092984408\n",
      "Gradient Descent(5630/9999): loss=190.1752523420398, w0=0.022952322590499057, w1=0.24188036076962244\n",
      "Gradient Descent(5631/9999): loss=190.13857373924188, w0=0.022950480649454045, w1=0.24185611290513034\n",
      "Gradient Descent(5632/9999): loss=190.1019037158561, w0=0.022948638886719747, w1=0.2418318673361478\n",
      "Gradient Descent(5633/9999): loss=190.06524226971578, w0=0.022946797302276223, w1=0.24180762406245487\n",
      "Gradient Descent(5634/9999): loss=190.02858939865493, w0=0.02294495589610354, w1=0.2417833830838316\n",
      "Gradient Descent(5635/9999): loss=189.99194510050805, w0=0.022943114668181774, w1=0.24175914440005808\n",
      "Gradient Descent(5636/9999): loss=189.9553093731104, w0=0.02294127361849099, w1=0.2417349080109144\n",
      "Gradient Descent(5637/9999): loss=189.91868221429755, w0=0.022939432747011258, w1=0.24171067391618067\n",
      "Gradient Descent(5638/9999): loss=189.88206362190581, w0=0.022937592053722658, w1=0.24168644211563706\n",
      "Gradient Descent(5639/9999): loss=189.84545359377213, w0=0.02293575153860527, w1=0.2416622126090637\n",
      "Gradient Descent(5640/9999): loss=189.80885212773387, w0=0.022933911201639165, w1=0.2416379853962408\n",
      "Gradient Descent(5641/9999): loss=189.77225922162907, w0=0.022932071042804435, w1=0.2416137604769485\n",
      "Gradient Descent(5642/9999): loss=189.73567487329626, w0=0.02293023106208116, w1=0.24158953785096707\n",
      "Gradient Descent(5643/9999): loss=189.6990990805747, w0=0.022928391259449423, w1=0.24156531751807675\n",
      "Gradient Descent(5644/9999): loss=189.66253184130412, w0=0.02292655163488932, w1=0.24154109947805777\n",
      "Gradient Descent(5645/9999): loss=189.6259731533248, w0=0.02292471218838094, w1=0.24151688373069044\n",
      "Gradient Descent(5646/9999): loss=189.5894230144776, w0=0.022922872919904375, w1=0.24149267027575505\n",
      "Gradient Descent(5647/9999): loss=189.55288142260414, w0=0.022921033829439722, w1=0.24146845911303194\n",
      "Gradient Descent(5648/9999): loss=189.5163483755464, w0=0.02291919491696708, w1=0.24144425024230143\n",
      "Gradient Descent(5649/9999): loss=189.47982387114695, w0=0.022917356182466545, w1=0.24142004366334388\n",
      "Gradient Descent(5650/9999): loss=189.44330790724902, w0=0.022915517625918227, w1=0.24139583937593967\n",
      "Gradient Descent(5651/9999): loss=189.40680048169648, w0=0.022913679247302227, w1=0.24137163737986922\n",
      "Gradient Descent(5652/9999): loss=189.37030159233362, w0=0.02291184104659865, w1=0.24134743767491293\n",
      "Gradient Descent(5653/9999): loss=189.33381123700528, w0=0.02291000302378761, w1=0.24132324026085125\n",
      "Gradient Descent(5654/9999): loss=189.29732941355707, w0=0.022908165178849214, w1=0.24129904513746464\n",
      "Gradient Descent(5655/9999): loss=189.2608561198351, w0=0.02290632751176358, w1=0.24127485230453358\n",
      "Gradient Descent(5656/9999): loss=189.22439135368592, w0=0.022904490022510823, w1=0.24125066176183857\n",
      "Gradient Descent(5657/9999): loss=189.18793511295686, w0=0.02290265271107106, w1=0.24122647350916016\n",
      "Gradient Descent(5658/9999): loss=189.1514873954957, w0=0.022900815577424412, w1=0.24120228754627884\n",
      "Gradient Descent(5659/9999): loss=189.11504819915075, w0=0.022898978621551002, w1=0.24117810387297522\n",
      "Gradient Descent(5660/9999): loss=189.078617521771, w0=0.022897141843430956, w1=0.24115392248902986\n",
      "Gradient Descent(5661/9999): loss=189.04219536120598, w0=0.022895305243044402, w1=0.24112974339422338\n",
      "Gradient Descent(5662/9999): loss=189.00578171530586, w0=0.02289346882037147, w1=0.24110556658833637\n",
      "Gradient Descent(5663/9999): loss=188.96937658192115, w0=0.02289163257539229, w1=0.2410813920711495\n",
      "Gradient Descent(5664/9999): loss=188.93297995890325, w0=0.022889796508087003, w1=0.24105721984244344\n",
      "Gradient Descent(5665/9999): loss=188.8965918441039, w0=0.022887960618435742, w1=0.24103304990199884\n",
      "Gradient Descent(5666/9999): loss=188.86021223537554, w0=0.022886124906418646, w1=0.2410088822495964\n",
      "Gradient Descent(5667/9999): loss=188.82384113057114, w0=0.022884289372015854, w1=0.24098471688501688\n",
      "Gradient Descent(5668/9999): loss=188.78747852754418, w0=0.022882454015207512, w1=0.240960553808041\n",
      "Gradient Descent(5669/9999): loss=188.75112442414877, w0=0.022880618835973764, w1=0.24093639301844952\n",
      "Gradient Descent(5670/9999): loss=188.71477881823975, w0=0.02287878383429476, w1=0.24091223451602323\n",
      "Gradient Descent(5671/9999): loss=188.67844170767214, w0=0.022876949010150652, w1=0.24088807830054293\n",
      "Gradient Descent(5672/9999): loss=188.64211309030193, w0=0.02287511436352159, w1=0.24086392437178944\n",
      "Gradient Descent(5673/9999): loss=188.6057929639855, w0=0.02287327989438773, w1=0.2408397727295436\n",
      "Gradient Descent(5674/9999): loss=188.5694813265798, w0=0.022871445602729232, w1=0.24081562337358628\n",
      "Gradient Descent(5675/9999): loss=188.53317817594234, w0=0.02286961148852625, w1=0.24079147630369835\n",
      "Gradient Descent(5676/9999): loss=188.4968835099313, w0=0.022867777551758946, w1=0.24076733151966073\n",
      "Gradient Descent(5677/9999): loss=188.4605973264053, w0=0.022865943792407493, w1=0.24074318902125433\n",
      "Gradient Descent(5678/9999): loss=188.42431962322368, w0=0.022864110210452048, w1=0.2407190488082601\n",
      "Gradient Descent(5679/9999): loss=188.3880503982462, w0=0.022862276805872786, w1=0.240694910880459\n",
      "Gradient Descent(5680/9999): loss=188.3517896493333, w0=0.022860443578649876, w1=0.240670775237632\n",
      "Gradient Descent(5681/9999): loss=188.31553737434598, w0=0.02285861052876349, w1=0.24064664187956014\n",
      "Gradient Descent(5682/9999): loss=188.2792935711457, w0=0.022856777656193802, w1=0.24062251080602443\n",
      "Gradient Descent(5683/9999): loss=188.24305823759465, w0=0.022854944960920994, w1=0.2405983820168059\n",
      "Gradient Descent(5684/9999): loss=188.20683137155544, w0=0.022853112442925247, w1=0.2405742555116856\n",
      "Gradient Descent(5685/9999): loss=188.17061297089143, w0=0.02285128010218674, w1=0.24055013129044464\n",
      "Gradient Descent(5686/9999): loss=188.13440303346633, w0=0.02284944793868566, w1=0.2405260093528641\n",
      "Gradient Descent(5687/9999): loss=188.09820155714468, w0=0.02284761595240219, w1=0.24050188969872513\n",
      "Gradient Descent(5688/9999): loss=188.0620085397912, w0=0.022845784143316523, w1=0.24047777232780884\n",
      "Gradient Descent(5689/9999): loss=188.02582397927165, w0=0.02284395251140885, w1=0.24045365723989642\n",
      "Gradient Descent(5690/9999): loss=187.98964787345201, w0=0.022842121056659365, w1=0.24042954443476902\n",
      "Gradient Descent(5691/9999): loss=187.95348022019903, w0=0.022840289779048263, w1=0.24040543391220787\n",
      "Gradient Descent(5692/9999): loss=187.9173210173799, w0=0.022838458678555745, w1=0.2403813256719942\n",
      "Gradient Descent(5693/9999): loss=187.88117026286236, w0=0.022836627755162008, w1=0.24035721971390922\n",
      "Gradient Descent(5694/9999): loss=187.8450279545149, w0=0.022834797008847257, w1=0.24033311603773422\n",
      "Gradient Descent(5695/9999): loss=187.8088940902065, w0=0.0228329664395917, w1=0.24030901464325047\n",
      "Gradient Descent(5696/9999): loss=187.77276866780653, w0=0.022831136047375543, w1=0.24028491553023928\n",
      "Gradient Descent(5697/9999): loss=187.7366516851852, w0=0.022829305832178997, w1=0.24026081869848198\n",
      "Gradient Descent(5698/9999): loss=187.70054314021303, w0=0.022827475793982272, w1=0.2402367241477599\n",
      "Gradient Descent(5699/9999): loss=187.6644430307614, w0=0.022825645932765582, w1=0.2402126318778544\n",
      "Gradient Descent(5700/9999): loss=187.62835135470198, w0=0.022823816248509148, w1=0.24018854188854688\n",
      "Gradient Descent(5701/9999): loss=187.59226810990708, w0=0.022821986741193186, w1=0.2401644541796187\n",
      "Gradient Descent(5702/9999): loss=187.55619329424977, w0=0.02282015741079792, w1=0.24014036875085132\n",
      "Gradient Descent(5703/9999): loss=187.5201269056035, w0=0.022818328257303573, w1=0.24011628560202616\n",
      "Gradient Descent(5704/9999): loss=187.48406894184217, w0=0.022816499280690367, w1=0.24009220473292472\n",
      "Gradient Descent(5705/9999): loss=187.4480194008406, w0=0.022814670480938534, w1=0.24006812614332845\n",
      "Gradient Descent(5706/9999): loss=187.41197828047385, w0=0.022812841858028305, w1=0.24004404983301886\n",
      "Gradient Descent(5707/9999): loss=187.37594557861772, w0=0.022811013411939914, w1=0.24001997580177747\n",
      "Gradient Descent(5708/9999): loss=187.33992129314854, w0=0.022809185142653593, w1=0.2399959040493858\n",
      "Gradient Descent(5709/9999): loss=187.3039054219432, w0=0.022807357050149583, w1=0.23997183457562546\n",
      "Gradient Descent(5710/9999): loss=187.26789796287915, w0=0.022805529134408118, w1=0.23994776738027798\n",
      "Gradient Descent(5711/9999): loss=187.23189891383439, w0=0.022803701395409447, w1=0.239923702463125\n",
      "Gradient Descent(5712/9999): loss=187.19590827268755, w0=0.022801873833133812, w1=0.23989963982394813\n",
      "Gradient Descent(5713/9999): loss=187.15992603731772, w0=0.02280004644756146, w1=0.23987557946252902\n",
      "Gradient Descent(5714/9999): loss=187.12395220560467, w0=0.02279821923867264, w1=0.23985152137864932\n",
      "Gradient Descent(5715/9999): loss=187.08798677542867, w0=0.0227963922064476, w1=0.2398274655720907\n",
      "Gradient Descent(5716/9999): loss=187.0520297446706, w0=0.0227945653508666, w1=0.23980341204263486\n",
      "Gradient Descent(5717/9999): loss=187.01608111121175, w0=0.02279273867190989, w1=0.23977936079006354\n",
      "Gradient Descent(5718/9999): loss=186.98014087293427, w0=0.022790912169557732, w1=0.2397553118141585\n",
      "Gradient Descent(5719/9999): loss=186.94420902772052, w0=0.022789085843790385, w1=0.23973126511470144\n",
      "Gradient Descent(5720/9999): loss=186.90828557345375, w0=0.022787259694588113, w1=0.23970722069147418\n",
      "Gradient Descent(5721/9999): loss=186.87237050801758, w0=0.022785433721931182, w1=0.23968317854425852\n",
      "Gradient Descent(5722/9999): loss=186.83646382929626, w0=0.022783607925799858, w1=0.23965913867283628\n",
      "Gradient Descent(5723/9999): loss=186.80056553517454, w0=0.02278178230617441, w1=0.2396351010769893\n",
      "Gradient Descent(5724/9999): loss=186.76467562353787, w0=0.022779956863035113, w1=0.23961106575649943\n",
      "Gradient Descent(5725/9999): loss=186.7287940922721, w0=0.022778131596362237, w1=0.23958703271114856\n",
      "Gradient Descent(5726/9999): loss=186.69292093926376, w0=0.022776306506136063, w1=0.23956300194071858\n",
      "Gradient Descent(5727/9999): loss=186.65705616239984, w0=0.02277448159233687, w1=0.23953897344499142\n",
      "Gradient Descent(5728/9999): loss=186.62119975956807, w0=0.022772656854944937, w1=0.23951494722374903\n",
      "Gradient Descent(5729/9999): loss=186.58535172865655, w0=0.022770832293940547, w1=0.23949092327677335\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(5730/9999): loss=186.54951206755405, w0=0.022769007909303987, w1=0.23946690160384637\n",
      "Gradient Descent(5731/9999): loss=186.5136807741499, w0=0.022767183701015544, w1=0.23944288220475007\n",
      "Gradient Descent(5732/9999): loss=186.47785784633385, w0=0.02276535966905551, w1=0.2394188650792665\n",
      "Gradient Descent(5733/9999): loss=186.44204328199643, w0=0.022763535813404182, w1=0.23939485022717766\n",
      "Gradient Descent(5734/9999): loss=186.4062370790287, w0=0.02276171213404185, w1=0.23937083764826564\n",
      "Gradient Descent(5735/9999): loss=186.37043923532207, w0=0.022759888630948813, w1=0.2393468273423125\n",
      "Gradient Descent(5736/9999): loss=186.33464974876875, w0=0.02275806530410537, w1=0.23932281930910035\n",
      "Gradient Descent(5737/9999): loss=186.2988686172614, w0=0.022756242153491823, w1=0.23929881354841132\n",
      "Gradient Descent(5738/9999): loss=186.26309583869332, w0=0.022754419179088478, w1=0.23927481006002752\n",
      "Gradient Descent(5739/9999): loss=186.22733141095821, w0=0.02275259638087564, w1=0.23925080884373112\n",
      "Gradient Descent(5740/9999): loss=186.19157533195045, w0=0.02275077375883362, w1=0.2392268098993043\n",
      "Gradient Descent(5741/9999): loss=186.15582759956496, w0=0.022748951312942727, w1=0.23920281322652925\n",
      "Gradient Descent(5742/9999): loss=186.12008821169735, w0=0.022747129043183278, w1=0.2391788188251882\n",
      "Gradient Descent(5743/9999): loss=186.0843571662435, w0=0.022745306949535585, w1=0.23915482669506338\n",
      "Gradient Descent(5744/9999): loss=186.0486344611001, w0=0.02274348503197997, w1=0.23913083683593706\n",
      "Gradient Descent(5745/9999): loss=186.01292009416434, w0=0.022741663290496754, w1=0.23910684924759149\n",
      "Gradient Descent(5746/9999): loss=185.9772140633339, w0=0.022739841725066257, w1=0.239082863929809\n",
      "Gradient Descent(5747/9999): loss=185.94151636650713, w0=0.022738020335668806, w1=0.23905888088237184\n",
      "Gradient Descent(5748/9999): loss=185.90582700158276, w0=0.022736199122284726, w1=0.23903490010506243\n",
      "Gradient Descent(5749/9999): loss=185.87014596646043, w0=0.02273437808489435, w1=0.23901092159766307\n",
      "Gradient Descent(5750/9999): loss=185.83447325903978, w0=0.02273255722347801, w1=0.23898694535995618\n",
      "Gradient Descent(5751/9999): loss=185.7988088772216, w0=0.02273073653801604, w1=0.23896297139172412\n",
      "Gradient Descent(5752/9999): loss=185.7631528189069, w0=0.022728916028488772, w1=0.23893899969274932\n",
      "Gradient Descent(5753/9999): loss=185.72750508199732, w0=0.02272709569487655, w1=0.23891503026281422\n",
      "Gradient Descent(5754/9999): loss=185.69186566439507, w0=0.022725275537159716, w1=0.23889106310170125\n",
      "Gradient Descent(5755/9999): loss=185.6562345640029, w0=0.022723455555318614, w1=0.2388670982091929\n",
      "Gradient Descent(5756/9999): loss=185.6206117787242, w0=0.02272163574933359, w1=0.2388431355850717\n",
      "Gradient Descent(5757/9999): loss=185.58499730646275, w0=0.02271981611918499, w1=0.2388191752291201\n",
      "Gradient Descent(5758/9999): loss=185.54939114512308, w0=0.022717996664853166, w1=0.23879521714112067\n",
      "Gradient Descent(5759/9999): loss=185.51379329261016, w0=0.02271617738631847, w1=0.23877126132085597\n",
      "Gradient Descent(5760/9999): loss=185.4782037468295, w0=0.022714358283561262, w1=0.23874730776810854\n",
      "Gradient Descent(5761/9999): loss=185.44262250568732, w0=0.02271253935656189, w1=0.23872335648266102\n",
      "Gradient Descent(5762/9999): loss=185.40704956709024, w0=0.022710720605300723, w1=0.238699407464296\n",
      "Gradient Descent(5763/9999): loss=185.37148492894545, w0=0.02270890202975812, w1=0.2386754607127961\n",
      "Gradient Descent(5764/9999): loss=185.33592858916083, w0=0.022707083629914446, w1=0.238651516227944\n",
      "Gradient Descent(5765/9999): loss=185.3003805456447, w0=0.022705265405750068, w1=0.23862757400952236\n",
      "Gradient Descent(5766/9999): loss=185.26484079630592, w0=0.022703447357245353, w1=0.23860363405731386\n",
      "Gradient Descent(5767/9999): loss=185.22930933905394, w0=0.022701629484380675, w1=0.2385796963711012\n",
      "Gradient Descent(5768/9999): loss=185.19378617179882, w0=0.02269981178713641, w1=0.23855576095066716\n",
      "Gradient Descent(5769/9999): loss=185.15827129245113, w0=0.02269799426549293, w1=0.23853182779579446\n",
      "Gradient Descent(5770/9999): loss=185.12276469892208, w0=0.022696176919430613, w1=0.2385078969062659\n",
      "Gradient Descent(5771/9999): loss=185.08726638912322, w0=0.022694359748929846, w1=0.23848396828186422\n",
      "Gradient Descent(5772/9999): loss=185.0517763609669, w0=0.02269254275397101, w1=0.23846004192237227\n",
      "Gradient Descent(5773/9999): loss=185.01629461236587, w0=0.022690725934534486, w1=0.23843611782757287\n",
      "Gradient Descent(5774/9999): loss=184.9808211412335, w0=0.022688909290600665, w1=0.23841219599724886\n",
      "Gradient Descent(5775/9999): loss=184.9453559454837, w0=0.022687092822149937, w1=0.23838827643118313\n",
      "Gradient Descent(5776/9999): loss=184.9098990230309, w0=0.022685276529162697, w1=0.23836435912915857\n",
      "Gradient Descent(5777/9999): loss=184.87445037179023, w0=0.022683460411619335, w1=0.23834044409095806\n",
      "Gradient Descent(5778/9999): loss=184.8390099896772, w0=0.02268164446950025, w1=0.23831653131636457\n",
      "Gradient Descent(5779/9999): loss=184.80357787460795, w0=0.02267982870278584, w1=0.23829262080516103\n",
      "Gradient Descent(5780/9999): loss=184.76815402449915, w0=0.022678013111456508, w1=0.2382687125571304\n",
      "Gradient Descent(5781/9999): loss=184.7327384372681, w0=0.022676197695492662, w1=0.23824480657205566\n",
      "Gradient Descent(5782/9999): loss=184.69733111083258, w0=0.022674382454874704, w1=0.23822090284971983\n",
      "Gradient Descent(5783/9999): loss=184.66193204311088, w0=0.022672567389583044, w1=0.23819700138990596\n",
      "Gradient Descent(5784/9999): loss=184.62654123202196, w0=0.022670752499598094, w1=0.23817310219239707\n",
      "Gradient Descent(5785/9999): loss=184.59115867548536, w0=0.022668937784900266, w1=0.23814920525697625\n",
      "Gradient Descent(5786/9999): loss=184.555784371421, w0=0.02266712324546998, w1=0.23812531058342656\n",
      "Gradient Descent(5787/9999): loss=184.52041831774946, w0=0.022665308881287642, w1=0.23810141817153113\n",
      "Gradient Descent(5788/9999): loss=184.4850605123919, w0=0.022663494692333683, w1=0.23807752802107307\n",
      "Gradient Descent(5789/9999): loss=184.44971095327, w0=0.022661680678588522, w1=0.23805364013183555\n",
      "Gradient Descent(5790/9999): loss=184.414369638306, w0=0.022659866840032586, w1=0.23802975450360173\n",
      "Gradient Descent(5791/9999): loss=184.3790365654226, w0=0.022658053176646303, w1=0.23800587113615476\n",
      "Gradient Descent(5792/9999): loss=184.34371173254323, w0=0.0226562396884101, w1=0.2379819900292779\n",
      "Gradient Descent(5793/9999): loss=184.30839513759182, w0=0.02265442637530441, w1=0.2379581111827543\n",
      "Gradient Descent(5794/9999): loss=184.2730867784927, w0=0.02265261323730967, w1=0.23793423459636728\n",
      "Gradient Descent(5795/9999): loss=184.23778665317093, w0=0.022650800274406312, w1=0.23791036026990006\n",
      "Gradient Descent(5796/9999): loss=184.20249475955208, w0=0.022648987486574776, w1=0.23788648820313593\n",
      "Gradient Descent(5797/9999): loss=184.16721109556218, w0=0.022647174873795504, w1=0.2378626183958582\n",
      "Gradient Descent(5798/9999): loss=184.1319356591279, w0=0.022645362436048942, w1=0.23783875084785022\n",
      "Gradient Descent(5799/9999): loss=184.09666844817656, w0=0.022643550173315534, w1=0.2378148855588953\n",
      "Gradient Descent(5800/9999): loss=184.06140946063587, w0=0.02264173808557573, w1=0.2377910225287768\n",
      "Gradient Descent(5801/9999): loss=184.02615869443403, w0=0.022639926172809976, w1=0.2377671617572781\n",
      "Gradient Descent(5802/9999): loss=183.99091614750006, w0=0.02263811443499873, w1=0.23774330324418264\n",
      "Gradient Descent(5803/9999): loss=183.95568181776326, w0=0.02263630287212244, w1=0.2377194469892738\n",
      "Gradient Descent(5804/9999): loss=183.92045570315364, w0=0.02263449148416157, w1=0.23769559299233506\n",
      "Gradient Descent(5805/9999): loss=183.88523780160176, w0=0.02263268027109658, w1=0.23767174125314985\n",
      "Gradient Descent(5806/9999): loss=183.8500281110386, w0=0.02263086923290793, w1=0.23764789177150167\n",
      "Gradient Descent(5807/9999): loss=183.81482662939587, w0=0.02262905836957609, w1=0.237624044547174\n",
      "Gradient Descent(5808/9999): loss=183.7796333546057, w0=0.022627247681081517, w1=0.23760019957995038\n",
      "Gradient Descent(5809/9999): loss=183.74444828460076, w0=0.02262543716740469, w1=0.2375763568696143\n",
      "Gradient Descent(5810/9999): loss=183.7092714173145, w0=0.02262362682852607, w1=0.2375525164159494\n",
      "Gradient Descent(5811/9999): loss=183.67410275068056, w0=0.022621816664426142, w1=0.23752867821873921\n",
      "Gradient Descent(5812/9999): loss=183.63894228263337, w0=0.022620006675085375, w1=0.23750484227776733\n",
      "Gradient Descent(5813/9999): loss=183.60379001110778, w0=0.02261819686048425, w1=0.2374810085928174\n",
      "Gradient Descent(5814/9999): loss=183.5686459340394, w0=0.02261638722060325, w1=0.23745717716367304\n",
      "Gradient Descent(5815/9999): loss=183.53351004936417, w0=0.022614577755422857, w1=0.2374333479901179\n",
      "Gradient Descent(5816/9999): loss=183.49838235501866, w0=0.022612768464923556, w1=0.23740952107193566\n",
      "Gradient Descent(5817/9999): loss=183.46326284894008, w0=0.022610959349085833, w1=0.23738569640891002\n",
      "Gradient Descent(5818/9999): loss=183.4281515290659, w0=0.02260915040789018, w1=0.23736187400082473\n",
      "Gradient Descent(5819/9999): loss=183.3930483933346, w0=0.02260734164131709, w1=0.23733805384746348\n",
      "Gradient Descent(5820/9999): loss=183.3579534396848, w0=0.022605533049347058, w1=0.23731423594861004\n",
      "Gradient Descent(5821/9999): loss=183.3228666660558, w0=0.02260372463196058, w1=0.23729042030404818\n",
      "Gradient Descent(5822/9999): loss=183.2877880703875, w0=0.022601916389138155, w1=0.23726660691356172\n",
      "Gradient Descent(5823/9999): loss=183.25271765062027, w0=0.022600108320860287, w1=0.23724279577693444\n",
      "Gradient Descent(5824/9999): loss=183.21765540469514, w0=0.022598300427107476, w1=0.2372189868939502\n",
      "Gradient Descent(5825/9999): loss=183.18260133055364, w0=0.022596492707860235, w1=0.23719518026439285\n",
      "Gradient Descent(5826/9999): loss=183.14755542613779, w0=0.02259468516309907, w1=0.23717137588804627\n",
      "Gradient Descent(5827/9999): loss=183.11251768939007, w0=0.02259287779280449, w1=0.23714757376469434\n",
      "Gradient Descent(5828/9999): loss=183.07748811825388, w0=0.02259107059695701, w1=0.23712377389412095\n",
      "Gradient Descent(5829/9999): loss=183.04246671067276, w0=0.022589263575537148, w1=0.23709997627611007\n",
      "Gradient Descent(5830/9999): loss=183.00745346459095, w0=0.022587456728525423, w1=0.23707618091044563\n",
      "Gradient Descent(5831/9999): loss=182.97244837795333, w0=0.02258565005590235, w1=0.23705238779691162\n",
      "Gradient Descent(5832/9999): loss=182.9374514487052, w0=0.022583843557648458, w1=0.23702859693529202\n",
      "Gradient Descent(5833/9999): loss=182.90246267479245, w0=0.022582037233744266, w1=0.23700480832537082\n",
      "Gradient Descent(5834/9999): loss=182.86748205416148, w0=0.022580231084170305, w1=0.2369810219669321\n",
      "Gradient Descent(5835/9999): loss=182.83250958475935, w0=0.022578425108907108, w1=0.23695723785975986\n",
      "Gradient Descent(5836/9999): loss=182.79754526453354, w0=0.022576619307935204, w1=0.2369334560036382\n",
      "Gradient Descent(5837/9999): loss=182.7625890914322, w0=0.022574813681235126, w1=0.2369096763983512\n",
      "Gradient Descent(5838/9999): loss=182.7276410634038, w0=0.022573008228787415, w1=0.23688589904368296\n",
      "Gradient Descent(5839/9999): loss=182.69270117839767, w0=0.022571202950572605, w1=0.2368621239394176\n",
      "Gradient Descent(5840/9999): loss=182.65776943436344, w0=0.02256939784657124, w1=0.23683835108533932\n",
      "Gradient Descent(5841/9999): loss=182.62284582925136, w0=0.022567592916763867, w1=0.23681458048123225\n",
      "Gradient Descent(5842/9999): loss=182.5879303610123, w0=0.02256578816113103, w1=0.23679081212688055\n",
      "Gradient Descent(5843/9999): loss=182.5530230275976, w0=0.022563983579653276, w1=0.23676704602206847\n",
      "Gradient Descent(5844/9999): loss=182.51812382695903, w0=0.022562179172311157, w1=0.23674328216658022\n",
      "Gradient Descent(5845/9999): loss=182.4832327570492, w0=0.022560374939085227, w1=0.23671952056020004\n",
      "Gradient Descent(5846/9999): loss=182.44834981582102, w0=0.022558570879956042, w1=0.2366957612027122\n",
      "Gradient Descent(5847/9999): loss=182.41347500122805, w0=0.022556766994904158, w1=0.23667200409390102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(5848/9999): loss=182.3786083112242, w0=0.022554963283910136, w1=0.23664824923355074\n",
      "Gradient Descent(5849/9999): loss=182.3437497437644, w0=0.02255315974695454, w1=0.23662449662144572\n",
      "Gradient Descent(5850/9999): loss=182.30889929680356, w0=0.022551356384017937, w1=0.2366007462573703\n",
      "Gradient Descent(5851/9999): loss=182.2740569682975, w0=0.02254955319508089, w1=0.23657699814110883\n",
      "Gradient Descent(5852/9999): loss=182.23922275620242, w0=0.022547750180123966, w1=0.2365532522724457\n",
      "Gradient Descent(5853/9999): loss=182.2043966584752, w0=0.022545947339127743, w1=0.23652950865116532\n",
      "Gradient Descent(5854/9999): loss=182.16957867307306, w0=0.022544144672072795, w1=0.2365057672770521\n",
      "Gradient Descent(5855/9999): loss=182.13476879795394, w0=0.022542342178939696, w1=0.2364820281498905\n",
      "Gradient Descent(5856/9999): loss=182.09996703107632, w0=0.022540539859709028, w1=0.23645829126946497\n",
      "Gradient Descent(5857/9999): loss=182.06517337039912, w0=0.02253873771436137, w1=0.23643455663555996\n",
      "Gradient Descent(5858/9999): loss=182.03038781388184, w0=0.022536935742877302, w1=0.23641082424796\n",
      "Gradient Descent(5859/9999): loss=181.99561035948457, w0=0.02253513394523742, w1=0.23638709410644962\n",
      "Gradient Descent(5860/9999): loss=181.96084100516788, w0=0.0225333323214223, w1=0.23636336621081333\n",
      "Gradient Descent(5861/9999): loss=181.926079748893, w0=0.022531530871412544, w1=0.23633964056083573\n",
      "Gradient Descent(5862/9999): loss=181.8913265886214, w0=0.02252972959518874, w1=0.23631591715630135\n",
      "Gradient Descent(5863/9999): loss=181.85658152231557, w0=0.022527928492731483, w1=0.2362921959969948\n",
      "Gradient Descent(5864/9999): loss=181.8218445479381, w0=0.02252612756402137, w1=0.2362684770827007\n",
      "Gradient Descent(5865/9999): loss=181.7871156634524, w0=0.022524326809039003, w1=0.23624476041320372\n",
      "Gradient Descent(5866/9999): loss=181.75239486682221, w0=0.022522526227764985, w1=0.23622104598828847\n",
      "Gradient Descent(5867/9999): loss=181.71768215601207, w0=0.02252072582017992, w1=0.23619733380773966\n",
      "Gradient Descent(5868/9999): loss=181.6829775289868, w0=0.022518925586264414, w1=0.23617362387134197\n",
      "Gradient Descent(5869/9999): loss=181.64828098371194, w0=0.022517125525999077, w1=0.2361499161788801\n",
      "Gradient Descent(5870/9999): loss=181.61359251815347, w0=0.02251532563936452, w1=0.23612621073013879\n",
      "Gradient Descent(5871/9999): loss=181.578912130278, w0=0.022513525926341357, w1=0.2361025075249028\n",
      "Gradient Descent(5872/9999): loss=181.54423981805257, w0=0.022511726386910208, w1=0.2360788065629569\n",
      "Gradient Descent(5873/9999): loss=181.50957557944488, w0=0.022509927021051687, w1=0.2360551078440859\n",
      "Gradient Descent(5874/9999): loss=181.4749194124231, w0=0.022508127828746418, w1=0.2360314113680746\n",
      "Gradient Descent(5875/9999): loss=181.44027131495585, w0=0.022506328809975024, w1=0.2360077171347078\n",
      "Gradient Descent(5876/9999): loss=181.40563128501253, w0=0.02250452996471813, w1=0.2359840251437704\n",
      "Gradient Descent(5877/9999): loss=181.37099932056293, w0=0.022502731292956366, w1=0.23596033539504724\n",
      "Gradient Descent(5878/9999): loss=181.33637541957728, w0=0.02250093279467036, w1=0.23593664788832322\n",
      "Gradient Descent(5879/9999): loss=181.3017595800265, w0=0.022499134469840747, w1=0.23591296262338327\n",
      "Gradient Descent(5880/9999): loss=181.2671517998822, w0=0.02249733631844816, w1=0.2358892796000123\n",
      "Gradient Descent(5881/9999): loss=181.23255207711614, w0=0.02249553834047324, w1=0.23586559881799524\n",
      "Gradient Descent(5882/9999): loss=181.19796040970087, w0=0.022493740535896624, w1=0.2358419202771171\n",
      "Gradient Descent(5883/9999): loss=181.16337679560945, w0=0.022491942904698952, w1=0.2358182439771628\n",
      "Gradient Descent(5884/9999): loss=181.12880123281542, w0=0.022490145446860874, w1=0.23579456991791742\n",
      "Gradient Descent(5885/9999): loss=181.09423371929296, w0=0.022488348162363034, w1=0.23577089809916596\n",
      "Gradient Descent(5886/9999): loss=181.05967425301677, w0=0.022486551051186084, w1=0.23574722852069346\n",
      "Gradient Descent(5887/9999): loss=181.02512283196194, w0=0.02248475411331067, w1=0.235723561182285\n",
      "Gradient Descent(5888/9999): loss=180.9905794541042, w0=0.02248295734871745, w1=0.23569989608372563\n",
      "Gradient Descent(5889/9999): loss=180.95604411741985, w0=0.02248116075738708, w1=0.23567623322480047\n",
      "Gradient Descent(5890/9999): loss=180.92151681988577, w0=0.022479364339300218, w1=0.23565257260529465\n",
      "Gradient Descent(5891/9999): loss=180.88699755947928, w0=0.022477568094437527, w1=0.23562891422499332\n",
      "Gradient Descent(5892/9999): loss=180.85248633417825, w0=0.022475772022779665, w1=0.23560525808368163\n",
      "Gradient Descent(5893/9999): loss=180.81798314196107, w0=0.022473976124307304, w1=0.23558160418114477\n",
      "Gradient Descent(5894/9999): loss=180.7834879808068, w0=0.022472180399001108, w1=0.23555795251716793\n",
      "Gradient Descent(5895/9999): loss=180.74900084869483, w0=0.02247038484684175, w1=0.23553430309153636\n",
      "Gradient Descent(5896/9999): loss=180.71452174360525, w0=0.0224685894678099, w1=0.23551065590403528\n",
      "Gradient Descent(5897/9999): loss=180.68005066351867, w0=0.022466794261886233, w1=0.23548701095444993\n",
      "Gradient Descent(5898/9999): loss=180.6455876064162, w0=0.022464999229051427, w1=0.2354633682425656\n",
      "Gradient Descent(5899/9999): loss=180.6111325702794, w0=0.022463204369286165, w1=0.23543972776816763\n",
      "Gradient Descent(5900/9999): loss=180.57668555309058, w0=0.022461409682571126, w1=0.2354160895310413\n",
      "Gradient Descent(5901/9999): loss=180.54224655283244, w0=0.022459615168886993, w1=0.23539245353097193\n",
      "Gradient Descent(5902/9999): loss=180.5078155674882, w0=0.022457820828214455, w1=0.23536881976774493\n",
      "Gradient Descent(5903/9999): loss=180.47339259504153, w0=0.0224560266605342, w1=0.23534518824114564\n",
      "Gradient Descent(5904/9999): loss=180.438977633477, w0=0.022454232665826922, w1=0.23532155895095946\n",
      "Gradient Descent(5905/9999): loss=180.40457068077936, w0=0.022452438844073313, w1=0.23529793189697182\n",
      "Gradient Descent(5906/9999): loss=180.37017173493402, w0=0.022450645195254068, w1=0.23527430707896813\n",
      "Gradient Descent(5907/9999): loss=180.33578079392694, w0=0.022448851719349884, w1=0.23525068449673386\n",
      "Gradient Descent(5908/9999): loss=180.30139785574463, w0=0.022447058416341464, w1=0.23522706415005448\n",
      "Gradient Descent(5909/9999): loss=180.26702291837407, w0=0.022445265286209513, w1=0.23520344603871549\n",
      "Gradient Descent(5910/9999): loss=180.2326559798027, w0=0.022443472328934736, w1=0.23517983016250238\n",
      "Gradient Descent(5911/9999): loss=180.19829703801875, w0=0.02244167954449784, w1=0.23515621652120072\n",
      "Gradient Descent(5912/9999): loss=180.16394609101073, w0=0.022439886932879533, w1=0.23513260511459602\n",
      "Gradient Descent(5913/9999): loss=180.1296031367679, w0=0.02243809449406053, w1=0.2351089959424739\n",
      "Gradient Descent(5914/9999): loss=180.0952681732799, w0=0.022436302228021548, w1=0.2350853890046199\n",
      "Gradient Descent(5915/9999): loss=180.0609411985369, w0=0.0224345101347433, w1=0.23506178430081964\n",
      "Gradient Descent(5916/9999): loss=180.02662221052972, w0=0.022432718214206507, w1=0.23503818183085878\n",
      "Gradient Descent(5917/9999): loss=179.99231120724957, w0=0.02243092646639189, w1=0.23501458159452296\n",
      "Gradient Descent(5918/9999): loss=179.95800818668837, w0=0.022429134891280178, w1=0.2349909835915978\n",
      "Gradient Descent(5919/9999): loss=179.92371314683845, w0=0.022427343488852093, w1=0.23496738782186907\n",
      "Gradient Descent(5920/9999): loss=179.88942608569263, w0=0.022425552259088363, w1=0.2349437942851224\n",
      "Gradient Descent(5921/9999): loss=179.85514700124446, w0=0.022423761201969723, w1=0.23492020298114358\n",
      "Gradient Descent(5922/9999): loss=179.8208758914878, w0=0.022421970317476905, w1=0.2348966139097183\n",
      "Gradient Descent(5923/9999): loss=179.7866127544171, w0=0.022420179605590645, w1=0.23487302707063235\n",
      "Gradient Descent(5924/9999): loss=179.7523575880275, w0=0.022418389066291684, w1=0.2348494424636715\n",
      "Gradient Descent(5925/9999): loss=179.71811039031454, w0=0.02241659869956076, w1=0.23482586008862158\n",
      "Gradient Descent(5926/9999): loss=179.68387115927422, w0=0.022414808505378615, w1=0.2348022799452684\n",
      "Gradient Descent(5927/9999): loss=179.64963989290325, w0=0.022413018483725997, w1=0.2347787020333978\n",
      "Gradient Descent(5928/9999): loss=179.61541658919873, w0=0.02241122863458365, w1=0.23475512635279563\n",
      "Gradient Descent(5929/9999): loss=179.58120124615843, w0=0.02240943895793233, w1=0.2347315529032478\n",
      "Gradient Descent(5930/9999): loss=179.54699386178052, w0=0.022407649453752784, w1=0.23470798168454018\n",
      "Gradient Descent(5931/9999): loss=179.5127944340637, w0=0.02240586012202577, w1=0.2346844126964587\n",
      "Gradient Descent(5932/9999): loss=179.47860296100737, w0=0.022404070962732045, w1=0.23466084593878933\n",
      "Gradient Descent(5933/9999): loss=179.44441944061117, w0=0.022402281975852367, w1=0.23463728141131798\n",
      "Gradient Descent(5934/9999): loss=179.41024387087566, w0=0.0224004931613675, w1=0.23461371911383067\n",
      "Gradient Descent(5935/9999): loss=179.3760762498016, w0=0.022398704519258204, w1=0.23459015904611336\n",
      "Gradient Descent(5936/9999): loss=179.3419165753904, w0=0.02239691604950525, w1=0.23456660120795209\n",
      "Gradient Descent(5937/9999): loss=179.30776484564396, w0=0.022395127752089402, w1=0.2345430455991329\n",
      "Gradient Descent(5938/9999): loss=179.27362105856488, w0=0.022393339626991436, w1=0.23451949221944182\n",
      "Gradient Descent(5939/9999): loss=179.23948521215607, w0=0.022391551674192125, w1=0.23449594106866495\n",
      "Gradient Descent(5940/9999): loss=179.20535730442109, w0=0.02238976389367224, w1=0.23447239214658838\n",
      "Gradient Descent(5941/9999): loss=179.171237333364, w0=0.022387976285412566, w1=0.23444884545299824\n",
      "Gradient Descent(5942/9999): loss=179.13712529698944, w0=0.02238618884939388, w1=0.23442530098768063\n",
      "Gradient Descent(5943/9999): loss=179.1030211933024, w0=0.022384401585596965, w1=0.2344017587504217\n",
      "Gradient Descent(5944/9999): loss=179.0689250203087, w0=0.022382614494002608, w1=0.23437821874100767\n",
      "Gradient Descent(5945/9999): loss=179.03483677601446, w0=0.022380827574591595, w1=0.23435468095922468\n",
      "Gradient Descent(5946/9999): loss=179.00075645842642, w0=0.022379040827344716, w1=0.23433114540485897\n",
      "Gradient Descent(5947/9999): loss=178.96668406555173, w0=0.022377254252242768, w1=0.23430761207769676\n",
      "Gradient Descent(5948/9999): loss=178.9326195953983, w0=0.02237546784926654, w1=0.2342840809775243\n",
      "Gradient Descent(5949/9999): loss=178.89856304597433, w0=0.02237368161839683, w1=0.23426055210412783\n",
      "Gradient Descent(5950/9999): loss=178.8645144152887, w0=0.022371895559614437, w1=0.2342370254572937\n",
      "Gradient Descent(5951/9999): loss=178.83047370135077, w0=0.022370109672900165, w1=0.23421350103680816\n",
      "Gradient Descent(5952/9999): loss=178.79644090217045, w0=0.022368323958234817, w1=0.23418997884245757\n",
      "Gradient Descent(5953/9999): loss=178.76241601575813, w0=0.022366538415599203, w1=0.23416645887402826\n",
      "Gradient Descent(5954/9999): loss=178.7283990401248, w0=0.022364753044974126, w1=0.2341429411313066\n",
      "Gradient Descent(5955/9999): loss=178.6943899732819, w0=0.0223629678463404, w1=0.23411942561407897\n",
      "Gradient Descent(5956/9999): loss=178.66038881324138, w0=0.02236118281967884, w1=0.23409591232213178\n",
      "Gradient Descent(5957/9999): loss=178.62639555801593, w0=0.022359397964970256, w1=0.23407240125525144\n",
      "Gradient Descent(5958/9999): loss=178.59241020561853, w0=0.022357613282195474, w1=0.2340488924132244\n",
      "Gradient Descent(5959/9999): loss=178.55843275406272, w0=0.02235582877133531, w1=0.23402538579583715\n",
      "Gradient Descent(5960/9999): loss=178.5244632013627, w0=0.022354044432370584, w1=0.23400188140287614\n",
      "Gradient Descent(5961/9999): loss=178.49050154553302, w0=0.02235226026528213, w1=0.23397837923412787\n",
      "Gradient Descent(5962/9999): loss=178.45654778458896, w0=0.022350476270050768, w1=0.23395487928937886\n",
      "Gradient Descent(5963/9999): loss=178.4226019165462, w0=0.02234869244665733, w1=0.23393138156841567\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(5964/9999): loss=178.38866393942092, w0=0.022346908795082648, w1=0.23390788607102483\n",
      "Gradient Descent(5965/9999): loss=178.35473385122992, w0=0.022345125315307558, w1=0.23388439279699294\n",
      "Gradient Descent(5966/9999): loss=178.32081164999047, w0=0.022343342007312895, w1=0.2338609017461066\n",
      "Gradient Descent(5967/9999): loss=178.2868973337204, w0=0.0223415588710795, w1=0.2338374129181524\n",
      "Gradient Descent(5968/9999): loss=178.25299090043802, w0=0.022339775906588217, w1=0.233813926312917\n",
      "Gradient Descent(5969/9999): loss=178.21909234816215, w0=0.022337993113819885, w1=0.23379044193018705\n",
      "Gradient Descent(5970/9999): loss=178.18520167491226, w0=0.022336210492755353, w1=0.2337669597697492\n",
      "Gradient Descent(5971/9999): loss=178.15131887870822, w0=0.02233442804337547, w1=0.23374347983139018\n",
      "Gradient Descent(5972/9999): loss=178.1174439575705, w0=0.022332645765661085, w1=0.23372000211489669\n",
      "Gradient Descent(5973/9999): loss=178.08357690951996, w0=0.022330863659593057, w1=0.23369652662005544\n",
      "Gradient Descent(5974/9999): loss=178.04971773257827, w0=0.022329081725152234, w1=0.23367305334665323\n",
      "Gradient Descent(5975/9999): loss=178.01586642476735, w0=0.02232729996231948, w1=0.23364958229447677\n",
      "Gradient Descent(5976/9999): loss=177.98202298410973, w0=0.022325518371075655, w1=0.2336261134633129\n",
      "Gradient Descent(5977/9999): loss=177.94818740862857, w0=0.02232373695140162, w1=0.23360264685294843\n",
      "Gradient Descent(5978/9999): loss=177.91435969634733, w0=0.022321955703278242, w1=0.23357918246317017\n",
      "Gradient Descent(5979/9999): loss=177.8805398452903, w0=0.022320174626686385, w1=0.23355572029376495\n",
      "Gradient Descent(5980/9999): loss=177.84672785348192, w0=0.02231839372160692, w1=0.23353226034451965\n",
      "Gradient Descent(5981/9999): loss=177.81292371894747, w0=0.02231661298802072, w1=0.23350880261522117\n",
      "Gradient Descent(5982/9999): loss=177.77912743971268, w0=0.022314832425908662, w1=0.2334853471056564\n",
      "Gradient Descent(5983/9999): loss=177.7453390138037, w0=0.02231305203525162, w1=0.23346189381561228\n",
      "Gradient Descent(5984/9999): loss=177.71155843924728, w0=0.022311271816030473, w1=0.23343844274487574\n",
      "Gradient Descent(5985/9999): loss=177.67778571407075, w0=0.0223094917682261, w1=0.23341499389323375\n",
      "Gradient Descent(5986/9999): loss=177.64402083630185, w0=0.022307711891819394, w1=0.2333915472604733\n",
      "Gradient Descent(5987/9999): loss=177.6102638039689, w0=0.022305932186791232, w1=0.23336810284638138\n",
      "Gradient Descent(5988/9999): loss=177.57651461510073, w0=0.02230415265312251, w1=0.23334466065074502\n",
      "Gradient Descent(5989/9999): loss=177.54277326772666, w0=0.022302373290794115, w1=0.23332122067335126\n",
      "Gradient Descent(5990/9999): loss=177.5090397598767, w0=0.02230059409978694, w1=0.23329778291398715\n",
      "Gradient Descent(5991/9999): loss=177.47531408958113, w0=0.02229881508008188, w1=0.23327434737243977\n",
      "Gradient Descent(5992/9999): loss=177.44159625487094, w0=0.022297036231659836, w1=0.23325091404849624\n",
      "Gradient Descent(5993/9999): loss=177.40788625377766, w0=0.022295257554501707, w1=0.23322748294194368\n",
      "Gradient Descent(5994/9999): loss=177.3741840843331, w0=0.0222934790485884, w1=0.2332040540525692\n",
      "Gradient Descent(5995/9999): loss=177.34048974456994, w0=0.022291700713900815, w1=0.23318062738015996\n",
      "Gradient Descent(5996/9999): loss=177.30680323252096, w0=0.02228992255041986, w1=0.23315720292450315\n",
      "Gradient Descent(5997/9999): loss=177.27312454622, w0=0.022288144558126444, w1=0.23313378068538596\n",
      "Gradient Descent(5998/9999): loss=177.23945368370087, w0=0.02228636673700148, w1=0.23311036066259558\n",
      "Gradient Descent(5999/9999): loss=177.20579064299832, w0=0.022284589087025884, w1=0.23308694285591927\n",
      "Gradient Descent(6000/9999): loss=177.17213542214745, w0=0.02228281160818057, w1=0.2330635272651443\n",
      "Gradient Descent(6001/9999): loss=177.13848801918374, w0=0.022281034300446464, w1=0.23304011389005788\n",
      "Gradient Descent(6002/9999): loss=177.10484843214354, w0=0.02227925716380448, w1=0.23301670273044736\n",
      "Gradient Descent(6003/9999): loss=177.07121665906345, w0=0.022277480198235545, w1=0.23299329378610004\n",
      "Gradient Descent(6004/9999): loss=177.03759269798067, w0=0.022275703403720586, w1=0.23296988705680322\n",
      "Gradient Descent(6005/9999): loss=177.0039765469329, w0=0.022273926780240528, w1=0.23294648254234426\n",
      "Gradient Descent(6006/9999): loss=176.9703682039584, w0=0.022272150327776307, w1=0.23292308024251057\n",
      "Gradient Descent(6007/9999): loss=176.93676766709592, w0=0.022270374046308855, w1=0.2328996801570895\n",
      "Gradient Descent(6008/9999): loss=176.90317493438474, w0=0.022268597935819104, w1=0.23287628228586846\n",
      "Gradient Descent(6009/9999): loss=176.86959000386466, w0=0.022266821996287996, w1=0.23285288662863488\n",
      "Gradient Descent(6010/9999): loss=176.836012873576, w0=0.02226504622769647, w1=0.23282949318517618\n",
      "Gradient Descent(6011/9999): loss=176.80244354155968, w0=0.022263270630025467, w1=0.23280610195527984\n",
      "Gradient Descent(6012/9999): loss=176.76888200585688, w0=0.022261495203255936, w1=0.23278271293873337\n",
      "Gradient Descent(6013/9999): loss=176.73532826450972, w0=0.022259719947368823, w1=0.23275932613532424\n",
      "Gradient Descent(6014/9999): loss=176.7017823155604, w0=0.022257944862345076, w1=0.23273594154483998\n",
      "Gradient Descent(6015/9999): loss=176.6682441570519, w0=0.022256169948165646, w1=0.23271255916706815\n",
      "Gradient Descent(6016/9999): loss=176.63471378702778, w0=0.02225439520481149, w1=0.23268917900179628\n",
      "Gradient Descent(6017/9999): loss=176.6011912035318, w0=0.022252620632263565, w1=0.23266580104881196\n",
      "Gradient Descent(6018/9999): loss=176.56767640460865, w0=0.02225084623050283, w1=0.2326424253079028\n",
      "Gradient Descent(6019/9999): loss=176.53416938830316, w0=0.022249071999510248, w1=0.23261905177885642\n",
      "Gradient Descent(6020/9999): loss=176.50067015266097, w0=0.022247297939266778, w1=0.23259568046146042\n",
      "Gradient Descent(6021/9999): loss=176.46717869572808, w0=0.02224552404975339, w1=0.23257231135550252\n",
      "Gradient Descent(6022/9999): loss=176.433695015551, w0=0.022243750330951054, w1=0.23254894446077035\n",
      "Gradient Descent(6023/9999): loss=176.4002191101769, w0=0.022241976782840737, w1=0.2325255797770516\n",
      "Gradient Descent(6024/9999): loss=176.3667509776532, w0=0.022240203405403412, w1=0.232502217304134\n",
      "Gradient Descent(6025/9999): loss=176.33329061602825, w0=0.022238430198620058, w1=0.2324788570418053\n",
      "Gradient Descent(6026/9999): loss=176.2998380233505, w0=0.02223665716247165, w1=0.2324554989898532\n",
      "Gradient Descent(6027/9999): loss=176.2663931976692, w0=0.02223488429693917, w1=0.23243214314806554\n",
      "Gradient Descent(6028/9999): loss=176.23295613703394, w0=0.022233111602003603, w1=0.23240878951623006\n",
      "Gradient Descent(6029/9999): loss=176.19952683949495, w0=0.02223133907764593, w1=0.2323854380941346\n",
      "Gradient Descent(6030/9999): loss=176.1661053031029, w0=0.02222956672384714, w1=0.23236208888156695\n",
      "Gradient Descent(6031/9999): loss=176.13269152590902, w0=0.022227794540588222, w1=0.232338741878315\n",
      "Gradient Descent(6032/9999): loss=176.09928550596513, w0=0.022226022527850168, w1=0.2323153970841666\n",
      "Gradient Descent(6033/9999): loss=176.06588724132334, w0=0.022224250685613973, w1=0.23229205449890963\n",
      "Gradient Descent(6034/9999): loss=176.03249673003654, w0=0.02222247901386063, w1=0.23226871412233202\n",
      "Gradient Descent(6035/9999): loss=175.99911397015796, w0=0.022220707512571148, w1=0.23224537595422168\n",
      "Gradient Descent(6036/9999): loss=175.9657389597414, w0=0.02221893618172652, w1=0.23222203999436652\n",
      "Gradient Descent(6037/9999): loss=175.93237169684122, w0=0.02221716502130775, w1=0.23219870624255456\n",
      "Gradient Descent(6038/9999): loss=175.89901217951217, w0=0.022215394031295847, w1=0.23217537469857377\n",
      "Gradient Descent(6039/9999): loss=175.86566040580976, w0=0.02221362321167182, w1=0.23215204536221212\n",
      "Gradient Descent(6040/9999): loss=175.83231637378975, w0=0.02221185256241668, w1=0.23212871823325767\n",
      "Gradient Descent(6041/9999): loss=175.79898008150852, w0=0.02221008208351144, w1=0.23210539331149843\n",
      "Gradient Descent(6042/9999): loss=175.765651527023, w0=0.02220831177493711, w1=0.23208207059672248\n",
      "Gradient Descent(6043/9999): loss=175.7323307083906, w0=0.022206541636674713, w1=0.23205875008871787\n",
      "Gradient Descent(6044/9999): loss=175.69901762366928, w0=0.022204771668705266, w1=0.23203543178727273\n",
      "Gradient Descent(6045/9999): loss=175.6657122709174, w0=0.022203001871009796, w1=0.23201211569217517\n",
      "Gradient Descent(6046/9999): loss=175.63241464819407, w0=0.022201232243569326, w1=0.23198880180321332\n",
      "Gradient Descent(6047/9999): loss=175.59912475355864, w0=0.022199462786364883, w1=0.23196549012017534\n",
      "Gradient Descent(6048/9999): loss=175.56584258507112, w0=0.022197693499377495, w1=0.23194218064284938\n",
      "Gradient Descent(6049/9999): loss=175.5325681407921, w0=0.022195924382588196, w1=0.23191887337102368\n",
      "Gradient Descent(6050/9999): loss=175.49930141878252, w0=0.02219415543597802, w1=0.2318955683044864\n",
      "Gradient Descent(6051/9999): loss=175.46604241710395, w0=0.022192386659528007, w1=0.2318722654430258\n",
      "Gradient Descent(6052/9999): loss=175.4327911338185, w0=0.02219061805321919, w1=0.23184896478643013\n",
      "Gradient Descent(6053/9999): loss=175.39954756698864, w0=0.022188849617032616, w1=0.23182566633448767\n",
      "Gradient Descent(6054/9999): loss=175.36631171467752, w0=0.022187081350949323, w1=0.2318023700869867\n",
      "Gradient Descent(6055/9999): loss=175.33308357494866, w0=0.022185313254950358, w1=0.23177907604371553\n",
      "Gradient Descent(6056/9999): loss=175.2998631458662, w0=0.022183545329016775, w1=0.23175578420446247\n",
      "Gradient Descent(6057/9999): loss=175.2666504254948, w0=0.02218177757312962, w1=0.23173249456901587\n",
      "Gradient Descent(6058/9999): loss=175.2334454118996, w0=0.022180009987269947, w1=0.23170920713716414\n",
      "Gradient Descent(6059/9999): loss=175.2002481031463, w0=0.022178242571418812, w1=0.2316859219086956\n",
      "Gradient Descent(6060/9999): loss=175.1670584973008, w0=0.022176475325557273, w1=0.23166263888339872\n",
      "Gradient Descent(6061/9999): loss=175.13387659243008, w0=0.02217470824966639, w1=0.23163935806106187\n",
      "Gradient Descent(6062/9999): loss=175.10070238660123, w0=0.022172941343727222, w1=0.23161607944147353\n",
      "Gradient Descent(6063/9999): loss=175.0675358778819, w0=0.022171174607720837, w1=0.23159280302442212\n",
      "Gradient Descent(6064/9999): loss=175.0343770643403, w0=0.022169408041628307, w1=0.23156952880969614\n",
      "Gradient Descent(6065/9999): loss=175.00122594404516, w0=0.022167641645430693, w1=0.2315462567970841\n",
      "Gradient Descent(6066/9999): loss=174.96808251506582, w0=0.022165875419109072, w1=0.23152298698637452\n",
      "Gradient Descent(6067/9999): loss=174.93494677547193, w0=0.022164109362644516, w1=0.23149971937735594\n",
      "Gradient Descent(6068/9999): loss=174.90181872333386, w0=0.022162343476018104, w1=0.23147645396981692\n",
      "Gradient Descent(6069/9999): loss=174.8686983567222, w0=0.022160577759210914, w1=0.231453190763546\n",
      "Gradient Descent(6070/9999): loss=174.83558567370844, w0=0.02215881221220402, w1=0.23142992975833182\n",
      "Gradient Descent(6071/9999): loss=174.80248067236423, w0=0.022157046834978517, w1=0.23140667095396297\n",
      "Gradient Descent(6072/9999): loss=174.76938335076193, w0=0.022155281627515483, w1=0.23138341435022808\n",
      "Gradient Descent(6073/9999): loss=174.73629370697444, w0=0.022153516589796012, w1=0.2313601599469158\n",
      "Gradient Descent(6074/9999): loss=174.703211739075, w0=0.022151751721801193, w1=0.23133690774381482\n",
      "Gradient Descent(6075/9999): loss=174.67013744513747, w0=0.02214998702351212, w1=0.23131365774071383\n",
      "Gradient Descent(6076/9999): loss=174.6370708232362, w0=0.022148222494909886, w1=0.23129040993740155\n",
      "Gradient Descent(6077/9999): loss=174.60401187144606, w0=0.02214645813597559, w1=0.23126716433366667\n",
      "Gradient Descent(6078/9999): loss=174.57096058784245, w0=0.02214469394669033, w1=0.23124392092929796\n",
      "Gradient Descent(6079/9999): loss=174.5379169705013, w0=0.02214292992703521, w1=0.2312206797240842\n",
      "Gradient Descent(6080/9999): loss=174.50488101749886, w0=0.022141166076991334, w1=0.23119744071781417\n",
      "Gradient Descent(6081/9999): loss=174.47185272691215, w0=0.02213940239653981, w1=0.23117420391027668\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(6082/9999): loss=174.43883209681863, w0=0.022137638885661746, w1=0.23115096930126053\n",
      "Gradient Descent(6083/9999): loss=174.40581912529615, w0=0.022135875544338254, w1=0.23112773689055457\n",
      "Gradient Descent(6084/9999): loss=174.37281381042317, w0=0.02213411237255045, w1=0.2311045066779477\n",
      "Gradient Descent(6085/9999): loss=174.33981615027855, w0=0.022132349370279452, w1=0.23108127866322878\n",
      "Gradient Descent(6086/9999): loss=174.3068261429419, w0=0.022130586537506373, w1=0.2310580528461867\n",
      "Gradient Descent(6087/9999): loss=174.27384378649313, w0=0.02212882387421234, w1=0.23103482922661037\n",
      "Gradient Descent(6088/9999): loss=174.24086907901273, w0=0.02212706138037847, w1=0.23101160780428875\n",
      "Gradient Descent(6089/9999): loss=174.20790201858156, w0=0.022125299055985898, w1=0.23098838857901083\n",
      "Gradient Descent(6090/9999): loss=174.17494260328132, w0=0.022123536901015744, w1=0.23096517155056553\n",
      "Gradient Descent(6091/9999): loss=174.14199083119385, w0=0.022121774915449144, w1=0.23094195671874188\n",
      "Gradient Descent(6092/9999): loss=174.1090467004017, w0=0.022120013099267227, w1=0.23091874408332888\n",
      "Gradient Descent(6093/9999): loss=174.07611020898796, w0=0.02211825145245113, w1=0.23089553364411558\n",
      "Gradient Descent(6094/9999): loss=174.04318135503607, w0=0.02211648997498199, w1=0.23087232540089103\n",
      "Gradient Descent(6095/9999): loss=174.01026013663014, w0=0.022114728666840947, w1=0.2308491193534443\n",
      "Gradient Descent(6096/9999): loss=173.97734655185465, w0=0.022112967528009144, w1=0.23082591550156445\n",
      "Gradient Descent(6097/9999): loss=173.94444059879464, w0=0.022111206558467725, w1=0.23080271384504064\n",
      "Gradient Descent(6098/9999): loss=173.9115422755358, w0=0.02210944575819784, w1=0.23077951438366198\n",
      "Gradient Descent(6099/9999): loss=173.87865158016407, w0=0.022107685127180633, w1=0.2307563171172176\n",
      "Gradient Descent(6100/9999): loss=173.84576851076608, w0=0.022105924665397257, w1=0.23073312204549673\n",
      "Gradient Descent(6101/9999): loss=173.81289306542888, w0=0.022104164372828868, w1=0.2307099291682885\n",
      "Gradient Descent(6102/9999): loss=173.78002524224013, w0=0.02210240424945662, w1=0.23068673848538213\n",
      "Gradient Descent(6103/9999): loss=173.7471650392878, w0=0.02210064429526168, w1=0.23066354999656685\n",
      "Gradient Descent(6104/9999): loss=173.71431245466064, w0=0.022098884510225198, w1=0.2306403637016319\n",
      "Gradient Descent(6105/9999): loss=173.6814674864477, w0=0.022097124894328343, w1=0.23061717960036657\n",
      "Gradient Descent(6106/9999): loss=173.6486301327386, w0=0.02209536544755228, w1=0.23059399769256012\n",
      "Gradient Descent(6107/9999): loss=173.61580039162345, w0=0.022093606169878176, w1=0.23057081797800183\n",
      "Gradient Descent(6108/9999): loss=173.58297826119292, w0=0.022091847061287202, w1=0.23054764045648105\n",
      "Gradient Descent(6109/9999): loss=173.55016373953808, w0=0.02209008812176053, w1=0.2305244651277871\n",
      "Gradient Descent(6110/9999): loss=173.51735682475064, w0=0.022088329351279336, w1=0.23050129199170938\n",
      "Gradient Descent(6111/9999): loss=173.48455751492273, w0=0.022086570749824796, w1=0.23047812104803722\n",
      "Gradient Descent(6112/9999): loss=173.45176580814697, w0=0.02208481231737809, w1=0.23045495229656004\n",
      "Gradient Descent(6113/9999): loss=173.4189817025166, w0=0.022083054053920408, w1=0.23043178573706727\n",
      "Gradient Descent(6114/9999): loss=173.38620519612522, w0=0.022081295959432923, w1=0.2304086213693483\n",
      "Gradient Descent(6115/9999): loss=173.353436287067, w0=0.022079538033896826, w1=0.23038545919319264\n",
      "Gradient Descent(6116/9999): loss=173.3206749734367, w0=0.022077780277293308, w1=0.23036229920838974\n",
      "Gradient Descent(6117/9999): loss=173.28792125332942, w0=0.02207602268960356, w1=0.23033914141472908\n",
      "Gradient Descent(6118/9999): loss=173.2551751248409, w0=0.022074265270808774, w1=0.2303159858120002\n",
      "Gradient Descent(6119/9999): loss=173.22243658606723, w0=0.022072508020890145, w1=0.2302928323999926\n",
      "Gradient Descent(6120/9999): loss=173.18970563510524, w0=0.022070750939828878, w1=0.23026968117849583\n",
      "Gradient Descent(6121/9999): loss=173.1569822700521, w0=0.02206899402760617, w1=0.23024653214729948\n",
      "Gradient Descent(6122/9999): loss=173.12426648900544, w0=0.02206723728420322, w1=0.23022338530619313\n",
      "Gradient Descent(6123/9999): loss=173.0915582900636, w0=0.02206548070960124, w1=0.2302002406549664\n",
      "Gradient Descent(6124/9999): loss=173.0588576713252, w0=0.022063724303781434, w1=0.23017709819340887\n",
      "Gradient Descent(6125/9999): loss=173.02616463088947, w0=0.022061968066725016, w1=0.23015395792131024\n",
      "Gradient Descent(6126/9999): loss=172.99347916685608, w0=0.022060211998413196, w1=0.23013081983846015\n",
      "Gradient Descent(6127/9999): loss=172.96080127732546, w0=0.022058456098827187, w1=0.23010768394464828\n",
      "Gradient Descent(6128/9999): loss=172.9281309603981, w0=0.022056700367948212, w1=0.23008455023966434\n",
      "Gradient Descent(6129/9999): loss=172.8954682141754, w0=0.022054944805757485, w1=0.23006141872329805\n",
      "Gradient Descent(6130/9999): loss=172.862813036759, w0=0.022053189412236232, w1=0.23003828939533913\n",
      "Gradient Descent(6131/9999): loss=172.83016542625117, w0=0.022051434187365673, w1=0.23001516225557736\n",
      "Gradient Descent(6132/9999): loss=172.79752538075468, w0=0.02204967913112704, w1=0.22999203730380252\n",
      "Gradient Descent(6133/9999): loss=172.76489289837275, w0=0.02204792424350156, w1=0.2299689145398044\n",
      "Gradient Descent(6134/9999): loss=172.73226797720915, w0=0.02204616952447046, w1=0.2299457939633728\n",
      "Gradient Descent(6135/9999): loss=172.69965061536809, w0=0.022044414974014978, w1=0.22992267557429757\n",
      "Gradient Descent(6136/9999): loss=172.66704081095435, w0=0.022042660592116348, w1=0.22989955937236858\n",
      "Gradient Descent(6137/9999): loss=172.63443856207323, w0=0.02204090637875581, w1=0.2298764453573757\n",
      "Gradient Descent(6138/9999): loss=172.60184386683048, w0=0.022039152333914604, w1=0.22985333352910878\n",
      "Gradient Descent(6139/9999): loss=172.56925672333222, w0=0.022037398457573975, w1=0.22983022388735777\n",
      "Gradient Descent(6140/9999): loss=172.53667712968542, w0=0.02203564474971517, w1=0.2298071164319126\n",
      "Gradient Descent(6141/9999): loss=172.5041050839972, w0=0.02203389121031943, w1=0.2297840111625632\n",
      "Gradient Descent(6142/9999): loss=172.47154058437545, w0=0.02203213783936801, w1=0.22976090807909955\n",
      "Gradient Descent(6143/9999): loss=172.4389836289282, w0=0.02203038463684216, w1=0.22973780718131165\n",
      "Gradient Descent(6144/9999): loss=172.40643421576453, w0=0.02202863160272314, w1=0.22971470846898948\n",
      "Gradient Descent(6145/9999): loss=172.37389234299354, w0=0.022026878736992196, w1=0.22969161194192309\n",
      "Gradient Descent(6146/9999): loss=172.34135800872502, w0=0.022025126039630596, w1=0.2296685175999025\n",
      "Gradient Descent(6147/9999): loss=172.30883121106928, w0=0.022023373510619604, w1=0.2296454254427178\n",
      "Gradient Descent(6148/9999): loss=172.27631194813705, w0=0.02202162114994048, w1=0.22962233547015903\n",
      "Gradient Descent(6149/9999): loss=172.24380021803955, w0=0.022019868957574486, w1=0.22959924768201634\n",
      "Gradient Descent(6150/9999): loss=172.2112960188887, w0=0.0220181169335029, w1=0.22957616207807982\n",
      "Gradient Descent(6151/9999): loss=172.17879934879673, w0=0.02201636507770699, w1=0.22955307865813962\n",
      "Gradient Descent(6152/9999): loss=172.14631020587632, w0=0.022014613390168025, w1=0.2295299974219859\n",
      "Gradient Descent(6153/9999): loss=172.11382858824084, w0=0.022012861870867286, w1=0.22950691836940884\n",
      "Gradient Descent(6154/9999): loss=172.08135449400402, w0=0.022011110519786048, w1=0.22948384150019863\n",
      "Gradient Descent(6155/9999): loss=172.04888792128017, w0=0.022009359336905593, w1=0.22946076681414546\n",
      "Gradient Descent(6156/9999): loss=172.01642886818402, w0=0.022007608322207203, w1=0.22943769431103958\n",
      "Gradient Descent(6157/9999): loss=171.9839773328309, w0=0.022005857475672165, w1=0.22941462399067128\n",
      "Gradient Descent(6158/9999): loss=171.95153331333648, w0=0.022004106797281764, w1=0.2293915558528308\n",
      "Gradient Descent(6159/9999): loss=171.91909680781723, w0=0.022002356287017293, w1=0.22936848989730843\n",
      "Gradient Descent(6160/9999): loss=171.88666781438977, w0=0.02200060594486004, w1=0.22934542612389447\n",
      "Gradient Descent(6161/9999): loss=171.85424633117137, w0=0.021998855770791303, w1=0.22932236453237928\n",
      "Gradient Descent(6162/9999): loss=171.82183235627986, w0=0.021997105764792378, w1=0.2292993051225532\n",
      "Gradient Descent(6163/9999): loss=171.78942588783346, w0=0.021995355926844564, w1=0.2292762478942066\n",
      "Gradient Descent(6164/9999): loss=171.757026923951, w0=0.02199360625692916, w1=0.2292531928471298\n",
      "Gradient Descent(6165/9999): loss=171.72463546275168, w0=0.021991856755027475, w1=0.2292301399811133\n",
      "Gradient Descent(6166/9999): loss=171.69225150235528, w0=0.021990107421120814, w1=0.22920708929594746\n",
      "Gradient Descent(6167/9999): loss=171.6598750408821, w0=0.021988358255190483, w1=0.22918404079142277\n",
      "Gradient Descent(6168/9999): loss=171.62750607645287, w0=0.021986609257217793, w1=0.22916099446732965\n",
      "Gradient Descent(6169/9999): loss=171.59514460718884, w0=0.021984860427184057, w1=0.2291379503234586\n",
      "Gradient Descent(6170/9999): loss=171.56279063121178, w0=0.021983111765070593, w1=0.22911490835960013\n",
      "Gradient Descent(6171/9999): loss=171.530444146644, w0=0.021981363270858718, w1=0.22909186857554475\n",
      "Gradient Descent(6172/9999): loss=171.49810515160817, w0=0.021979614944529755, w1=0.229068830971083\n",
      "Gradient Descent(6173/9999): loss=171.46577364422754, w0=0.02197786678606502, w1=0.2290457955460054\n",
      "Gradient Descent(6174/9999): loss=171.43344962262591, w0=0.021976118795445844, w1=0.22902276230010257\n",
      "Gradient Descent(6175/9999): loss=171.40113308492747, w0=0.02197437097265355, w1=0.2289997312331651\n",
      "Gradient Descent(6176/9999): loss=171.368824029257, w0=0.02197262331766947, w1=0.2289767023449836\n",
      "Gradient Descent(6177/9999): loss=171.33652245373975, w0=0.021970875830474936, w1=0.22895367563534869\n",
      "Gradient Descent(6178/9999): loss=171.30422835650137, w0=0.021969128511051282, w1=0.22893065110405103\n",
      "Gradient Descent(6179/9999): loss=171.2719417356682, w0=0.021967381359379843, w1=0.2289076287508813\n",
      "Gradient Descent(6180/9999): loss=171.23966258936687, w0=0.02196563437544196, w1=0.22888460857563017\n",
      "Gradient Descent(6181/9999): loss=171.2073909157247, w0=0.02196388755921897, w1=0.22886159057808836\n",
      "Gradient Descent(6182/9999): loss=171.17512671286931, w0=0.021962140910692225, w1=0.2288385747580466\n",
      "Gradient Descent(6183/9999): loss=171.14286997892896, w0=0.021960394429843065, w1=0.2288155611152956\n",
      "Gradient Descent(6184/9999): loss=171.11062071203241, w0=0.021958648116652837, w1=0.2287925496496262\n",
      "Gradient Descent(6185/9999): loss=171.07837891030877, w0=0.021956901971102897, w1=0.2287695403608291\n",
      "Gradient Descent(6186/9999): loss=171.04614457188782, w0=0.021955155993174595, w1=0.22874653324869518\n",
      "Gradient Descent(6187/9999): loss=171.01391769489965, w0=0.021953410182849287, w1=0.2287235283130152\n",
      "Gradient Descent(6188/9999): loss=170.9816982774751, w0=0.02195166454010833, w1=0.22870052555358003\n",
      "Gradient Descent(6189/9999): loss=170.94948631774523, w0=0.02194991906493308, w1=0.22867752497018054\n",
      "Gradient Descent(6190/9999): loss=170.91728181384178, w0=0.021948173757304908, w1=0.2286545265626076\n",
      "Gradient Descent(6191/9999): loss=170.885084763897, w0=0.021946428617205173, w1=0.2286315303306521\n",
      "Gradient Descent(6192/9999): loss=170.8528951660434, w0=0.021944683644615243, w1=0.22860853627410493\n",
      "Gradient Descent(6193/9999): loss=170.82071301841424, w0=0.021942938839516486, w1=0.22858554439275708\n",
      "Gradient Descent(6194/9999): loss=170.7885383191432, w0=0.021941194201890276, w1=0.22856255468639947\n",
      "Gradient Descent(6195/9999): loss=170.7563710663644, w0=0.021939449731717985, w1=0.22853956715482307\n",
      "Gradient Descent(6196/9999): loss=170.72421125821253, w0=0.02193770542898099, w1=0.2285165817978189\n",
      "Gradient Descent(6197/9999): loss=170.69205889282264, w0=0.021935961293660673, w1=0.22849359861517796\n",
      "Gradient Descent(6198/9999): loss=170.6599139683305, w0=0.021934217325738407, w1=0.22847061760669127\n",
      "Gradient Descent(6199/9999): loss=170.62777648287212, w0=0.021932473525195582, w1=0.22844763877214988\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(6200/9999): loss=170.5956464345842, w0=0.02193072989201358, w1=0.22842466211134488\n",
      "Gradient Descent(6201/9999): loss=170.56352382160384, w0=0.02192898642617379, w1=0.22840168762406735\n",
      "Gradient Descent(6202/9999): loss=170.53140864206864, w0=0.021927243127657606, w1=0.22837871531010837\n",
      "Gradient Descent(6203/9999): loss=170.49930089411666, w0=0.021925499996446416, w1=0.22835574516925908\n",
      "Gradient Descent(6204/9999): loss=170.4672005758866, w0=0.02192375703252162, w1=0.22833277720131062\n",
      "Gradient Descent(6205/9999): loss=170.43510768551747, w0=0.02192201423586461, w1=0.22830981140605416\n",
      "Gradient Descent(6206/9999): loss=170.40302222114892, w0=0.021920271606456786, w1=0.2282868477832809\n",
      "Gradient Descent(6207/9999): loss=170.3709441809209, w0=0.021918529144279553, w1=0.228263886332782\n",
      "Gradient Descent(6208/9999): loss=170.33887356297413, w0=0.021916786849314314, w1=0.2282409270543487\n",
      "Gradient Descent(6209/9999): loss=170.3068103654496, w0=0.021915044721542477, w1=0.22821796994777224\n",
      "Gradient Descent(6210/9999): loss=170.27475458648888, w0=0.021913302760945448, w1=0.22819501501284387\n",
      "Gradient Descent(6211/9999): loss=170.24270622423396, w0=0.021911560967504642, w1=0.22817206224935488\n",
      "Gradient Descent(6212/9999): loss=170.21066527682748, w0=0.02190981934120147, w1=0.22814911165709656\n",
      "Gradient Descent(6213/9999): loss=170.17863174241236, w0=0.02190807788201735, w1=0.2281261632358602\n",
      "Gradient Descent(6214/9999): loss=170.1466056191322, w0=0.021906336589933698, w1=0.22810321698543717\n",
      "Gradient Descent(6215/9999): loss=170.114586905131, w0=0.021904595464931936, w1=0.2280802729056188\n",
      "Gradient Descent(6216/9999): loss=170.08257559855326, w0=0.021902854506993485, w1=0.22805733099619646\n",
      "Gradient Descent(6217/9999): loss=170.0505716975439, w0=0.021901113716099773, w1=0.22803439125696154\n",
      "Gradient Descent(6218/9999): loss=170.0185752002485, w0=0.021899373092232225, w1=0.22801145368770545\n",
      "Gradient Descent(6219/9999): loss=169.986586104813, w0=0.021897632635372276, w1=0.22798851828821962\n",
      "Gradient Descent(6220/9999): loss=169.9546044093839, w0=0.02189589234550135, w1=0.2279655850582955\n",
      "Gradient Descent(6221/9999): loss=169.9226301121081, w0=0.02189415222260089, w1=0.22794265399772456\n",
      "Gradient Descent(6222/9999): loss=169.89066321113307, w0=0.02189241226665233, w1=0.22791972510629827\n",
      "Gradient Descent(6223/9999): loss=169.85870370460685, w0=0.021890672477637103, w1=0.22789679838380814\n",
      "Gradient Descent(6224/9999): loss=169.8267515906777, w0=0.02188893285553666, w1=0.22787387383004568\n",
      "Gradient Descent(6225/9999): loss=169.79480686749466, w0=0.02188719340033244, w1=0.22785095144480244\n",
      "Gradient Descent(6226/9999): loss=169.7628695332071, w0=0.021885454112005893, w1=0.22782803122787\n",
      "Gradient Descent(6227/9999): loss=169.7309395859649, w0=0.021883714990538462, w1=0.2278051131790399\n",
      "Gradient Descent(6228/9999): loss=169.69901702391846, w0=0.021881976035911604, w1=0.22778219729810376\n",
      "Gradient Descent(6229/9999): loss=169.66710184521872, w0=0.021880237248106768, w1=0.22775928358485317\n",
      "Gradient Descent(6230/9999): loss=169.63519404801696, w0=0.021878498627105412, w1=0.2277363720390798\n",
      "Gradient Descent(6231/9999): loss=169.60329363046512, w0=0.021876760172888993, w1=0.22771346266057527\n",
      "Gradient Descent(6232/9999): loss=169.57140059071548, w0=0.02187502188543897, w1=0.22769055544913128\n",
      "Gradient Descent(6233/9999): loss=169.53951492692093, w0=0.02187328376473681, w1=0.2276676504045395\n",
      "Gradient Descent(6234/9999): loss=169.50763663723475, w0=0.021871545810763973, w1=0.2276447475265917\n",
      "Gradient Descent(6235/9999): loss=169.47576571981082, w0=0.021869808023501927, w1=0.22762184681507955\n",
      "Gradient Descent(6236/9999): loss=169.44390217280332, w0=0.021868070402932142, w1=0.2275989482697948\n",
      "Gradient Descent(6237/9999): loss=169.4120459943672, w0=0.02186633294903609, w1=0.22757605189052924\n",
      "Gradient Descent(6238/9999): loss=169.38019718265764, w0=0.021864595661795248, w1=0.22755315767707462\n",
      "Gradient Descent(6239/9999): loss=169.3483557358304, w0=0.021862858541191087, w1=0.2275302656292228\n",
      "Gradient Descent(6240/9999): loss=169.3165216520418, w0=0.02186112158720509, w1=0.22750737574676555\n",
      "Gradient Descent(6241/9999): loss=169.28469492944853, w0=0.021859384799818738, w1=0.22748448802949475\n",
      "Gradient Descent(6242/9999): loss=169.25287556620785, w0=0.021857648179013513, w1=0.22746160247720223\n",
      "Gradient Descent(6243/9999): loss=169.2210635604775, w0=0.0218559117247709, w1=0.2274387190896799\n",
      "Gradient Descent(6244/9999): loss=169.18925891041562, w0=0.021854175437072392, w1=0.22741583786671968\n",
      "Gradient Descent(6245/9999): loss=169.15746161418096, w0=0.021852439315899478, w1=0.22739295880811342\n",
      "Gradient Descent(6246/9999): loss=169.1256716699327, w0=0.021850703361233646, w1=0.2273700819136531\n",
      "Gradient Descent(6247/9999): loss=169.09388907583048, w0=0.021848967573056398, w1=0.22734720718313067\n",
      "Gradient Descent(6248/9999): loss=169.06211383003452, w0=0.021847231951349227, w1=0.2273243346163381\n",
      "Gradient Descent(6249/9999): loss=169.03034593070538, w0=0.02184549649609363, w1=0.22730146421306738\n",
      "Gradient Descent(6250/9999): loss=168.99858537600423, w0=0.021843761207271117, w1=0.22727859597311054\n",
      "Gradient Descent(6251/9999): loss=168.96683216409266, w0=0.021842026084863186, w1=0.22725572989625958\n",
      "Gradient Descent(6252/9999): loss=168.93508629313288, w0=0.021840291128851348, w1=0.22723286598230658\n",
      "Gradient Descent(6253/9999): loss=168.90334776128736, w0=0.021838556339217108, w1=0.2272100042310436\n",
      "Gradient Descent(6254/9999): loss=168.87161656671924, w0=0.021836821715941982, w1=0.22718714464226272\n",
      "Gradient Descent(6255/9999): loss=168.839892707592, w0=0.02183508725900748, w1=0.22716428721575604\n",
      "Gradient Descent(6256/9999): loss=168.80817618206987, w0=0.02183335296839512, w1=0.2271414319513157\n",
      "Gradient Descent(6257/9999): loss=168.77646698831717, w0=0.021831618844086423, w1=0.22711857884873385\n",
      "Gradient Descent(6258/9999): loss=168.74476512449903, w0=0.021829884886062904, w1=0.22709572790780264\n",
      "Gradient Descent(6259/9999): loss=168.7130705887809, w0=0.02182815109430609, w1=0.22707287912831425\n",
      "Gradient Descent(6260/9999): loss=168.68138337932888, w0=0.021826417468797505, w1=0.2270500325100609\n",
      "Gradient Descent(6261/9999): loss=168.6497034943093, w0=0.021824684009518678, w1=0.22702718805283478\n",
      "Gradient Descent(6262/9999): loss=168.61803093188922, w0=0.02182295071645114, w1=0.22700434575642814\n",
      "Gradient Descent(6263/9999): loss=168.58636569023608, w0=0.02182121758957642, w1=0.22698150562063324\n",
      "Gradient Descent(6264/9999): loss=168.55470776751784, w0=0.021819484628876052, w1=0.22695866764524236\n",
      "Gradient Descent(6265/9999): loss=168.5230571619028, w0=0.021817751834331576, w1=0.2269358318300478\n",
      "Gradient Descent(6266/9999): loss=168.49141387155996, w0=0.021816019205924533, w1=0.22691299817484187\n",
      "Gradient Descent(6267/9999): loss=168.45977789465869, w0=0.021814286743636464, w1=0.2268901666794169\n",
      "Gradient Descent(6268/9999): loss=168.4281492293688, w0=0.02181255444744891, w1=0.2268673373435652\n",
      "Gradient Descent(6269/9999): loss=168.39652787386072, w0=0.021810822317343416, w1=0.2268445101670792\n",
      "Gradient Descent(6270/9999): loss=168.3649138263053, w0=0.021809090353301534, w1=0.2268216851497513\n",
      "Gradient Descent(6271/9999): loss=168.33330708487375, w0=0.021807358555304815, w1=0.22679886229137383\n",
      "Gradient Descent(6272/9999): loss=168.301707647738, w0=0.021805626923334813, w1=0.22677604159173928\n",
      "Gradient Descent(6273/9999): loss=168.27011551307027, w0=0.02180389545737308, w1=0.22675322305064008\n",
      "Gradient Descent(6274/9999): loss=168.23853067904332, w0=0.021802164157401178, w1=0.2267304066678687\n",
      "Gradient Descent(6275/9999): loss=168.20695314383042, w0=0.021800433023400664, w1=0.22670759244321761\n",
      "Gradient Descent(6276/9999): loss=168.17538290560537, w0=0.021798702055353104, w1=0.22668478037647932\n",
      "Gradient Descent(6277/9999): loss=168.14381996254227, w0=0.02179697125324006, w1=0.22666197046744635\n",
      "Gradient Descent(6278/9999): loss=168.11226431281597, w0=0.021795240617043098, w1=0.22663916271591122\n",
      "Gradient Descent(6279/9999): loss=168.08071595460157, w0=0.02179351014674379, w1=0.2266163571216665\n",
      "Gradient Descent(6280/9999): loss=168.04917488607478, w0=0.021791779842323708, w1=0.2265935536845048\n",
      "Gradient Descent(6281/9999): loss=168.0176411054117, w0=0.021790049703764424, w1=0.22657075240421864\n",
      "Gradient Descent(6282/9999): loss=167.98611461078897, w0=0.021788319731047518, w1=0.2265479532806007\n",
      "Gradient Descent(6283/9999): loss=167.9545954003838, w0=0.021786589924154567, w1=0.2265251563134436\n",
      "Gradient Descent(6284/9999): loss=167.92308347237372, w0=0.021784860283067153, w1=0.22650236150254\n",
      "Gradient Descent(6285/9999): loss=167.8915788249368, w0=0.021783130807766858, w1=0.22647956884768256\n",
      "Gradient Descent(6286/9999): loss=167.86008145625163, w0=0.021781401498235268, w1=0.22645677834866396\n",
      "Gradient Descent(6287/9999): loss=167.82859136449724, w0=0.02177967235445397, w1=0.22643399000527692\n",
      "Gradient Descent(6288/9999): loss=167.79710854785318, w0=0.021777943376404558, w1=0.22641120381731417\n",
      "Gradient Descent(6289/9999): loss=167.7656330044995, w0=0.02177621456406862, w1=0.22638841978456845\n",
      "Gradient Descent(6290/9999): loss=167.73416473261656, w0=0.021774485917427755, w1=0.2263656379068325\n",
      "Gradient Descent(6291/9999): loss=167.70270373038548, w0=0.02177275743646356, w1=0.22634285818389915\n",
      "Gradient Descent(6292/9999): loss=167.67124999598758, w0=0.021771029121157633, w1=0.2263200806155612\n",
      "Gradient Descent(6293/9999): loss=167.63980352760495, w0=0.021769300971491577, w1=0.22629730520161143\n",
      "Gradient Descent(6294/9999): loss=167.60836432341986, w0=0.021767572987446998, w1=0.22627453194184272\n",
      "Gradient Descent(6295/9999): loss=167.57693238161534, w0=0.0217658451690055, w1=0.2262517608360479\n",
      "Gradient Descent(6296/9999): loss=167.5455077003746, w0=0.02176411751614869, w1=0.22622899188401985\n",
      "Gradient Descent(6297/9999): loss=167.5140902778816, w0=0.021762390028858183, w1=0.2262062250855515\n",
      "Gradient Descent(6298/9999): loss=167.4826801123207, w0=0.021760662707115592, w1=0.22618346044043572\n",
      "Gradient Descent(6299/9999): loss=167.45127720187662, w0=0.021758935550902532, w1=0.22616069794846547\n",
      "Gradient Descent(6300/9999): loss=167.41988154473478, w0=0.021757208560200622, w1=0.2261379376094337\n",
      "Gradient Descent(6301/9999): loss=167.3884931390809, w0=0.02175548173499148, w1=0.2261151794231334\n",
      "Gradient Descent(6302/9999): loss=167.35711198310113, w0=0.021753755075256734, w1=0.2260924233893575\n",
      "Gradient Descent(6303/9999): loss=167.32573807498244, w0=0.021752028580978005, w1=0.22606966950789908\n",
      "Gradient Descent(6304/9999): loss=167.29437141291183, w0=0.021750302252136925, w1=0.22604691777855115\n",
      "Gradient Descent(6305/9999): loss=167.26301199507716, w0=0.021748576088715117, w1=0.22602416820110674\n",
      "Gradient Descent(6306/9999): loss=167.23165981966645, w0=0.02174685009069422, w1=0.22600142077535892\n",
      "Gradient Descent(6307/9999): loss=167.20031488486845, w0=0.021745124258055866, w1=0.22597867550110076\n",
      "Gradient Descent(6308/9999): loss=167.16897718887228, w0=0.021743398590781688, w1=0.2259559323781254\n",
      "Gradient Descent(6309/9999): loss=167.13764672986755, w0=0.02174167308885333, w1=0.22593319140622592\n",
      "Gradient Descent(6310/9999): loss=167.10632350604433, w0=0.021739947752252433, w1=0.2259104525851955\n",
      "Gradient Descent(6311/9999): loss=167.07500751559317, w0=0.02173822258096064, w1=0.22588771591482726\n",
      "Gradient Descent(6312/9999): loss=167.04369875670528, w0=0.02173649757495959, w1=0.2258649813949144\n",
      "Gradient Descent(6313/9999): loss=167.01239722757197, w0=0.021734772734230943, w1=0.22584224902525013\n",
      "Gradient Descent(6314/9999): loss=166.98110292638538, w0=0.02173304805875634, w1=0.22581951880562764\n",
      "Gradient Descent(6315/9999): loss=166.94981585133795, w0=0.021731323548517444, w1=0.22579679073584016\n",
      "Gradient Descent(6316/9999): loss=166.91853600062254, w0=0.021729599203495903, w1=0.22577406481568096\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(6317/9999): loss=166.88726337243278, w0=0.021727875023673373, w1=0.22575134104494332\n",
      "Gradient Descent(6318/9999): loss=166.8559979649624, w0=0.02172615100903152, w1=0.2257286194234205\n",
      "Gradient Descent(6319/9999): loss=166.82473977640592, w0=0.021724427159552005, w1=0.22570589995090587\n",
      "Gradient Descent(6320/9999): loss=166.79348880495826, w0=0.021722703475216487, w1=0.2256831826271927\n",
      "Gradient Descent(6321/9999): loss=166.76224504881463, w0=0.02172097995600664, w1=0.22566046745207435\n",
      "Gradient Descent(6322/9999): loss=166.7310085061709, w0=0.02171925660190413, w1=0.2256377544253442\n",
      "Gradient Descent(6323/9999): loss=166.6997791752234, w0=0.021717533412890625, w1=0.22561504354679562\n",
      "Gradient Descent(6324/9999): loss=166.66855705416884, w0=0.021715810388947803, w1=0.225592334816222\n",
      "Gradient Descent(6325/9999): loss=166.6373421412046, w0=0.021714087530057342, w1=0.2255696282334168\n",
      "Gradient Descent(6326/9999): loss=166.6061344345283, w0=0.021712364836200915, w1=0.22554692379817345\n",
      "Gradient Descent(6327/9999): loss=166.57493393233815, w0=0.02171064230736021, w1=0.2255242215102854\n",
      "Gradient Descent(6328/9999): loss=166.5437406328329, w0=0.021708919943516904, w1=0.2255015213695461\n",
      "Gradient Descent(6329/9999): loss=166.51255453421166, w0=0.021707197744652684, w1=0.2254788233757491\n",
      "Gradient Descent(6330/9999): loss=166.48137563467412, w0=0.021705475710749236, w1=0.22545612752868788\n",
      "Gradient Descent(6331/9999): loss=166.45020393242032, w0=0.021703753841788253, w1=0.22543343382815598\n",
      "Gradient Descent(6332/9999): loss=166.41903942565094, w0=0.021702032137751428, w1=0.22541074227394697\n",
      "Gradient Descent(6333/9999): loss=166.38788211256698, w0=0.021700310598620452, w1=0.22538805286585442\n",
      "Gradient Descent(6334/9999): loss=166.35673199136994, w0=0.021698589224377025, w1=0.2253653656036719\n",
      "Gradient Descent(6335/9999): loss=166.32558906026193, w0=0.02169686801500284, w1=0.22534268048719303\n",
      "Gradient Descent(6336/9999): loss=166.29445331744543, w0=0.021695146970479605, w1=0.22531999751621143\n",
      "Gradient Descent(6337/9999): loss=166.26332476112336, w0=0.021693426090789024, w1=0.22529731669052075\n",
      "Gradient Descent(6338/9999): loss=166.23220338949915, w0=0.0216917053759128, w1=0.22527463800991468\n",
      "Gradient Descent(6339/9999): loss=166.20108920077678, w0=0.02168998482583264, w1=0.22525196147418688\n",
      "Gradient Descent(6340/9999): loss=166.16998219316056, w0=0.02168826444053026, w1=0.22522928708313106\n",
      "Gradient Descent(6341/9999): loss=166.13888236485542, w0=0.021686544219987374, w1=0.22520661483654092\n",
      "Gradient Descent(6342/9999): loss=166.10778971406668, w0=0.02168482416418569, w1=0.22518394473421022\n",
      "Gradient Descent(6343/9999): loss=166.07670423900018, w0=0.021683104273106935, w1=0.22516127677593273\n",
      "Gradient Descent(6344/9999): loss=166.04562593786216, w0=0.02168138454673282, w1=0.2251386109615022\n",
      "Gradient Descent(6345/9999): loss=166.01455480885934, w0=0.02167966498504507, w1=0.22511594729071246\n",
      "Gradient Descent(6346/9999): loss=165.98349085019908, w0=0.021677945588025414, w1=0.2250932857633573\n",
      "Gradient Descent(6347/9999): loss=165.95243406008896, w0=0.021676226355655574, w1=0.22507062637923056\n",
      "Gradient Descent(6348/9999): loss=165.92138443673736, w0=0.02167450728791728, w1=0.2250479691381261\n",
      "Gradient Descent(6349/9999): loss=165.89034197835272, w0=0.021672788384792266, w1=0.2250253140398378\n",
      "Gradient Descent(6350/9999): loss=165.85930668314427, w0=0.021671069646262264, w1=0.22500266108415953\n",
      "Gradient Descent(6351/9999): loss=165.8282785493216, w0=0.02166935107230901, w1=0.2249800102708852\n",
      "Gradient Descent(6352/9999): loss=165.79725757509482, w0=0.02166763266291424, w1=0.22495736159980875\n",
      "Gradient Descent(6353/9999): loss=165.76624375867442, w0=0.0216659144180597, w1=0.2249347150707241\n",
      "Gradient Descent(6354/9999): loss=165.73523709827145, w0=0.02166419633772713, w1=0.22491207068342525\n",
      "Gradient Descent(6355/9999): loss=165.70423759209748, w0=0.02166247842189828, w1=0.22488942843770615\n",
      "Gradient Descent(6356/9999): loss=165.6732452383643, w0=0.02166076067055489, w1=0.22486678833336082\n",
      "Gradient Descent(6357/9999): loss=165.64226003528452, w0=0.021659043083678716, w1=0.2248441503701833\n",
      "Gradient Descent(6358/9999): loss=165.611281981071, w0=0.02165732566125151, w1=0.2248215145479676\n",
      "Gradient Descent(6359/9999): loss=165.58031107393708, w0=0.021655608403255023, w1=0.2247988808665078\n",
      "Gradient Descent(6360/9999): loss=165.54934731209676, w0=0.021653891309671013, w1=0.22477624932559795\n",
      "Gradient Descent(6361/9999): loss=165.51839069376416, w0=0.02165217438048124, w1=0.22475361992503218\n",
      "Gradient Descent(6362/9999): loss=165.48744121715418, w0=0.021650457615667466, w1=0.22473099266460458\n",
      "Gradient Descent(6363/9999): loss=165.45649888048214, w0=0.021648741015211456, w1=0.22470836754410928\n",
      "Gradient Descent(6364/9999): loss=165.42556368196375, w0=0.021647024579094974, w1=0.22468574456334045\n",
      "Gradient Descent(6365/9999): loss=165.39463561981518, w0=0.021645308307299792, w1=0.22466312372209224\n",
      "Gradient Descent(6366/9999): loss=165.3637146922532, w0=0.021643592199807674, w1=0.22464050502015886\n",
      "Gradient Descent(6367/9999): loss=165.3328008974949, w0=0.021641876256600398, w1=0.22461788845733452\n",
      "Gradient Descent(6368/9999): loss=165.3018942337579, w0=0.021640160477659737, w1=0.22459527403341342\n",
      "Gradient Descent(6369/9999): loss=165.27099469926043, w0=0.02163844486296747, w1=0.22457266174818982\n",
      "Gradient Descent(6370/9999): loss=165.24010229222088, w0=0.02163672941250538, w1=0.224550051601458\n",
      "Gradient Descent(6371/9999): loss=165.20921701085842, w0=0.021635014126255243, w1=0.22452744359301222\n",
      "Gradient Descent(6372/9999): loss=165.17833885339255, w0=0.021633299004198848, w1=0.2245048377226468\n",
      "Gradient Descent(6373/9999): loss=165.14746781804317, w0=0.021631584046317983, w1=0.22448223399015604\n",
      "Gradient Descent(6374/9999): loss=165.11660390303086, w0=0.02162986925259443, w1=0.2244596323953343\n",
      "Gradient Descent(6375/9999): loss=165.08574710657643, w0=0.021628154623009987, w1=0.22443703293797593\n",
      "Gradient Descent(6376/9999): loss=165.0548974269013, w0=0.021626440157546445, w1=0.2244144356178753\n",
      "Gradient Descent(6377/9999): loss=165.02405486222736, w0=0.021624725856185602, w1=0.2243918404348268\n",
      "Gradient Descent(6378/9999): loss=164.99321941077696, w0=0.021623011718909256, w1=0.22436924738862485\n",
      "Gradient Descent(6379/9999): loss=164.96239107077287, w0=0.02162129774569921, w1=0.22434665647906388\n",
      "Gradient Descent(6380/9999): loss=164.93156984043833, w0=0.021619583936537262, w1=0.22432406770593835\n",
      "Gradient Descent(6381/9999): loss=164.90075571799713, w0=0.02161787029140522, w1=0.22430148106904274\n",
      "Gradient Descent(6382/9999): loss=164.86994870167348, w0=0.021616156810284892, w1=0.22427889656817152\n",
      "Gradient Descent(6383/9999): loss=164.83914878969202, w0=0.021614443493158084, w1=0.22425631420311917\n",
      "Gradient Descent(6384/9999): loss=164.80835598027795, w0=0.021612730340006613, w1=0.22423373397368027\n",
      "Gradient Descent(6385/9999): loss=164.77757027165686, w0=0.021611017350812292, w1=0.22421115587964932\n",
      "Gradient Descent(6386/9999): loss=164.74679166205487, w0=0.02160930452555694, w1=0.22418857992082092\n",
      "Gradient Descent(6387/9999): loss=164.7160201496985, w0=0.021607591864222373, w1=0.2241660060969896\n",
      "Gradient Descent(6388/9999): loss=164.68525573281474, w0=0.021605879366790412, w1=0.22414343440795004\n",
      "Gradient Descent(6389/9999): loss=164.65449840963115, w0=0.02160416703324288, w1=0.22412086485349678\n",
      "Gradient Descent(6390/9999): loss=164.62374817837568, w0=0.021602454863561608, w1=0.2240982974334245\n",
      "Gradient Descent(6391/9999): loss=164.59300503727673, w0=0.02160074285772842, w1=0.22407573214752785\n",
      "Gradient Descent(6392/9999): loss=164.56226898456322, w0=0.021599031015725145, w1=0.2240531689956015\n",
      "Gradient Descent(6393/9999): loss=164.53154001846448, w0=0.021597319337533623, w1=0.22403060797744012\n",
      "Gradient Descent(6394/9999): loss=164.50081813721033, w0=0.021595607823135684, w1=0.22400804909283847\n",
      "Gradient Descent(6395/9999): loss=164.4701033390312, w0=0.021593896472513165, w1=0.22398549234159126\n",
      "Gradient Descent(6396/9999): loss=164.43939562215763, w0=0.021592185285647908, w1=0.22396293772349324\n",
      "Gradient Descent(6397/9999): loss=164.4086949848211, w0=0.021590474262521753, w1=0.22394038523833917\n",
      "Gradient Descent(6398/9999): loss=164.37800142525307, w0=0.021588763403116547, w1=0.22391783488592384\n",
      "Gradient Descent(6399/9999): loss=164.3473149416859, w0=0.021587052707414133, w1=0.22389528666604205\n",
      "Gradient Descent(6400/9999): loss=164.31663553235214, w0=0.021585342175396365, w1=0.22387274057848863\n",
      "Gradient Descent(6401/9999): loss=164.2859631954849, w0=0.02158363180704509, w1=0.22385019662305844\n",
      "Gradient Descent(6402/9999): loss=164.2552979293178, w0=0.021581921602342163, w1=0.2238276547995463\n",
      "Gradient Descent(6403/9999): loss=164.22463973208477, w0=0.02158021156126944, w1=0.22380511510774714\n",
      "Gradient Descent(6404/9999): loss=164.19398860202043, w0=0.021578501683808777, w1=0.22378257754745584\n",
      "Gradient Descent(6405/9999): loss=164.16334453735956, w0=0.021576791969942038, w1=0.2237600421184673\n",
      "Gradient Descent(6406/9999): loss=164.13270753633785, w0=0.021575082419651084, w1=0.22373750882057647\n",
      "Gradient Descent(6407/9999): loss=164.102077597191, w0=0.02157337303291778, w1=0.22371497765357828\n",
      "Gradient Descent(6408/9999): loss=164.07145471815545, w0=0.021571663809723994, w1=0.22369244861726775\n",
      "Gradient Descent(6409/9999): loss=164.04083889746806, w0=0.021569954750051596, w1=0.22366992171143985\n",
      "Gradient Descent(6410/9999): loss=164.01023013336604, w0=0.021568245853882458, w1=0.22364739693588956\n",
      "Gradient Descent(6411/9999): loss=163.97962842408725, w0=0.02156653712119845, w1=0.22362487429041195\n",
      "Gradient Descent(6412/9999): loss=163.9490337678698, w0=0.021564828551981455, w1=0.22360235377480206\n",
      "Gradient Descent(6413/9999): loss=163.91844616295253, w0=0.02156312014621335, w1=0.22357983538885495\n",
      "Gradient Descent(6414/9999): loss=163.88786560757444, w0=0.02156141190387601, w1=0.2235573191323657\n",
      "Gradient Descent(6415/9999): loss=163.85729209997524, w0=0.02155970382495133, w1=0.22353480500512943\n",
      "Gradient Descent(6416/9999): loss=163.8267256383951, w0=0.021557995909421184, w1=0.22351229300694125\n",
      "Gradient Descent(6417/9999): loss=163.79616622107437, w0=0.02155628815726747, w1=0.2234897831375963\n",
      "Gradient Descent(6418/9999): loss=163.76561384625427, w0=0.02155458056847207, w1=0.22346727539688974\n",
      "Gradient Descent(6419/9999): loss=163.73506851217607, w0=0.02155287314301688, w1=0.22344476978461675\n",
      "Gradient Descent(6420/9999): loss=163.70453021708195, w0=0.021551165880883797, w1=0.22342226630057252\n",
      "Gradient Descent(6421/9999): loss=163.6739989592142, w0=0.021549458782054713, w1=0.22339976494455227\n",
      "Gradient Descent(6422/9999): loss=163.64347473681562, w0=0.02154775184651153, w1=0.22337726571635125\n",
      "Gradient Descent(6423/9999): loss=163.61295754812966, w0=0.02154604507423615, w1=0.2233547686157647\n",
      "Gradient Descent(6424/9999): loss=163.58244739140008, w0=0.02154433846521048, w1=0.22333227364258787\n",
      "Gradient Descent(6425/9999): loss=163.5519442648711, w0=0.021542632019416426, w1=0.2233097807966161\n",
      "Gradient Descent(6426/9999): loss=163.52144816678756, w0=0.02154092573683589, w1=0.22328729007764464\n",
      "Gradient Descent(6427/9999): loss=163.49095909539457, w0=0.021539219617450787, w1=0.22326480148546884\n",
      "Gradient Descent(6428/9999): loss=163.46047704893783, w0=0.02153751366124303, w1=0.22324231501988406\n",
      "Gradient Descent(6429/9999): loss=163.43000202566338, w0=0.021535807868194538, w1=0.22321983068068565\n",
      "Gradient Descent(6430/9999): loss=163.39953402381784, w0=0.021534102238287223, w1=0.223197348467669\n",
      "Gradient Descent(6431/9999): loss=163.36907304164828, w0=0.02153239677150301, w1=0.22317486838062953\n",
      "Gradient Descent(6432/9999): loss=163.33861907740226, w0=0.02153069146782382, w1=0.22315239041936263\n",
      "Gradient Descent(6433/9999): loss=163.3081721293276, w0=0.021528986327231575, w1=0.22312991458366374\n",
      "Gradient Descent(6434/9999): loss=163.27773219567285, w0=0.021527281349708206, w1=0.22310744087332834\n",
      "Gradient Descent(6435/9999): loss=163.24729927468687, w0=0.021525576535235635, w1=0.22308496928815189\n",
      "Gradient Descent(6436/9999): loss=163.21687336461898, w0=0.0215238718837958, w1=0.2230624998279299\n",
      "Gradient Descent(6437/9999): loss=163.18645446371906, w0=0.021522167395370634, w1=0.22304003249245785\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(6438/9999): loss=163.15604257023736, w0=0.021520463069942073, w1=0.2230175672815313\n",
      "Gradient Descent(6439/9999): loss=163.1256376824246, w0=0.021518758907492053, w1=0.22299510419494578\n",
      "Gradient Descent(6440/9999): loss=163.09523979853205, w0=0.021517054908002518, w1=0.22297264323249688\n",
      "Gradient Descent(6441/9999): loss=163.06484891681137, w0=0.02151535107145541, w1=0.22295018439398018\n",
      "Gradient Descent(6442/9999): loss=163.0344650355146, w0=0.021513647397832673, w1=0.22292772767919128\n",
      "Gradient Descent(6443/9999): loss=163.00408815289438, w0=0.021511943887116254, w1=0.2229052730879258\n",
      "Gradient Descent(6444/9999): loss=162.9737182672038, w0=0.021510240539288104, w1=0.22288282061997938\n",
      "Gradient Descent(6445/9999): loss=162.94335537669622, w0=0.021508537354330174, w1=0.2228603702751477\n",
      "Gradient Descent(6446/9999): loss=162.9129994796258, w0=0.02150683433222442, w1=0.22283792205322647\n",
      "Gradient Descent(6447/9999): loss=162.8826505742469, w0=0.0215051314729528, w1=0.22281547595401133\n",
      "Gradient Descent(6448/9999): loss=162.8523086588144, w0=0.021503428776497267, w1=0.22279303197729802\n",
      "Gradient Descent(6449/9999): loss=162.82197373158368, w0=0.02150172624283979, w1=0.22277059012288228\n",
      "Gradient Descent(6450/9999): loss=162.79164579081058, w0=0.02150002387196233, w1=0.22274815039055987\n",
      "Gradient Descent(6451/9999): loss=162.76132483475124, w0=0.021498321663846846, w1=0.22272571278012654\n",
      "Gradient Descent(6452/9999): loss=162.7310108616625, w0=0.021496619618475318, w1=0.2227032772913781\n",
      "Gradient Descent(6453/9999): loss=162.70070386980157, w0=0.02149491773582971, w1=0.22268084392411036\n",
      "Gradient Descent(6454/9999): loss=162.67040385742607, w0=0.02149321601589199, w1=0.22265841267811914\n",
      "Gradient Descent(6455/9999): loss=162.64011082279416, w0=0.021491514458644143, w1=0.2226359835532003\n",
      "Gradient Descent(6456/9999): loss=162.6098247641643, w0=0.02148981306406814, w1=0.22261355654914972\n",
      "Gradient Descent(6457/9999): loss=162.57954567979564, w0=0.021488111832145964, w1=0.22259113166576325\n",
      "Gradient Descent(6458/9999): loss=162.5492735679476, w0=0.021486410762859592, w1=0.2225687089028368\n",
      "Gradient Descent(6459/9999): loss=162.5190084268802, w0=0.02148470985619101, w1=0.22254628826016634\n",
      "Gradient Descent(6460/9999): loss=162.4887502548538, w0=0.021483009112122208, w1=0.22252386973754776\n",
      "Gradient Descent(6461/9999): loss=162.45849905012932, w0=0.02148130853063517, w1=0.22250145333477703\n",
      "Gradient Descent(6462/9999): loss=162.42825481096807, w0=0.021479608111711887, w1=0.22247903905165012\n",
      "Gradient Descent(6463/9999): loss=162.3980175356318, w0=0.021477907855334356, w1=0.22245662688796303\n",
      "Gradient Descent(6464/9999): loss=162.36778722238273, w0=0.02147620776148457, w1=0.22243421684351178\n",
      "Gradient Descent(6465/9999): loss=162.33756386948372, w0=0.021474507830144526, w1=0.22241180891809242\n",
      "Gradient Descent(6466/9999): loss=162.30734747519773, w0=0.021472808061296228, w1=0.222389403111501\n",
      "Gradient Descent(6467/9999): loss=162.27713803778855, w0=0.021471108454921672, w1=0.22236699942353355\n",
      "Gradient Descent(6468/9999): loss=162.2469355555202, w0=0.021469409011002867, w1=0.2223445978539862\n",
      "Gradient Descent(6469/9999): loss=162.2167400266572, w0=0.02146770972952182, w1=0.22232219840265502\n",
      "Gradient Descent(6470/9999): loss=162.18655144946456, w0=0.02146601061046054, w1=0.22229980106933617\n",
      "Gradient Descent(6471/9999): loss=162.15636982220767, w0=0.02146431165380104, w1=0.2222774058538258\n",
      "Gradient Descent(6472/9999): loss=162.1261951431526, w0=0.021462612859525332, w1=0.22225501275592005\n",
      "Gradient Descent(6473/9999): loss=162.0960274105656, w0=0.02146091422761543, w1=0.2222326217754151\n",
      "Gradient Descent(6474/9999): loss=162.06586662271354, w0=0.021459215758053353, w1=0.22221023291210718\n",
      "Gradient Descent(6475/9999): loss=162.03571277786367, w0=0.021457517450821122, w1=0.22218784616579249\n",
      "Gradient Descent(6476/9999): loss=162.00556587428372, w0=0.021455819305900764, w1=0.22216546153626726\n",
      "Gradient Descent(6477/9999): loss=161.97542591024197, w0=0.0214541213232743, w1=0.22214307902332775\n",
      "Gradient Descent(6478/9999): loss=161.945292884007, w0=0.021452423502923756, w1=0.22212069862677025\n",
      "Gradient Descent(6479/9999): loss=161.9151667938479, w0=0.021450725844831164, w1=0.22209832034639104\n",
      "Gradient Descent(6480/9999): loss=161.88504763803437, w0=0.021449028348978556, w1=0.22207594418198642\n",
      "Gradient Descent(6481/9999): loss=161.8549354148363, w0=0.021447331015347966, w1=0.22205357013335275\n",
      "Gradient Descent(6482/9999): loss=161.82483012252422, w0=0.02144563384392143, w1=0.22203119820028636\n",
      "Gradient Descent(6483/9999): loss=161.79473175936906, w0=0.02144393683468099, w1=0.22200882838258362\n",
      "Gradient Descent(6484/9999): loss=161.76464032364223, w0=0.021442239987608684, w1=0.2219864606800409\n",
      "Gradient Descent(6485/9999): loss=161.7345558136155, w0=0.021440543302686556, w1=0.22196409509245463\n",
      "Gradient Descent(6486/9999): loss=161.70447822756134, w0=0.02143884677989665, w1=0.22194173161962122\n",
      "Gradient Descent(6487/9999): loss=161.67440756375237, w0=0.02143715041922102, w1=0.22191937026133712\n",
      "Gradient Descent(6488/9999): loss=161.64434382046187, w0=0.02143545422064171, w1=0.22189701101739878\n",
      "Gradient Descent(6489/9999): loss=161.6142869959635, w0=0.021433758184140773, w1=0.2218746538876027\n",
      "Gradient Descent(6490/9999): loss=161.58423708853138, w0=0.021432062309700267, w1=0.22185229887174535\n",
      "Gradient Descent(6491/9999): loss=161.55419409644009, w0=0.02143036659730225, w1=0.22182994596962324\n",
      "Gradient Descent(6492/9999): loss=161.52415801796468, w0=0.021428671046928777, w1=0.22180759518103293\n",
      "Gradient Descent(6493/9999): loss=161.49412885138062, w0=0.02142697565856191, w1=0.22178524650577097\n",
      "Gradient Descent(6494/9999): loss=161.4641065949639, w0=0.021425280432183717, w1=0.22176289994363393\n",
      "Gradient Descent(6495/9999): loss=161.4340912469909, w0=0.02142358536777626, w1=0.2217405554944184\n",
      "Gradient Descent(6496/9999): loss=161.40408280573848, w0=0.021421890465321607, w1=0.22171821315792098\n",
      "Gradient Descent(6497/9999): loss=161.37408126948398, w0=0.021420195724801833, w1=0.2216958729339383\n",
      "Gradient Descent(6498/9999): loss=161.34408663650504, w0=0.021418501146199007, w1=0.221673534822267\n",
      "Gradient Descent(6499/9999): loss=161.31409890508002, w0=0.021416806729495207, w1=0.22165119882270376\n",
      "Gradient Descent(6500/9999): loss=161.28411807348755, w0=0.021415112474672506, w1=0.22162886493504524\n",
      "Gradient Descent(6501/9999): loss=161.25414414000676, w0=0.021413418381712988, w1=0.22160653315908815\n",
      "Gradient Descent(6502/9999): loss=161.22417710291722, w0=0.021411724450598735, w1=0.22158420349462923\n",
      "Gradient Descent(6503/9999): loss=161.19421696049892, w0=0.021410030681311833, w1=0.2215618759414652\n",
      "Gradient Descent(6504/9999): loss=161.16426371103248, w0=0.021408337073834365, w1=0.2215395504993928\n",
      "Gradient Descent(6505/9999): loss=161.1343173527988, w0=0.02140664362814842, w1=0.22151722716820885\n",
      "Gradient Descent(6506/9999): loss=161.1043778840792, w0=0.021404950344236093, w1=0.2214949059477101\n",
      "Gradient Descent(6507/9999): loss=161.07444530315558, w0=0.021403257222079475, w1=0.22147258683769339\n",
      "Gradient Descent(6508/9999): loss=161.04451960831025, w0=0.02140156426166066, w1=0.22145026983795552\n",
      "Gradient Descent(6509/9999): loss=161.01460079782598, w0=0.021399871462961746, w1=0.22142795494829337\n",
      "Gradient Descent(6510/9999): loss=160.9846888699859, w0=0.021398178825964835, w1=0.22140564216850378\n",
      "Gradient Descent(6511/9999): loss=160.95478382307374, w0=0.021396486350652028, w1=0.22138333149838366\n",
      "Gradient Descent(6512/9999): loss=160.92488565537354, w0=0.021394794037005428, w1=0.2213610229377299\n",
      "Gradient Descent(6513/9999): loss=160.89499436517, w0=0.02139310188500715, w1=0.22133871648633946\n",
      "Gradient Descent(6514/9999): loss=160.86510995074804, w0=0.021391409894639297, w1=0.22131641214400924\n",
      "Gradient Descent(6515/9999): loss=160.83523241039313, w0=0.02138971806588398, w1=0.2212941099105362\n",
      "Gradient Descent(6516/9999): loss=160.8053617423912, w0=0.02138802639872332, w1=0.22127180978571734\n",
      "Gradient Descent(6517/9999): loss=160.7754979450287, w0=0.021386334893139423, w1=0.22124951176934965\n",
      "Gradient Descent(6518/9999): loss=160.74564101659237, w0=0.021384643549114415, w1=0.22122721586123015\n",
      "Gradient Descent(6519/9999): loss=160.71579095536953, w0=0.021382952366630414, w1=0.22120492206115586\n",
      "Gradient Descent(6520/9999): loss=160.68594775964786, w0=0.02138126134566954, w1=0.22118263036892383\n",
      "Gradient Descent(6521/9999): loss=160.65611142771556, w0=0.021379570486213924, w1=0.22116034078433114\n",
      "Gradient Descent(6522/9999): loss=160.62628195786135, w0=0.02137787978824569, w1=0.2211380533071749\n",
      "Gradient Descent(6523/9999): loss=160.5964593483742, w0=0.021376189251746967, w1=0.22111576793725218\n",
      "Gradient Descent(6524/9999): loss=160.5666435975437, w0=0.021374498876699892, w1=0.22109348467436013\n",
      "Gradient Descent(6525/9999): loss=160.5368347036598, w0=0.021372808663086593, w1=0.2210712035182959\n",
      "Gradient Descent(6526/9999): loss=160.507032665013, w0=0.02137111861088921, w1=0.22104892446885663\n",
      "Gradient Descent(6527/9999): loss=160.47723747989411, w0=0.02136942872008988, w1=0.22102664752583953\n",
      "Gradient Descent(6528/9999): loss=160.44744914659452, w0=0.021367738990670748, w1=0.22100437268904177\n",
      "Gradient Descent(6529/9999): loss=160.41766766340604, w0=0.021366049422613952, w1=0.2209820999582606\n",
      "Gradient Descent(6530/9999): loss=160.38789302862085, w0=0.02136436001590164, w1=0.22095982933329325\n",
      "Gradient Descent(6531/9999): loss=160.35812524053168, w0=0.02136267077051596, w1=0.22093756081393698\n",
      "Gradient Descent(6532/9999): loss=160.32836429743168, w0=0.021360981686439063, w1=0.22091529439998903\n",
      "Gradient Descent(6533/9999): loss=160.29861019761435, w0=0.0213592927636531, w1=0.22089303009124672\n",
      "Gradient Descent(6534/9999): loss=160.26886293937378, w0=0.021357604002140228, w1=0.22087076788750737\n",
      "Gradient Descent(6535/9999): loss=160.23912252100456, w0=0.021355915401882603, w1=0.2208485077885683\n",
      "Gradient Descent(6536/9999): loss=160.2093889408015, w0=0.021354226962862384, w1=0.22082624979422685\n",
      "Gradient Descent(6537/9999): loss=160.17966219706005, w0=0.02135253868506173, w1=0.2208039939042804\n",
      "Gradient Descent(6538/9999): loss=160.14994228807598, w0=0.02135085056846281, w1=0.22078174011852633\n",
      "Gradient Descent(6539/9999): loss=160.12022921214566, w0=0.021349162613047784, w1=0.22075948843676205\n",
      "Gradient Descent(6540/9999): loss=160.09052296756573, w0=0.021347474818798822, w1=0.22073723885878496\n",
      "Gradient Descent(6541/9999): loss=160.06082355263348, w0=0.021345787185698097, w1=0.22071499138439254\n",
      "Gradient Descent(6542/9999): loss=160.0311309656465, w0=0.02134409971372778, w1=0.22069274601338223\n",
      "Gradient Descent(6543/9999): loss=160.00144520490286, w0=0.021342412402870048, w1=0.2206705027455515\n",
      "Gradient Descent(6544/9999): loss=159.97176626870112, w0=0.02134072525310708, w1=0.22064826158069784\n",
      "Gradient Descent(6545/9999): loss=159.9420941553402, w0=0.02133903826442105, w1=0.2206260225186188\n",
      "Gradient Descent(6546/9999): loss=159.91242886311957, w0=0.02133735143679414, w1=0.22060378555911186\n",
      "Gradient Descent(6547/9999): loss=159.88277039033912, w0=0.02133566477020854, w1=0.2205815507019746\n",
      "Gradient Descent(6548/9999): loss=159.85311873529918, w0=0.02133397826464643, w1=0.22055931794700462\n",
      "Gradient Descent(6549/9999): loss=159.82347389630038, w0=0.021332291920090004, w1=0.22053708729399946\n",
      "Gradient Descent(6550/9999): loss=159.79383587164418, w0=0.02133060573652145, w1=0.22051485874275675\n",
      "Gradient Descent(6551/9999): loss=159.7642046596321, w0=0.02132891971392296, w1=0.22049263229307411\n",
      "Gradient Descent(6552/9999): loss=159.73458025856627, w0=0.021327233852276733, w1=0.2204704079447492\n",
      "Gradient Descent(6553/9999): loss=159.70496266674925, w0=0.021325548151564965, w1=0.22044818569757965\n",
      "Gradient Descent(6554/9999): loss=159.67535188248402, w0=0.021323862611769856, w1=0.22042596555136318\n",
      "Gradient Descent(6555/9999): loss=159.6457479040742, w0=0.021322177232873607, w1=0.22040374750589747\n",
      "Gradient Descent(6556/9999): loss=159.6161507298235, w0=0.021320492014858423, w1=0.22038153156098023\n",
      "Gradient Descent(6557/9999): loss=159.5865603580363, w0=0.02131880695770651, w1=0.2203593177164092\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(6558/9999): loss=159.55697678701753, w0=0.02131712206140008, w1=0.22033710597198214\n",
      "Gradient Descent(6559/9999): loss=159.52740001507235, w0=0.02131543732592134, w1=0.22031489632749682\n",
      "Gradient Descent(6560/9999): loss=159.4978300405064, w0=0.021313752751252506, w1=0.22029268878275104\n",
      "Gradient Descent(6561/9999): loss=159.46826686162584, w0=0.021312068337375794, w1=0.2202704833375426\n",
      "Gradient Descent(6562/9999): loss=159.4387104767373, w0=0.021310384084273423, w1=0.22024827999166932\n",
      "Gradient Descent(6563/9999): loss=159.4091608841478, w0=0.021308699991927613, w1=0.22022607874492908\n",
      "Gradient Descent(6564/9999): loss=159.37961808216485, w0=0.021307016060320583, w1=0.2202038795971197\n",
      "Gradient Descent(6565/9999): loss=159.35008206909626, w0=0.02130533228943456, w1=0.2201816825480391\n",
      "Gradient Descent(6566/9999): loss=159.3205528432505, w0=0.02130364867925177, w1=0.2201594875974852\n",
      "Gradient Descent(6567/9999): loss=159.29103040293637, w0=0.021301965229754447, w1=0.22013729474525584\n",
      "Gradient Descent(6568/9999): loss=159.26151474646306, w0=0.021300281940924818, w1=0.22011510399114903\n",
      "Gradient Descent(6569/9999): loss=159.23200587214035, w0=0.021298598812745116, w1=0.22009291533496272\n",
      "Gradient Descent(6570/9999): loss=159.20250377827836, w0=0.02129691584519758, w1=0.2200707287764949\n",
      "Gradient Descent(6571/9999): loss=159.17300846318776, w0=0.02129523303826445, w1=0.2200485443155435\n",
      "Gradient Descent(6572/9999): loss=159.14351992517948, w0=0.021293550391927964, w1=0.22002636195190658\n",
      "Gradient Descent(6573/9999): loss=159.11403816256507, w0=0.021291867906170363, w1=0.22000418168538216\n",
      "Gradient Descent(6574/9999): loss=159.08456317365642, w0=0.021290185580973894, w1=0.2199820035157683\n",
      "Gradient Descent(6575/9999): loss=159.05509495676588, w0=0.021288503416320805, w1=0.21995982744286305\n",
      "Gradient Descent(6576/9999): loss=159.02563351020635, w0=0.021286821412193346, w1=0.21993765346646452\n",
      "Gradient Descent(6577/9999): loss=158.99617883229112, w0=0.021285139568573766, w1=0.2199154815863708\n",
      "Gradient Descent(6578/9999): loss=158.96673092133383, w0=0.02128345788544432, w1=0.21989331180238003\n",
      "Gradient Descent(6579/9999): loss=158.93728977564854, w0=0.021281776362787268, w1=0.21987114411429032\n",
      "Gradient Descent(6580/9999): loss=158.90785539355002, w0=0.021280095000584864, w1=0.21984897852189986\n",
      "Gradient Descent(6581/9999): loss=158.87842777335322, w0=0.02127841379881937, w1=0.2198268150250068\n",
      "Gradient Descent(6582/9999): loss=158.84900691337367, w0=0.021276732757473053, w1=0.2198046536234094\n",
      "Gradient Descent(6583/9999): loss=158.81959281192732, w0=0.021275051876528173, w1=0.2197824943169058\n",
      "Gradient Descent(6584/9999): loss=158.79018546733047, w0=0.021273371155967, w1=0.21976033710529427\n",
      "Gradient Descent(6585/9999): loss=158.76078487789994, w0=0.021271690595771803, w1=0.21973818198837305\n",
      "Gradient Descent(6586/9999): loss=158.73139104195306, w0=0.02127001019592486, w1=0.21971602896594042\n",
      "Gradient Descent(6587/9999): loss=158.70200395780748, w0=0.021268329956408434, w1=0.21969387803779467\n",
      "Gradient Descent(6588/9999): loss=158.67262362378133, w0=0.02126664987720481, w1=0.2196717292037341\n",
      "Gradient Descent(6589/9999): loss=158.64325003819323, w0=0.021264969958296267, w1=0.21964958246355704\n",
      "Gradient Descent(6590/9999): loss=158.61388319936222, w0=0.021263290199665084, w1=0.21962743781706184\n",
      "Gradient Descent(6591/9999): loss=158.58452310560776, w0=0.021261610601293543, w1=0.21960529526404687\n",
      "Gradient Descent(6592/9999): loss=158.55516975524975, w0=0.02125993116316393, w1=0.2195831548043105\n",
      "Gradient Descent(6593/9999): loss=158.52582314660867, w0=0.021258251885258535, w1=0.21956101643765114\n",
      "Gradient Descent(6594/9999): loss=158.4964832780051, w0=0.02125657276755965, w1=0.2195388801638672\n",
      "Gradient Descent(6595/9999): loss=158.46715014776046, w0=0.021254893810049563, w1=0.2195167459827571\n",
      "Gradient Descent(6596/9999): loss=158.43782375419636, w0=0.02125321501271057, w1=0.21949461389411934\n",
      "Gradient Descent(6597/9999): loss=158.408504095635, w0=0.02125153637552497, w1=0.21947248389775234\n",
      "Gradient Descent(6598/9999): loss=158.37919117039885, w0=0.02124985789847506, w1=0.2194503559934546\n",
      "Gradient Descent(6599/9999): loss=158.349884976811, w0=0.02124817958154314, w1=0.21942823018102467\n",
      "Gradient Descent(6600/9999): loss=158.32058551319497, w0=0.021246501424711513, w1=0.21940610646026107\n",
      "Gradient Descent(6601/9999): loss=158.29129277787447, w0=0.02124482342796249, w1=0.21938398483096233\n",
      "Gradient Descent(6602/9999): loss=158.26200676917395, w0=0.02124314559127838, w1=0.219361865292927\n",
      "Gradient Descent(6603/9999): loss=158.2327274854182, w0=0.021241467914641488, w1=0.2193397478459537\n",
      "Gradient Descent(6604/9999): loss=158.2034549249324, w0=0.021239790398034128, w1=0.219317632489841\n",
      "Gradient Descent(6605/9999): loss=158.17418908604225, w0=0.021238113041438616, w1=0.21929551922438756\n",
      "Gradient Descent(6606/9999): loss=158.1449299670738, w0=0.021236435844837268, w1=0.21927340804939197\n",
      "Gradient Descent(6607/9999): loss=158.1156775663536, w0=0.021234758808212403, w1=0.21925129896465292\n",
      "Gradient Descent(6608/9999): loss=158.0864318822087, w0=0.021233081931546344, w1=0.21922919196996907\n",
      "Gradient Descent(6609/9999): loss=158.05719291296643, w0=0.021231405214821416, w1=0.21920708706513914\n",
      "Gradient Descent(6610/9999): loss=158.02796065695475, w0=0.02122972865801994, w1=0.2191849842499618\n",
      "Gradient Descent(6611/9999): loss=157.99873511250186, w0=0.02122805226112425, w1=0.21916288352423582\n",
      "Gradient Descent(6612/9999): loss=157.96951627793663, w0=0.021226376024116676, w1=0.21914078488775993\n",
      "Gradient Descent(6613/9999): loss=157.94030415158818, w0=0.021224699946979547, w1=0.2191186883403329\n",
      "Gradient Descent(6614/9999): loss=157.91109873178607, w0=0.0212230240296952, w1=0.21909659388175354\n",
      "Gradient Descent(6615/9999): loss=157.88190001686047, w0=0.021221348272245977, w1=0.21907450151182062\n",
      "Gradient Descent(6616/9999): loss=157.85270800514184, w0=0.021219672674614207, w1=0.21905241123033298\n",
      "Gradient Descent(6617/9999): loss=157.82352269496116, w0=0.021217997236782242, w1=0.21903032303708947\n",
      "Gradient Descent(6618/9999): loss=157.79434408464974, w0=0.021216321958732423, w1=0.21900823693188892\n",
      "Gradient Descent(6619/9999): loss=157.76517217253942, w0=0.021214646840447094, w1=0.21898615291453025\n",
      "Gradient Descent(6620/9999): loss=157.7360069569626, w0=0.021212971881908606, w1=0.21896407098481233\n",
      "Gradient Descent(6621/9999): loss=157.70684843625176, w0=0.02121129708309931, w1=0.21894199114253407\n",
      "Gradient Descent(6622/9999): loss=157.6776966087402, w0=0.021209622444001555, w1=0.21891991338749442\n",
      "Gradient Descent(6623/9999): loss=157.64855147276145, w0=0.021207947964597698, w1=0.2188978377194923\n",
      "Gradient Descent(6624/9999): loss=157.6194130266495, w0=0.021206273644870103, w1=0.2188757641383267\n",
      "Gradient Descent(6625/9999): loss=157.59028126873886, w0=0.021204599484801124, w1=0.21885369264379664\n",
      "Gradient Descent(6626/9999): loss=157.5611561973644, w0=0.021202925484373122, w1=0.2188316232357011\n",
      "Gradient Descent(6627/9999): loss=157.53203781086142, w0=0.021201251643568467, w1=0.21880955591383908\n",
      "Gradient Descent(6628/9999): loss=157.5029261075658, w0=0.021199577962369518, w1=0.21878749067800965\n",
      "Gradient Descent(6629/9999): loss=157.47382108581365, w0=0.02119790444075865, w1=0.21876542752801187\n",
      "Gradient Descent(6630/9999): loss=157.44472274394164, w0=0.021196231078718226, w1=0.2187433664636448\n",
      "Gradient Descent(6631/9999): loss=157.41563108028683, w0=0.021194557876230626, w1=0.21872130748470758\n",
      "Gradient Descent(6632/9999): loss=157.3865460931868, w0=0.021192884833278223, w1=0.21869925059099932\n",
      "Gradient Descent(6633/9999): loss=157.35746778097953, w0=0.021191211949843396, w1=0.21867719578231912\n",
      "Gradient Descent(6634/9999): loss=157.32839614200336, w0=0.021189539225908525, w1=0.21865514305846614\n",
      "Gradient Descent(6635/9999): loss=157.2993311745971, w0=0.02118786666145599, w1=0.2186330924192396\n",
      "Gradient Descent(6636/9999): loss=157.27027287710007, w0=0.02118619425646818, w1=0.21861104386443864\n",
      "Gradient Descent(6637/9999): loss=157.24122124785202, w0=0.021184522010927475, w1=0.21858899739386248\n",
      "Gradient Descent(6638/9999): loss=157.21217628519307, w0=0.02118284992481627, w1=0.21856695300731036\n",
      "Gradient Descent(6639/9999): loss=157.18313798746374, w0=0.021181177998116953, w1=0.2185449107045815\n",
      "Gradient Descent(6640/9999): loss=157.1541063530051, w0=0.02117950623081192, w1=0.21852287048547522\n",
      "Gradient Descent(6641/9999): loss=157.12508138015863, w0=0.021177834622883564, w1=0.21850083234979076\n",
      "Gradient Descent(6642/9999): loss=157.09606306726627, w0=0.021176163174314283, w1=0.21847879629732742\n",
      "Gradient Descent(6643/9999): loss=157.0670514126702, w0=0.02117449188508648, w1=0.21845676232788452\n",
      "Gradient Descent(6644/9999): loss=157.03804641471334, w0=0.021172820755182554, w1=0.2184347304412614\n",
      "Gradient Descent(6645/9999): loss=157.00904807173876, w0=0.02117114978458491, w1=0.21841270063725743\n",
      "Gradient Descent(6646/9999): loss=156.98005638209025, w0=0.02116947897327596, w1=0.21839067291567196\n",
      "Gradient Descent(6647/9999): loss=156.95107134411177, w0=0.021167808321238102, w1=0.2183686472763044\n",
      "Gradient Descent(6648/9999): loss=156.92209295614788, w0=0.021166137828453755, w1=0.21834662371895414\n",
      "Gradient Descent(6649/9999): loss=156.89312121654353, w0=0.02116446749490533, w1=0.21832460224342062\n",
      "Gradient Descent(6650/9999): loss=156.86415612364408, w0=0.021162797320575247, w1=0.2183025828495033\n",
      "Gradient Descent(6651/9999): loss=156.83519767579537, w0=0.02116112730544592, w1=0.21828056553700162\n",
      "Gradient Descent(6652/9999): loss=156.80624587134363, w0=0.02115945744949977, w1=0.2182585503057151\n",
      "Gradient Descent(6653/9999): loss=156.77730070863558, w0=0.02115778775271922, w1=0.2182365371554432\n",
      "Gradient Descent(6654/9999): loss=156.74836218601834, w0=0.021156118215086697, w1=0.21821452608598546\n",
      "Gradient Descent(6655/9999): loss=156.71943030183945, w0=0.021154448836584624, w1=0.21819251709714144\n",
      "Gradient Descent(6656/9999): loss=156.69050505444687, w0=0.02115277961719543, w1=0.21817051018871067\n",
      "Gradient Descent(6657/9999): loss=156.66158644218913, w0=0.02115111055690155, w1=0.21814850536049274\n",
      "Gradient Descent(6658/9999): loss=156.632674463415, w0=0.021149441655685414, w1=0.21812650261228725\n",
      "Gradient Descent(6659/9999): loss=156.6037691164738, w0=0.02114777291352946, w1=0.2181045019438938\n",
      "Gradient Descent(6660/9999): loss=156.5748703997154, w0=0.021146104330416124, w1=0.21808250335511203\n",
      "Gradient Descent(6661/9999): loss=156.5459783114897, w0=0.02114443590632785, w1=0.21806050684574158\n",
      "Gradient Descent(6662/9999): loss=156.51709285014758, w0=0.021142767641247076, w1=0.21803851241558211\n",
      "Gradient Descent(6663/9999): loss=156.48821401403984, w0=0.02114109953515625, w1=0.2180165200644333\n",
      "Gradient Descent(6664/9999): loss=156.45934180151812, w0=0.021139431588037817, w1=0.2179945297920949\n",
      "Gradient Descent(6665/9999): loss=156.43047621093424, w0=0.021137763799874226, w1=0.2179725415983666\n",
      "Gradient Descent(6666/9999): loss=156.40161724064052, w0=0.02113609617064793, w1=0.21795055548304812\n",
      "Gradient Descent(6667/9999): loss=156.3727648889898, w0=0.021134428700341382, w1=0.21792857144593925\n",
      "Gradient Descent(6668/9999): loss=156.34391915433525, w0=0.02113276138893704, w1=0.21790658948683975\n",
      "Gradient Descent(6669/9999): loss=156.31508003503043, w0=0.02113109423641736, w1=0.21788460960554942\n",
      "Gradient Descent(6670/9999): loss=156.28624752942952, w0=0.021129427242764804, w1=0.2178626318018681\n",
      "Gradient Descent(6671/9999): loss=156.257421635887, w0=0.02112776040796183, w1=0.2178406560755956\n",
      "Gradient Descent(6672/9999): loss=156.2286023527578, w0=0.021126093731990907, w1=0.21781868242653177\n",
      "Gradient Descent(6673/9999): loss=156.1997896783973, w0=0.021124427214834502, w1=0.21779671085447647\n",
      "Gradient Descent(6674/9999): loss=156.1709836111613, w0=0.021122760856475086, w1=0.2177747413592296\n",
      "Gradient Descent(6675/9999): loss=156.14218414940598, w0=0.02112109465689513, w1=0.2177527739405911\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(6676/9999): loss=156.11339129148809, w0=0.0211194286160771, w1=0.21773080859836083\n",
      "Gradient Descent(6677/9999): loss=156.0846050357647, w0=0.02111776273400348, w1=0.21770884533233875\n",
      "Gradient Descent(6678/9999): loss=156.05582538059323, w0=0.021116097010656747, w1=0.21768688414232484\n",
      "Gradient Descent(6679/9999): loss=156.02705232433183, w0=0.02111443144601938, w1=0.21766492502811907\n",
      "Gradient Descent(6680/9999): loss=155.9982858653388, w0=0.021112766040073862, w1=0.21764296798952143\n",
      "Gradient Descent(6681/9999): loss=155.96952600197298, w0=0.02111110079280268, w1=0.21762101302633194\n",
      "Gradient Descent(6682/9999): loss=155.9407727325936, w0=0.021109435704188315, w1=0.21759906013835065\n",
      "Gradient Descent(6683/9999): loss=155.91202605556037, w0=0.021107770774213264, w1=0.2175771093253776\n",
      "Gradient Descent(6684/9999): loss=155.8832859692335, w0=0.021106106002860017, w1=0.21755516058721286\n",
      "Gradient Descent(6685/9999): loss=155.8545524719734, w0=0.021104441390111067, w1=0.21753321392365652\n",
      "Gradient Descent(6686/9999): loss=155.82582556214112, w0=0.021102776935948905, w1=0.21751126933450868\n",
      "Gradient Descent(6687/9999): loss=155.7971052380981, w0=0.021101112640356034, w1=0.21748932681956945\n",
      "Gradient Descent(6688/9999): loss=155.76839149820609, w0=0.021099448503314955, w1=0.21746738637863902\n",
      "Gradient Descent(6689/9999): loss=155.73968434082755, w0=0.021097784524808167, w1=0.21744544801151752\n",
      "Gradient Descent(6690/9999): loss=155.71098376432502, w0=0.02109612070481818, w1=0.21742351171800514\n",
      "Gradient Descent(6691/9999): loss=155.68228976706172, w0=0.0210944570433275, w1=0.21740157749790207\n",
      "Gradient Descent(6692/9999): loss=155.6536023474012, w0=0.021092793540318632, w1=0.2173796453510085\n",
      "Gradient Descent(6693/9999): loss=155.62492150370747, w0=0.02109113019577409, w1=0.21735771527712472\n",
      "Gradient Descent(6694/9999): loss=155.59624723434496, w0=0.021089467009676392, w1=0.21733578727605096\n",
      "Gradient Descent(6695/9999): loss=155.56757953767857, w0=0.021087803982008047, w1=0.21731386134758748\n",
      "Gradient Descent(6696/9999): loss=155.53891841207349, w0=0.021086141112751578, w1=0.2172919374915346\n",
      "Gradient Descent(6697/9999): loss=155.51026385589546, w0=0.021084478401889505, w1=0.2172700157076926\n",
      "Gradient Descent(6698/9999): loss=155.48161586751075, w0=0.02108281584940435, w1=0.21724809599586184\n",
      "Gradient Descent(6699/9999): loss=155.45297444528583, w0=0.02108115345527864, w1=0.21722617835584263\n",
      "Gradient Descent(6700/9999): loss=155.42433958758778, w0=0.021079491219494897, w1=0.21720426278743532\n",
      "Gradient Descent(6701/9999): loss=155.395711292784, w0=0.021077829142035653, w1=0.21718234929044034\n",
      "Gradient Descent(6702/9999): loss=155.36708955924234, w0=0.02107616722288344, w1=0.21716043786465805\n",
      "Gradient Descent(6703/9999): loss=155.33847438533115, w0=0.021074505462020793, w1=0.21713852850988888\n",
      "Gradient Descent(6704/9999): loss=155.30986576941908, w0=0.021072843859430247, w1=0.21711662122593328\n",
      "Gradient Descent(6705/9999): loss=155.28126370987545, w0=0.021071182415094338, w1=0.2170947160125917\n",
      "Gradient Descent(6706/9999): loss=155.25266820506965, w0=0.021069521128995614, w1=0.2170728128696646\n",
      "Gradient Descent(6707/9999): loss=155.22407925337183, w0=0.021067860001116608, w1=0.21705091179695246\n",
      "Gradient Descent(6708/9999): loss=155.19549685315232, w0=0.021066199031439872, w1=0.21702901279425582\n",
      "Gradient Descent(6709/9999): loss=155.16692100278212, w0=0.02106453821994795, w1=0.2170071158613752\n",
      "Gradient Descent(6710/9999): loss=155.13835170063246, w0=0.02106287756662339, w1=0.21698522099811113\n",
      "Gradient Descent(6711/9999): loss=155.10978894507508, w0=0.02106121707144875, w1=0.2169633282042642\n",
      "Gradient Descent(6712/9999): loss=155.08123273448209, w0=0.021059556734406574, w1=0.21694143747963499\n",
      "Gradient Descent(6713/9999): loss=155.05268306722616, w0=0.021057896555479426, w1=0.21691954882402406\n",
      "Gradient Descent(6714/9999): loss=155.0241399416803, w0=0.02105623653464986, w1=0.2168976622372321\n",
      "Gradient Descent(6715/9999): loss=154.99560335621786, w0=0.02105457667190044, w1=0.21687577771905966\n",
      "Gradient Descent(6716/9999): loss=154.9670733092128, w0=0.021052916967213724, w1=0.21685389526930746\n",
      "Gradient Descent(6717/9999): loss=154.93854979903935, w0=0.021051257420572282, w1=0.21683201488777615\n",
      "Gradient Descent(6718/9999): loss=154.9100328240723, w0=0.021049598031958678, w1=0.21681013657426643\n",
      "Gradient Descent(6719/9999): loss=154.8815223826867, w0=0.02104793880135548, w1=0.216788260328579\n",
      "Gradient Descent(6720/9999): loss=154.85301847325826, w0=0.021046279728745263, w1=0.2167663861505146\n",
      "Gradient Descent(6721/9999): loss=154.8245210941629, w0=0.0210446208141106, w1=0.216744514039874\n",
      "Gradient Descent(6722/9999): loss=154.79603024377704, w0=0.021042962057434063, w1=0.2167226439964579\n",
      "Gradient Descent(6723/9999): loss=154.7675459204776, w0=0.021041303458698236, w1=0.21670077602006715\n",
      "Gradient Descent(6724/9999): loss=154.7390681226419, w0=0.021039645017885696, w1=0.21667891011050253\n",
      "Gradient Descent(6725/9999): loss=154.71059684864753, w0=0.021037986734979027, w1=0.21665704626756482\n",
      "Gradient Descent(6726/9999): loss=154.68213209687266, w0=0.021036328609960814, w1=0.2166351844910549\n",
      "Gradient Descent(6727/9999): loss=154.65367386569596, w0=0.021034670642813644, w1=0.2166133247807736\n",
      "Gradient Descent(6728/9999): loss=154.62522215349634, w0=0.021033012833520107, w1=0.21659146713652183\n",
      "Gradient Descent(6729/9999): loss=154.5967769586532, w0=0.02103135518206279, w1=0.21656961155810048\n",
      "Gradient Descent(6730/9999): loss=154.56833827954642, w0=0.02102969768842429, w1=0.21654775804531043\n",
      "Gradient Descent(6731/9999): loss=154.5399061145563, w0=0.021028040352587204, w1=0.21652590659795262\n",
      "Gradient Descent(6732/9999): loss=154.51148046206345, w0=0.021026383174534126, w1=0.21650405721582802\n",
      "Gradient Descent(6733/9999): loss=154.48306132044905, w0=0.02102472615424766, w1=0.21648220989873757\n",
      "Gradient Descent(6734/9999): loss=154.45464868809466, w0=0.02102306929171041, w1=0.21646036464648227\n",
      "Gradient Descent(6735/9999): loss=154.42624256338217, w0=0.02102141258690498, w1=0.2164385214588631\n",
      "Gradient Descent(6736/9999): loss=154.39784294469416, w0=0.021019756039813972, w1=0.21641668033568112\n",
      "Gradient Descent(6737/9999): loss=154.3694498304133, w0=0.021018099650419998, w1=0.21639484127673733\n",
      "Gradient Descent(6738/9999): loss=154.34106321892284, w0=0.02101644341870567, w1=0.21637300428183281\n",
      "Gradient Descent(6739/9999): loss=154.31268310860654, w0=0.0210147873446536, w1=0.21635116935076865\n",
      "Gradient Descent(6740/9999): loss=154.2843094978484, w0=0.021013131428246408, w1=0.2163293364833459\n",
      "Gradient Descent(6741/9999): loss=154.25594238503302, w0=0.02101147566946671, w1=0.2163075056793657\n",
      "Gradient Descent(6742/9999): loss=154.2275817685453, w0=0.021009820068297123, w1=0.21628567693862918\n",
      "Gradient Descent(6743/9999): loss=154.19922764677062, w0=0.02100816462472027, w1=0.21626385026093747\n",
      "Gradient Descent(6744/9999): loss=154.17088001809483, w0=0.02100650933871878, w1=0.21624202564609177\n",
      "Gradient Descent(6745/9999): loss=154.1425388809041, w0=0.021004854210275276, w1=0.21622020309389323\n",
      "Gradient Descent(6746/9999): loss=154.1142042335851, w0=0.02100319923937239, w1=0.21619838260414306\n",
      "Gradient Descent(6747/9999): loss=154.08587607452492, w0=0.021001544425992752, w1=0.2161765641766425\n",
      "Gradient Descent(6748/9999): loss=154.05755440211095, w0=0.020999889770118994, w1=0.2161547478111928\n",
      "Gradient Descent(6749/9999): loss=154.02923921473123, w0=0.020998235271733754, w1=0.21613293350759516\n",
      "Gradient Descent(6750/9999): loss=154.00093051077408, w0=0.02099658093081967, w1=0.2161111212656509\n",
      "Gradient Descent(6751/9999): loss=153.97262828862821, w0=0.02099492674735938, w1=0.21608931108516133\n",
      "Gradient Descent(6752/9999): loss=153.94433254668274, w0=0.020993272721335526, w1=0.21606750296592772\n",
      "Gradient Descent(6753/9999): loss=153.91604328332753, w0=0.020991618852730758, w1=0.21604569690775144\n",
      "Gradient Descent(6754/9999): loss=153.88776049695238, w0=0.020989965141527716, w1=0.2160238929104338\n",
      "Gradient Descent(6755/9999): loss=153.85948418594782, w0=0.02098831158770905, w1=0.2160020909737762\n",
      "Gradient Descent(6756/9999): loss=153.8312143487048, w0=0.020986658191257414, w1=0.21598029109758002\n",
      "Gradient Descent(6757/9999): loss=153.80295098361452, w0=0.02098500495215546, w1=0.21595849328164665\n",
      "Gradient Descent(6758/9999): loss=153.77469408906882, w0=0.020983351870385844, w1=0.21593669752577752\n",
      "Gradient Descent(6759/9999): loss=153.74644366345976, w0=0.020981698945931222, w1=0.21591490382977405\n",
      "Gradient Descent(6760/9999): loss=153.71819970517987, w0=0.020980046178774255, w1=0.2158931121934377\n",
      "Gradient Descent(6761/9999): loss=153.68996221262228, w0=0.020978393568897604, w1=0.21587132261657\n",
      "Gradient Descent(6762/9999): loss=153.66173118418035, w0=0.020976741116283934, w1=0.2158495350989724\n",
      "Gradient Descent(6763/9999): loss=153.63350661824785, w0=0.020975088820915914, w1=0.2158277496404464\n",
      "Gradient Descent(6764/9999): loss=153.60528851321914, w0=0.020973436682776212, w1=0.21580596624079357\n",
      "Gradient Descent(6765/9999): loss=153.57707686748887, w0=0.0209717847018475, w1=0.21578418489981543\n",
      "Gradient Descent(6766/9999): loss=153.5488716794521, w0=0.020970132878112446, w1=0.21576240561731355\n",
      "Gradient Descent(6767/9999): loss=153.52067294750444, w0=0.02096848121155373, w1=0.21574062839308952\n",
      "Gradient Descent(6768/9999): loss=153.49248067004172, w0=0.02096682970215403, w1=0.21571885322694495\n",
      "Gradient Descent(6769/9999): loss=153.46429484546044, w0=0.020965178349896023, w1=0.21569708011868147\n",
      "Gradient Descent(6770/9999): loss=153.4361154721573, w0=0.020963527154762393, w1=0.2156753090681007\n",
      "Gradient Descent(6771/9999): loss=153.40794254852958, w0=0.020961876116735823, w1=0.2156535400750043\n",
      "Gradient Descent(6772/9999): loss=153.3797760729749, w0=0.020960225235798997, w1=0.215631773139194\n",
      "Gradient Descent(6773/9999): loss=153.35161604389123, w0=0.020958574511934606, w1=0.21561000826047141\n",
      "Gradient Descent(6774/9999): loss=153.32346245967722, w0=0.020956923945125345, w1=0.2155882454386383\n",
      "Gradient Descent(6775/9999): loss=153.29531531873153, w0=0.0209552735353539, w1=0.2155664846734964\n",
      "Gradient Descent(6776/9999): loss=153.26717461945364, w0=0.02095362328260297, w1=0.21554472596484744\n",
      "Gradient Descent(6777/9999): loss=153.2390403602433, w0=0.020951973186855252, w1=0.21552296931249318\n",
      "Gradient Descent(6778/9999): loss=153.2109125395006, w0=0.020950323248093445, w1=0.21550121471623543\n",
      "Gradient Descent(6779/9999): loss=153.18279115562618, w0=0.020948673466300253, w1=0.21547946217587596\n",
      "Gradient Descent(6780/9999): loss=153.15467620702094, w0=0.020947023841458377, w1=0.21545771169121664\n",
      "Gradient Descent(6781/9999): loss=153.12656769208638, w0=0.020945374373550527, w1=0.21543596326205927\n",
      "Gradient Descent(6782/9999): loss=153.09846560922435, w0=0.020943725062559405, w1=0.21541421688820572\n",
      "Gradient Descent(6783/9999): loss=153.070369956837, w0=0.020942075908467726, w1=0.2153924725694579\n",
      "Gradient Descent(6784/9999): loss=153.04228073332715, w0=0.020940426911258202, w1=0.21537073030561765\n",
      "Gradient Descent(6785/9999): loss=153.01419793709783, w0=0.020938778070913548, w1=0.21534899009648692\n",
      "Gradient Descent(6786/9999): loss=152.98612156655253, w0=0.02093712938741648, w1=0.21532725194186764\n",
      "Gradient Descent(6787/9999): loss=152.95805162009523, w0=0.02093548086074972, w1=0.21530551584156174\n",
      "Gradient Descent(6788/9999): loss=152.92998809613027, w0=0.02093383249089599, w1=0.2152837817953712\n",
      "Gradient Descent(6789/9999): loss=152.90193099306245, w0=0.020932184277838013, w1=0.21526204980309802\n",
      "Gradient Descent(6790/9999): loss=152.87388030929696, w0=0.02093053622155851, w1=0.21524031986454417\n",
      "Gradient Descent(6791/9999): loss=152.8458360432394, w0=0.020928888322040217, w1=0.2152185919795117\n",
      "Gradient Descent(6792/9999): loss=152.81779819329577, w0=0.02092724057926586, w1=0.21519686614780265\n",
      "Gradient Descent(6793/9999): loss=152.78976675787257, w0=0.020925592993218174, w1=0.21517514236921906\n",
      "Gradient Descent(6794/9999): loss=152.76174173537666, w0=0.02092394556387989, w1=0.21515342064356302\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(6795/9999): loss=152.73372312421535, w0=0.020922298291233744, w1=0.21513170097063664\n",
      "Gradient Descent(6796/9999): loss=152.7057109227963, w0=0.02092065117526248, w1=0.215109983350242\n",
      "Gradient Descent(6797/9999): loss=152.67770512952762, w0=0.02091900421594884, w1=0.21508826778218126\n",
      "Gradient Descent(6798/9999): loss=152.6497057428179, w0=0.020917357413275564, w1=0.21506655426625657\n",
      "Gradient Descent(6799/9999): loss=152.6217127610761, w0=0.0209157107672254, w1=0.21504484280227007\n",
      "Gradient Descent(6800/9999): loss=152.59372618271163, w0=0.020914064277781093, w1=0.21502313339002396\n",
      "Gradient Descent(6801/9999): loss=152.56574600613413, w0=0.020912417944925394, w1=0.21500142602932046\n",
      "Gradient Descent(6802/9999): loss=152.5377722297541, w0=0.02091077176864106, w1=0.21497972071996177\n",
      "Gradient Descent(6803/9999): loss=152.50980485198193, w0=0.020909125748910837, w1=0.21495801746175014\n",
      "Gradient Descent(6804/9999): loss=152.48184387122865, w0=0.02090747988571749, w1=0.21493631625448784\n",
      "Gradient Descent(6805/9999): loss=152.4538892859059, w0=0.02090583417904377, w1=0.21491461709797713\n",
      "Gradient Descent(6806/9999): loss=152.42594109442547, w0=0.020904188628872448, w1=0.21489291999202031\n",
      "Gradient Descent(6807/9999): loss=152.39799929519972, w0=0.02090254323518628, w1=0.2148712249364197\n",
      "Gradient Descent(6808/9999): loss=152.37006388664125, w0=0.020900897997968033, w1=0.2148495319309776\n",
      "Gradient Descent(6809/9999): loss=152.34213486716334, w0=0.020899252917200475, w1=0.21482784097549643\n",
      "Gradient Descent(6810/9999): loss=152.31421223517938, w0=0.020897607992866375, w1=0.2148061520697785\n",
      "Gradient Descent(6811/9999): loss=152.2862959891035, w0=0.020895963224948508, w1=0.21478446521362618\n",
      "Gradient Descent(6812/9999): loss=152.25838612734998, w0=0.020894318613429646, w1=0.21476278040684194\n",
      "Gradient Descent(6813/9999): loss=152.2304826483337, w0=0.020892674158292563, w1=0.21474109764922814\n",
      "Gradient Descent(6814/9999): loss=152.20258555046976, w0=0.02089102985952004, w1=0.21471941694058724\n",
      "Gradient Descent(6815/9999): loss=152.17469483217394, w0=0.02088938571709486, w1=0.21469773828072172\n",
      "Gradient Descent(6816/9999): loss=152.14681049186217, w0=0.020887741730999804, w1=0.21467606166943404\n",
      "Gradient Descent(6817/9999): loss=152.11893252795096, w0=0.02088609790121766, w1=0.2146543871065267\n",
      "Gradient Descent(6818/9999): loss=152.09106093885725, w0=0.02088445422773121, w1=0.21463271459180222\n",
      "Gradient Descent(6819/9999): loss=152.06319572299822, w0=0.020882810710523246, w1=0.2146110441250631\n",
      "Gradient Descent(6820/9999): loss=152.0353368787917, w0=0.02088116734957656, w1=0.21458937570611192\n",
      "Gradient Descent(6821/9999): loss=152.00748440465574, w0=0.020879524144873946, w1=0.21456770933475122\n",
      "Gradient Descent(6822/9999): loss=151.97963829900888, w0=0.0208778810963982, w1=0.21454604501078361\n",
      "Gradient Descent(6823/9999): loss=151.95179856027016, w0=0.02087623820413212, w1=0.2145243827340117\n",
      "Gradient Descent(6824/9999): loss=151.92396518685888, w0=0.020874595468058504, w1=0.21450272250423807\n",
      "Gradient Descent(6825/9999): loss=151.89613817719484, w0=0.02087295288816016, w1=0.2144810643212654\n",
      "Gradient Descent(6826/9999): loss=151.86831752969823, w0=0.020871310464419892, w1=0.21445940818489634\n",
      "Gradient Descent(6827/9999): loss=151.84050324278977, w0=0.020869668196820505, w1=0.21443775409493357\n",
      "Gradient Descent(6828/9999): loss=151.81269531489036, w0=0.02086802608534481, w1=0.21441610205117978\n",
      "Gradient Descent(6829/9999): loss=151.78489374442154, w0=0.020866384129975612, w1=0.21439445205343768\n",
      "Gradient Descent(6830/9999): loss=151.75709852980515, w0=0.020864742330695733, w1=0.21437280410151\n",
      "Gradient Descent(6831/9999): loss=151.72930966946345, w0=0.020863100687487986, w1=0.21435115819519948\n",
      "Gradient Descent(6832/9999): loss=151.70152716181912, w0=0.02086145920033519, w1=0.2143295143343089\n",
      "Gradient Descent(6833/9999): loss=151.6737510052953, w0=0.02085981786922016, w1=0.21430787251864106\n",
      "Gradient Descent(6834/9999): loss=151.64598119831552, w0=0.020858176694125725, w1=0.2142862327479987\n",
      "Gradient Descent(6835/9999): loss=151.61821773930365, w0=0.020856535675034704, w1=0.21426459502218473\n",
      "Gradient Descent(6836/9999): loss=151.59046062668406, w0=0.02085489481192993, w1=0.21424295934100193\n",
      "Gradient Descent(6837/9999): loss=151.56270985888156, w0=0.020853254104794224, w1=0.21422132570425317\n",
      "Gradient Descent(6838/9999): loss=151.5349654343213, w0=0.020851613553610423, w1=0.21419969411174133\n",
      "Gradient Descent(6839/9999): loss=151.50722735142884, w0=0.02084997315836136, w1=0.2141780645632693\n",
      "Gradient Descent(6840/9999): loss=151.4794956086302, w0=0.020848332919029867, w1=0.21415643705864\n",
      "Gradient Descent(6841/9999): loss=151.45177020435182, w0=0.020846692835598785, w1=0.21413481159765635\n",
      "Gradient Descent(6842/9999): loss=151.42405113702048, w0=0.020845052908050953, w1=0.21411318818012132\n",
      "Gradient Descent(6843/9999): loss=151.3963384050635, w0=0.020843413136369213, w1=0.21409156680583785\n",
      "Gradient Descent(6844/9999): loss=151.36863200690837, w0=0.020841773520536407, w1=0.21406994747460895\n",
      "Gradient Descent(6845/9999): loss=151.34093194098332, w0=0.020840134060535382, w1=0.2140483301862376\n",
      "Gradient Descent(6846/9999): loss=151.31323820571671, w0=0.02083849475634899, w1=0.2140267149405268\n",
      "Gradient Descent(6847/9999): loss=151.2855507995376, w0=0.02083685560796008, w1=0.21400510173727963\n",
      "Gradient Descent(6848/9999): loss=151.25786972087516, w0=0.020835216615351503, w1=0.21398349057629915\n",
      "Gradient Descent(6849/9999): loss=151.23019496815914, w0=0.020833577778506116, w1=0.2139618814573884\n",
      "Gradient Descent(6850/9999): loss=151.20252653981962, w0=0.020831939097406778, w1=0.2139402743803505\n",
      "Gradient Descent(6851/9999): loss=151.1748644342872, w0=0.020830300572036345, w1=0.21391866934498854\n",
      "Gradient Descent(6852/9999): loss=151.14720864999282, w0=0.020828662202377682, w1=0.21389706635110567\n",
      "Gradient Descent(6853/9999): loss=151.11955918536785, w0=0.02082702398841365, w1=0.21387546539850505\n",
      "Gradient Descent(6854/9999): loss=151.09191603884403, w0=0.020825385930127113, w1=0.2138538664869898\n",
      "Gradient Descent(6855/9999): loss=151.0642792088536, w0=0.020823748027500942, w1=0.21383226961636315\n",
      "Gradient Descent(6856/9999): loss=151.03664869382908, w0=0.02082211028051801, w1=0.21381067478642826\n",
      "Gradient Descent(6857/9999): loss=151.0090244922036, w0=0.020820472689161186, w1=0.2137890819969884\n",
      "Gradient Descent(6858/9999): loss=150.98140660241046, w0=0.020818835253413347, w1=0.21376749124784677\n",
      "Gradient Descent(6859/9999): loss=150.95379502288358, w0=0.020817197973257366, w1=0.21374590253880665\n",
      "Gradient Descent(6860/9999): loss=150.92618975205718, w0=0.020815560848676126, w1=0.2137243158696713\n",
      "Gradient Descent(6861/9999): loss=150.8985907883659, w0=0.020813923879652505, w1=0.213702731240244\n",
      "Gradient Descent(6862/9999): loss=150.87099813024477, w0=0.02081228706616939, w1=0.21368114865032808\n",
      "Gradient Descent(6863/9999): loss=150.84341177612933, w0=0.020810650408209664, w1=0.21365956809972686\n",
      "Gradient Descent(6864/9999): loss=150.81583172445545, w0=0.020809013905756218, w1=0.2136379895882437\n",
      "Gradient Descent(6865/9999): loss=150.78825797365943, w0=0.020807377558791938, w1=0.21361641311568194\n",
      "Gradient Descent(6866/9999): loss=150.76069052217795, w0=0.020805741367299718, w1=0.21359483868184498\n",
      "Gradient Descent(6867/9999): loss=150.73312936844812, w0=0.02080410533126245, w1=0.21357326628653622\n",
      "Gradient Descent(6868/9999): loss=150.70557451090758, w0=0.020802469450663035, w1=0.21355169592955908\n",
      "Gradient Descent(6869/9999): loss=150.67802594799414, w0=0.020800833725484368, w1=0.213530127610717\n",
      "Gradient Descent(6870/9999): loss=150.65048367814617, w0=0.02079919815570935, w1=0.2135085613298134\n",
      "Gradient Descent(6871/9999): loss=150.62294769980244, w0=0.020797562741320885, w1=0.21348699708665184\n",
      "Gradient Descent(6872/9999): loss=150.59541801140216, w0=0.020795927482301878, w1=0.21346543488103573\n",
      "Gradient Descent(6873/9999): loss=150.56789461138487, w0=0.020794292378635237, w1=0.2134438747127686\n",
      "Gradient Descent(6874/9999): loss=150.54037749819057, w0=0.020792657430303868, w1=0.21342231658165398\n",
      "Gradient Descent(6875/9999): loss=150.5128666702596, w0=0.020791022637290688, w1=0.21340076048749543\n",
      "Gradient Descent(6876/9999): loss=150.48536212603287, w0=0.020789387999578607, w1=0.2133792064300965\n",
      "Gradient Descent(6877/9999): loss=150.45786386395147, w0=0.020787753517150544, w1=0.21335765440926077\n",
      "Gradient Descent(6878/9999): loss=150.43037188245717, w0=0.020786119189989412, w1=0.21333610442479184\n",
      "Gradient Descent(6879/9999): loss=150.4028861799919, w0=0.020784485018078135, w1=0.21331455647649333\n",
      "Gradient Descent(6880/9999): loss=150.37540675499807, w0=0.020782851001399635, w1=0.21329301056416888\n",
      "Gradient Descent(6881/9999): loss=150.34793360591863, w0=0.020781217139936838, w1=0.21327146668762215\n",
      "Gradient Descent(6882/9999): loss=150.3204667311968, w0=0.020779583433672668, w1=0.2132499248466568\n",
      "Gradient Descent(6883/9999): loss=150.2930061292762, w0=0.020777949882590057, w1=0.21322838504107655\n",
      "Gradient Descent(6884/9999): loss=150.26555179860094, w0=0.020776316486671935, w1=0.2132068472706851\n",
      "Gradient Descent(6885/9999): loss=150.23810373761552, w0=0.02077468324590124, w1=0.21318531153528614\n",
      "Gradient Descent(6886/9999): loss=150.2106619447648, w0=0.0207730501602609, w1=0.21316377783468343\n",
      "Gradient Descent(6887/9999): loss=150.18322641849412, w0=0.020771417229733857, w1=0.21314224616868074\n",
      "Gradient Descent(6888/9999): loss=150.15579715724917, w0=0.02076978445430305, w1=0.21312071653708187\n",
      "Gradient Descent(6889/9999): loss=150.12837415947604, w0=0.02076815183395142, w1=0.21309918893969057\n",
      "Gradient Descent(6890/9999): loss=150.10095742362125, w0=0.020766519368661914, w1=0.2130776633763107\n",
      "Gradient Descent(6891/9999): loss=150.07354694813182, w0=0.020764887058417475, w1=0.2130561398467461\n",
      "Gradient Descent(6892/9999): loss=150.04614273145498, w0=0.020763254903201054, w1=0.2130346183508006\n",
      "Gradient Descent(6893/9999): loss=150.01874477203847, w0=0.0207616229029956, w1=0.21301309888827805\n",
      "Gradient Descent(6894/9999): loss=149.99135306833048, w0=0.020759991057784064, w1=0.21299158145898237\n",
      "Gradient Descent(6895/9999): loss=149.9639676187796, w0=0.020758359367549408, w1=0.21297006606271746\n",
      "Gradient Descent(6896/9999): loss=149.9365884218348, w0=0.020756727832274585, w1=0.21294855269928725\n",
      "Gradient Descent(6897/9999): loss=149.9092154759454, w0=0.020755096451942553, w1=0.2129270413684957\n",
      "Gradient Descent(6898/9999): loss=149.88184877956124, w0=0.020753465226536278, w1=0.21290553207014673\n",
      "Gradient Descent(6899/9999): loss=149.85448833113244, w0=0.020751834156038717, w1=0.21288402480404434\n",
      "Gradient Descent(6900/9999): loss=149.82713412910962, w0=0.02075020324043284, w1=0.21286251956999253\n",
      "Gradient Descent(6901/9999): loss=149.79978617194377, w0=0.02074857247970161, w1=0.21284101636779532\n",
      "Gradient Descent(6902/9999): loss=149.77244445808634, w0=0.020746941873828008, w1=0.21281951519725675\n",
      "Gradient Descent(6903/9999): loss=149.7451089859891, w0=0.020745311422795, w1=0.21279801605818086\n",
      "Gradient Descent(6904/9999): loss=149.7177797541043, w0=0.020743681126585558, w1=0.21277651895037172\n",
      "Gradient Descent(6905/9999): loss=149.6904567608845, w0=0.020742050985182663, w1=0.21275502387363343\n",
      "Gradient Descent(6906/9999): loss=149.66314000478275, w0=0.020740420998569292, w1=0.21273353082777008\n",
      "Gradient Descent(6907/9999): loss=149.63582948425258, w0=0.020738791166728426, w1=0.2127120398125858\n",
      "Gradient Descent(6908/9999): loss=149.60852519774772, w0=0.02073716148964305, w1=0.2126905508278847\n",
      "Gradient Descent(6909/9999): loss=149.58122714372243, w0=0.020735531967296147, w1=0.212669063873471\n",
      "Gradient Descent(6910/9999): loss=149.5539353206314, w0=0.020733902599670707, w1=0.21264757894914885\n",
      "Gradient Descent(6911/9999): loss=149.52664972692972, w0=0.02073227338674972, w1=0.21262609605472244\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(6912/9999): loss=149.49937036107275, w0=0.020730644328516176, w1=0.212604615189996\n",
      "Gradient Descent(6913/9999): loss=149.47209722151646, w0=0.02072901542495307, w1=0.21258313635477372\n",
      "Gradient Descent(6914/9999): loss=149.4448303067171, w0=0.020727386676043395, w1=0.2125616595488599\n",
      "Gradient Descent(6915/9999): loss=149.41756961513127, w0=0.020725758081770154, w1=0.21254018477205877\n",
      "Gradient Descent(6916/9999): loss=149.3903151452161, w0=0.020724129642116344, w1=0.21251871202417466\n",
      "Gradient Descent(6917/9999): loss=149.36306689542914, w0=0.02072250135706497, w1=0.21249724130501185\n",
      "Gradient Descent(6918/9999): loss=149.3358248642282, w0=0.020720873226599032, w1=0.21247577261437467\n",
      "Gradient Descent(6919/9999): loss=149.3085890500716, w0=0.020719245250701544, w1=0.21245430595206746\n",
      "Gradient Descent(6920/9999): loss=149.28135945141804, w0=0.02071761742935551, w1=0.21243284131789456\n",
      "Gradient Descent(6921/9999): loss=149.2541360667266, w0=0.020715989762543945, w1=0.21241137871166035\n",
      "Gradient Descent(6922/9999): loss=149.2269188944569, w0=0.02071436225024986, w1=0.21238991813316926\n",
      "Gradient Descent(6923/9999): loss=149.19970793306868, w0=0.020712734892456272, w1=0.21236845958222567\n",
      "Gradient Descent(6924/9999): loss=149.17250318102242, w0=0.020711107689146198, w1=0.21234700305863402\n",
      "Gradient Descent(6925/9999): loss=149.14530463677875, w0=0.020709480640302657, w1=0.21232554856219876\n",
      "Gradient Descent(6926/9999): loss=149.1181122987987, w0=0.020707853745908673, w1=0.21230409609272435\n",
      "Gradient Descent(6927/9999): loss=149.090926165544, w0=0.02070622700594727, w1=0.21228264565001528\n",
      "Gradient Descent(6928/9999): loss=149.0637462354765, w0=0.020704600420401474, w1=0.21226119723387604\n",
      "Gradient Descent(6929/9999): loss=149.03657250705848, w0=0.020702973989254313, w1=0.21223975084411117\n",
      "Gradient Descent(6930/9999): loss=149.0094049787527, w0=0.02070134771248882, w1=0.2122183064805252\n",
      "Gradient Descent(6931/9999): loss=148.9822436490224, w0=0.020699721590088024, w1=0.2121968641429227\n",
      "Gradient Descent(6932/9999): loss=148.955088516331, w0=0.020698095622034965, w1=0.2121754238311082\n",
      "Gradient Descent(6933/9999): loss=148.92793957914245, w0=0.020696469808312676, w1=0.21215398554488638\n",
      "Gradient Descent(6934/9999): loss=148.90079683592114, w0=0.020694844148904195, w1=0.21213254928406178\n",
      "Gradient Descent(6935/9999): loss=148.87366028513182, w0=0.020693218643792567, w1=0.21211111504843905\n",
      "Gradient Descent(6936/9999): loss=148.84652992523965, w0=0.020691593292960837, w1=0.2120896828378228\n",
      "Gradient Descent(6937/9999): loss=148.81940575471017, w0=0.020689968096392047, w1=0.21206825265201776\n",
      "Gradient Descent(6938/9999): loss=148.7922877720094, w0=0.020688343054069247, w1=0.21204682449082857\n",
      "Gradient Descent(6939/9999): loss=148.7651759756035, w0=0.020686718165975487, w1=0.21202539835405995\n",
      "Gradient Descent(6940/9999): loss=148.73807036395948, w0=0.020685093432093817, w1=0.2120039742415166\n",
      "Gradient Descent(6941/9999): loss=148.71097093554437, w0=0.020683468852407298, w1=0.21198255215300327\n",
      "Gradient Descent(6942/9999): loss=148.68387768882585, w0=0.02068184442689898, w1=0.2119611320883247\n",
      "Gradient Descent(6943/9999): loss=148.65679062227176, w0=0.02068022015555192, w1=0.2119397140472857\n",
      "Gradient Descent(6944/9999): loss=148.62970973435046, w0=0.02067859603834919, w1=0.21191829802969103\n",
      "Gradient Descent(6945/9999): loss=148.60263502353087, w0=0.02067697207527384, w1=0.2118968840353455\n",
      "Gradient Descent(6946/9999): loss=148.57556648828194, w0=0.020675348266308947, w1=0.21187547206405397\n",
      "Gradient Descent(6947/9999): loss=148.54850412707347, w0=0.020673724611437568, w1=0.21185406211562124\n",
      "Gradient Descent(6948/9999): loss=148.52144793837536, w0=0.020672101110642778, w1=0.2118326541898522\n",
      "Gradient Descent(6949/9999): loss=148.49439792065795, w0=0.020670477763907645, w1=0.21181124828655173\n",
      "Gradient Descent(6950/9999): loss=148.46735407239203, w0=0.020668854571215246, w1=0.21178984440552473\n",
      "Gradient Descent(6951/9999): loss=148.44031639204874, w0=0.020667231532548656, w1=0.2117684425465761\n",
      "Gradient Descent(6952/9999): loss=148.41328487809977, w0=0.020665608647890955, w1=0.21174704270951078\n",
      "Gradient Descent(6953/9999): loss=148.38625952901702, w0=0.020663985917225222, w1=0.21172564489413373\n",
      "Gradient Descent(6954/9999): loss=148.35924034327283, w0=0.02066236334053454, w1=0.21170424910024993\n",
      "Gradient Descent(6955/9999): loss=148.3322273193401, w0=0.02066074091780199, w1=0.21168285532766437\n",
      "Gradient Descent(6956/9999): loss=148.30522045569188, w0=0.020659118649010665, w1=0.21166146357618204\n",
      "Gradient Descent(6957/9999): loss=148.27821975080187, w0=0.020657496534143647, w1=0.21164007384560798\n",
      "Gradient Descent(6958/9999): loss=148.251225203144, w0=0.02065587457318403, w1=0.21161868613574722\n",
      "Gradient Descent(6959/9999): loss=148.2242368111926, w0=0.020654252766114906, w1=0.21159730044640482\n",
      "Gradient Descent(6960/9999): loss=148.19725457342253, w0=0.020652631112919373, w1=0.21157591677738588\n",
      "Gradient Descent(6961/9999): loss=148.17027848830898, w0=0.020651009613580528, w1=0.21155453512849548\n",
      "Gradient Descent(6962/9999): loss=148.14330855432746, w0=0.020649388268081468, w1=0.21153315549953874\n",
      "Gradient Descent(6963/9999): loss=148.11634476995397, w0=0.020647767076405296, w1=0.21151177789032077\n",
      "Gradient Descent(6964/9999): loss=148.0893871336649, w0=0.020646146038535117, w1=0.21149040230064675\n",
      "Gradient Descent(6965/9999): loss=148.062435643937, w0=0.020644525154454037, w1=0.21146902873032186\n",
      "Gradient Descent(6966/9999): loss=148.0354902992475, w0=0.020642904424145164, w1=0.21144765717915126\n",
      "Gradient Descent(6967/9999): loss=148.00855109807392, w0=0.020641283847591608, w1=0.21142628764694016\n",
      "Gradient Descent(6968/9999): loss=147.98161803889434, w0=0.02063966342477648, w1=0.2114049201334938\n",
      "Gradient Descent(6969/9999): loss=147.954691120187, w0=0.0206380431556829, w1=0.21138355463861738\n",
      "Gradient Descent(6970/9999): loss=147.92777034043075, w0=0.020636423040293977, w1=0.21136219116211621\n",
      "Gradient Descent(6971/9999): loss=147.9008556981048, w0=0.020634803078592837, w1=0.21134082970379553\n",
      "Gradient Descent(6972/9999): loss=147.87394719168864, w0=0.020633183270562596, w1=0.21131947026346065\n",
      "Gradient Descent(6973/9999): loss=147.84704481966227, w0=0.02063156361618638, w1=0.2112981128409169\n",
      "Gradient Descent(6974/9999): loss=147.82014858050604, w0=0.02062994411544731, w1=0.21127675743596958\n",
      "Gradient Descent(6975/9999): loss=147.79325847270076, w0=0.02062832476832852, w1=0.21125540404842405\n",
      "Gradient Descent(6976/9999): loss=147.76637449472753, w0=0.020626705574813133, w1=0.21123405267808568\n",
      "Gradient Descent(6977/9999): loss=147.739496645068, w0=0.020625086534884287, w1=0.21121270332475986\n",
      "Gradient Descent(6978/9999): loss=147.71262492220401, w0=0.02062346764852511, w1=0.21119135598825198\n",
      "Gradient Descent(6979/9999): loss=147.685759324618, w0=0.020621848915718746, w1=0.21117001066836746\n",
      "Gradient Descent(6980/9999): loss=147.65889985079272, w0=0.020620230336448327, w1=0.21114866736491175\n",
      "Gradient Descent(6981/9999): loss=147.63204649921133, w0=0.020618611910696992, w1=0.2111273260776903\n",
      "Gradient Descent(6982/9999): loss=147.60519926835738, w0=0.02061699363844789, w1=0.2111059868065086\n",
      "Gradient Descent(6983/9999): loss=147.57835815671478, w0=0.02061537551968416, w1=0.21108464955117212\n",
      "Gradient Descent(6984/9999): loss=147.5515231627679, w0=0.020613757554388948, w1=0.2110633143114864\n",
      "Gradient Descent(6985/9999): loss=147.52469428500146, w0=0.020612139742545405, w1=0.21104198108725694\n",
      "Gradient Descent(6986/9999): loss=147.49787152190063, w0=0.020610522084136683, w1=0.2110206498782893\n",
      "Gradient Descent(6987/9999): loss=147.47105487195097, w0=0.020608904579145937, w1=0.21099932068438904\n",
      "Gradient Descent(6988/9999): loss=147.44424433363832, w0=0.02060728722755632, w1=0.21097799350536173\n",
      "Gradient Descent(6989/9999): loss=147.4174399054491, w0=0.02060567002935099, w1=0.21095666834101298\n",
      "Gradient Descent(6990/9999): loss=147.39064158587001, w0=0.020604052984513104, w1=0.21093534519114843\n",
      "Gradient Descent(6991/9999): loss=147.36384937338815, w0=0.02060243609302583, w1=0.2109140240555737\n",
      "Gradient Descent(6992/9999): loss=147.33706326649101, w0=0.020600819354872325, w1=0.21089270493409443\n",
      "Gradient Descent(6993/9999): loss=147.3102832636666, w0=0.020599202770035758, w1=0.21087138782651632\n",
      "Gradient Descent(6994/9999): loss=147.2835093634032, w0=0.020597586338499296, w1=0.21085007273264503\n",
      "Gradient Descent(6995/9999): loss=147.25674156418944, w0=0.020595970060246112, w1=0.21082875965228629\n",
      "Gradient Descent(6996/9999): loss=147.22997986451455, w0=0.020594353935259376, w1=0.2108074485852458\n",
      "Gradient Descent(6997/9999): loss=147.20322426286788, w0=0.020592737963522265, w1=0.21078613953132935\n",
      "Gradient Descent(6998/9999): loss=147.17647475773944, w0=0.020591122145017952, w1=0.21076483249034267\n",
      "Gradient Descent(6999/9999): loss=147.14973134761945, w0=0.020589506479729618, w1=0.21074352746209155\n",
      "Gradient Descent(7000/9999): loss=147.12299403099868, w0=0.020587890967640445, w1=0.21072222444638178\n",
      "Gradient Descent(7001/9999): loss=147.09626280636812, w0=0.020586275608733613, w1=0.2107009234430192\n",
      "Gradient Descent(7002/9999): loss=147.0695376722193, w0=0.02058466040299231, w1=0.2106796244518096\n",
      "Gradient Descent(7003/9999): loss=147.04281862704406, w0=0.020583045350399724, w1=0.21065832747255886\n",
      "Gradient Descent(7004/9999): loss=147.01610566933473, w0=0.02058143045093904, w1=0.21063703250507285\n",
      "Gradient Descent(7005/9999): loss=146.98939879758387, w0=0.020579815704593458, w1=0.21061573954915744\n",
      "Gradient Descent(7006/9999): loss=146.9626980102846, w0=0.020578201111346162, w1=0.21059444860461857\n",
      "Gradient Descent(7007/9999): loss=146.9360033059304, w0=0.020576586671180355, w1=0.21057315967126214\n",
      "Gradient Descent(7008/9999): loss=146.90931468301505, w0=0.02057497238407923, w1=0.2105518727488941\n",
      "Gradient Descent(7009/9999): loss=146.88263214003283, w0=0.020573358250025993, w1=0.21053058783732037\n",
      "Gradient Descent(7010/9999): loss=146.8559556754783, w0=0.020571744269003843, w1=0.21050930493634698\n",
      "Gradient Descent(7011/9999): loss=146.82928528784663, w0=0.020570130440995982, w1=0.2104880240457799\n",
      "Gradient Descent(7012/9999): loss=146.80262097563315, w0=0.02056851676598562, w1=0.21046674516542516\n",
      "Gradient Descent(7013/9999): loss=146.77596273733369, w0=0.020566903243955968, w1=0.21044546829508878\n",
      "Gradient Descent(7014/9999): loss=146.74931057144443, w0=0.020565289874890234, w1=0.2104241934345768\n",
      "Gradient Descent(7015/9999): loss=146.72266447646203, w0=0.02056367665877163, w1=0.2104029205836953\n",
      "Gradient Descent(7016/9999): loss=146.6960244508834, w0=0.020562063595583373, w1=0.21038164974225038\n",
      "Gradient Descent(7017/9999): loss=146.66939049320612, w0=0.02056045068530868, w1=0.2103603809100481\n",
      "Gradient Descent(7018/9999): loss=146.64276260192776, w0=0.020558837927930767, w1=0.21033911408689462\n",
      "Gradient Descent(7019/9999): loss=146.61614077554663, w0=0.02055722532343286, w1=0.21031784927259606\n",
      "Gradient Descent(7020/9999): loss=146.58952501256127, w0=0.02055561287179818, w1=0.21029658646695856\n",
      "Gradient Descent(7021/9999): loss=146.5629153114706, w0=0.020554000573009955, w1=0.21027532566978835\n",
      "Gradient Descent(7022/9999): loss=146.536311670774, w0=0.02055238842705141, w1=0.21025406688089157\n",
      "Gradient Descent(7023/9999): loss=146.50971408897132, w0=0.02055077643390578, w1=0.21023281010007444\n",
      "Gradient Descent(7024/9999): loss=146.48312256456256, w0=0.020549164593556293, w1=0.2102115553271432\n",
      "Gradient Descent(7025/9999): loss=146.45653709604835, w0=0.020547552905986182, w1=0.2101903025619041\n",
      "Gradient Descent(7026/9999): loss=146.42995768192964, w0=0.020545941371178687, w1=0.2101690518041634\n",
      "Gradient Descent(7027/9999): loss=146.4033843207076, w0=0.020544329989117045, w1=0.21014780305372738\n",
      "Gradient Descent(7028/9999): loss=146.37681701088414, w0=0.020542718759784497, w1=0.21012655631040233\n",
      "Gradient Descent(7029/9999): loss=146.35025575096122, w0=0.020541107683164285, w1=0.2101053115739946\n",
      "Gradient Descent(7030/9999): loss=146.32370053944146, w0=0.020539496759239655, w1=0.21008406884431047\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(7031/9999): loss=146.29715137482765, w0=0.020537885987993852, w1=0.21006282812115634\n",
      "Gradient Descent(7032/9999): loss=146.2706082556231, w0=0.020536275369410127, w1=0.21004158940433856\n",
      "Gradient Descent(7033/9999): loss=146.24407118033147, w0=0.020534664903471733, w1=0.21002035269366354\n",
      "Gradient Descent(7034/9999): loss=146.2175401474569, w0=0.02053305459016192, w1=0.20999911798893767\n",
      "Gradient Descent(7035/9999): loss=146.1910151555038, w0=0.02053144442946395, w1=0.2099778852899674\n",
      "Gradient Descent(7036/9999): loss=146.164496202977, w0=0.02052983442136107, w1=0.20995665459655913\n",
      "Gradient Descent(7037/9999): loss=146.1379832883818, w0=0.02052822456583655, w1=0.20993542590851935\n",
      "Gradient Descent(7038/9999): loss=146.11147641022382, w0=0.020526614862873646, w1=0.20991419922565455\n",
      "Gradient Descent(7039/9999): loss=146.084975567009, w0=0.020525005312455626, w1=0.2098929745477712\n",
      "Gradient Descent(7040/9999): loss=146.05848075724387, w0=0.020523395914565756, w1=0.20987175187467583\n",
      "Gradient Descent(7041/9999): loss=146.03199197943516, w0=0.0205217866691873, w1=0.20985053120617497\n",
      "Gradient Descent(7042/9999): loss=146.0055092320901, w0=0.02052017757630353, w1=0.20982931254207515\n",
      "Gradient Descent(7043/9999): loss=145.9790325137163, w0=0.02051856863589772, w1=0.20980809588218297\n",
      "Gradient Descent(7044/9999): loss=145.9525618228217, w0=0.020516959847953146, w1=0.209786881226305\n",
      "Gradient Descent(7045/9999): loss=145.9260971579147, w0=0.020515351212453084, w1=0.20976566857424786\n",
      "Gradient Descent(7046/9999): loss=145.899638517504, w0=0.02051374272938081, w1=0.20974445792581817\n",
      "Gradient Descent(7047/9999): loss=145.87318590009883, w0=0.020512134398719607, w1=0.20972324928082256\n",
      "Gradient Descent(7048/9999): loss=145.8467393042087, w0=0.020510526220452758, w1=0.2097020426390677\n",
      "Gradient Descent(7049/9999): loss=145.82029872834357, w0=0.02050891819456355, w1=0.20968083800036022\n",
      "Gradient Descent(7050/9999): loss=145.7938641710137, w0=0.02050731032103527, w1=0.20965963536450688\n",
      "Gradient Descent(7051/9999): loss=145.76743563072984, w0=0.020505702599851204, w1=0.20963843473131435\n",
      "Gradient Descent(7052/9999): loss=145.74101310600298, w0=0.02050409503099465, w1=0.20961723610058938\n",
      "Gradient Descent(7053/9999): loss=145.71459659534483, w0=0.020502487614448898, w1=0.2095960394721387\n",
      "Gradient Descent(7054/9999): loss=145.68818609726713, w0=0.020500880350197242, w1=0.20957484484576908\n",
      "Gradient Descent(7055/9999): loss=145.66178161028216, w0=0.020499273238222985, w1=0.2095536522212873\n",
      "Gradient Descent(7056/9999): loss=145.6353831329026, w0=0.02049766627850942, w1=0.20953246159850017\n",
      "Gradient Descent(7057/9999): loss=145.60899066364144, w0=0.020496059471039857, w1=0.20951127297721453\n",
      "Gradient Descent(7058/9999): loss=145.58260420101226, w0=0.020494452815797596, w1=0.20949008635723718\n",
      "Gradient Descent(7059/9999): loss=145.5562237435288, w0=0.020492846312765944, w1=0.20946890173837499\n",
      "Gradient Descent(7060/9999): loss=145.5298492897052, w0=0.02049123996192821, w1=0.20944771912043483\n",
      "Gradient Descent(7061/9999): loss=145.50348083805622, w0=0.020489633763267703, w1=0.2094265385032236\n",
      "Gradient Descent(7062/9999): loss=145.4771183870967, w0=0.02048802771676774, w1=0.2094053598865482\n",
      "Gradient Descent(7063/9999): loss=145.45076193534214, w0=0.020486421822411637, w1=0.20938418327021557\n",
      "Gradient Descent(7064/9999): loss=145.42441148130825, w0=0.020484816080182704, w1=0.20936300865403265\n",
      "Gradient Descent(7065/9999): loss=145.39806702351123, w0=0.020483210490064265, w1=0.20934183603780637\n",
      "Gradient Descent(7066/9999): loss=145.37172856046757, w0=0.020481605052039643, w1=0.20932066542134375\n",
      "Gradient Descent(7067/9999): loss=145.34539609069432, w0=0.020479999766092157, w1=0.20929949680445178\n",
      "Gradient Descent(7068/9999): loss=145.3190696127087, w0=0.020478394632205136, w1=0.20927833018693745\n",
      "Gradient Descent(7069/9999): loss=145.29274912502845, w0=0.020476789650361905, w1=0.20925716556860782\n",
      "Gradient Descent(7070/9999): loss=145.26643462617164, w0=0.0204751848205458, w1=0.20923600294926994\n",
      "Gradient Descent(7071/9999): loss=145.24012611465682, w0=0.020473580142740144, w1=0.20921484232873086\n",
      "Gradient Descent(7072/9999): loss=145.21382358900283, w0=0.02047197561692828, w1=0.2091936837067977\n",
      "Gradient Descent(7073/9999): loss=145.18752704772893, w0=0.020470371243093536, w1=0.20917252708327752\n",
      "Gradient Descent(7074/9999): loss=145.16123648935476, w0=0.020468767021219254, w1=0.20915137245797746\n",
      "Gradient Descent(7075/9999): loss=145.13495191240042, w0=0.020467162951288774, w1=0.20913021983070468\n",
      "Gradient Descent(7076/9999): loss=145.10867331538628, w0=0.02046555903328544, w1=0.20910906920126632\n",
      "Gradient Descent(7077/9999): loss=145.08240069683316, w0=0.020463955267192597, w1=0.20908792056946957\n",
      "Gradient Descent(7078/9999): loss=145.05613405526228, w0=0.020462351652993586, w1=0.20906677393512163\n",
      "Gradient Descent(7079/9999): loss=145.02987338919522, w0=0.02046074819067176, w1=0.2090456292980297\n",
      "Gradient Descent(7080/9999): loss=145.0036186971539, w0=0.020459144880210473, w1=0.209024486658001\n",
      "Gradient Descent(7081/9999): loss=144.9773699776608, w0=0.020457541721593073, w1=0.2090033460148428\n",
      "Gradient Descent(7082/9999): loss=144.95112722923858, w0=0.020455938714802915, w1=0.20898220736836234\n",
      "Gradient Descent(7083/9999): loss=144.9248904504104, w0=0.02045433585982336, w1=0.20896107071836692\n",
      "Gradient Descent(7084/9999): loss=144.89865963969973, w0=0.020452733156637766, w1=0.20893993606466385\n",
      "Gradient Descent(7085/9999): loss=144.8724347956305, w0=0.020451130605229496, w1=0.20891880340706043\n",
      "Gradient Descent(7086/9999): loss=144.8462159167271, w0=0.020449528205581914, w1=0.208897672745364\n",
      "Gradient Descent(7087/9999): loss=144.8200030015141, w0=0.020447925957678383, w1=0.20887654407938194\n",
      "Gradient Descent(7088/9999): loss=144.79379604851655, w0=0.02044632386150227, w1=0.2088554174089216\n",
      "Gradient Descent(7089/9999): loss=144.76759505626, w0=0.02044472191703695, w1=0.20883429273379037\n",
      "Gradient Descent(7090/9999): loss=144.74140002327022, w0=0.020443120124265787, w1=0.20881317005379565\n",
      "Gradient Descent(7091/9999): loss=144.71521094807343, w0=0.02044151848317216, w1=0.2087920493687449\n",
      "Gradient Descent(7092/9999): loss=144.68902782919628, w0=0.020439916993739447, w1=0.20877093067844554\n",
      "Gradient Descent(7093/9999): loss=144.66285066516573, w0=0.020438315655951025, w1=0.20874981398270503\n",
      "Gradient Descent(7094/9999): loss=144.63667945450922, w0=0.020436714469790274, w1=0.20872869928133087\n",
      "Gradient Descent(7095/9999): loss=144.61051419575438, w0=0.020435113435240577, w1=0.20870758657413055\n",
      "Gradient Descent(7096/9999): loss=144.58435488742944, w0=0.02043351255228532, w1=0.20868647586091157\n",
      "Gradient Descent(7097/9999): loss=144.5582015280629, w0=0.020431911820907887, w1=0.20866536714148148\n",
      "Gradient Descent(7098/9999): loss=144.53205411618382, w0=0.02043031124109167, w1=0.20864426041564782\n",
      "Gradient Descent(7099/9999): loss=144.5059126503214, w0=0.02042871081282006, w1=0.20862315568321815\n",
      "Gradient Descent(7100/9999): loss=144.47977712900527, w0=0.02042711053607645, w1=0.20860205294400008\n",
      "Gradient Descent(7101/9999): loss=144.45364755076557, w0=0.02042551041084423, w1=0.2085809521978012\n",
      "Gradient Descent(7102/9999): loss=144.42752391413273, w0=0.020423910437106802, w1=0.20855985344442915\n",
      "Gradient Descent(7103/9999): loss=144.40140621763766, w0=0.020422310614847568, w1=0.20853875668369157\n",
      "Gradient Descent(7104/9999): loss=144.37529445981153, w0=0.020420710944049923, w1=0.20851766191539609\n",
      "Gradient Descent(7105/9999): loss=144.34918863918594, w0=0.020419111424697278, w1=0.2084965691393504\n",
      "Gradient Descent(7106/9999): loss=144.3230887542929, w0=0.020417512056773033, w1=0.20847547835536223\n",
      "Gradient Descent(7107/9999): loss=144.29699480366483, w0=0.0204159128402606, w1=0.20845438956323925\n",
      "Gradient Descent(7108/9999): loss=144.27090678583448, w0=0.020414313775143383, w1=0.20843330276278918\n",
      "Gradient Descent(7109/9999): loss=144.2448246993349, w0=0.0204127148614048, w1=0.20841221795381978\n",
      "Gradient Descent(7110/9999): loss=144.21874854269973, w0=0.02041111609902826, w1=0.20839113513613886\n",
      "Gradient Descent(7111/9999): loss=144.19267831446288, w0=0.020409517487997186, w1=0.20837005430955413\n",
      "Gradient Descent(7112/9999): loss=144.16661401315855, w0=0.020407919028294995, w1=0.20834897547387343\n",
      "Gradient Descent(7113/9999): loss=144.14055563732154, w0=0.020406320719905103, w1=0.20832789862890458\n",
      "Gradient Descent(7114/9999): loss=144.11450318548688, w0=0.020404722562810937, w1=0.20830682377445542\n",
      "Gradient Descent(7115/9999): loss=144.08845665619, w0=0.02040312455699592, w1=0.2082857509103338\n",
      "Gradient Descent(7116/9999): loss=144.06241604796668, w0=0.020401526702443478, w1=0.20826468003634757\n",
      "Gradient Descent(7117/9999): loss=144.0363813593532, w0=0.02039992899913704, w1=0.20824361115230466\n",
      "Gradient Descent(7118/9999): loss=144.01035258888615, w0=0.02039833144706004, w1=0.20822254425801295\n",
      "Gradient Descent(7119/9999): loss=143.98432973510248, w0=0.02039673404619591, w1=0.20820147935328037\n",
      "Gradient Descent(7120/9999): loss=143.95831279653962, w0=0.020395136796528084, w1=0.20818041643791488\n",
      "Gradient Descent(7121/9999): loss=143.93230177173527, w0=0.02039353969804, w1=0.2081593555117244\n",
      "Gradient Descent(7122/9999): loss=143.90629665922754, w0=0.020391942750715097, w1=0.20813829657451693\n",
      "Gradient Descent(7123/9999): loss=143.88029745755495, w0=0.02039034595453682, w1=0.2081172396261005\n",
      "Gradient Descent(7124/9999): loss=143.8543041652564, w0=0.020388749309488606, w1=0.20809618466628307\n",
      "Gradient Descent(7125/9999): loss=143.82831678087115, w0=0.020387152815553904, w1=0.2080751316948727\n",
      "Gradient Descent(7126/9999): loss=143.80233530293884, w0=0.020385556472716164, w1=0.20805408071167747\n",
      "Gradient Descent(7127/9999): loss=143.77635972999954, w0=0.020383960280958832, w1=0.2080330317165054\n",
      "Gradient Descent(7128/9999): loss=143.7503900605937, w0=0.020382364240265363, w1=0.2080119847091646\n",
      "Gradient Descent(7129/9999): loss=143.724426293262, w0=0.02038076835061921, w1=0.20799093968946317\n",
      "Gradient Descent(7130/9999): loss=143.6984684265458, w0=0.020379172612003834, w1=0.20796989665720922\n",
      "Gradient Descent(7131/9999): loss=143.6725164589865, w0=0.020377577024402686, w1=0.2079488556122109\n",
      "Gradient Descent(7132/9999): loss=143.64657038912617, w0=0.020375981587799227, w1=0.2079278165542764\n",
      "Gradient Descent(7133/9999): loss=143.62063021550705, w0=0.020374386302176927, w1=0.20790677948321382\n",
      "Gradient Descent(7134/9999): loss=143.59469593667188, w0=0.020372791167519243, w1=0.2078857443988314\n",
      "Gradient Descent(7135/9999): loss=143.56876755116375, w0=0.020371196183809646, w1=0.20786471130093734\n",
      "Gradient Descent(7136/9999): loss=143.5428450575261, w0=0.0203696013510316, w1=0.20784368018933988\n",
      "Gradient Descent(7137/9999): loss=143.51692845430279, w0=0.020368006669168583, w1=0.20782265106384726\n",
      "Gradient Descent(7138/9999): loss=143.49101774003813, w0=0.02036641213820406, w1=0.20780162392426774\n",
      "Gradient Descent(7139/9999): loss=143.4651129132766, w0=0.020364817758121514, w1=0.2077805987704096\n",
      "Gradient Descent(7140/9999): loss=143.43921397256335, w0=0.020363223528904417, w1=0.20775957560208116\n",
      "Gradient Descent(7141/9999): loss=143.41332091644355, w0=0.02036162945053625, w1=0.20773855441909073\n",
      "Gradient Descent(7142/9999): loss=143.38743374346313, w0=0.020360035523000496, w1=0.2077175352212466\n",
      "Gradient Descent(7143/9999): loss=143.3615524521682, w0=0.020358441746280636, w1=0.2076965180083572\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(7144/9999): loss=143.33567704110513, w0=0.020356848120360156, w1=0.20767550278023084\n",
      "Gradient Descent(7145/9999): loss=143.309807508821, w0=0.020355254645222545, w1=0.20765448953667592\n",
      "Gradient Descent(7146/9999): loss=143.28394385386295, w0=0.020353661320851292, w1=0.20763347827750087\n",
      "Gradient Descent(7147/9999): loss=143.25808607477876, w0=0.02035206814722989, w1=0.2076124690025141\n",
      "Gradient Descent(7148/9999): loss=143.23223417011636, w0=0.020350475124341832, w1=0.20759146171152404\n",
      "Gradient Descent(7149/9999): loss=143.20638813842416, w0=0.020348882252170613, w1=0.20757045640433916\n",
      "Gradient Descent(7150/9999): loss=143.18054797825096, w0=0.02034728953069973, w1=0.20754945308076794\n",
      "Gradient Descent(7151/9999): loss=143.15471368814602, w0=0.02034569695991269, w1=0.20752845174061887\n",
      "Gradient Descent(7152/9999): loss=143.12888526665878, w0=0.02034410453979299, w1=0.20750745238370047\n",
      "Gradient Descent(7153/9999): loss=143.10306271233924, w0=0.020342512270324135, w1=0.20748645500982127\n",
      "Gradient Descent(7154/9999): loss=143.0772460237377, w0=0.020340920151489632, w1=0.20746545961878982\n",
      "Gradient Descent(7155/9999): loss=143.05143519940478, w0=0.02033932818327299, w1=0.20744446621041465\n",
      "Gradient Descent(7156/9999): loss=143.02563023789162, w0=0.02033773636565772, w1=0.2074234747845044\n",
      "Gradient Descent(7157/9999): loss=142.99983113774965, w0=0.020336144698627334, w1=0.20740248534086764\n",
      "Gradient Descent(7158/9999): loss=142.97403789753068, w0=0.020334553182165347, w1=0.20738149787931298\n",
      "Gradient Descent(7159/9999): loss=142.94825051578692, w0=0.02033296181625528, w1=0.2073605123996491\n",
      "Gradient Descent(7160/9999): loss=142.92246899107096, w0=0.020331370600880645, w1=0.2073395289016846\n",
      "Gradient Descent(7161/9999): loss=142.89669332193574, w0=0.020329779536024967, w1=0.2073185473852282\n",
      "Gradient Descent(7162/9999): loss=142.87092350693467, w0=0.020328188621671767, w1=0.20729756785008857\n",
      "Gradient Descent(7163/9999): loss=142.8451595446213, w0=0.020326597857804572, w1=0.20727659029607443\n",
      "Gradient Descent(7164/9999): loss=142.81940143354993, w0=0.02032500724440691, w1=0.20725561472299447\n",
      "Gradient Descent(7165/9999): loss=142.79364917227485, w0=0.02032341678146231, w1=0.20723464113065748\n",
      "Gradient Descent(7166/9999): loss=142.76790275935105, w0=0.020321826468954304, w1=0.2072136695188722\n",
      "Gradient Descent(7167/9999): loss=142.74216219333366, w0=0.020320236306866427, w1=0.20719269988744743\n",
      "Gradient Descent(7168/9999): loss=142.7164274727784, w0=0.02031864629518221, w1=0.20717173223619192\n",
      "Gradient Descent(7169/9999): loss=142.6906985962411, w0=0.020317056433885195, w1=0.20715076656491452\n",
      "Gradient Descent(7170/9999): loss=142.66497556227822, w0=0.020315466722958923, w1=0.20712980287342406\n",
      "Gradient Descent(7171/9999): loss=142.63925836944645, w0=0.020313877162386935, w1=0.2071088411615294\n",
      "Gradient Descent(7172/9999): loss=142.61354701630296, w0=0.020312287752152772, w1=0.20708788142903936\n",
      "Gradient Descent(7173/9999): loss=142.58784150140525, w0=0.020310698492239983, w1=0.20706692367576288\n",
      "Gradient Descent(7174/9999): loss=142.56214182331107, w0=0.020309109382632116, w1=0.20704596790150884\n",
      "Gradient Descent(7175/9999): loss=142.5364479805788, w0=0.02030752042331272, w1=0.20702501410608617\n",
      "Gradient Descent(7176/9999): loss=142.51075997176696, w0=0.02030593161426535, w1=0.20700406228930382\n",
      "Gradient Descent(7177/9999): loss=142.48507779543462, w0=0.020304342955473557, w1=0.2069831124509707\n",
      "Gradient Descent(7178/9999): loss=142.45940145014117, w0=0.020302754446920902, w1=0.20696216459089584\n",
      "Gradient Descent(7179/9999): loss=142.43373093444626, w0=0.020301166088590943, w1=0.2069412187088882\n",
      "Gradient Descent(7180/9999): loss=142.40806624691015, w0=0.020299577880467237, w1=0.2069202748047568\n",
      "Gradient Descent(7181/9999): loss=142.38240738609323, w0=0.02029798982253335, w1=0.20689933287831067\n",
      "Gradient Descent(7182/9999): loss=142.35675435055643, w0=0.020296401914772846, w1=0.20687839292935883\n",
      "Gradient Descent(7183/9999): loss=142.33110713886106, w0=0.020294814157169293, w1=0.20685745495771038\n",
      "Gradient Descent(7184/9999): loss=142.30546574956864, w0=0.02029322654970626, w1=0.2068365189631744\n",
      "Gradient Descent(7185/9999): loss=142.27983018124124, w0=0.02029163909236732, w1=0.20681558494555996\n",
      "Gradient Descent(7186/9999): loss=142.25420043244122, w0=0.020290051785136043, w1=0.2067946529046762\n",
      "Gradient Descent(7187/9999): loss=142.22857650173142, w0=0.020288464627996005, w1=0.20677372284033224\n",
      "Gradient Descent(7188/9999): loss=142.2029583876749, w0=0.020286877620930784, w1=0.20675279475233724\n",
      "Gradient Descent(7189/9999): loss=142.1773460888352, w0=0.02028529076392396, w1=0.20673186864050036\n",
      "Gradient Descent(7190/9999): loss=142.1517396037762, w0=0.020283704056959114, w1=0.20671094450463082\n",
      "Gradient Descent(7191/9999): loss=142.12613893106217, w0=0.02028211750001983, w1=0.2066900223445378\n",
      "Gradient Descent(7192/9999): loss=142.10054406925772, w0=0.020280531093089697, w1=0.2066691021600305\n",
      "Gradient Descent(7193/9999): loss=142.07495501692782, w0=0.020278944836152297, w1=0.20664818395091822\n",
      "Gradient Descent(7194/9999): loss=142.04937177263798, w0=0.020277358729191224, w1=0.20662726771701018\n",
      "Gradient Descent(7195/9999): loss=142.0237943349539, w0=0.02027577277219007, w1=0.20660635345811565\n",
      "Gradient Descent(7196/9999): loss=141.99822270244167, w0=0.020274186965132425, w1=0.20658544117404393\n",
      "Gradient Descent(7197/9999): loss=141.9726568736679, w0=0.02027260130800189, w1=0.20656453086460433\n",
      "Gradient Descent(7198/9999): loss=141.9470968471994, w0=0.020271015800782062, w1=0.2065436225296062\n",
      "Gradient Descent(7199/9999): loss=141.9215426216034, w0=0.02026943044345654, w1=0.20652271616885887\n",
      "Gradient Descent(7200/9999): loss=141.8959941954476, w0=0.020267845236008927, w1=0.2065018117821717\n",
      "Gradient Descent(7201/9999): loss=141.87045156730008, w0=0.02026626017842283, w1=0.20648090936935407\n",
      "Gradient Descent(7202/9999): loss=141.84491473572902, w0=0.020264675270681855, w1=0.2064600089302154\n",
      "Gradient Descent(7203/9999): loss=141.81938369930336, w0=0.020263090512769606, w1=0.20643911046456512\n",
      "Gradient Descent(7204/9999): loss=141.7938584565922, w0=0.020261505904669696, w1=0.20641821397221263\n",
      "Gradient Descent(7205/9999): loss=141.76833900616495, w0=0.02025992144636574, w1=0.20639731945296738\n",
      "Gradient Descent(7206/9999): loss=141.74282534659156, w0=0.02025833713784135, w1=0.20637642690663888\n",
      "Gradient Descent(7207/9999): loss=141.71731747644228, w0=0.02025675297908015, w1=0.20635553633303658\n",
      "Gradient Descent(7208/9999): loss=141.6918153942877, w0=0.02025516897006575, w1=0.20633464773197\n",
      "Gradient Descent(7209/9999): loss=141.6663190986988, w0=0.020253585110781777, w1=0.20631376110324867\n",
      "Gradient Descent(7210/9999): loss=141.64082858824708, w0=0.020252001401211854, w1=0.20629287644668215\n",
      "Gradient Descent(7211/9999): loss=141.61534386150413, w0=0.020250417841339602, w1=0.20627199376207997\n",
      "Gradient Descent(7212/9999): loss=141.58986491704223, w0=0.020248834431148652, w1=0.20625111304925173\n",
      "Gradient Descent(7213/9999): loss=141.56439175343365, w0=0.02024725117062263, w1=0.206230234308007\n",
      "Gradient Descent(7214/9999): loss=141.5389243692515, w0=0.020245668059745175, w1=0.2062093575381554\n",
      "Gradient Descent(7215/9999): loss=141.51346276306882, w0=0.020244085098499915, w1=0.20618848273950657\n",
      "Gradient Descent(7216/9999): loss=141.48800693345936, w0=0.020242502286870485, w1=0.20616760991187016\n",
      "Gradient Descent(7217/9999): loss=141.462556878997, w0=0.020240919624840523, w1=0.20614673905505582\n",
      "Gradient Descent(7218/9999): loss=141.43711259825614, w0=0.02023933711239367, w1=0.20612587016887324\n",
      "Gradient Descent(7219/9999): loss=141.41167408981147, w0=0.02023775474951357, w1=0.20610500325313214\n",
      "Gradient Descent(7220/9999): loss=141.3862413522382, w0=0.02023617253618386, w1=0.20608413830764222\n",
      "Gradient Descent(7221/9999): loss=141.36081438411162, w0=0.020234590472388195, w1=0.20606327533221322\n",
      "Gradient Descent(7222/9999): loss=141.33539318400778, w0=0.020233008558110215, w1=0.2060424143266549\n",
      "Gradient Descent(7223/9999): loss=141.30997775050278, w0=0.020231426793333576, w1=0.20602155529077704\n",
      "Gradient Descent(7224/9999): loss=141.28456808217317, w0=0.020229845178041925, w1=0.2060006982243894\n",
      "Gradient Descent(7225/9999): loss=141.25916417759592, w0=0.020228263712218923, w1=0.2059798431273018\n",
      "Gradient Descent(7226/9999): loss=141.23376603534845, w0=0.02022668239584822, w1=0.20595898999932408\n",
      "Gradient Descent(7227/9999): loss=141.20837365400843, w0=0.02022510122891348, w1=0.20593813884026607\n",
      "Gradient Descent(7228/9999): loss=141.1829870321539, w0=0.020223520211398362, w1=0.20591728964993763\n",
      "Gradient Descent(7229/9999): loss=141.15760616836334, w0=0.020221939343286525, w1=0.20589644242814864\n",
      "Gradient Descent(7230/9999): loss=141.1322310612155, w0=0.020220358624561636, w1=0.205875597174709\n",
      "Gradient Descent(7231/9999): loss=141.10686170928963, w0=0.020218778055207363, w1=0.20585475388942862\n",
      "Gradient Descent(7232/9999): loss=141.08149811116527, w0=0.020217197635207372, w1=0.20583391257211742\n",
      "Gradient Descent(7233/9999): loss=141.05614026542236, w0=0.020215617364545337, w1=0.20581307322258538\n",
      "Gradient Descent(7234/9999): loss=141.0307881706412, w0=0.02021403724320493, w1=0.20579223584064243\n",
      "Gradient Descent(7235/9999): loss=141.0054418254024, w0=0.020212457271169822, w1=0.2057714004260986\n",
      "Gradient Descent(7236/9999): loss=140.9801012282871, w0=0.020210877448423696, w1=0.20575056697876384\n",
      "Gradient Descent(7237/9999): loss=140.9547663778767, w0=0.020209297774950227, w1=0.2057297354984482\n",
      "Gradient Descent(7238/9999): loss=140.9294372727529, w0=0.020207718250733097, w1=0.20570890598496172\n",
      "Gradient Descent(7239/9999): loss=140.90411391149792, w0=0.020206138875755994, w1=0.20568807843811446\n",
      "Gradient Descent(7240/9999): loss=140.87879629269423, w0=0.020204559650002597, w1=0.20566725285771648\n",
      "Gradient Descent(7241/9999): loss=140.85348441492476, w0=0.020202980573456597, w1=0.20564642924357784\n",
      "Gradient Descent(7242/9999): loss=140.8281782767728, w0=0.020201401646101683, w1=0.20562560759550869\n",
      "Gradient Descent(7243/9999): loss=140.80287787682198, w0=0.020199822867921546, w1=0.20560478791331915\n",
      "Gradient Descent(7244/9999): loss=140.77758321365624, w0=0.02019824423889988, w1=0.20558397019681937\n",
      "Gradient Descent(7245/9999): loss=140.75229428586, w0=0.02019666575902038, w1=0.2055631544458195\n",
      "Gradient Descent(7246/9999): loss=140.72701109201796, w0=0.020195087428266745, w1=0.2055423406601297\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(7247/9999): loss=140.70173363071527, w0=0.020193509246622672, w1=0.20552152883956018\n",
      "Gradient Descent(7248/9999): loss=140.67646190053748, w0=0.020191931214071868, w1=0.20550071898392117\n",
      "Gradient Descent(7249/9999): loss=140.6511959000703, w0=0.020190353330598033, w1=0.20547991109302288\n",
      "Gradient Descent(7250/9999): loss=140.62593562790002, w0=0.02018877559618487, w1=0.2054591051666756\n",
      "Gradient Descent(7251/9999): loss=140.60068108261322, w0=0.020187198010816094, w1=0.20543830120468953\n",
      "Gradient Descent(7252/9999): loss=140.57543226279688, w0=0.02018562057447541, w1=0.20541749920687502\n",
      "Gradient Descent(7253/9999): loss=140.5501891670383, w0=0.02018404328714653, w1=0.20539669917304235\n",
      "Gradient Descent(7254/9999): loss=140.5249517939252, w0=0.020182466148813173, w1=0.20537590110300183\n",
      "Gradient Descent(7255/9999): loss=140.4997201420456, w0=0.020180889159459054, w1=0.2053551049965638\n",
      "Gradient Descent(7256/9999): loss=140.474494209988, w0=0.020179312319067888, w1=0.2053343108535386\n",
      "Gradient Descent(7257/9999): loss=140.4492739963411, w0=0.0201777356276234, w1=0.20531351867373662\n",
      "Gradient Descent(7258/9999): loss=140.42405949969418, w0=0.020176159085109308, w1=0.20529272845696828\n",
      "Gradient Descent(7259/9999): loss=140.39885071863674, w0=0.02017458269150934, w1=0.20527194020304393\n",
      "Gradient Descent(7260/9999): loss=140.37364765175866, w0=0.02017300644680722, w1=0.20525115391177404\n",
      "Gradient Descent(7261/9999): loss=140.3484502976502, w0=0.020171430350986678, w1=0.20523036958296903\n",
      "Gradient Descent(7262/9999): loss=140.32325865490208, w0=0.020169854404031445, w1=0.2052095872164394\n",
      "Gradient Descent(7263/9999): loss=140.29807272210527, w0=0.020168278605925252, w1=0.20518880681199558\n",
      "Gradient Descent(7264/9999): loss=140.2728924978511, w0=0.020166702956651836, w1=0.20516802836944809\n",
      "Gradient Descent(7265/9999): loss=140.24771798073138, w0=0.020165127456194934, w1=0.20514725188860744\n",
      "Gradient Descent(7266/9999): loss=140.2225491693382, w0=0.020163552104538282, w1=0.20512647736928416\n",
      "Gradient Descent(7267/9999): loss=140.19738606226406, w0=0.020161976901665626, w1=0.2051057048112888\n",
      "Gradient Descent(7268/9999): loss=140.17222865810177, w0=0.020160401847560707, w1=0.20508493421443197\n",
      "Gradient Descent(7269/9999): loss=140.14707695544456, w0=0.02015882694220727, w1=0.2050641655785242\n",
      "Gradient Descent(7270/9999): loss=140.1219309528861, w0=0.02015725218558906, w1=0.20504339890337608\n",
      "Gradient Descent(7271/9999): loss=140.09679064902016, w0=0.020155677577689823, w1=0.2050226341887983\n",
      "Gradient Descent(7272/9999): loss=140.07165604244122, w0=0.02015410311849332, w1=0.20500187143460144\n",
      "Gradient Descent(7273/9999): loss=140.0465271317439, w0=0.020152528807983298, w1=0.20498111064059618\n",
      "Gradient Descent(7274/9999): loss=140.02140391552322, w0=0.020150954646143512, w1=0.20496035180659317\n",
      "Gradient Descent(7275/9999): loss=139.99628639237463, w0=0.020149380632957726, w1=0.20493959493240313\n",
      "Gradient Descent(7276/9999): loss=139.9711745608939, w0=0.020147806768409694, w1=0.20491884001783675\n",
      "Gradient Descent(7277/9999): loss=139.94606841967723, w0=0.020146233052483177, w1=0.20489808706270476\n",
      "Gradient Descent(7278/9999): loss=139.92096796732108, w0=0.02014465948516194, w1=0.2048773360668179\n",
      "Gradient Descent(7279/9999): loss=139.8958732024224, w0=0.02014308606642975, w1=0.20485658702998694\n",
      "Gradient Descent(7280/9999): loss=139.8707841235783, w0=0.020141512796270376, w1=0.20483583995202265\n",
      "Gradient Descent(7281/9999): loss=139.84570072938655, w0=0.020139939674667585, w1=0.20481509483273583\n",
      "Gradient Descent(7282/9999): loss=139.82062301844502, w0=0.02013836670160515, w1=0.2047943516719373\n",
      "Gradient Descent(7283/9999): loss=139.79555098935217, w0=0.020136793877066843, w1=0.20477361046943787\n",
      "Gradient Descent(7284/9999): loss=139.77048464070663, w0=0.020135221201036443, w1=0.20475287122504843\n",
      "Gradient Descent(7285/9999): loss=139.74542397110747, w0=0.020133648673497727, w1=0.2047321339385798\n",
      "Gradient Descent(7286/9999): loss=139.72036897915416, w0=0.020132076294434476, w1=0.20471139860984292\n",
      "Gradient Descent(7287/9999): loss=139.6953196634465, w0=0.02013050406383047, w1=0.20469066523864865\n",
      "Gradient Descent(7288/9999): loss=139.67027602258472, w0=0.02012893198166949, w1=0.20466993382480791\n",
      "Gradient Descent(7289/9999): loss=139.6452380551693, w0=0.02012736004793533, w1=0.20464920436813167\n",
      "Gradient Descent(7290/9999): loss=139.62020575980117, w0=0.02012578826261177, w1=0.20462847686843086\n",
      "Gradient Descent(7291/9999): loss=139.5951791350816, w0=0.02012421662568261, w1=0.20460775132551645\n",
      "Gradient Descent(7292/9999): loss=139.57015817961215, w0=0.020122645137131637, w1=0.20458702773919943\n",
      "Gradient Descent(7293/9999): loss=139.545142891995, w0=0.020121073796942647, w1=0.20456630610929083\n",
      "Gradient Descent(7294/9999): loss=139.52013327083233, w0=0.020119502605099433, w1=0.20454558643560164\n",
      "Gradient Descent(7295/9999): loss=139.49512931472697, w0=0.020117931561585797, w1=0.20452486871794293\n",
      "Gradient Descent(7296/9999): loss=139.47013102228198, w0=0.020116360666385537, w1=0.20450415295612576\n",
      "Gradient Descent(7297/9999): loss=139.4451383921008, w0=0.02011478991948246, w1=0.2044834391499612\n",
      "Gradient Descent(7298/9999): loss=139.42015142278737, w0=0.020113219320860368, w1=0.20446272729926035\n",
      "Gradient Descent(7299/9999): loss=139.39517011294566, w0=0.020111648870503068, w1=0.20444201740383433\n",
      "Gradient Descent(7300/9999): loss=139.37019446118043, w0=0.02011007856839437, w1=0.20442130946349427\n",
      "Gradient Descent(7301/9999): loss=139.3452244660965, w0=0.02010850841451808, w1=0.2044006034780513\n",
      "Gradient Descent(7302/9999): loss=139.3202601262992, w0=0.020106938408858015, w1=0.2043798994473166\n",
      "Gradient Descent(7303/9999): loss=139.2953014403941, w0=0.02010536855139799, w1=0.20435919737110136\n",
      "Gradient Descent(7304/9999): loss=139.2703484069873, w0=0.020103798842121823, w1=0.2043384972492168\n",
      "Gradient Descent(7305/9999): loss=139.24540102468507, w0=0.02010222928101333, w1=0.2043177990814741\n",
      "Gradient Descent(7306/9999): loss=139.22045929209415, w0=0.020100659868056335, w1=0.20429710286768452\n",
      "Gradient Descent(7307/9999): loss=139.19552320782174, w0=0.02009909060323466, w1=0.2042764086076593\n",
      "Gradient Descent(7308/9999): loss=139.17059277047528, w0=0.020097521486532127, w1=0.20425571630120973\n",
      "Gradient Descent(7309/9999): loss=139.1456679786625, w0=0.02009595251793257, w1=0.20423502594814708\n",
      "Gradient Descent(7310/9999): loss=139.12074883099166, w0=0.02009438369741981, w1=0.20421433754828267\n",
      "Gradient Descent(7311/9999): loss=139.09583532607127, w0=0.020092815024977683, w1=0.20419365110142781\n",
      "Gradient Descent(7312/9999): loss=139.07092746251035, w0=0.020091246500590024, w1=0.20417296660739387\n",
      "Gradient Descent(7313/9999): loss=139.04602523891808, w0=0.020089678124240666, w1=0.20415228406599217\n",
      "Gradient Descent(7314/9999): loss=139.02112865390407, w0=0.020088109895913447, w1=0.20413160347703413\n",
      "Gradient Descent(7315/9999): loss=138.9962377060784, w0=0.020086541815592208, w1=0.20411092484033114\n",
      "Gradient Descent(7316/9999): loss=138.97135239405142, w0=0.02008497388326079, w1=0.20409024815569457\n",
      "Gradient Descent(7317/9999): loss=138.94647271643385, w0=0.020083406098903037, w1=0.20406957342293588\n",
      "Gradient Descent(7318/9999): loss=138.9215986718368, w0=0.02008183846250279, w1=0.20404890064186654\n",
      "Gradient Descent(7319/9999): loss=138.8967302588716, w0=0.020080270974043902, w1=0.20402822981229798\n",
      "Gradient Descent(7320/9999): loss=138.87186747615024, w0=0.02007870363351022, w1=0.2040075609340417\n",
      "Gradient Descent(7321/9999): loss=138.8470103222848, w0=0.020077136440885598, w1=0.20398689400690923\n",
      "Gradient Descent(7322/9999): loss=138.8221587958878, w0=0.02007556939615389, w1=0.20396622903071204\n",
      "Gradient Descent(7323/9999): loss=138.79731289557225, w0=0.020074002499298946, w1=0.20394556600526167\n",
      "Gradient Descent(7324/9999): loss=138.7724726199513, w0=0.02007243575030463, w1=0.20392490493036972\n",
      "Gradient Descent(7325/9999): loss=138.74763796763864, w0=0.0200708691491548, w1=0.2039042458058477\n",
      "Gradient Descent(7326/9999): loss=138.7228089372482, w0=0.020069302695833322, w1=0.20388358863150724\n",
      "Gradient Descent(7327/9999): loss=138.69798552739437, w0=0.020067736390324054, w1=0.20386293340715994\n",
      "Gradient Descent(7328/9999): loss=138.6731677366919, w0=0.020066170232610866, w1=0.2038422801326174\n",
      "Gradient Descent(7329/9999): loss=138.64835556375576, w0=0.020064604222677623, w1=0.20382162880769128\n",
      "Gradient Descent(7330/9999): loss=138.6235490072014, w0=0.020063038360508198, w1=0.20380097943219325\n",
      "Gradient Descent(7331/9999): loss=138.59874806564466, w0=0.02006147264608646, w1=0.20378033200593498\n",
      "Gradient Descent(7332/9999): loss=138.5739527377017, w0=0.020059907079396287, w1=0.20375968652872817\n",
      "Gradient Descent(7333/9999): loss=138.549163021989, w0=0.02005834166042155, w1=0.2037390430003845\n",
      "Gradient Descent(7334/9999): loss=138.52437891712344, w0=0.020056776389146134, w1=0.20371840142071573\n",
      "Gradient Descent(7335/9999): loss=138.49960042172222, w0=0.020055211265553916, w1=0.2036977617895336\n",
      "Gradient Descent(7336/9999): loss=138.474827534403, w0=0.020053646289628777, w1=0.20367712410664987\n",
      "Gradient Descent(7337/9999): loss=138.4500602537837, w0=0.020052081461354603, w1=0.20365648837187633\n",
      "Gradient Descent(7338/9999): loss=138.4252985784827, w0=0.02005051678071528, w1=0.20363585458502476\n",
      "Gradient Descent(7339/9999): loss=138.4005425071186, w0=0.0200489522476947, w1=0.203615222745907\n",
      "Gradient Descent(7340/9999): loss=138.37579203831044, w0=0.02004738786227675, w1=0.20359459285433487\n",
      "Gradient Descent(7341/9999): loss=138.35104717067765, w0=0.020045823624445323, w1=0.20357396491012023\n",
      "Gradient Descent(7342/9999): loss=138.32630790283997, w0=0.02004425953418431, w1=0.20355333891307495\n",
      "Gradient Descent(7343/9999): loss=138.30157423341754, w0=0.020042695591477613, w1=0.20353271486301092\n",
      "Gradient Descent(7344/9999): loss=138.2768461610308, w0=0.02004113179630913, w1=0.20351209275974003\n",
      "Gradient Descent(7345/9999): loss=138.2521236843006, w0=0.020039568148662763, w1=0.2034914726030742\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(7346/9999): loss=138.22740680184822, w0=0.02003800464852241, w1=0.20347085439282542\n",
      "Gradient Descent(7347/9999): loss=138.20269551229507, w0=0.020036441295871982, w1=0.20345023812880558\n",
      "Gradient Descent(7348/9999): loss=138.17798981426313, w0=0.02003487809069538, w1=0.20342962381082672\n",
      "Gradient Descent(7349/9999): loss=138.15328970637472, w0=0.020033315032976517, w1=0.20340901143870077\n",
      "Gradient Descent(7350/9999): loss=138.1285951872525, w0=0.020031752122699302, w1=0.2033884010122398\n",
      "Gradient Descent(7351/9999): loss=138.10390625551935, w0=0.020030189359847645, w1=0.20336779253125578\n",
      "Gradient Descent(7352/9999): loss=138.07922290979866, w0=0.020028626744405465, w1=0.2033471859955608\n",
      "Gradient Descent(7353/9999): loss=138.05454514871417, w0=0.020027064276356678, w1=0.20332658140496687\n",
      "Gradient Descent(7354/9999): loss=138.0298729708899, w0=0.020025501955685202, w1=0.20330597875928613\n",
      "Gradient Descent(7355/9999): loss=138.00520637495035, w0=0.02002393978237496, w1=0.20328537805833066\n",
      "Gradient Descent(7356/9999): loss=137.98054535952025, w0=0.020022377756409877, w1=0.20326477930191258\n",
      "Gradient Descent(7357/9999): loss=137.95588992322485, w0=0.020020815877773872, w1=0.20324418248984402\n",
      "Gradient Descent(7358/9999): loss=137.9312400646895, w0=0.02001925414645088, w1=0.20322358762193712\n",
      "Gradient Descent(7359/9999): loss=137.90659578254014, w0=0.020017692562424826, w1=0.20320299469800404\n",
      "Gradient Descent(7360/9999): loss=137.881957075403, w0=0.020016131125679636, w1=0.20318240371785698\n",
      "Gradient Descent(7361/9999): loss=137.85732394190464, w0=0.02001456983619925, w1=0.20316181468130814\n",
      "Gradient Descent(7362/9999): loss=137.83269638067196, w0=0.020013008693967604, w1=0.20314122758816974\n",
      "Gradient Descent(7363/9999): loss=137.8080743903324, w0=0.02001144769896863, w1=0.20312064243825403\n",
      "Gradient Descent(7364/9999): loss=137.78345796951342, w0=0.020009886851186272, w1=0.20310005923137325\n",
      "Gradient Descent(7365/9999): loss=137.75884711684319, w0=0.02000832615060447, w1=0.2030794779673397\n",
      "Gradient Descent(7366/9999): loss=137.73424183095, w0=0.020006765597207165, w1=0.20305889864596563\n",
      "Gradient Descent(7367/9999): loss=137.70964211046254, w0=0.020005205190978308, w1=0.2030383212670634\n",
      "Gradient Descent(7368/9999): loss=137.68504795400997, w0=0.02000364493190184, w1=0.20301774583044527\n",
      "Gradient Descent(7369/9999): loss=137.66045936022172, w0=0.020002084819961716, w1=0.20299717233592363\n",
      "Gradient Descent(7370/9999): loss=137.63587632772754, w0=0.020000524855141883, w1=0.20297660078331084\n",
      "Gradient Descent(7371/9999): loss=137.61129885515766, w0=0.0199989650374263, w1=0.20295603117241928\n",
      "Gradient Descent(7372/9999): loss=137.58672694114253, w0=0.019997405366798917, w1=0.20293546350306133\n",
      "Gradient Descent(7373/9999): loss=137.562160584313, w0=0.019995845843243695, w1=0.2029148977750494\n",
      "Gradient Descent(7374/9999): loss=137.5375997833004, w0=0.019994286466744594, w1=0.20289433398819592\n",
      "Gradient Descent(7375/9999): loss=137.5130445367362, w0=0.01999272723728557, w1=0.20287377214231336\n",
      "Gradient Descent(7376/9999): loss=137.48849484325237, w0=0.019991168154850594, w1=0.2028532122372142\n",
      "Gradient Descent(7377/9999): loss=137.4639507014813, w0=0.01998960921942363, w1=0.20283265427271088\n",
      "Gradient Descent(7378/9999): loss=137.43941211005549, w0=0.019988050430988644, w1=0.20281209824861593\n",
      "Gradient Descent(7379/9999): loss=137.4148790676081, w0=0.019986491789529607, w1=0.20279154416474185\n",
      "Gradient Descent(7380/9999): loss=137.39035157277235, w0=0.01998493329503049, w1=0.2027709920209012\n",
      "Gradient Descent(7381/9999): loss=137.36582962418206, w0=0.019983374947475267, w1=0.20275044181690652\n",
      "Gradient Descent(7382/9999): loss=137.34131322047128, w0=0.019981816746847913, w1=0.20272989355257037\n",
      "Gradient Descent(7383/9999): loss=137.31680236027447, w0=0.019980258693132406, w1=0.20270934722770537\n",
      "Gradient Descent(7384/9999): loss=137.29229704222635, w0=0.01997870078631273, w1=0.2026888028421241\n",
      "Gradient Descent(7385/9999): loss=137.26779726496213, w0=0.01997714302637286, w1=0.20266826039563918\n",
      "Gradient Descent(7386/9999): loss=137.2433030271173, w0=0.01997558541329679, w1=0.20264771988806327\n",
      "Gradient Descent(7387/9999): loss=137.2188143273277, w0=0.019974027947068495, w1=0.20262718131920904\n",
      "Gradient Descent(7388/9999): loss=137.19433116422954, w0=0.01997247062767197, w1=0.20260664468888914\n",
      "Gradient Descent(7389/9999): loss=137.16985353645936, w0=0.0199709134550912, w1=0.20258610999691629\n",
      "Gradient Descent(7390/9999): loss=137.14538144265418, w0=0.019969356429310185, w1=0.20256557724310317\n",
      "Gradient Descent(7391/9999): loss=137.1209148814512, w0=0.01996779955031291, w1=0.20254504642726254\n",
      "Gradient Descent(7392/9999): loss=137.09645385148806, w0=0.019966242818083377, w1=0.2025245175492071\n",
      "Gradient Descent(7393/9999): loss=137.07199835140278, w0=0.019964686232605583, w1=0.20250399060874966\n",
      "Gradient Descent(7394/9999): loss=137.04754837983364, w0=0.019963129793863527, w1=0.202483465605703\n",
      "Gradient Descent(7395/9999): loss=137.0231039354194, w0=0.01996157350184121, w1=0.20246294253987987\n",
      "Gradient Descent(7396/9999): loss=136.9986650167991, w0=0.019960017356522638, w1=0.20244242141109314\n",
      "Gradient Descent(7397/9999): loss=136.97423162261217, w0=0.019958461357891817, w1=0.20242190221915562\n",
      "Gradient Descent(7398/9999): loss=136.94980375149828, w0=0.019956905505932758, w1=0.2024013849638802\n",
      "Gradient Descent(7399/9999): loss=136.92538140209766, w0=0.019955349800629466, w1=0.20238086964507968\n",
      "Gradient Descent(7400/9999): loss=136.90096457305071, w0=0.019953794241965958, w1=0.202360356262567\n",
      "Gradient Descent(7401/9999): loss=136.8765532629983, w0=0.019952238829926243, w1=0.20233984481615502\n",
      "Gradient Descent(7402/9999): loss=136.85214747058154, w0=0.01995068356449434, w1=0.2023193353056567\n",
      "Gradient Descent(7403/9999): loss=136.82774719444203, w0=0.01994912844565427, w1=0.20229882773088498\n",
      "Gradient Descent(7404/9999): loss=136.80335243322156, w0=0.019947573473390053, w1=0.2022783220916528\n",
      "Gradient Descent(7405/9999): loss=136.77896318556256, w0=0.019946018647685707, w1=0.20225781838777315\n",
      "Gradient Descent(7406/9999): loss=136.75457945010743, w0=0.01994446396852526, w1=0.202237316619059\n",
      "Gradient Descent(7407/9999): loss=136.73020122549923, w0=0.019942909435892737, w1=0.20221681678532336\n",
      "Gradient Descent(7408/9999): loss=136.7058285103812, w0=0.019941355049772166, w1=0.20219631888637926\n",
      "Gradient Descent(7409/9999): loss=136.6814613033971, w0=0.01993980081014758, w1=0.20217582292203976\n",
      "Gradient Descent(7410/9999): loss=136.6570996031908, w0=0.019938246717003012, w1=0.2021553288921179\n",
      "Gradient Descent(7411/9999): loss=136.63274340840672, w0=0.01993669277032249, w1=0.20213483679642677\n",
      "Gradient Descent(7412/9999): loss=136.60839271768958, w0=0.019935138970090055, w1=0.20211434663477948\n",
      "Gradient Descent(7413/9999): loss=136.58404752968454, w0=0.019933585316289745, w1=0.20209385840698912\n",
      "Gradient Descent(7414/9999): loss=136.55970784303676, w0=0.0199320318089056, w1=0.20207337211286883\n",
      "Gradient Descent(7415/9999): loss=136.53537365639227, w0=0.019930478447921667, w1=0.20205288775223176\n",
      "Gradient Descent(7416/9999): loss=136.51104496839713, w0=0.019928925233321987, w1=0.20203240532489108\n",
      "Gradient Descent(7417/9999): loss=136.48672177769777, w0=0.019927372165090607, w1=0.20201192483065997\n",
      "Gradient Descent(7418/9999): loss=136.46240408294113, w0=0.019925819243211573, w1=0.20199144626935162\n",
      "Gradient Descent(7419/9999): loss=136.43809188277424, w0=0.019924266467668938, w1=0.20197096964077926\n",
      "Gradient Descent(7420/9999): loss=136.4137851758447, w0=0.019922713838446755, w1=0.20195049494475614\n",
      "Gradient Descent(7421/9999): loss=136.38948396080045, w0=0.019921161355529078, w1=0.2019300221810955\n",
      "Gradient Descent(7422/9999): loss=136.36518823628967, w0=0.019919609018899963, w1=0.20190955134961058\n",
      "Gradient Descent(7423/9999): loss=136.34089800096103, w0=0.01991805682854347, w1=0.20188908245011472\n",
      "Gradient Descent(7424/9999): loss=136.31661325346332, w0=0.019916504784443657, w1=0.20186861548242122\n",
      "Gradient Descent(7425/9999): loss=136.29233399244606, w0=0.01991495288658459, w1=0.20184815044634336\n",
      "Gradient Descent(7426/9999): loss=136.2680602165587, w0=0.019913401134950333, w1=0.2018276873416945\n",
      "Gradient Descent(7427/9999): loss=136.24379192445136, w0=0.01991184952952495, w1=0.20180722616828806\n",
      "Gradient Descent(7428/9999): loss=136.21952911477436, w0=0.019910298070292515, w1=0.20178676692593733\n",
      "Gradient Descent(7429/9999): loss=136.1952717861784, w0=0.019908746757237094, w1=0.20176630961445574\n",
      "Gradient Descent(7430/9999): loss=136.17101993731455, w0=0.019907195590342763, w1=0.2017458542336567\n",
      "Gradient Descent(7431/9999): loss=136.14677356683418, w0=0.019905644569593595, w1=0.20172540078335366\n",
      "Gradient Descent(7432/9999): loss=136.12253267338906, w0=0.019904093694973667, w1=0.20170494926336002\n",
      "Gradient Descent(7433/9999): loss=136.09829725563137, w0=0.019902542966467058, w1=0.20168449967348928\n",
      "Gradient Descent(7434/9999): loss=136.0740673122135, w0=0.019900992384057847, w1=0.2016640520135549\n",
      "Gradient Descent(7435/9999): loss=136.0498428417882, w0=0.01989944194773012, w1=0.20164360628337036\n",
      "Gradient Descent(7436/9999): loss=136.02562384300873, w0=0.019897891657467963, w1=0.20162316248274922\n",
      "Gradient Descent(7437/9999): loss=136.0014103145286, w0=0.019896341513255457, w1=0.201602720611505\n",
      "Gradient Descent(7438/9999): loss=135.97720225500166, w0=0.019894791515076695, w1=0.20158228066945125\n",
      "Gradient Descent(7439/9999): loss=135.9529996630821, w0=0.019893241662915766, w1=0.2015618426564015\n",
      "Gradient Descent(7440/9999): loss=135.92880253742447, w0=0.019891691956756767, w1=0.2015414065721694\n",
      "Gradient Descent(7441/9999): loss=135.90461087668376, w0=0.019890142396583786, w1=0.2015209724165685\n",
      "Gradient Descent(7442/9999): loss=135.88042467951522, w0=0.019888592982380927, w1=0.20150054018941244\n",
      "Gradient Descent(7443/9999): loss=135.85624394457443, w0=0.019887043714132285, w1=0.20148010989051485\n",
      "Gradient Descent(7444/9999): loss=135.83206867051732, w0=0.01988549459182196, w1=0.2014596815196894\n",
      "Gradient Descent(7445/9999): loss=135.80789885600026, w0=0.019883945615434058, w1=0.20143925507674976\n",
      "Gradient Descent(7446/9999): loss=135.78373449967995, w0=0.01988239678495268, w1=0.2014188305615096\n",
      "Gradient Descent(7447/9999): loss=135.7595756002133, w0=0.019880848100361934, w1=0.20139840797378267\n",
      "Gradient Descent(7448/9999): loss=135.73542215625775, w0=0.01987929956164593, w1=0.20137798731338266\n",
      "Gradient Descent(7449/9999): loss=135.71127416647101, w0=0.01987775116878878, w1=0.20135756858012333\n",
      "Gradient Descent(7450/9999): loss=135.68713162951119, w0=0.019876202921774597, w1=0.20133715177381842\n",
      "Gradient Descent(7451/9999): loss=135.6629945440366, w0=0.019874654820587496, w1=0.20131673689428173\n",
      "Gradient Descent(7452/9999): loss=135.63886290870607, w0=0.01987310686521159, w1=0.20129632394132704\n",
      "Gradient Descent(7453/9999): loss=135.61473672217866, w0=0.019871559055631004, w1=0.20127591291476815\n",
      "Gradient Descent(7454/9999): loss=135.59061598311393, w0=0.019870011391829853, w1=0.20125550381441892\n",
      "Gradient Descent(7455/9999): loss=135.5665006901716, w0=0.019868463873792264, w1=0.20123509664009318\n",
      "Gradient Descent(7456/9999): loss=135.5423908420118, w0=0.01986691650150236, w1=0.20121469139160478\n",
      "Gradient Descent(7457/9999): loss=135.51828643729522, w0=0.019865369274944266, w1=0.20119428806876763\n",
      "Gradient Descent(7458/9999): loss=135.49418747468255, w0=0.019863822194102116, w1=0.2011738866713956\n",
      "Gradient Descent(7459/9999): loss=135.470093952835, w0=0.019862275258960036, w1=0.20115348719930265\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(7460/9999): loss=135.44600587041415, w0=0.019860728469502164, w1=0.20113308965230267\n",
      "Gradient Descent(7461/9999): loss=135.42192322608196, w0=0.01985918182571263, w1=0.20111269403020965\n",
      "Gradient Descent(7462/9999): loss=135.39784601850064, w0=0.019857635327575576, w1=0.2010923003328375\n",
      "Gradient Descent(7463/9999): loss=135.37377424633274, w0=0.01985608897507514, w1=0.2010719085600003\n",
      "Gradient Descent(7464/9999): loss=135.34970790824127, w0=0.01985454276819546, w1=0.20105151871151195\n",
      "Gradient Descent(7465/9999): loss=135.3256470028895, w0=0.019852996706920683, w1=0.20103113078718654\n",
      "Gradient Descent(7466/9999): loss=135.30159152894117, w0=0.01985145079123495, w1=0.20101074478683809\n",
      "Gradient Descent(7467/9999): loss=135.2775414850601, w0=0.019849905021122407, w1=0.20099036071028065\n",
      "Gradient Descent(7468/9999): loss=135.25349686991075, w0=0.019848359396567206, w1=0.2009699785573283\n",
      "Gradient Descent(7469/9999): loss=135.22945768215774, w0=0.019846813917553496, w1=0.2009495983277951\n",
      "Gradient Descent(7470/9999): loss=135.2054239204662, w0=0.019845268584065434, w1=0.20092922002149521\n",
      "Gradient Descent(7471/9999): loss=135.18139558350137, w0=0.019843723396087174, w1=0.20090884363824274\n",
      "Gradient Descent(7472/9999): loss=135.15737266992915, w0=0.01984217835360287, w1=0.20088846917785183\n",
      "Gradient Descent(7473/9999): loss=135.13335517841546, w0=0.019840633456596684, w1=0.20086809664013663\n",
      "Gradient Descent(7474/9999): loss=135.1093431076268, w0=0.019839088705052774, w1=0.20084772602491133\n",
      "Gradient Descent(7475/9999): loss=135.08533645623, w0=0.019837544098955304, w1=0.20082735733199011\n",
      "Gradient Descent(7476/9999): loss=135.06133522289204, w0=0.01983599963828844, w1=0.2008069905611872\n",
      "Gradient Descent(7477/9999): loss=135.03733940628055, w0=0.01983445532303635, w1=0.2007866257123168\n",
      "Gradient Descent(7478/9999): loss=135.0133490050631, w0=0.019832911153183197, w1=0.20076626278519322\n",
      "Gradient Descent(7479/9999): loss=134.98936401790806, w0=0.01983136712871316, w1=0.20074590177963067\n",
      "Gradient Descent(7480/9999): loss=134.9653844434839, w0=0.01982982324961041, w1=0.20072554269544346\n",
      "Gradient Descent(7481/9999): loss=134.94141028045945, w0=0.019828279515859117, w1=0.20070518553244587\n",
      "Gradient Descent(7482/9999): loss=134.91744152750388, w0=0.019826735927443464, w1=0.20068483029045223\n",
      "Gradient Descent(7483/9999): loss=134.8934781832868, w0=0.019825192484347626, w1=0.20066447696927686\n",
      "Gradient Descent(7484/9999): loss=134.86952024647803, w0=0.019823649186555787, w1=0.20064412556873412\n",
      "Gradient Descent(7485/9999): loss=134.84556771574788, w0=0.019822106034052128, w1=0.2006237760886384\n",
      "Gradient Descent(7486/9999): loss=134.8216205897668, w0=0.019820563026820833, w1=0.20060342852880406\n",
      "Gradient Descent(7487/9999): loss=134.79767886720592, w0=0.01981902016484609, w1=0.20058308288904553\n",
      "Gradient Descent(7488/9999): loss=134.77374254673637, w0=0.01981747744811209, w1=0.2005627391691772\n",
      "Gradient Descent(7489/9999): loss=134.74981162702983, w0=0.01981593487660302, w1=0.20054239736901353\n",
      "Gradient Descent(7490/9999): loss=134.72588610675817, w0=0.019814392450303076, w1=0.20052205748836896\n",
      "Gradient Descent(7491/9999): loss=134.70196598459384, w0=0.019812850169196452, w1=0.20050171952705798\n",
      "Gradient Descent(7492/9999): loss=134.67805125920944, w0=0.019811308033267343, w1=0.20048138348489508\n",
      "Gradient Descent(7493/9999): loss=134.65414192927798, w0=0.019809766042499954, w1=0.20046104936169476\n",
      "Gradient Descent(7494/9999): loss=134.63023799347278, w0=0.01980822419687848, w1=0.20044071715727155\n",
      "Gradient Descent(7495/9999): loss=134.60633945046752, w0=0.019806682496387125, w1=0.20042038687143998\n",
      "Gradient Descent(7496/9999): loss=134.58244629893633, w0=0.019805140941010095, w1=0.20040005850401463\n",
      "Gradient Descent(7497/9999): loss=134.55855853755352, w0=0.0198035995307316, w1=0.20037973205481008\n",
      "Gradient Descent(7498/9999): loss=134.53467616499384, w0=0.01980205826553584, w1=0.20035940752364093\n",
      "Gradient Descent(7499/9999): loss=134.51079917993238, w0=0.019800517145407034, w1=0.20033908491032176\n",
      "Gradient Descent(7500/9999): loss=134.48692758104457, w0=0.019798976170329392, w1=0.20031876421466724\n",
      "Gradient Descent(7501/9999): loss=134.4630613670061, w0=0.01979743534028713, w1=0.20029844543649197\n",
      "Gradient Descent(7502/9999): loss=134.43920053649313, w0=0.019795894655264462, w1=0.20027812857561067\n",
      "Gradient Descent(7503/9999): loss=134.4153450881821, w0=0.01979435411524561, w1=0.20025781363183798\n",
      "Gradient Descent(7504/9999): loss=134.39149502074986, w0=0.019792813720214793, w1=0.20023750060498863\n",
      "Gradient Descent(7505/9999): loss=134.36765033287347, w0=0.019791273470156235, w1=0.20021718949487732\n",
      "Gradient Descent(7506/9999): loss=134.34381102323042, w0=0.01978973336505416, w1=0.2001968803013188\n",
      "Gradient Descent(7507/9999): loss=134.31997709049864, w0=0.019788193404892795, w1=0.20017657302412778\n",
      "Gradient Descent(7508/9999): loss=134.29614853335616, w0=0.01978665358965637, w1=0.20015626766311906\n",
      "Gradient Descent(7509/9999): loss=134.27232535048165, w0=0.019785113919329116, w1=0.20013596421810742\n",
      "Gradient Descent(7510/9999): loss=134.24850754055387, w0=0.019783574393895262, w1=0.20011566268890768\n",
      "Gradient Descent(7511/9999): loss=134.224695102252, w0=0.019782035013339045, w1=0.20009536307533465\n",
      "Gradient Descent(7512/9999): loss=134.20088803425568, w0=0.019780495777644704, w1=0.20007506537720318\n",
      "Gradient Descent(7513/9999): loss=134.17708633524484, w0=0.019778956686796474, w1=0.2000547695943281\n",
      "Gradient Descent(7514/9999): loss=134.15329000389957, w0=0.019777417740778597, w1=0.20003447572652433\n",
      "Gradient Descent(7515/9999): loss=134.12949903890055, w0=0.019775878939575317, w1=0.20001418377360672\n",
      "Gradient Descent(7516/9999): loss=134.10571343892863, w0=0.01977434028317088, w1=0.19999389373539017\n",
      "Gradient Descent(7517/9999): loss=134.0819332026651, w0=0.01977280177154953, w1=0.19997360561168964\n",
      "Gradient Descent(7518/9999): loss=134.05815832879168, w0=0.019771263404695514, w1=0.19995331940232006\n",
      "Gradient Descent(7519/9999): loss=134.03438881599013, w0=0.019769725182593085, w1=0.19993303510709637\n",
      "Gradient Descent(7520/9999): loss=134.01062466294292, w0=0.019768187105226497, w1=0.19991275272583356\n",
      "Gradient Descent(7521/9999): loss=133.98686586833253, w0=0.019766649172580003, w1=0.19989247225834664\n",
      "Gradient Descent(7522/9999): loss=133.96311243084207, w0=0.019765111384637858, w1=0.1998721937044506\n",
      "Gradient Descent(7523/9999): loss=133.93936434915483, w0=0.019763573741384325, w1=0.1998519170639605\n",
      "Gradient Descent(7524/9999): loss=133.91562162195444, w0=0.01976203624280366, w1=0.19983164233669135\n",
      "Gradient Descent(7525/9999): loss=133.8918842479249, w0=0.019760498888880127, w1=0.19981136952245823\n",
      "Gradient Descent(7526/9999): loss=133.86815222575066, w0=0.019758961679597994, w1=0.19979109862107625\n",
      "Gradient Descent(7527/9999): loss=133.8444255541163, w0=0.019757424614941524, w1=0.19977082963236048\n",
      "Gradient Descent(7528/9999): loss=133.82070423170688, w0=0.019755887694894985, w1=0.19975056255612603\n",
      "Gradient Descent(7529/9999): loss=133.79698825720777, w0=0.019754350919442647, w1=0.19973029739218803\n",
      "Gradient Descent(7530/9999): loss=133.77327762930477, w0=0.019752814288568787, w1=0.19971003414036168\n",
      "Gradient Descent(7531/9999): loss=133.74957234668383, w0=0.019751277802257678, w1=0.1996897728004621\n",
      "Gradient Descent(7532/9999): loss=133.72587240803148, w0=0.019749741460493592, w1=0.19966951337230449\n",
      "Gradient Descent(7533/9999): loss=133.7021778120343, w0=0.019748205263260814, w1=0.19964925585570403\n",
      "Gradient Descent(7534/9999): loss=133.67848855737947, w0=0.019746669210543622, w1=0.19962900025047597\n",
      "Gradient Descent(7535/9999): loss=133.65480464275447, w0=0.019745133302326297, w1=0.19960874655643557\n",
      "Gradient Descent(7536/9999): loss=133.63112606684697, w0=0.019743597538593123, w1=0.19958849477339805\n",
      "Gradient Descent(7537/9999): loss=133.60745282834512, w0=0.019742061919328386, w1=0.19956824490117867\n",
      "Gradient Descent(7538/9999): loss=133.58378492593735, w0=0.019740526444516376, w1=0.19954799693959274\n",
      "Gradient Descent(7539/9999): loss=133.5601223583125, w0=0.019738991114141385, w1=0.19952775088845556\n",
      "Gradient Descent(7540/9999): loss=133.5364651241597, w0=0.0197374559281877, w1=0.19950750674758247\n",
      "Gradient Descent(7541/9999): loss=133.51281322216835, w0=0.01973592088663962, w1=0.1994872645167888\n",
      "Gradient Descent(7542/9999): loss=133.48916665102828, w0=0.01973438598948144, w1=0.19946702419588994\n",
      "Gradient Descent(7543/9999): loss=133.46552540942977, w0=0.01973285123669746, w1=0.1994467857847012\n",
      "Gradient Descent(7544/9999): loss=133.44188949606323, w0=0.019731316628271975, w1=0.19942654928303805\n",
      "Gradient Descent(7545/9999): loss=133.41825890961945, w0=0.019729782164189292, w1=0.19940631469071585\n",
      "Gradient Descent(7546/9999): loss=133.3946336487896, w0=0.019728247844433715, w1=0.19938608200755004\n",
      "Gradient Descent(7547/9999): loss=133.3710137122653, w0=0.019726713668989548, w1=0.19936585123335607\n",
      "Gradient Descent(7548/9999): loss=133.34739909873832, w0=0.0197251796378411, w1=0.1993456223679494\n",
      "Gradient Descent(7549/9999): loss=133.32378980690092, w0=0.019723645750972678, w1=0.1993253954111455\n",
      "Gradient Descent(7550/9999): loss=133.3001858354456, w0=0.0197221120083686, w1=0.19930517036275988\n",
      "Gradient Descent(7551/9999): loss=133.2765871830653, w0=0.01972057841001318, w1=0.19928494722260806\n",
      "Gradient Descent(7552/9999): loss=133.25299384845314, w0=0.01971904495589073, w1=0.19926472599050557\n",
      "Gradient Descent(7553/9999): loss=133.22940583030274, w0=0.019717511645985567, w1=0.19924450666626795\n",
      "Gradient Descent(7554/9999): loss=133.20582312730792, w0=0.019715978480282014, w1=0.19922428924971078\n",
      "Gradient Descent(7555/9999): loss=133.18224573816303, w0=0.01971444545876439, w1=0.19920407374064963\n",
      "Gradient Descent(7556/9999): loss=133.1586736615626, w0=0.019712912581417025, w1=0.19918386013890013\n",
      "Gradient Descent(7557/9999): loss=133.13510689620153, w0=0.01971137984822424, w1=0.19916364844427786\n",
      "Gradient Descent(7558/9999): loss=133.1115454407751, w0=0.019709847259170365, w1=0.19914343865659848\n",
      "Gradient Descent(7559/9999): loss=133.0879892939789, w0=0.01970831481423973, w1=0.19912323077567765\n",
      "Gradient Descent(7560/9999): loss=133.06443845450883, w0=0.019706782513416662, w1=0.19910302480133102\n",
      "Gradient Descent(7561/9999): loss=133.04089292106116, w0=0.0197052503566855, w1=0.1990828207333743\n",
      "Gradient Descent(7562/9999): loss=133.0173526923326, w0=0.019703718344030584, w1=0.19906261857162316\n",
      "Gradient Descent(7563/9999): loss=132.99381776702, w0=0.019702186475436242, w1=0.19904241831589337\n",
      "Gradient Descent(7564/9999): loss=132.97028814382068, w0=0.01970065475088682, w1=0.19902221996600064\n",
      "Gradient Descent(7565/9999): loss=132.9467638214323, w0=0.019699123170366657, w1=0.19900202352176072\n",
      "Gradient Descent(7566/9999): loss=132.92324479855276, w0=0.019697591733860098, w1=0.1989818289829894\n",
      "Gradient Descent(7567/9999): loss=132.89973107388045, w0=0.019696060441351488, w1=0.1989616363495025\n",
      "Gradient Descent(7568/9999): loss=132.87622264611392, w0=0.019694529292825176, w1=0.1989414456211158\n",
      "Gradient Descent(7569/9999): loss=132.85271951395222, w0=0.01969299828826551, w1=0.1989212567976451\n",
      "Gradient Descent(7570/9999): loss=132.82922167609465, w0=0.019691467427656838, w1=0.1989010698789063\n",
      "Gradient Descent(7571/9999): loss=132.80572913124087, w0=0.01968993671098352, w1=0.19888088486471522\n",
      "Gradient Descent(7572/9999): loss=132.78224187809084, w0=0.01968840613822991, w1=0.19886070175488776\n",
      "Gradient Descent(7573/9999): loss=132.758759915345, w0=0.019686875709380363, w1=0.1988405205492398\n",
      "Gradient Descent(7574/9999): loss=132.73528324170394, w0=0.019685345424419243, w1=0.19882034124758727\n",
      "Gradient Descent(7575/9999): loss=132.71181185586863, w0=0.019683815283330908, w1=0.19880016384974608\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(7576/9999): loss=132.6883457565405, w0=0.01968228528609972, w1=0.19877998835553218\n",
      "Gradient Descent(7577/9999): loss=132.66488494242122, w0=0.019680755432710046, w1=0.19875981476476154\n",
      "Gradient Descent(7578/9999): loss=132.64142941221283, w0=0.019679225723146256, w1=0.19873964307725017\n",
      "Gradient Descent(7579/9999): loss=132.6179791646176, w0=0.019677696157392715, w1=0.19871947329281403\n",
      "Gradient Descent(7580/9999): loss=132.59453419833832, w0=0.019676166735433796, w1=0.19869930541126915\n",
      "Gradient Descent(7581/9999): loss=132.57109451207793, w0=0.019674637457253875, w1=0.19867913943243157\n",
      "Gradient Descent(7582/9999): loss=132.54766010453994, w0=0.019673108322837324, w1=0.19865897535611735\n",
      "Gradient Descent(7583/9999): loss=132.52423097442798, w0=0.01967157933216852, w1=0.19863881318214255\n",
      "Gradient Descent(7584/9999): loss=132.50080712044607, w0=0.019670050485231846, w1=0.19861865291032324\n",
      "Gradient Descent(7585/9999): loss=132.4773885412986, w0=0.01966852178201168, w1=0.19859849454047554\n",
      "Gradient Descent(7586/9999): loss=132.45397523569042, w0=0.019666993222492404, w1=0.19857833807241557\n",
      "Gradient Descent(7587/9999): loss=132.43056720232642, w0=0.019665464806658404, w1=0.19855818350595947\n",
      "Gradient Descent(7588/9999): loss=132.40716443991204, w0=0.019663936534494067, w1=0.1985380308409234\n",
      "Gradient Descent(7589/9999): loss=132.38376694715302, w0=0.019662408405983784, w1=0.1985178800771235\n",
      "Gradient Descent(7590/9999): loss=132.36037472275547, w0=0.019660880421111946, w1=0.19849773121437603\n",
      "Gradient Descent(7591/9999): loss=132.33698776542573, w0=0.019659352579862944, w1=0.19847758425249712\n",
      "Gradient Descent(7592/9999): loss=132.31360607387057, w0=0.019657824882221172, w1=0.19845743919130304\n",
      "Gradient Descent(7593/9999): loss=132.29022964679697, w0=0.01965629732817103, w1=0.19843729603061\n",
      "Gradient Descent(7594/9999): loss=132.2668584829125, w0=0.019654769917696917, w1=0.19841715477023428\n",
      "Gradient Descent(7595/9999): loss=132.24349258092485, w0=0.01965324265078323, w1=0.19839701540999216\n",
      "Gradient Descent(7596/9999): loss=132.22013193954203, w0=0.019651715527414376, w1=0.19837687794969994\n",
      "Gradient Descent(7597/9999): loss=132.19677655747256, w0=0.019650188547574755, w1=0.1983567423891739\n",
      "Gradient Descent(7598/9999): loss=132.17342643342508, w0=0.019648661711248776, w1=0.19833660872823036\n",
      "Gradient Descent(7599/9999): loss=132.1500815661088, w0=0.019647135018420848, w1=0.19831647696668572\n",
      "Gradient Descent(7600/9999): loss=132.1267419542331, w0=0.01964560846907538, w1=0.1982963471043563\n",
      "Gradient Descent(7601/9999): loss=132.10340759650774, w0=0.01964408206319679, w1=0.1982762191410585\n",
      "Gradient Descent(7602/9999): loss=132.08007849164278, w0=0.019642555800769484, w1=0.1982560930766087\n",
      "Gradient Descent(7603/9999): loss=132.05675463834862, w0=0.019641029681777885, w1=0.19823596891082335\n",
      "Gradient Descent(7604/9999): loss=132.03343603533614, w0=0.01963950370620641, w1=0.19821584664351882\n",
      "Gradient Descent(7605/9999): loss=132.01012268131643, w0=0.019637977874039477, w1=0.1981957262745116\n",
      "Gradient Descent(7606/9999): loss=131.98681457500086, w0=0.01963645218526151, w1=0.19817560780361815\n",
      "Gradient Descent(7607/9999): loss=131.9635117151012, w0=0.019634926639856932, w1=0.19815549123065496\n",
      "Gradient Descent(7608/9999): loss=131.94021410032954, w0=0.019633401237810173, w1=0.19813537655543853\n",
      "Gradient Descent(7609/9999): loss=131.91692172939838, w0=0.019631875979105658, w1=0.19811526377778538\n",
      "Gradient Descent(7610/9999): loss=131.89363460102047, w0=0.01963035086372782, w1=0.198095152897512\n",
      "Gradient Descent(7611/9999): loss=131.87035271390891, w0=0.019628825891661088, w1=0.198075043914435\n",
      "Gradient Descent(7612/9999): loss=131.84707606677716, w0=0.019627301062889898, w1=0.19805493682837094\n",
      "Gradient Descent(7613/9999): loss=131.82380465833901, w0=0.019625776377398686, w1=0.19803483163913638\n",
      "Gradient Descent(7614/9999): loss=131.80053848730853, w0=0.019624251835171892, w1=0.19801472834654793\n",
      "Gradient Descent(7615/9999): loss=131.77727755240025, w0=0.019622727436193952, w1=0.19799462695042225\n",
      "Gradient Descent(7616/9999): loss=131.7540218523288, w0=0.019621203180449312, w1=0.19797452745057592\n",
      "Gradient Descent(7617/9999): loss=131.73077138580945, w0=0.01961967906792241, w1=0.19795442984682565\n",
      "Gradient Descent(7618/9999): loss=131.70752615155754, w0=0.019618155098597698, w1=0.1979343341389881\n",
      "Gradient Descent(7619/9999): loss=131.6842861482889, w0=0.01961663127245962, w1=0.1979142403268799\n",
      "Gradient Descent(7620/9999): loss=131.66105137471965, w0=0.019615107589492628, w1=0.19789414841031783\n",
      "Gradient Descent(7621/9999): loss=131.63782182956624, w0=0.019613584049681176, w1=0.1978740583891186\n",
      "Gradient Descent(7622/9999): loss=131.61459751154538, w0=0.019612060653009713, w1=0.19785397026309892\n",
      "Gradient Descent(7623/9999): loss=131.59137841937428, w0=0.019610537399462696, w1=0.19783388403207558\n",
      "Gradient Descent(7624/9999): loss=131.56816455177037, w0=0.019609014289024587, w1=0.19781379969586535\n",
      "Gradient Descent(7625/9999): loss=131.5449559074514, w0=0.01960749132167984, w1=0.19779371725428502\n",
      "Gradient Descent(7626/9999): loss=131.5217524851355, w0=0.019605968497412918, w1=0.1977736367071514\n",
      "Gradient Descent(7627/9999): loss=131.49855428354113, w0=0.019604445816208283, w1=0.19775355805428133\n",
      "Gradient Descent(7628/9999): loss=131.47536130138707, w0=0.019602923278050403, w1=0.19773348129549165\n",
      "Gradient Descent(7629/9999): loss=131.4521735373924, w0=0.019601400882923746, w1=0.1977134064305992\n",
      "Gradient Descent(7630/9999): loss=131.4289909902766, w0=0.01959987863081278, w1=0.1976933334594209\n",
      "Gradient Descent(7631/9999): loss=131.40581365875946, w0=0.01959835652170198, w1=0.1976732623817736\n",
      "Gradient Descent(7632/9999): loss=131.38264154156101, w0=0.019596834555575815, w1=0.19765319319747426\n",
      "Gradient Descent(7633/9999): loss=131.35947463740183, w0=0.019595312732418758, w1=0.1976331259063398\n",
      "Gradient Descent(7634/9999): loss=131.33631294500256, w0=0.019593791052215294, w1=0.19761306050818717\n",
      "Gradient Descent(7635/9999): loss=131.31315646308448, w0=0.019592269514949894, w1=0.1975929970028333\n",
      "Gradient Descent(7636/9999): loss=131.29000519036882, w0=0.019590748120607043, w1=0.19757293539009524\n",
      "Gradient Descent(7637/9999): loss=131.26685912557747, w0=0.019589226869171226, w1=0.19755287566978993\n",
      "Gradient Descent(7638/9999): loss=131.24371826743257, w0=0.019587705760626926, w1=0.1975328178417344\n",
      "Gradient Descent(7639/9999): loss=131.22058261465648, w0=0.01958618479495863, w1=0.1975127619057457\n",
      "Gradient Descent(7640/9999): loss=131.19745216597204, w0=0.019584663972150824, w1=0.1974927078616409\n",
      "Gradient Descent(7641/9999): loss=131.17432692010223, w0=0.019583143292188004, w1=0.197472655709237\n",
      "Gradient Descent(7642/9999): loss=131.1512068757706, w0=0.019581622755054658, w1=0.19745260544835117\n",
      "Gradient Descent(7643/9999): loss=131.1280920317009, w0=0.01958010236073528, w1=0.19743255707880045\n",
      "Gradient Descent(7644/9999): loss=131.1049823866172, w0=0.019578582109214374, w1=0.197412510600402\n",
      "Gradient Descent(7645/9999): loss=131.0818779392439, w0=0.019577062000476434, w1=0.19739246601297294\n",
      "Gradient Descent(7646/9999): loss=131.0587786883058, w0=0.019575542034505962, w1=0.19737242331633043\n",
      "Gradient Descent(7647/9999): loss=131.03568463252802, w0=0.01957402221128746, w1=0.19735238251029164\n",
      "Gradient Descent(7648/9999): loss=131.0125957706359, w0=0.019572502530805433, w1=0.19733234359467375\n",
      "Gradient Descent(7649/9999): loss=130.98951210135522, w0=0.019570982993044387, w1=0.197312306569294\n",
      "Gradient Descent(7650/9999): loss=130.9664336234121, w0=0.019569463597988827, w1=0.19729227143396957\n",
      "Gradient Descent(7651/9999): loss=130.9433603355329, w0=0.01956794434562327, w1=0.19727223818851775\n",
      "Gradient Descent(7652/9999): loss=130.92029223644442, w0=0.019566425235932224, w1=0.19725220683275577\n",
      "Gradient Descent(7653/9999): loss=130.89722932487368, w0=0.0195649062689002, w1=0.1972321773665009\n",
      "Gradient Descent(7654/9999): loss=130.8741715995481, w0=0.01956338744451172, w1=0.19721214978957044\n",
      "Gradient Descent(7655/9999): loss=130.8511190591954, w0=0.019561868762751303, w1=0.19719212410178172\n",
      "Gradient Descent(7656/9999): loss=130.82807170254378, w0=0.019560350223603463, w1=0.19717210030295204\n",
      "Gradient Descent(7657/9999): loss=130.80502952832143, w0=0.019558831827052727, w1=0.19715207839289878\n",
      "Gradient Descent(7658/9999): loss=130.78199253525722, w0=0.01955731357308362, w1=0.19713205837143927\n",
      "Gradient Descent(7659/9999): loss=130.75896072208016, w0=0.019555795461680663, w1=0.19711204023839088\n",
      "Gradient Descent(7660/9999): loss=130.73593408751967, w0=0.019554277492828388, w1=0.19709202399357104\n",
      "Gradient Descent(7661/9999): loss=130.71291263030537, w0=0.019552759666511325, w1=0.19707200963679714\n",
      "Gradient Descent(7662/9999): loss=130.68989634916747, w0=0.019551241982714, w1=0.1970519971678866\n",
      "Gradient Descent(7663/9999): loss=130.66688524283614, w0=0.019549724441420953, w1=0.1970319865866569\n",
      "Gradient Descent(7664/9999): loss=130.64387931004228, w0=0.019548207042616717, w1=0.1970119778929255\n",
      "Gradient Descent(7665/9999): loss=130.62087854951673, w0=0.01954668978628583, w1=0.19699197108650984\n",
      "Gradient Descent(7666/9999): loss=130.59788295999107, w0=0.019545172672412833, w1=0.19697196616722748\n",
      "Gradient Descent(7667/9999): loss=130.57489254019688, w0=0.019543655700982265, w1=0.1969519631348959\n",
      "Gradient Descent(7668/9999): loss=130.55190728886615, w0=0.019542138871978674, w1=0.19693196198933266\n",
      "Gradient Descent(7669/9999): loss=130.5289272047313, w0=0.019540622185386602, w1=0.19691196273035527\n",
      "Gradient Descent(7670/9999): loss=130.505952286525, w0=0.019539105641190595, w1=0.19689196535778133\n",
      "Gradient Descent(7671/9999): loss=130.48298253298026, w0=0.019537589239375205, w1=0.1968719698714284\n",
      "Gradient Descent(7672/9999): loss=130.4600179428304, w0=0.019536072979924983, w1=0.19685197627111412\n",
      "Gradient Descent(7673/9999): loss=130.43705851480914, w0=0.01953455686282448, w1=0.19683198455665607\n",
      "Gradient Descent(7674/9999): loss=130.41410424765039, w0=0.01953304088805825, w1=0.19681199472787192\n",
      "Gradient Descent(7675/9999): loss=130.3911551400885, w0=0.019531525055610854, w1=0.1967920067845793\n",
      "Gradient Descent(7676/9999): loss=130.36821119085818, w0=0.019530009365466852, w1=0.19677202072659591\n",
      "Gradient Descent(7677/9999): loss=130.34527239869442, w0=0.019528493817610803, w1=0.1967520365537394\n",
      "Gradient Descent(7678/9999): loss=130.32233876233246, w0=0.019526978412027267, w1=0.19673205426582752\n",
      "Gradient Descent(7679/9999): loss=130.299410280508, w0=0.01952546314870081, w1=0.19671207386267794\n",
      "Gradient Descent(7680/9999): loss=130.27648695195694, w0=0.019523948027616, w1=0.19669209534410845\n",
      "Gradient Descent(7681/9999): loss=130.25356877541566, w0=0.019522433048757407, w1=0.19667211870993676\n",
      "Gradient Descent(7682/9999): loss=130.23065574962078, w0=0.0195209182121096, w1=0.19665214395998068\n",
      "Gradient Descent(7683/9999): loss=130.20774787330916, w0=0.01951940351765715, w1=0.19663217109405798\n",
      "Gradient Descent(7684/9999): loss=130.18484514521816, w0=0.019517888965384637, w1=0.1966122001119865\n",
      "Gradient Descent(7685/9999): loss=130.16194756408538, w0=0.019516374555276633, w1=0.19659223101358403\n",
      "Gradient Descent(7686/9999): loss=130.13905512864872, w0=0.019514860287317717, w1=0.19657226379866843\n",
      "Gradient Descent(7687/9999): loss=130.1161678376465, w0=0.01951334616149247, w1=0.19655229846705757\n",
      "Gradient Descent(7688/9999): loss=130.0932856898173, w0=0.019511832177785474, w1=0.19653233501856932\n",
      "Gradient Descent(7689/9999): loss=130.07040868389998, w0=0.01951031833618131, w1=0.19651237345302155\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(7690/9999): loss=130.0475368186338, w0=0.01950880463666457, w1=0.1964924137702322\n",
      "Gradient Descent(7691/9999): loss=130.0246700927584, w0=0.01950729107921984, w1=0.19647245597001917\n",
      "Gradient Descent(7692/9999): loss=130.00180850501366, w0=0.019505777663831707, w1=0.19645250005220044\n",
      "Gradient Descent(7693/9999): loss=129.97895205413974, w0=0.019504264390484766, w1=0.19643254601659396\n",
      "Gradient Descent(7694/9999): loss=129.95610073887724, w0=0.019502751259163613, w1=0.1964125938630177\n",
      "Gradient Descent(7695/9999): loss=129.9332545579671, w0=0.01950123826985284, w1=0.1963926435912897\n",
      "Gradient Descent(7696/9999): loss=129.9104135101504, w0=0.019499725422537043, w1=0.1963726952012279\n",
      "Gradient Descent(7697/9999): loss=129.88757759416876, w0=0.019498212717200827, w1=0.1963527486926504\n",
      "Gradient Descent(7698/9999): loss=129.86474680876407, w0=0.019496700153828792, w1=0.1963328040653752\n",
      "Gradient Descent(7699/9999): loss=129.84192115267842, w0=0.019495187732405543, w1=0.1963128613192204\n",
      "Gradient Descent(7700/9999): loss=129.8191006246544, w0=0.019493675452915683, w1=0.19629292045400407\n",
      "Gradient Descent(7701/9999): loss=129.79628522343486, w0=0.019492163315343817, w1=0.1962729814695443\n",
      "Gradient Descent(7702/9999): loss=129.7734749477629, w0=0.01949065131967456, w1=0.19625304436565924\n",
      "Gradient Descent(7703/9999): loss=129.75066979638214, w0=0.019489139465892518, w1=0.196233109142167\n",
      "Gradient Descent(7704/9999): loss=129.7278697680362, w0=0.019487627753982308, w1=0.1962131757988857\n",
      "Gradient Descent(7705/9999): loss=129.70507486146937, w0=0.019486116183928544, w1=0.19619324433563357\n",
      "Gradient Descent(7706/9999): loss=129.6822850754261, w0=0.019484604755715843, w1=0.19617331475222877\n",
      "Gradient Descent(7707/9999): loss=129.65950040865113, w0=0.019483093469328826, w1=0.1961533870484895\n",
      "Gradient Descent(7708/9999): loss=129.6367208598897, w0=0.019481582324752115, w1=0.19613346122423397\n",
      "Gradient Descent(7709/9999): loss=129.61394642788713, w0=0.019480071321970327, w1=0.19611353727928044\n",
      "Gradient Descent(7710/9999): loss=129.5911771113893, w0=0.019478560460968092, w1=0.19609361521344715\n",
      "Gradient Descent(7711/9999): loss=129.56841290914218, w0=0.019477049741730035, w1=0.19607369502655236\n",
      "Gradient Descent(7712/9999): loss=129.54565381989235, w0=0.019475539164240784, w1=0.1960537767184144\n",
      "Gradient Descent(7713/9999): loss=129.52289984238647, w0=0.01947402872848497, w1=0.19603386028885153\n",
      "Gradient Descent(7714/9999): loss=129.5001509753716, w0=0.019472518434447226, w1=0.1960139457376821\n",
      "Gradient Descent(7715/9999): loss=129.4774072175952, w0=0.019471008282112186, w1=0.19599403306472446\n",
      "Gradient Descent(7716/9999): loss=129.454668567805, w0=0.019469498271464488, w1=0.19597412226979694\n",
      "Gradient Descent(7717/9999): loss=129.43193502474892, w0=0.01946798840248877, w1=0.19595421335271793\n",
      "Gradient Descent(7718/9999): loss=129.40920658717553, w0=0.019466478675169673, w1=0.1959343063133058\n",
      "Gradient Descent(7719/9999): loss=129.38648325383338, w0=0.019464969089491836, w1=0.19591440115137898\n",
      "Gradient Descent(7720/9999): loss=129.3637650234716, w0=0.019463459645439905, w1=0.1958944978667559\n",
      "Gradient Descent(7721/9999): loss=129.34105189483938, w0=0.019461950342998528, w1=0.19587459645925498\n",
      "Gradient Descent(7722/9999): loss=129.31834386668658, w0=0.01946044118215235, w1=0.1958546969286947\n",
      "Gradient Descent(7723/9999): loss=129.29564093776312, w0=0.019458932162886022, w1=0.19583479927489356\n",
      "Gradient Descent(7724/9999): loss=129.27294310681933, w0=0.019457423285184197, w1=0.19581490349767\n",
      "Gradient Descent(7725/9999): loss=129.25025037260582, w0=0.01945591454903153, w1=0.19579500959684257\n",
      "Gradient Descent(7726/9999): loss=129.22756273387358, w0=0.01945440595441267, w1=0.1957751175722298\n",
      "Gradient Descent(7727/9999): loss=129.20488018937394, w0=0.019452897501312284, w1=0.1957552274236502\n",
      "Gradient Descent(7728/9999): loss=129.1822027378585, w0=0.019451389189715024, w1=0.19573533915092237\n",
      "Gradient Descent(7729/9999): loss=129.15953037807918, w0=0.019449881019605555, w1=0.19571545275386487\n",
      "Gradient Descent(7730/9999): loss=129.13686310878825, w0=0.019448372990968542, w1=0.1956955682322963\n",
      "Gradient Descent(7731/9999): loss=129.11420092873834, w0=0.019446865103788645, w1=0.1956756855860353\n",
      "Gradient Descent(7732/9999): loss=129.09154383668232, w0=0.019445357358050537, w1=0.19565580481490044\n",
      "Gradient Descent(7733/9999): loss=129.0688918313734, w0=0.019443849753738886, w1=0.19563592591871043\n",
      "Gradient Descent(7734/9999): loss=129.04624491156525, w0=0.01944234229083836, w1=0.1956160488972839\n",
      "Gradient Descent(7735/9999): loss=129.02360307601168, w0=0.019440834969333638, w1=0.19559617375043956\n",
      "Gradient Descent(7736/9999): loss=129.00096632346694, w0=0.019439327789209388, w1=0.19557630047799607\n",
      "Gradient Descent(7737/9999): loss=128.97833465268548, w0=0.01943782075045029, w1=0.19555642907977216\n",
      "Gradient Descent(7738/9999): loss=128.9557080624222, w0=0.019436313853041027, w1=0.1955365595555866\n",
      "Gradient Descent(7739/9999): loss=128.93308655143227, w0=0.019434807096966274, w1=0.1955166919052581\n",
      "Gradient Descent(7740/9999): loss=128.91047011847124, w0=0.019433300482210713, w1=0.19549682612860547\n",
      "Gradient Descent(7741/9999): loss=128.88785876229485, w0=0.019431794008759034, w1=0.19547696222544744\n",
      "Gradient Descent(7742/9999): loss=128.86525248165935, w0=0.019430287676595923, w1=0.19545710019560286\n",
      "Gradient Descent(7743/9999): loss=128.8426512753211, w0=0.019428781485706065, w1=0.19543724003889051\n",
      "Gradient Descent(7744/9999): loss=128.82005514203698, w0=0.01942727543607415, w1=0.19541738175512927\n",
      "Gradient Descent(7745/9999): loss=128.79746408056403, w0=0.019425769527684874, w1=0.19539752534413796\n",
      "Gradient Descent(7746/9999): loss=128.77487808965967, w0=0.019424263760522927, w1=0.19537767080573545\n",
      "Gradient Descent(7747/9999): loss=128.75229716808172, w0=0.01942275813457301, w1=0.19535781813974065\n",
      "Gradient Descent(7748/9999): loss=128.7297213145883, w0=0.01942125264981982, w1=0.19533796734597245\n",
      "Gradient Descent(7749/9999): loss=128.7071505279377, w0=0.019419747306248052, w1=0.19531811842424976\n",
      "Gradient Descent(7750/9999): loss=128.68458480688872, w0=0.019418242103842413, w1=0.19529827137439154\n",
      "Gradient Descent(7751/9999): loss=128.66202415020035, w0=0.019416737042587607, w1=0.19527842619621674\n",
      "Gradient Descent(7752/9999): loss=128.639468556632, w0=0.019415232122468335, w1=0.19525858288954434\n",
      "Gradient Descent(7753/9999): loss=128.6169180249434, w0=0.019413727343469307, w1=0.19523874145419332\n",
      "Gradient Descent(7754/9999): loss=128.5943725538944, w0=0.019412222705575237, w1=0.1952189018899827\n",
      "Gradient Descent(7755/9999): loss=128.57183214224554, w0=0.019410718208770834, w1=0.1951990641967315\n",
      "Gradient Descent(7756/9999): loss=128.5492967887574, w0=0.01940921385304081, w1=0.19517922837425875\n",
      "Gradient Descent(7757/9999): loss=128.52676649219086, w0=0.01940770963836988, w1=0.1951593944223835\n",
      "Gradient Descent(7758/9999): loss=128.50424125130732, w0=0.019406205564742762, w1=0.19513956234092486\n",
      "Gradient Descent(7759/9999): loss=128.48172106486837, w0=0.019404701632144177, w1=0.1951197321297019\n",
      "Gradient Descent(7760/9999): loss=128.45920593163595, w0=0.019403197840558845, w1=0.19509990378853376\n",
      "Gradient Descent(7761/9999): loss=128.43669585037233, w0=0.019401694189971487, w1=0.1950800773172395\n",
      "Gradient Descent(7762/9999): loss=128.41419081984006, w0=0.01940019068036683, w1=0.19506025271563832\n",
      "Gradient Descent(7763/9999): loss=128.39169083880205, w0=0.0193986873117296, w1=0.19504042998354937\n",
      "Gradient Descent(7764/9999): loss=128.36919590602153, w0=0.01939718408404453, w1=0.19502060912079183\n",
      "Gradient Descent(7765/9999): loss=128.34670602026208, w0=0.019395680997296345, w1=0.19500079012718488\n",
      "Gradient Descent(7766/9999): loss=128.3242211802875, w0=0.019394178051469778, w1=0.19498097300254774\n",
      "Gradient Descent(7767/9999): loss=128.3017413848621, w0=0.019392675246549567, w1=0.19496115774669964\n",
      "Gradient Descent(7768/9999): loss=128.27926663275025, w0=0.019391172582520447, w1=0.19494134435945984\n",
      "Gradient Descent(7769/9999): loss=128.25679692271677, w0=0.019389670059367155, w1=0.1949215328406476\n",
      "Gradient Descent(7770/9999): loss=128.2343322535269, w0=0.019388167677074433, w1=0.19490172319008217\n",
      "Gradient Descent(7771/9999): loss=128.2118726239461, w0=0.019386665435627025, w1=0.19488191540758287\n",
      "Gradient Descent(7772/9999): loss=128.18941803274004, w0=0.01938516333500967, w1=0.19486210949296903\n",
      "Gradient Descent(7773/9999): loss=128.16696847867496, w0=0.01938366137520712, w1=0.19484230544605996\n",
      "Gradient Descent(7774/9999): loss=128.1445239605172, w0=0.019382159556204117, w1=0.19482250326667502\n",
      "Gradient Descent(7775/9999): loss=128.12208447703355, w0=0.019380657877985416, w1=0.19480270295463356\n",
      "Gradient Descent(7776/9999): loss=128.09965002699107, w0=0.019379156340535764, w1=0.194782904509755\n",
      "Gradient Descent(7777/9999): loss=128.07722060915714, w0=0.019377654943839917, w1=0.1947631079318587\n",
      "Gradient Descent(7778/9999): loss=128.05479622229947, w0=0.019376153687882633, w1=0.19474331322076407\n",
      "Gradient Descent(7779/9999): loss=128.03237686518608, w0=0.019374652572648667, w1=0.1947235203762906\n",
      "Gradient Descent(7780/9999): loss=128.00996253658528, w0=0.01937315159812278, w1=0.1947037293982577\n",
      "Gradient Descent(7781/9999): loss=127.9875532352658, w0=0.01937165076428973, w1=0.19468394028648484\n",
      "Gradient Descent(7782/9999): loss=127.9651489599966, w0=0.01937015007113428, w1=0.19466415304079152\n",
      "Gradient Descent(7783/9999): loss=127.94274970954699, w0=0.019368649518641205, w1=0.19464436766099721\n",
      "Gradient Descent(7784/9999): loss=127.92035548268655, w0=0.019367149106795264, w1=0.19462458414692146\n",
      "Gradient Descent(7785/9999): loss=127.89796627818524, w0=0.019365648835581225, w1=0.1946048024983838\n",
      "Gradient Descent(7786/9999): loss=127.87558209481332, w0=0.01936414870498386, w1=0.19458502271520378\n",
      "Gradient Descent(7787/9999): loss=127.85320293134134, w0=0.019362648714987947, w1=0.19456524479720097\n",
      "Gradient Descent(7788/9999): loss=127.8308287865403, w0=0.019361148865578253, w1=0.19454546874419495\n",
      "Gradient Descent(7789/9999): loss=127.80845965918131, w0=0.01935964915673956, w1=0.19452569455600535\n",
      "Gradient Descent(7790/9999): loss=127.78609554803589, w0=0.019358149588456643, w1=0.19450592223245178\n",
      "Gradient Descent(7791/9999): loss=127.76373645187599, w0=0.019356650160714285, w1=0.19448615177335388\n",
      "Gradient Descent(7792/9999): loss=127.74138236947367, w0=0.01935515087349727, w1=0.1944663831785313\n",
      "Gradient Descent(7793/9999): loss=127.71903329960152, w0=0.019353651726790377, w1=0.1944466164478037\n",
      "Gradient Descent(7794/9999): loss=127.69668924103226, w0=0.019352152720578396, w1=0.1944268515809908\n",
      "Gradient Descent(7795/9999): loss=127.6743501925391, w0=0.019350653854846116, w1=0.19440708857791228\n",
      "Gradient Descent(7796/9999): loss=127.65201615289541, w0=0.019349155129578324, w1=0.19438732743838788\n",
      "Gradient Descent(7797/9999): loss=127.62968712087502, w0=0.01934765654475981, w1=0.19436756816223733\n",
      "Gradient Descent(7798/9999): loss=127.60736309525194, w0=0.019346158100375377, w1=0.1943478107492804\n",
      "Gradient Descent(7799/9999): loss=127.58504407480056, w0=0.019344659796409813, w1=0.19432805519933685\n",
      "Gradient Descent(7800/9999): loss=127.5627300582957, w0=0.019343161632847918, w1=0.19430830151222647\n",
      "Gradient Descent(7801/9999): loss=127.54042104451226, w0=0.01934166360967449, w1=0.1942885496877691\n",
      "Gradient Descent(7802/9999): loss=127.51811703222567, w0=0.019340165726874334, w1=0.19426879972578454\n",
      "Gradient Descent(7803/9999): loss=127.49581802021162, w0=0.01933866798443225, w1=0.19424905162609263\n",
      "Gradient Descent(7804/9999): loss=127.473524007246, w0=0.019337170382333044, w1=0.19422930538851324\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(7805/9999): loss=127.45123499210514, w0=0.019335672920561525, w1=0.19420956101286624\n",
      "Gradient Descent(7806/9999): loss=127.42895097356572, w0=0.0193341755991025, w1=0.19418981849897154\n",
      "Gradient Descent(7807/9999): loss=127.40667195040459, w0=0.019332678417940778, w1=0.19417007784664905\n",
      "Gradient Descent(7808/9999): loss=127.38439792139913, w0=0.019331181377061177, w1=0.19415033905571868\n",
      "Gradient Descent(7809/9999): loss=127.36212888532675, w0=0.01932968447644851, w1=0.1941306021260004\n",
      "Gradient Descent(7810/9999): loss=127.33986484096542, w0=0.01932818771608759, w1=0.19411086705731412\n",
      "Gradient Descent(7811/9999): loss=127.31760578709336, w0=0.019326691095963237, w1=0.19409113384947987\n",
      "Gradient Descent(7812/9999): loss=127.29535172248907, w0=0.019325194616060276, w1=0.19407140250231764\n",
      "Gradient Descent(7813/9999): loss=127.27310264593137, w0=0.019323698276363526, w1=0.19405167301564744\n",
      "Gradient Descent(7814/9999): loss=127.2508585561994, w0=0.019322202076857812, w1=0.19403194538928928\n",
      "Gradient Descent(7815/9999): loss=127.22861945207269, w0=0.01932070601752796, w1=0.19401221962306323\n",
      "Gradient Descent(7816/9999): loss=127.20638533233095, w0=0.0193192100983588, w1=0.19399249571678934\n",
      "Gradient Descent(7817/9999): loss=127.18415619575433, w0=0.01931771431933516, w1=0.1939727736702877\n",
      "Gradient Descent(7818/9999): loss=127.16193204112324, w0=0.019316218680441873, w1=0.19395305348337843\n",
      "Gradient Descent(7819/9999): loss=127.1397128672184, w0=0.019314723181663773, w1=0.19393333515588163\n",
      "Gradient Descent(7820/9999): loss=127.11749867282082, w0=0.019313227822985692, w1=0.19391361868761742\n",
      "Gradient Descent(7821/9999): loss=127.09528945671202, w0=0.01931173260439247, w1=0.19389390407840595\n",
      "Gradient Descent(7822/9999): loss=127.07308521767345, w0=0.019310237525868953, w1=0.19387419132806738\n",
      "Gradient Descent(7823/9999): loss=127.05088595448737, w0=0.019308742587399976, w1=0.19385448043642192\n",
      "Gradient Descent(7824/9999): loss=127.02869166593585, w0=0.01930724778897038, w1=0.19383477140328975\n",
      "Gradient Descent(7825/9999): loss=127.00650235080165, w0=0.019305753130565016, w1=0.1938150642284911\n",
      "Gradient Descent(7826/9999): loss=126.98431800786766, w0=0.019304258612168727, w1=0.19379535891184618\n",
      "Gradient Descent(7827/9999): loss=126.96213863591717, w0=0.019302764233766364, w1=0.19377565545317527\n",
      "Gradient Descent(7828/9999): loss=126.93996423373376, w0=0.019301269995342776, w1=0.19375595385229863\n",
      "Gradient Descent(7829/9999): loss=126.91779480010132, w0=0.01929977589688282, w1=0.19373625410903653\n",
      "Gradient Descent(7830/9999): loss=126.89563033380398, w0=0.019298281938371346, w1=0.1937165562232093\n",
      "Gradient Descent(7831/9999): loss=126.87347083362634, w0=0.019296788119793216, w1=0.19369686019463722\n",
      "Gradient Descent(7832/9999): loss=126.85131629835325, w0=0.01929529444113328, w1=0.19367716602314067\n",
      "Gradient Descent(7833/9999): loss=126.8291667267698, w0=0.019293800902376406, w1=0.19365747370853997\n",
      "Gradient Descent(7834/9999): loss=126.80702211766146, w0=0.019292307503507456, w1=0.1936377832506555\n",
      "Gradient Descent(7835/9999): loss=126.78488246981402, w0=0.019290814244511293, w1=0.19361809464930765\n",
      "Gradient Descent(7836/9999): loss=126.76274778201359, w0=0.01928932112537278, w1=0.19359840790431684\n",
      "Gradient Descent(7837/9999): loss=126.74061805304655, w0=0.01928782814607679, w1=0.19357872301550347\n",
      "Gradient Descent(7838/9999): loss=126.71849328169965, w0=0.01928633530660819, w1=0.193559039982688\n",
      "Gradient Descent(7839/9999): loss=126.69637346675991, w0=0.01928484260695185, w1=0.19353935880569087\n",
      "Gradient Descent(7840/9999): loss=126.67425860701466, w0=0.019283350047092647, w1=0.19351967948433255\n",
      "Gradient Descent(7841/9999): loss=126.65214870125159, w0=0.019281857627015456, w1=0.19350000201843354\n",
      "Gradient Descent(7842/9999): loss=126.63004374825869, w0=0.019280365346705155, w1=0.19348032640781432\n",
      "Gradient Descent(7843/9999): loss=126.60794374682426, w0=0.019278873206146623, w1=0.19346065265229545\n",
      "Gradient Descent(7844/9999): loss=126.5858486957368, w0=0.019277381205324743, w1=0.19344098075169744\n",
      "Gradient Descent(7845/9999): loss=126.56375859378538, w0=0.019275889344224396, w1=0.19342131070584087\n",
      "Gradient Descent(7846/9999): loss=126.54167343975917, w0=0.019274397622830466, w1=0.1934016425145463\n",
      "Gradient Descent(7847/9999): loss=126.51959323244766, w0=0.019272906041127844, w1=0.19338197617763433\n",
      "Gradient Descent(7848/9999): loss=126.49751797064083, w0=0.019271414599101415, w1=0.19336231169492557\n",
      "Gradient Descent(7849/9999): loss=126.47544765312873, w0=0.01926992329673607, w1=0.19334264906624063\n",
      "Gradient Descent(7850/9999): loss=126.4533822787019, w0=0.019268432134016706, w1=0.19332298829140016\n",
      "Gradient Descent(7851/9999): loss=126.43132184615122, w0=0.01926694111092821, w1=0.19330332937022482\n",
      "Gradient Descent(7852/9999): loss=126.40926635426767, w0=0.019265450227455485, w1=0.1932836723025353\n",
      "Gradient Descent(7853/9999): loss=126.38721580184274, w0=0.019263959483583427, w1=0.19326401708815225\n",
      "Gradient Descent(7854/9999): loss=126.36517018766823, w0=0.01926246887929694, w1=0.1932443637268964\n",
      "Gradient Descent(7855/9999): loss=126.34312951053604, w0=0.01926097841458092, w1=0.1932247122185885\n",
      "Gradient Descent(7856/9999): loss=126.32109376923871, w0=0.019259488089420275, w1=0.19320506256304928\n",
      "Gradient Descent(7857/9999): loss=126.29906296256877, w0=0.01925799790379991, w1=0.1931854147600995\n",
      "Gradient Descent(7858/9999): loss=126.2770370893193, w0=0.01925650785770473, w1=0.19316576880955996\n",
      "Gradient Descent(7859/9999): loss=126.25501614828362, w0=0.019255017951119648, w1=0.19314612471125142\n",
      "Gradient Descent(7860/9999): loss=126.2330001382553, w0=0.019253528184029575, w1=0.1931264824649947\n",
      "Gradient Descent(7861/9999): loss=126.21098905802829, w0=0.019252038556419427, w1=0.19310684207061063\n",
      "Gradient Descent(7862/9999): loss=126.18898290639676, w0=0.019250549068274114, w1=0.19308720352792005\n",
      "Gradient Descent(7863/9999): loss=126.16698168215538, w0=0.01924905971957856, w1=0.19306756683674384\n",
      "Gradient Descent(7864/9999): loss=126.14498538409899, w0=0.01924757051031768, w1=0.19304793199690287\n",
      "Gradient Descent(7865/9999): loss=126.1229940110227, w0=0.019246081440476393, w1=0.19302829900821805\n",
      "Gradient Descent(7866/9999): loss=126.10100756172203, w0=0.019244592510039623, w1=0.19300866787051027\n",
      "Gradient Descent(7867/9999): loss=126.07902603499282, w0=0.0192431037189923, w1=0.19298903858360048\n",
      "Gradient Descent(7868/9999): loss=126.05704942963109, w0=0.019241615067319348, w1=0.19296941114730962\n",
      "Gradient Descent(7869/9999): loss=126.03507774443348, w0=0.019240126555005694, w1=0.19294978556145864\n",
      "Gradient Descent(7870/9999): loss=126.01311097819642, w0=0.019238638182036267, w1=0.19293016182586853\n",
      "Gradient Descent(7871/9999): loss=125.99114912971724, w0=0.019237149948396003, w1=0.1929105399403603\n",
      "Gradient Descent(7872/9999): loss=125.96919219779312, w0=0.019235661854069833, w1=0.19289091990475496\n",
      "Gradient Descent(7873/9999): loss=125.9472401812218, w0=0.019234173899042698, w1=0.19287130171887354\n",
      "Gradient Descent(7874/9999): loss=125.92529307880127, w0=0.019232686083299533, w1=0.19285168538253708\n",
      "Gradient Descent(7875/9999): loss=125.9033508893298, w0=0.019231198406825276, w1=0.19283207089556667\n",
      "Gradient Descent(7876/9999): loss=125.881413611606, w0=0.01922971086960487, w1=0.19281245825778337\n",
      "Gradient Descent(7877/9999): loss=125.85948124442874, w0=0.019228223471623264, w1=0.1927928474690083\n",
      "Gradient Descent(7878/9999): loss=125.83755378659741, w0=0.019226736212865397, w1=0.19277323852906256\n",
      "Gradient Descent(7879/9999): loss=125.81563123691139, w0=0.01922524909331622, w1=0.19275363143776728\n",
      "Gradient Descent(7880/9999): loss=125.79371359417057, w0=0.01922376211296068, w1=0.19273402619494362\n",
      "Gradient Descent(7881/9999): loss=125.77180085717515, w0=0.01922227527178373, w1=0.19271442280041273\n",
      "Gradient Descent(7882/9999): loss=125.74989302472552, w0=0.019220788569770324, w1=0.19269482125399584\n",
      "Gradient Descent(7883/9999): loss=125.72799009562256, w0=0.019219302006905414, w1=0.19267522155551411\n",
      "Gradient Descent(7884/9999): loss=125.70609206866735, w0=0.01921781558317396, w1=0.19265562370478878\n",
      "Gradient Descent(7885/9999): loss=125.68419894266115, w0=0.019216329298560916, w1=0.19263602770164107\n",
      "Gradient Descent(7886/9999): loss=125.66231071640587, w0=0.01921484315305125, w1=0.19261643354589222\n",
      "Gradient Descent(7887/9999): loss=125.64042738870342, w0=0.01921335714662992, w1=0.19259684123736354\n",
      "Gradient Descent(7888/9999): loss=125.61854895835613, w0=0.01921187127928189, w1=0.19257725077587629\n",
      "Gradient Descent(7889/9999): loss=125.59667542416668, w0=0.019210385550992127, w1=0.19255766216125178\n",
      "Gradient Descent(7890/9999): loss=125.57480678493805, w0=0.0192088999617456, w1=0.19253807539331133\n",
      "Gradient Descent(7891/9999): loss=125.55294303947343, w0=0.01920741451152728, w1=0.19251849047187627\n",
      "Gradient Descent(7892/9999): loss=125.53108418657645, w0=0.019205929200322137, w1=0.19249890739676795\n",
      "Gradient Descent(7893/9999): loss=125.50923022505093, w0=0.019204444028115147, w1=0.19247932616780775\n",
      "Gradient Descent(7894/9999): loss=125.48738115370111, w0=0.019202958994891283, w1=0.19245974678481706\n",
      "Gradient Descent(7895/9999): loss=125.46553697133156, w0=0.019201474100635524, w1=0.19244016924761728\n",
      "Gradient Descent(7896/9999): loss=125.44369767674692, w0=0.01919998934533285, w1=0.19242059355602983\n",
      "Gradient Descent(7897/9999): loss=125.42186326875249, w0=0.01919850472896824, w1=0.19240101970987616\n",
      "Gradient Descent(7898/9999): loss=125.40003374615354, w0=0.019197020251526684, w1=0.19238144770897772\n",
      "Gradient Descent(7899/9999): loss=125.37820910775596, w0=0.01919553591299316, w1=0.19236187755315598\n",
      "Gradient Descent(7900/9999): loss=125.35638935236572, w0=0.01919405171335266, w1=0.19234230924223242\n",
      "Gradient Descent(7901/9999): loss=125.33457447878915, w0=0.019192567652590167, w1=0.19232274277602854\n",
      "Gradient Descent(7902/9999): loss=125.31276448583291, w0=0.019191083730690676, w1=0.19230317815436587\n",
      "Gradient Descent(7903/9999): loss=125.2909593723041, w0=0.01918959994763918, w1=0.19228361537706598\n",
      "Gradient Descent(7904/9999): loss=125.26915913700988, w0=0.019188116303420672, w1=0.1922640544439504\n",
      "Gradient Descent(7905/9999): loss=125.24736377875792, w0=0.01918663279802015, w1=0.1922444953548407\n",
      "Gradient Descent(7906/9999): loss=125.22557329635602, w0=0.019185149431422608, w1=0.19222493810955849\n",
      "Gradient Descent(7907/9999): loss=125.20378768861248, w0=0.019183666203613052, w1=0.19220538270792534\n",
      "Gradient Descent(7908/9999): loss=125.1820069543358, w0=0.019182183114576482, w1=0.1921858291497629\n",
      "Gradient Descent(7909/9999): loss=125.16023109233478, w0=0.0191807001642979, w1=0.1921662774348928\n",
      "Gradient Descent(7910/9999): loss=125.13846010141862, w0=0.019179217352762314, w1=0.19214672756313675\n",
      "Gradient Descent(7911/9999): loss=125.11669398039666, w0=0.019177734679954733, w1=0.19212717953431638\n",
      "Gradient Descent(7912/9999): loss=125.09493272807873, w0=0.019176252145860162, w1=0.19210763334825337\n",
      "Gradient Descent(7913/9999): loss=125.07317634327491, w0=0.019174769750463616, w1=0.19208808900476945\n",
      "Gradient Descent(7914/9999): loss=125.05142482479548, w0=0.019173287493750107, w1=0.19206854650368635\n",
      "Gradient Descent(7915/9999): loss=125.02967817145121, w0=0.019171805375704655, w1=0.1920490058448258\n",
      "Gradient Descent(7916/9999): loss=125.00793638205296, w0=0.01917032339631227, w1=0.19202946702800955\n",
      "Gradient Descent(7917/9999): loss=124.98619945541218, w0=0.019168841555557974, w1=0.19200993005305939\n",
      "Gradient Descent(7918/9999): loss=124.96446739034035, w0=0.019167359853426786, w1=0.1919903949197971\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(7919/9999): loss=124.94274018564937, w0=0.01916587828990373, w1=0.19197086162804453\n",
      "Gradient Descent(7920/9999): loss=124.92101784015163, w0=0.019164396864973832, w1=0.19195133017762347\n",
      "Gradient Descent(7921/9999): loss=124.89930035265941, w0=0.019162915578622118, w1=0.1919318005683558\n",
      "Gradient Descent(7922/9999): loss=124.87758772198563, w0=0.019161434430833617, w1=0.19191227280006334\n",
      "Gradient Descent(7923/9999): loss=124.85587994694347, w0=0.019159953421593354, w1=0.191892746872568\n",
      "Gradient Descent(7924/9999): loss=124.8341770263464, w0=0.019158472550886367, w1=0.19187322278569163\n",
      "Gradient Descent(7925/9999): loss=124.81247895900803, w0=0.019156991818697688, w1=0.19185370053925618\n",
      "Gradient Descent(7926/9999): loss=124.79078574374252, w0=0.019155511225012353, w1=0.19183418013308356\n",
      "Gradient Descent(7927/9999): loss=124.76909737936427, w0=0.019154030769815398, w1=0.19181466156699573\n",
      "Gradient Descent(7928/9999): loss=124.74741386468784, w0=0.019152550453091867, w1=0.19179514484081464\n",
      "Gradient Descent(7929/9999): loss=124.72573519852828, w0=0.0191510702748268, w1=0.1917756299543623\n",
      "Gradient Descent(7930/9999): loss=124.70406137970086, w0=0.019149590235005235, w1=0.19175611690746064\n",
      "Gradient Descent(7931/9999): loss=124.68239240702117, w0=0.019148110333612224, w1=0.19173660569993173\n",
      "Gradient Descent(7932/9999): loss=124.66072827930508, w0=0.01914663057063281, w1=0.1917170963315976\n",
      "Gradient Descent(7933/9999): loss=124.63906899536882, w0=0.019145150946052042, w1=0.19169758880228027\n",
      "Gradient Descent(7934/9999): loss=124.61741455402891, w0=0.019143671459854975, w1=0.19167808311180182\n",
      "Gradient Descent(7935/9999): loss=124.59576495410214, w0=0.019142192112026654, w1=0.1916585792599843\n",
      "Gradient Descent(7936/9999): loss=124.57412019440565, w0=0.01914071290255214, w1=0.19163907724664986\n",
      "Gradient Descent(7937/9999): loss=124.55248027375688, w0=0.01913923383141649, w1=0.19161957707162058\n",
      "Gradient Descent(7938/9999): loss=124.53084519097354, w0=0.01913775489860476, w1=0.1916000787347186\n",
      "Gradient Descent(7939/9999): loss=124.50921494487366, w0=0.01913627610410201, w1=0.19158058223576604\n",
      "Gradient Descent(7940/9999): loss=124.48758953427561, w0=0.0191347974478933, w1=0.19156108757458512\n",
      "Gradient Descent(7941/9999): loss=124.465968957998, w0=0.0191333189299637, w1=0.19154159475099797\n",
      "Gradient Descent(7942/9999): loss=124.44435321485983, w0=0.01913184055029827, w1=0.1915221037648268\n",
      "Gradient Descent(7943/9999): loss=124.42274230368035, w0=0.019130362308882078, w1=0.19150261461589385\n",
      "Gradient Descent(7944/9999): loss=124.40113622327918, w0=0.019128884205700195, w1=0.19148312730402134\n",
      "Gradient Descent(7945/9999): loss=124.37953497247612, w0=0.019127406240737693, w1=0.19146364182903153\n",
      "Gradient Descent(7946/9999): loss=124.3579385500913, w0=0.019125928413979646, w1=0.19144415819074664\n",
      "Gradient Descent(7947/9999): loss=124.33634695494538, w0=0.019124450725411128, w1=0.191424676388989\n",
      "Gradient Descent(7948/9999): loss=124.31476018585899, w0=0.019122973175017214, w1=0.1914051964235809\n",
      "Gradient Descent(7949/9999): loss=124.29317824165328, w0=0.019121495762782985, w1=0.19138571829434464\n",
      "Gradient Descent(7950/9999): loss=124.27160112114969, w0=0.019120018488693522, w1=0.19136624200110258\n",
      "Gradient Descent(7951/9999): loss=124.25002882316984, w0=0.019118541352733907, w1=0.19134676754367705\n",
      "Gradient Descent(7952/9999): loss=124.22846134653581, w0=0.019117064354889222, w1=0.1913272949218904\n",
      "Gradient Descent(7953/9999): loss=124.20689869006985, w0=0.019115587495144557, w1=0.19130782413556505\n",
      "Gradient Descent(7954/9999): loss=124.18534085259462, w0=0.019114110773485, w1=0.1912883551845234\n",
      "Gradient Descent(7955/9999): loss=124.16378783293302, w0=0.019112634189895634, w1=0.19126888806858783\n",
      "Gradient Descent(7956/9999): loss=124.14223962990829, w0=0.01911115774436156, w1=0.1912494227875808\n",
      "Gradient Descent(7957/9999): loss=124.12069624234397, w0=0.019109681436867868, w1=0.19122995934132478\n",
      "Gradient Descent(7958/9999): loss=124.0991576690639, w0=0.019108205267399658, w1=0.1912104977296422\n",
      "Gradient Descent(7959/9999): loss=124.07762390889219, w0=0.01910672923594202, w1=0.19119103795235554\n",
      "Gradient Descent(7960/9999): loss=124.0560949606533, w0=0.019105253342480057, w1=0.19117158000928733\n",
      "Gradient Descent(7961/9999): loss=124.034570823172, w0=0.019103777586998872, w1=0.1911521239002601\n",
      "Gradient Descent(7962/9999): loss=124.01305149527333, w0=0.01910230196948357, w1=0.19113266962509634\n",
      "Gradient Descent(7963/9999): loss=123.99153697578261, w0=0.01910082648991925, w1=0.19111321718361865\n",
      "Gradient Descent(7964/9999): loss=123.97002726352551, w0=0.01909935114829102, w1=0.19109376657564958\n",
      "Gradient Descent(7965/9999): loss=123.94852235732806, w0=0.019097875944583988, w1=0.19107431780101172\n",
      "Gradient Descent(7966/9999): loss=123.92702225601646, w0=0.019096400878783268, w1=0.19105487085952766\n",
      "Gradient Descent(7967/9999): loss=123.90552695841727, w0=0.019094925950873972, w1=0.19103542575102003\n",
      "Gradient Descent(7968/9999): loss=123.88403646335752, w0=0.019093451160841213, w1=0.19101598247531146\n",
      "Gradient Descent(7969/9999): loss=123.86255076966413, w0=0.019091976508670106, w1=0.1909965410322246\n",
      "Gradient Descent(7970/9999): loss=123.84106987616474, w0=0.01909050199434577, w1=0.19097710142158214\n",
      "Gradient Descent(7971/9999): loss=123.81959378168709, w0=0.01908902761785333, w1=0.19095766364320674\n",
      "Gradient Descent(7972/9999): loss=123.79812248505932, w0=0.019087553379177898, w1=0.19093822769692112\n",
      "Gradient Descent(7973/9999): loss=123.7766559851098, w0=0.019086079278304604, w1=0.190918793582548\n",
      "Gradient Descent(7974/9999): loss=123.75519428066717, w0=0.01908460531521857, w1=0.19089936129991014\n",
      "Gradient Descent(7975/9999): loss=123.73373737056049, w0=0.01908313148990493, w1=0.19087993084883026\n",
      "Gradient Descent(7976/9999): loss=123.71228525361903, w0=0.019081657802348804, w1=0.19086050222913112\n",
      "Gradient Descent(7977/9999): loss=123.69083792867235, w0=0.01908018425253533, w1=0.19084107544063553\n",
      "Gradient Descent(7978/9999): loss=123.6693953945504, w0=0.019078710840449636, w1=0.1908216504831663\n",
      "Gradient Descent(7979/9999): loss=123.64795765008346, w0=0.01907723756607686, w1=0.19080222735654623\n",
      "Gradient Descent(7980/9999): loss=123.62652469410189, w0=0.019075764429402137, w1=0.1907828060605982\n",
      "Gradient Descent(7981/9999): loss=123.60509652543666, w0=0.019074291430410607, w1=0.19076338659514502\n",
      "Gradient Descent(7982/9999): loss=123.58367314291874, w0=0.019072818569087407, w1=0.19074396896000959\n",
      "Gradient Descent(7983/9999): loss=123.56225454537962, w0=0.019071345845417684, w1=0.19072455315501477\n",
      "Gradient Descent(7984/9999): loss=123.54084073165102, w0=0.01906987325938658, w1=0.1907051391799835\n",
      "Gradient Descent(7985/9999): loss=123.51943170056495, w0=0.019068400810979238, w1=0.19068572703473868\n",
      "Gradient Descent(7986/9999): loss=123.49802745095376, w0=0.01906692850018081, w1=0.19066631671910325\n",
      "Gradient Descent(7987/9999): loss=123.47662798165, w0=0.019065456326976444, w1=0.19064690823290018\n",
      "Gradient Descent(7988/9999): loss=123.45523329148665, w0=0.01906398429135129, w1=0.19062750157595243\n",
      "Gradient Descent(7989/9999): loss=123.43384337929693, w0=0.0190625123932905, w1=0.190608096748083\n",
      "Gradient Descent(7990/9999): loss=123.41245824391437, w0=0.019061040632779233, w1=0.19058869374911488\n",
      "Gradient Descent(7991/9999): loss=123.39107788417286, w0=0.019059569009802647, w1=0.19056929257887112\n",
      "Gradient Descent(7992/9999): loss=123.3697022989064, w0=0.019058097524345896, w1=0.19054989323717475\n",
      "Gradient Descent(7993/9999): loss=123.34833148694956, w0=0.019056626176394146, w1=0.1905304957238488\n",
      "Gradient Descent(7994/9999): loss=123.326965447137, w0=0.019055154965932554, w1=0.19051110003871638\n",
      "Gradient Descent(7995/9999): loss=123.30560417830377, w0=0.019053683892946288, w1=0.19049170618160055\n",
      "Gradient Descent(7996/9999): loss=123.28424767928517, w0=0.019052212957420513, w1=0.19047231415232443\n",
      "Gradient Descent(7997/9999): loss=123.26289594891693, w0=0.019050742159340397, w1=0.19045292395071117\n",
      "Gradient Descent(7998/9999): loss=123.24154898603499, w0=0.019049271498691112, w1=0.19043353557658388\n",
      "Gradient Descent(7999/9999): loss=123.2202067894755, w0=0.01904780097545783, w1=0.19041414902976572\n",
      "Gradient Descent(8000/9999): loss=123.19886935807497, w0=0.019046330589625723, w1=0.19039476431007987\n",
      "Gradient Descent(8001/9999): loss=123.17753669067042, w0=0.019044860341179966, w1=0.19037538141734953\n",
      "Gradient Descent(8002/9999): loss=123.15620878609884, w0=0.019043390230105737, w1=0.19035600035139788\n",
      "Gradient Descent(8003/9999): loss=123.13488564319779, w0=0.019041920256388217, w1=0.19033662111204816\n",
      "Gradient Descent(8004/9999): loss=123.11356726080491, w0=0.019040450420012584, w1=0.19031724369912362\n",
      "Gradient Descent(8005/9999): loss=123.09225363775828, w0=0.019038980720964025, w1=0.1902978681124475\n",
      "Gradient Descent(8006/9999): loss=123.07094477289627, w0=0.01903751115922772, w1=0.1902784943518431\n",
      "Gradient Descent(8007/9999): loss=123.04964066505747, w0=0.019036041734788862, w1=0.19025912241713372\n",
      "Gradient Descent(8008/9999): loss=123.02834131308092, w0=0.019034572447632635, w1=0.19023975230814263\n",
      "Gradient Descent(8009/9999): loss=123.00704671580583, w0=0.01903310329774423, w1=0.19022038402469318\n",
      "Gradient Descent(8010/9999): loss=122.98575687207168, w0=0.01903163428510884, w1=0.1902010175666087\n",
      "Gradient Descent(8011/9999): loss=122.96447178071838, w0=0.01903016540971166, w1=0.19018165293371256\n",
      "Gradient Descent(8012/9999): loss=122.94319144058605, w0=0.019028696671537882, w1=0.19016229012582814\n",
      "Gradient Descent(8013/9999): loss=122.92191585051516, w0=0.019027228070572708, w1=0.19014292914277883\n",
      "Gradient Descent(8014/9999): loss=122.90064500934645, w0=0.019025759606801335, w1=0.19012356998438804\n",
      "Gradient Descent(8015/9999): loss=122.87937891592095, w0=0.019024291280208962, w1=0.19010421265047917\n",
      "Gradient Descent(8016/9999): loss=122.85811756908005, w0=0.019022823090780797, w1=0.1900848571408757\n",
      "Gradient Descent(8017/9999): loss=122.83686096766529, w0=0.019021355038502046, w1=0.19006550345540105\n",
      "Gradient Descent(8018/9999): loss=122.81560911051871, w0=0.019019887123357913, w1=0.19004615159387872\n",
      "Gradient Descent(8019/9999): loss=122.79436199648251, w0=0.019018419345333607, w1=0.19002680155613222\n",
      "Gradient Descent(8020/9999): loss=122.7731196243993, w0=0.01901695170441434, w1=0.19000745334198504\n",
      "Gradient Descent(8021/9999): loss=122.75188199311184, w0=0.019015484200585325, w1=0.1899881069512607\n",
      "Gradient Descent(8022/9999): loss=122.73064910146329, w0=0.019014016833831776, w1=0.18996876238378277\n",
      "Gradient Descent(8023/9999): loss=122.70942094829714, w0=0.01901254960413891, w1=0.18994941963937478\n",
      "Gradient Descent(8024/9999): loss=122.68819753245698, w0=0.01901108251149194, w1=0.18993007871786033\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(8025/9999): loss=122.66697885278707, w0=0.019009615555876092, w1=0.189910739619063\n",
      "Gradient Descent(8026/9999): loss=122.6457649081316, w0=0.019008148737276586, w1=0.18989140234280638\n",
      "Gradient Descent(8027/9999): loss=122.6245556973352, w0=0.019006682055678646, w1=0.18987206688891414\n",
      "Gradient Descent(8028/9999): loss=122.60335121924281, w0=0.019005215511067496, w1=0.18985273325720992\n",
      "Gradient Descent(8029/9999): loss=122.58215147269973, w0=0.019003749103428367, w1=0.18983340144751737\n",
      "Gradient Descent(8030/9999): loss=122.56095645655148, w0=0.01900228283274648, w1=0.18981407145966014\n",
      "Gradient Descent(8031/9999): loss=122.5397661696438, w0=0.019000816699007075, w1=0.18979474329346196\n",
      "Gradient Descent(8032/9999): loss=122.51858061082294, w0=0.01899935070219538, w1=0.18977541694874653\n",
      "Gradient Descent(8033/9999): loss=122.49739977893523, w0=0.01899788484229663, w1=0.18975609242533759\n",
      "Gradient Descent(8034/9999): loss=122.47622367282739, w0=0.018996419119296063, w1=0.18973676972305886\n",
      "Gradient Descent(8035/9999): loss=122.45505229134653, w0=0.018994953533178916, w1=0.18971744884173414\n",
      "Gradient Descent(8036/9999): loss=122.43388563333988, w0=0.01899348808393043, w1=0.18969812978118716\n",
      "Gradient Descent(8037/9999): loss=122.4127236976551, w0=0.01899202277153585, w1=0.18967881254124175\n",
      "Gradient Descent(8038/9999): loss=122.3915664831401, w0=0.018990557595980416, w1=0.1896594971217217\n",
      "Gradient Descent(8039/9999): loss=122.37041398864305, w0=0.018989092557249374, w1=0.18964018352245085\n",
      "Gradient Descent(8040/9999): loss=122.3492662130125, w0=0.018987627655327972, w1=0.18962087174325304\n",
      "Gradient Descent(8041/9999): loss=122.32812315509724, w0=0.018986162890201462, w1=0.18960156178395213\n",
      "Gradient Descent(8042/9999): loss=122.30698481374642, w0=0.018984698261855094, w1=0.189582253644372\n",
      "Gradient Descent(8043/9999): loss=122.28585118780943, w0=0.01898323377027412, w1=0.18956294732433657\n",
      "Gradient Descent(8044/9999): loss=122.26472227613591, w0=0.018981769415443796, w1=0.18954364282366973\n",
      "Gradient Descent(8045/9999): loss=122.24359807757591, w0=0.01898030519734938, w1=0.1895243401421954\n",
      "Gradient Descent(8046/9999): loss=122.22247859097972, w0=0.018978841115976128, w1=0.18950503927973755\n",
      "Gradient Descent(8047/9999): loss=122.20136381519787, w0=0.0189773771713093, w1=0.18948574023612014\n",
      "Gradient Descent(8048/9999): loss=122.18025374908136, w0=0.018975913363334162, w1=0.1894664430111671\n",
      "Gradient Descent(8049/9999): loss=122.15914839148127, w0=0.018974449692035978, w1=0.1894471476047025\n",
      "Gradient Descent(8050/9999): loss=122.13804774124911, w0=0.01897298615740001, w1=0.1894278540165503\n",
      "Gradient Descent(8051/9999): loss=122.1169517972367, w0=0.01897152275941153, w1=0.18940856224653455\n",
      "Gradient Descent(8052/9999): loss=122.0958605582961, w0=0.018970059498055806, w1=0.18938927229447927\n",
      "Gradient Descent(8053/9999): loss=122.07477402327963, w0=0.01896859637331811, w1=0.18936998416020856\n",
      "Gradient Descent(8054/9999): loss=122.05369219104006, w0=0.018967133385183712, w1=0.1893506978435465\n",
      "Gradient Descent(8055/9999): loss=122.03261506043023, w0=0.01896567053363789, w1=0.18933141334431716\n",
      "Gradient Descent(8056/9999): loss=122.01154263030345, w0=0.018964207818665924, w1=0.18931213066234465\n",
      "Gradient Descent(8057/9999): loss=121.99047489951333, w0=0.018962745240253088, w1=0.18929284979745312\n",
      "Gradient Descent(8058/9999): loss=121.96941186691369, w0=0.018961282798384667, w1=0.1892735707494667\n",
      "Gradient Descent(8059/9999): loss=121.9483535313586, w0=0.01895982049304594, w1=0.18925429351820958\n",
      "Gradient Descent(8060/9999): loss=121.92729989170263, w0=0.018958358324222192, w1=0.1892350181035059\n",
      "Gradient Descent(8061/9999): loss=121.90625094680047, w0=0.01895689629189871, w1=0.1892157445051799\n",
      "Gradient Descent(8062/9999): loss=121.88520669550708, w0=0.018955434396060785, w1=0.18919647272305576\n",
      "Gradient Descent(8063/9999): loss=121.86416713667792, w0=0.018953972636693704, w1=0.18917720275695774\n",
      "Gradient Descent(8064/9999): loss=121.84313226916856, w0=0.018952511013782758, w1=0.18915793460671004\n",
      "Gradient Descent(8065/9999): loss=121.8221020918349, w0=0.018951049527313243, w1=0.18913866827213696\n",
      "Gradient Descent(8066/9999): loss=121.80107660353322, w0=0.01894958817727045, w1=0.18911940375306277\n",
      "Gradient Descent(8067/9999): loss=121.78005580311995, w0=0.018948126963639685, w1=0.18910014104931178\n",
      "Gradient Descent(8068/9999): loss=121.75903968945202, w0=0.018946665886406238, w1=0.1890808801607083\n",
      "Gradient Descent(8069/9999): loss=121.7380282613864, w0=0.018945204945555412, w1=0.18906162108707664\n",
      "Gradient Descent(8070/9999): loss=121.71702151778057, w0=0.018943744141072513, w1=0.18904236382824116\n",
      "Gradient Descent(8071/9999): loss=121.69601945749224, w0=0.018942283472942842, w1=0.18902310838402622\n",
      "Gradient Descent(8072/9999): loss=121.67502207937933, w0=0.018940822941151708, w1=0.1890038547542562\n",
      "Gradient Descent(8073/9999): loss=121.65402938230017, w0=0.01893936254568442, w1=0.18898460293875552\n",
      "Gradient Descent(8074/9999): loss=121.6330413651134, w0=0.018937902286526285, w1=0.18896535293734856\n",
      "Gradient Descent(8075/9999): loss=121.6120580266778, w0=0.018936442163662616, w1=0.18894610474985976\n",
      "Gradient Descent(8076/9999): loss=121.59107936585252, w0=0.01893498217707873, w1=0.1889268583761136\n",
      "Gradient Descent(8077/9999): loss=121.57010538149714, w0=0.01893352232675994, w1=0.1889076138159345\n",
      "Gradient Descent(8078/9999): loss=121.54913607247133, w0=0.018932062612691564, w1=0.18888837106914694\n",
      "Gradient Descent(8079/9999): loss=121.52817143763518, w0=0.018930603034858922, w1=0.18886913013557544\n",
      "Gradient Descent(8080/9999): loss=121.50721147584902, w0=0.018929143593247332, w1=0.1888498910150445\n",
      "Gradient Descent(8081/9999): loss=121.48625618597349, w0=0.01892768428784212, w1=0.1888306537073787\n",
      "Gradient Descent(8082/9999): loss=121.46530556686955, w0=0.018926225118628605, w1=0.18881141821240252\n",
      "Gradient Descent(8083/9999): loss=121.4443596173984, w0=0.018924766085592123, w1=0.18879218452994057\n",
      "Gradient Descent(8084/9999): loss=121.4234183364216, w0=0.018923307188717996, w1=0.1887729526598174\n",
      "Gradient Descent(8085/9999): loss=121.40248172280096, w0=0.018921848427991557, w1=0.1887537226018576\n",
      "Gradient Descent(8086/9999): loss=121.38154977539857, w0=0.01892038980339814, w1=0.18873449435588582\n",
      "Gradient Descent(8087/9999): loss=121.36062249307685, w0=0.018918931314923074, w1=0.1887152679217267\n",
      "Gradient Descent(8088/9999): loss=121.33969987469851, w0=0.018917472962551696, w1=0.1886960432992048\n",
      "Gradient Descent(8089/9999): loss=121.31878191912652, w0=0.018916014746269343, w1=0.18867682048814488\n",
      "Gradient Descent(8090/9999): loss=121.2978686252242, w0=0.018914556666061358, w1=0.18865759948837157\n",
      "Gradient Descent(8091/9999): loss=121.2769599918551, w0=0.01891309872191308, w1=0.1886383802997096\n",
      "Gradient Descent(8092/9999): loss=121.25605601788308, w0=0.018911640913809855, w1=0.1886191629219837\n",
      "Gradient Descent(8093/9999): loss=121.23515670217239, w0=0.018910183241737023, w1=0.18859994735501853\n",
      "Gradient Descent(8094/9999): loss=121.2142620435874, w0=0.018908725705679932, w1=0.1885807335986389\n",
      "Gradient Descent(8095/9999): loss=121.19337204099288, w0=0.018907268305623934, w1=0.18856152165266954\n",
      "Gradient Descent(8096/9999): loss=121.17248669325396, w0=0.018905811041554376, w1=0.18854231151693523\n",
      "Gradient Descent(8097/9999): loss=121.15160599923591, w0=0.01890435391345661, w1=0.1885231031912608\n",
      "Gradient Descent(8098/9999): loss=121.13072995780439, w0=0.01890289692131599, w1=0.18850389667547104\n",
      "Gradient Descent(8099/9999): loss=121.1098585678252, w0=0.018901440065117875, w1=0.1884846919693908\n",
      "Gradient Descent(8100/9999): loss=121.0889918281648, w0=0.01889998334484762, w1=0.18846548907284488\n",
      "Gradient Descent(8101/9999): loss=121.06812973768949, w0=0.018898526760490586, w1=0.18844628798565818\n",
      "Gradient Descent(8102/9999): loss=121.04727229526617, w0=0.018897070312032135, w1=0.1884270887076556\n",
      "Gradient Descent(8103/9999): loss=121.02641949976191, w0=0.018895613999457626, w1=0.188407891238662\n",
      "Gradient Descent(8104/9999): loss=121.00557135004414, w0=0.018894157822752427, w1=0.18838869557850232\n",
      "Gradient Descent(8105/9999): loss=120.98472784498045, w0=0.018892701781901906, w1=0.18836950172700148\n",
      "Gradient Descent(8106/9999): loss=120.96388898343893, w0=0.018891245876891432, w1=0.18835030968398445\n",
      "Gradient Descent(8107/9999): loss=120.94305476428781, w0=0.01888979010770637, w1=0.18833111944927616\n",
      "Gradient Descent(8108/9999): loss=120.92222518639561, w0=0.0188883344743321, w1=0.1883119310227016\n",
      "Gradient Descent(8109/9999): loss=120.90140024863119, w0=0.018886878976753993, w1=0.18829274440408578\n",
      "Gradient Descent(8110/9999): loss=120.88057994986373, w0=0.018885423614957424, w1=0.1882735595932537\n",
      "Gradient Descent(8111/9999): loss=120.8597642889626, w0=0.01888396838892777, w1=0.18825437659003041\n",
      "Gradient Descent(8112/9999): loss=120.83895326479765, w0=0.01888251329865041, w1=0.18823519539424094\n",
      "Gradient Descent(8113/9999): loss=120.81814687623876, w0=0.018881058344110732, w1=0.18821601600571036\n",
      "Gradient Descent(8114/9999): loss=120.79734512215626, w0=0.018879603525294112, w1=0.18819683842426377\n",
      "Gradient Descent(8115/9999): loss=120.77654800142078, w0=0.018878148842185936, w1=0.18817766264972624\n",
      "Gradient Descent(8116/9999): loss=120.7557555129033, w0=0.018876694294771592, w1=0.1881584886819229\n",
      "Gradient Descent(8117/9999): loss=120.73496765547489, w0=0.018875239883036472, w1=0.18813931652067886\n",
      "Gradient Descent(8118/9999): loss=120.71418442800707, w0=0.01887378560696596, w1=0.1881201461658193\n",
      "Gradient Descent(8119/9999): loss=120.69340582937157, w0=0.018872331466545454, w1=0.18810097761716935\n",
      "Gradient Descent(8120/9999): loss=120.6726318584405, w0=0.018870877461760346, w1=0.1880818108745542\n",
      "Gradient Descent(8121/9999): loss=120.65186251408616, w0=0.018869423592596034, w1=0.18806264593779906\n",
      "Gradient Descent(8122/9999): loss=120.63109779518123, w0=0.018867969859037916, w1=0.18804348280672914\n",
      "Gradient Descent(8123/9999): loss=120.61033770059865, w0=0.018866516261071387, w1=0.18802432148116968\n",
      "Gradient Descent(8124/9999): loss=120.58958222921156, w0=0.01886506279868185, w1=0.1880051619609459\n",
      "Gradient Descent(8125/9999): loss=120.56883137989361, w0=0.018863609471854715, w1=0.1879860042458831\n",
      "Gradient Descent(8126/9999): loss=120.54808515151846, w0=0.018862156280575377, w1=0.18796684833580654\n",
      "Gradient Descent(8127/9999): loss=120.52734354296031, w0=0.01886070322482925, w1=0.18794769423054153\n",
      "Gradient Descent(8128/9999): loss=120.50660655309352, w0=0.018859250304601737, w1=0.18792854192991337\n",
      "Gradient Descent(8129/9999): loss=120.48587418079275, w0=0.018857797519878254, w1=0.18790939143374738\n",
      "Gradient Descent(8130/9999): loss=120.46514642493297, w0=0.01885634487064421, w1=0.18789024274186894\n",
      "Gradient Descent(8131/9999): loss=120.44442328438942, w0=0.018854892356885023, w1=0.1878710958541034\n",
      "Gradient Descent(8132/9999): loss=120.42370475803763, w0=0.018853439978586108, w1=0.1878519507702761\n",
      "Gradient Descent(8133/9999): loss=120.40299084475353, w0=0.018851987735732878, w1=0.1878328074902125\n",
      "Gradient Descent(8134/9999): loss=120.38228154341319, w0=0.01885053562831076, w1=0.187813666013738\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(8135/9999): loss=120.361576852893, w0=0.01884908365630517, w1=0.187794526340678\n",
      "Gradient Descent(8136/9999): loss=120.34087677206968, w0=0.01884763181970153, w1=0.18777538847085798\n",
      "Gradient Descent(8137/9999): loss=120.32018129982025, w0=0.018846180118485267, w1=0.18775625240410337\n",
      "Gradient Descent(8138/9999): loss=120.29949043502202, w0=0.01884472855264181, w1=0.1877371181402397\n",
      "Gradient Descent(8139/9999): loss=120.27880417655248, w0=0.01884327712215659, w1=0.18771798567909243\n",
      "Gradient Descent(8140/9999): loss=120.2581225232896, w0=0.018841825827015032, w1=0.18769885502048708\n",
      "Gradient Descent(8141/9999): loss=120.23744547411144, w0=0.01884037466720257, w1=0.1876797261642492\n",
      "Gradient Descent(8142/9999): loss=120.21677302789656, w0=0.01883892364270464, w1=0.18766059911020427\n",
      "Gradient Descent(8143/9999): loss=120.1961051835236, w0=0.018837472753506678, w1=0.18764147385817792\n",
      "Gradient Descent(8144/9999): loss=120.17544193987158, w0=0.01883602199959412, w1=0.1876223504079957\n",
      "Gradient Descent(8145/9999): loss=120.15478329581987, w0=0.018834571380952406, w1=0.18760322875948326\n",
      "Gradient Descent(8146/9999): loss=120.13412925024801, w0=0.018833120897566977, w1=0.18758410891246616\n",
      "Gradient Descent(8147/9999): loss=120.11347980203597, w0=0.01883167054942328, w1=0.18756499086677003\n",
      "Gradient Descent(8148/9999): loss=120.09283495006385, w0=0.018830220336506755, w1=0.18754587462222053\n",
      "Gradient Descent(8149/9999): loss=120.07219469321217, w0=0.018828770258802852, w1=0.1875267601786433\n",
      "Gradient Descent(8150/9999): loss=120.05155903036172, w0=0.018827320316297018, w1=0.18750764753586407\n",
      "Gradient Descent(8151/9999): loss=120.03092796039348, w0=0.018825870508974704, w1=0.18748853669370852\n",
      "Gradient Descent(8152/9999): loss=120.0103014821888, w0=0.018824420836821364, w1=0.18746942765200234\n",
      "Gradient Descent(8153/9999): loss=119.98967959462931, w0=0.01882297129982245, w1=0.18745032041057128\n",
      "Gradient Descent(8154/9999): loss=119.96906229659692, w0=0.01882152189796342, w1=0.18743121496924106\n",
      "Gradient Descent(8155/9999): loss=119.94844958697384, w0=0.018820072631229732, w1=0.18741211132783747\n",
      "Gradient Descent(8156/9999): loss=119.92784146464254, w0=0.01881862349960684, w1=0.18739300948618628\n",
      "Gradient Descent(8157/9999): loss=119.9072379284859, w0=0.018817174503080212, w1=0.18737390944411328\n",
      "Gradient Descent(8158/9999): loss=119.88663897738681, w0=0.018815725641635306, w1=0.1873548112014443\n",
      "Gradient Descent(8159/9999): loss=119.86604461022873, w0=0.01881427691525759, w1=0.18733571475800515\n",
      "Gradient Descent(8160/9999): loss=119.8454548258953, w0=0.018812828323932534, w1=0.18731662011362168\n",
      "Gradient Descent(8161/9999): loss=119.82486962327044, w0=0.0188113798676456, w1=0.18729752726811977\n",
      "Gradient Descent(8162/9999): loss=119.80428900123837, w0=0.018809931546382264, w1=0.18727843622132528\n",
      "Gradient Descent(8163/9999): loss=119.7837129586836, w0=0.018808483360127994, w1=0.18725934697306412\n",
      "Gradient Descent(8164/9999): loss=119.76314149449094, w0=0.018807035308868267, w1=0.1872402595231622\n",
      "Gradient Descent(8165/9999): loss=119.7425746075454, w0=0.01880558739258856, w1=0.18722117387144543\n",
      "Gradient Descent(8166/9999): loss=119.72201229673247, w0=0.018804139611274348, w1=0.18720209001773977\n",
      "Gradient Descent(8167/9999): loss=119.70145456093769, w0=0.018802691964911113, w1=0.18718300796187118\n",
      "Gradient Descent(8168/9999): loss=119.68090139904707, w0=0.01880124445348433, w1=0.18716392770366563\n",
      "Gradient Descent(8169/9999): loss=119.66035280994683, w0=0.018799797076979492, w1=0.18714484924294914\n",
      "Gradient Descent(8170/9999): loss=119.63980879252344, w0=0.018798349835382075, w1=0.18712577257954768\n",
      "Gradient Descent(8171/9999): loss=119.61926934566377, w0=0.01879690272867757, w1=0.18710669771328733\n",
      "Gradient Descent(8172/9999): loss=119.59873446825492, w0=0.018795455756851466, w1=0.18708762464399412\n",
      "Gradient Descent(8173/9999): loss=119.57820415918425, w0=0.018794008919889253, w1=0.18706855337149408\n",
      "Gradient Descent(8174/9999): loss=119.55767841733937, w0=0.01879256221777642, w1=0.18704948389561332\n",
      "Gradient Descent(8175/9999): loss=119.5371572416083, w0=0.018791115650498465, w1=0.1870304162161779\n",
      "Gradient Descent(8176/9999): loss=119.51664063087928, w0=0.018789669218040882, w1=0.187011350333014\n",
      "Gradient Descent(8177/9999): loss=119.49612858404087, w0=0.01878822292038917, w1=0.1869922862459477\n",
      "Gradient Descent(8178/9999): loss=119.47562109998177, w0=0.01878677675752883, w1=0.18697322395480512\n",
      "Gradient Descent(8179/9999): loss=119.45511817759117, w0=0.018785330729445358, w1=0.18695416345941246\n",
      "Gradient Descent(8180/9999): loss=119.43461981575848, w0=0.018783884836124257, w1=0.18693510475959588\n",
      "Gradient Descent(8181/9999): loss=119.4141260133733, w0=0.018782439077551037, w1=0.1869160478551816\n",
      "Gradient Descent(8182/9999): loss=119.39363676932562, w0=0.0187809934537112, w1=0.1868969927459958\n",
      "Gradient Descent(8183/9999): loss=119.37315208250568, w0=0.018779547964590255, w1=0.18687793943186473\n",
      "Gradient Descent(8184/9999): loss=119.35267195180404, w0=0.018778102610173715, w1=0.18685888791261462\n",
      "Gradient Descent(8185/9999): loss=119.3321963761115, w0=0.018776657390447093, w1=0.18683983818807173\n",
      "Gradient Descent(8186/9999): loss=119.3117253543192, w0=0.0187752123053959, w1=0.18682079025806234\n",
      "Gradient Descent(8187/9999): loss=119.29125888531847, w0=0.01877376735500565, w1=0.18680174412241274\n",
      "Gradient Descent(8188/9999): loss=119.27079696800095, w0=0.018772322539261865, w1=0.18678269978094925\n",
      "Gradient Descent(8189/9999): loss=119.25033960125877, w0=0.01877087785815006, w1=0.1867636572334982\n",
      "Gradient Descent(8190/9999): loss=119.22988678398407, w0=0.01876943331165576, w1=0.1867446164798859\n",
      "Gradient Descent(8191/9999): loss=119.20943851506937, w0=0.01876798889976448, w1=0.18672557751993876\n",
      "Gradient Descent(8192/9999): loss=119.18899479340749, w0=0.018766544622461755, w1=0.18670654035348314\n",
      "Gradient Descent(8193/9999): loss=119.16855561789158, w0=0.018765100479733107, w1=0.18668750498034542\n",
      "Gradient Descent(8194/9999): loss=119.14812098741501, w0=0.018763656471564066, w1=0.186668471400352\n",
      "Gradient Descent(8195/9999): loss=119.12769090087149, w0=0.018762212597940156, w1=0.18664943961332933\n",
      "Gradient Descent(8196/9999): loss=119.10726535715489, w0=0.018760768858846914, w1=0.18663040961910385\n",
      "Gradient Descent(8197/9999): loss=119.08684435515954, w0=0.018759325254269873, w1=0.186611381417502\n",
      "Gradient Descent(8198/9999): loss=119.06642789377997, w0=0.018757881784194567, w1=0.18659235500835028\n",
      "Gradient Descent(8199/9999): loss=119.04601597191096, w0=0.018756438448606538, w1=0.18657333039147517\n",
      "Gradient Descent(8200/9999): loss=119.02560858844761, w0=0.01875499524749132, w1=0.1865543075667032\n",
      "Gradient Descent(8201/9999): loss=119.00520574228531, w0=0.018753552180834455, w1=0.18653528653386084\n",
      "Gradient Descent(8202/9999): loss=118.98480743231977, w0=0.018752109248621487, w1=0.18651626729277468\n",
      "Gradient Descent(8203/9999): loss=118.96441365744691, w0=0.01875066645083796, w1=0.18649724984327126\n",
      "Gradient Descent(8204/9999): loss=118.94402441656297, w0=0.01874922378746942, w1=0.18647823418517717\n",
      "Gradient Descent(8205/9999): loss=118.92363970856452, w0=0.018747781258501413, w1=0.186459220318319\n",
      "Gradient Descent(8206/9999): loss=118.9032595323483, w0=0.01874633886391949, w1=0.18644020824252333\n",
      "Gradient Descent(8207/9999): loss=118.88288388681143, w0=0.018744896603709205, w1=0.18642119795761683\n",
      "Gradient Descent(8208/9999): loss=118.86251277085134, w0=0.018743454477856112, w1=0.18640218946342613\n",
      "Gradient Descent(8209/9999): loss=118.84214618336564, w0=0.01874201248634576, w1=0.18638318275977786\n",
      "Gradient Descent(8210/9999): loss=118.82178412325229, w0=0.018740570629163712, w1=0.18636417784649872\n",
      "Gradient Descent(8211/9999): loss=118.80142658940954, w0=0.018739128906295525, w1=0.18634517472341539\n",
      "Gradient Descent(8212/9999): loss=118.78107358073586, w0=0.01873768731772676, w1=0.18632617339035457\n",
      "Gradient Descent(8213/9999): loss=118.76072509613007, w0=0.01873624586344298, w1=0.186307173847143\n",
      "Gradient Descent(8214/9999): loss=118.7403811344913, w0=0.018734804543429746, w1=0.1862881760936074\n",
      "Gradient Descent(8215/9999): loss=118.72004169471884, w0=0.018733363357672627, w1=0.18626918012957455\n",
      "Gradient Descent(8216/9999): loss=118.69970677571241, w0=0.01873192230615719, w1=0.18625018595487122\n",
      "Gradient Descent(8217/9999): loss=118.67937637637186, w0=0.018730481388869007, w1=0.1862311935693242\n",
      "Gradient Descent(8218/9999): loss=118.65905049559751, w0=0.018729040605793647, w1=0.18621220297276028\n",
      "Gradient Descent(8219/9999): loss=118.63872913228981, w0=0.018727599956916682, w1=0.1861932141650063\n",
      "Gradient Descent(8220/9999): loss=118.61841228534956, w0=0.01872615944222369, w1=0.1861742271458891\n",
      "Gradient Descent(8221/9999): loss=118.59809995367782, w0=0.018724719061700246, w1=0.18615524191523553\n",
      "Gradient Descent(8222/9999): loss=118.57779213617589, w0=0.01872327881533193, w1=0.18613625847287246\n",
      "Gradient Descent(8223/9999): loss=118.55748883174552, w0=0.018721838703104322, w1=0.18611727681862678\n",
      "Gradient Descent(8224/9999): loss=118.53719003928853, w0=0.018720398725003, w1=0.1860982969523254\n",
      "Gradient Descent(8225/9999): loss=118.51689575770716, w0=0.018718958881013555, w1=0.18607931887379525\n",
      "Gradient Descent(8226/9999): loss=118.49660598590384, w0=0.01871751917112157, w1=0.18606034258286325\n",
      "Gradient Descent(8227/9999): loss=118.47632072278148, w0=0.018716079595312633, w1=0.18604136807935637\n",
      "Gradient Descent(8228/9999): loss=118.45603996724296, w0=0.018714640153572333, w1=0.18602239536310158\n",
      "Gradient Descent(8229/9999): loss=118.43576371819172, w0=0.01871320084588626, w1=0.18600342443392587\n",
      "Gradient Descent(8230/9999): loss=118.41549197453135, w0=0.018711761672240007, w1=0.18598445529165625\n",
      "Gradient Descent(8231/9999): loss=118.39522473516571, w0=0.01871032263261917, w1=0.18596548793611972\n",
      "Gradient Descent(8232/9999): loss=118.37496199899903, w0=0.018708883727009344, w1=0.18594652236714335\n",
      "Gradient Descent(8233/9999): loss=118.35470376493578, w0=0.01870744495539613, w1=0.18592755858455415\n",
      "Gradient Descent(8234/9999): loss=118.33445003188066, w0=0.018706006317765124, w1=0.18590859658817924\n",
      "Gradient Descent(8235/9999): loss=118.31420079873871, w0=0.018704567814101934, w1=0.18588963637784567\n",
      "Gradient Descent(8236/9999): loss=118.29395606441527, w0=0.018703129444392157, w1=0.18587067795338055\n",
      "Gradient Descent(8237/9999): loss=118.27371582781585, w0=0.018701691208621404, w1=0.18585172131461103\n",
      "Gradient Descent(8238/9999): loss=118.25348008784641, w0=0.01870025310677528, w1=0.1858327664613642\n",
      "Gradient Descent(8239/9999): loss=118.23324884341308, w0=0.018698815138839392, w1=0.18581381339346725\n",
      "Gradient Descent(8240/9999): loss=118.21302209342232, w0=0.018697377304799354, w1=0.18579486211074733\n",
      "Gradient Descent(8241/9999): loss=118.19279983678084, w0=0.018695939604640777, w1=0.18577591261303164\n",
      "Gradient Descent(8242/9999): loss=118.17258207239556, w0=0.018694502038349277, w1=0.1857569649001474\n",
      "Gradient Descent(8243/9999): loss=118.15236879917389, w0=0.01869306460591047, w1=0.1857380189719218\n",
      "Gradient Descent(8244/9999): loss=118.13216001602328, w0=0.018691627307309975, w1=0.1857190748281821\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(8245/9999): loss=118.11195572185167, w0=0.018690190142533408, w1=0.18570013246875552\n",
      "Gradient Descent(8246/9999): loss=118.09175591556719, w0=0.018688753111566394, w1=0.18568119189346935\n",
      "Gradient Descent(8247/9999): loss=118.07156059607813, w0=0.018687316214394553, w1=0.18566225310215087\n",
      "Gradient Descent(8248/9999): loss=118.05136976229328, w0=0.018685879451003515, w1=0.1856433160946274\n",
      "Gradient Descent(8249/9999): loss=118.03118341312158, w0=0.018684442821378905, w1=0.18562438087072622\n",
      "Gradient Descent(8250/9999): loss=118.0110015474723, w0=0.01868300632550635, w1=0.18560544743027468\n",
      "Gradient Descent(8251/9999): loss=117.99082416425497, w0=0.018681569963371482, w1=0.18558651577310017\n",
      "Gradient Descent(8252/9999): loss=117.97065126237939, w0=0.018680133734959935, w1=0.18556758589903002\n",
      "Gradient Descent(8253/9999): loss=117.9504828407557, w0=0.01867869764025734, w1=0.18554865780789162\n",
      "Gradient Descent(8254/9999): loss=117.93031889829422, w0=0.018677261679249338, w1=0.18552973149951238\n",
      "Gradient Descent(8255/9999): loss=117.9101594339056, w0=0.01867582585192156, w1=0.1855108069737197\n",
      "Gradient Descent(8256/9999): loss=117.89000444650082, w0=0.018674390158259647, w1=0.18549188423034102\n",
      "Gradient Descent(8257/9999): loss=117.86985393499113, w0=0.018672954598249244, w1=0.18547296326920382\n",
      "Gradient Descent(8258/9999): loss=117.8497078982879, w0=0.01867151917187599, w1=0.18545404409013552\n",
      "Gradient Descent(8259/9999): loss=117.82956633530304, w0=0.018670083879125533, w1=0.18543512669296364\n",
      "Gradient Descent(8260/9999): loss=117.80942924494856, w0=0.01866864871998352, w1=0.18541621107751566\n",
      "Gradient Descent(8261/9999): loss=117.78929662613679, w0=0.018667213694435598, w1=0.18539729724361909\n",
      "Gradient Descent(8262/9999): loss=117.76916847778038, w0=0.018665778802467413, w1=0.18537838519110147\n",
      "Gradient Descent(8263/9999): loss=117.74904479879221, w0=0.018664344044064623, w1=0.18535947491979038\n",
      "Gradient Descent(8264/9999): loss=117.7289255880855, w0=0.018662909419212878, w1=0.18534056642951335\n",
      "Gradient Descent(8265/9999): loss=117.70881084457366, w0=0.018661474927897836, w1=0.18532165972009798\n",
      "Gradient Descent(8266/9999): loss=117.6887005671704, w0=0.018660040570105153, w1=0.18530275479137187\n",
      "Gradient Descent(8267/9999): loss=117.66859475478982, w0=0.01865860634582049, w1=0.1852838516431626\n",
      "Gradient Descent(8268/9999): loss=117.64849340634622, w0=0.01865717225502951, w1=0.18526495027529785\n",
      "Gradient Descent(8269/9999): loss=117.62839652075407, w0=0.018655738297717873, w1=0.18524605068760525\n",
      "Gradient Descent(8270/9999): loss=117.60830409692831, w0=0.01865430447387124, w1=0.18522715287991245\n",
      "Gradient Descent(8271/9999): loss=117.58821613378413, w0=0.01865287078347528, w1=0.18520825685204714\n",
      "Gradient Descent(8272/9999): loss=117.56813263023685, w0=0.018651437226515664, w1=0.18518936260383703\n",
      "Gradient Descent(8273/9999): loss=117.54805358520225, w0=0.018650003802978057, w1=0.18517047013510982\n",
      "Gradient Descent(8274/9999): loss=117.52797899759622, w0=0.018648570512848132, w1=0.18515157944569324\n",
      "Gradient Descent(8275/9999): loss=117.50790886633509, w0=0.018647137356111565, w1=0.18513269053541503\n",
      "Gradient Descent(8276/9999): loss=117.4878431903354, w0=0.018645704332754028, w1=0.185113803404103\n",
      "Gradient Descent(8277/9999): loss=117.46778196851388, w0=0.018644271442761203, w1=0.18509491805158487\n",
      "Gradient Descent(8278/9999): loss=117.4477251997877, w0=0.01864283868611876, w1=0.18507603447768847\n",
      "Gradient Descent(8279/9999): loss=117.4276728830742, w0=0.018641406062812386, w1=0.1850571526822416\n",
      "Gradient Descent(8280/9999): loss=117.40762501729111, w0=0.01863997357282776, w1=0.18503827266507208\n",
      "Gradient Descent(8281/9999): loss=117.3875816013562, w0=0.01863854121615057, w1=0.1850193944260078\n",
      "Gradient Descent(8282/9999): loss=117.36754263418783, w0=0.0186371089927665, w1=0.18500051796487657\n",
      "Gradient Descent(8283/9999): loss=117.3475081147044, w0=0.018635676902661238, w1=0.1849816432815063\n",
      "Gradient Descent(8284/9999): loss=117.3274780418247, w0=0.018634244945820472, w1=0.18496277037572487\n",
      "Gradient Descent(8285/9999): loss=117.3074524144678, w0=0.018632813122229894, w1=0.18494389924736018\n",
      "Gradient Descent(8286/9999): loss=117.28743123155301, w0=0.018631381431875198, w1=0.18492502989624018\n",
      "Gradient Descent(8287/9999): loss=117.2674144919999, w0=0.018629949874742077, w1=0.18490616232219284\n",
      "Gradient Descent(8288/9999): loss=117.24740219472841, w0=0.018628518450816225, w1=0.18488729652504607\n",
      "Gradient Descent(8289/9999): loss=117.22739433865864, w0=0.018627087160083346, w1=0.18486843250462787\n",
      "Gradient Descent(8290/9999): loss=117.207390922711, w0=0.018625656002529137, w1=0.18484957026076623\n",
      "Gradient Descent(8291/9999): loss=117.18739194580634, w0=0.0186242249781393, w1=0.18483070979328917\n",
      "Gradient Descent(8292/9999): loss=117.16739740686552, w0=0.018622794086899538, w1=0.1848118511020247\n",
      "Gradient Descent(8293/9999): loss=117.14740730480986, w0=0.01862136332879556, w1=0.18479299418680087\n",
      "Gradient Descent(8294/9999): loss=117.1274216385609, w0=0.01861993270381307, w1=0.18477413904744575\n",
      "Gradient Descent(8295/9999): loss=117.10744040704044, w0=0.01861850221193778, w1=0.1847552856837874\n",
      "Gradient Descent(8296/9999): loss=117.08746360917064, w0=0.018617071853155396, w1=0.18473643409565393\n",
      "Gradient Descent(8297/9999): loss=117.06749124387387, w0=0.01861564162745163, w1=0.18471758428287344\n",
      "Gradient Descent(8298/9999): loss=117.04752331007275, w0=0.018614211534812204, w1=0.18469873624527405\n",
      "Gradient Descent(8299/9999): loss=117.02755980669023, w0=0.018612781575222826, w1=0.1846798899826839\n",
      "Gradient Descent(8300/9999): loss=117.00760073264954, w0=0.01861135174866922, w1=0.18466104549493115\n",
      "Gradient Descent(8301/9999): loss=116.98764608687415, w0=0.018609922055137097, w1=0.18464220278184398\n",
      "Gradient Descent(8302/9999): loss=116.9676958682878, w0=0.01860849249461219, w1=0.1846233618432506\n",
      "Gradient Descent(8303/9999): loss=116.9477500758146, w0=0.018607063067080213, w1=0.18460452267897917\n",
      "Gradient Descent(8304/9999): loss=116.92780870837885, w0=0.018605633772526897, w1=0.18458568528885794\n",
      "Gradient Descent(8305/9999): loss=116.90787176490515, w0=0.018604204610937963, w1=0.18456684967271514\n",
      "Gradient Descent(8306/9999): loss=116.88793924431835, w0=0.01860277558229914, w1=0.18454801583037905\n",
      "Gradient Descent(8307/9999): loss=116.86801114554363, w0=0.018601346686596167, w1=0.18452918376167793\n",
      "Gradient Descent(8308/9999): loss=116.84808746750637, w0=0.018599917923814765, w1=0.18451035346644007\n",
      "Gradient Descent(8309/9999): loss=116.82816820913233, w0=0.018598489293940673, w1=0.18449152494449378\n",
      "Gradient Descent(8310/9999): loss=116.80825336934753, w0=0.018597060796959625, w1=0.18447269819566736\n",
      "Gradient Descent(8311/9999): loss=116.78834294707808, w0=0.018595632432857362, w1=0.18445387321978918\n",
      "Gradient Descent(8312/9999): loss=116.7684369412507, w0=0.01859420420161962, w1=0.18443505001668759\n",
      "Gradient Descent(8313/9999): loss=116.74853535079207, w0=0.01859277610323214, w1=0.18441622858619094\n",
      "Gradient Descent(8314/9999): loss=116.72863817462931, w0=0.018591348137680663, w1=0.18439740892812764\n",
      "Gradient Descent(8315/9999): loss=116.70874541168986, w0=0.018589920304950935, w1=0.1843785910423261\n",
      "Gradient Descent(8316/9999): loss=116.68885706090124, w0=0.018588492605028706, w1=0.18435977492861472\n",
      "Gradient Descent(8317/9999): loss=116.66897312119143, w0=0.018587065037899718, w1=0.18434096058682195\n",
      "Gradient Descent(8318/9999): loss=116.64909359148865, w0=0.018585637603549726, w1=0.18432214801677624\n",
      "Gradient Descent(8319/9999): loss=116.62921847072128, w0=0.018584210301964478, w1=0.18430333721830605\n",
      "Gradient Descent(8320/9999): loss=116.60934775781818, w0=0.018582783133129728, w1=0.18428452819123986\n",
      "Gradient Descent(8321/9999): loss=116.58948145170827, w0=0.018581356097031234, w1=0.1842657209354062\n",
      "Gradient Descent(8322/9999): loss=116.5696195513209, w0=0.018579929193654747, w1=0.18424691545063357\n",
      "Gradient Descent(8323/9999): loss=116.54976205558559, w0=0.01857850242298603, w1=0.18422811173675052\n",
      "Gradient Descent(8324/9999): loss=116.52990896343226, w0=0.01857707578501084, w1=0.1842093097935856\n",
      "Gradient Descent(8325/9999): loss=116.51006027379097, w0=0.018575649279714942, w1=0.18419050962096736\n",
      "Gradient Descent(8326/9999): loss=116.49021598559219, w0=0.0185742229070841, w1=0.18417171121872442\n",
      "Gradient Descent(8327/9999): loss=116.4703760977665, w0=0.01857279666710408, w1=0.18415291458668534\n",
      "Gradient Descent(8328/9999): loss=116.45054060924498, w0=0.018571370559760648, w1=0.18413411972467877\n",
      "Gradient Descent(8329/9999): loss=116.43070951895868, w0=0.018569944585039574, w1=0.18411532663253333\n",
      "Gradient Descent(8330/9999): loss=116.41088282583922, w0=0.01856851874292663, w1=0.18409653531007766\n",
      "Gradient Descent(8331/9999): loss=116.39106052881834, w0=0.018567093033407582, w1=0.18407774575714045\n",
      "Gradient Descent(8332/9999): loss=116.37124262682815, w0=0.01856566745646821, w1=0.18405895797355037\n",
      "Gradient Descent(8333/9999): loss=116.35142911880082, w0=0.018564242012094292, w1=0.18404017195913613\n",
      "Gradient Descent(8334/9999): loss=116.33162000366912, w0=0.018562816700271604, w1=0.18402138771372642\n",
      "Gradient Descent(8335/9999): loss=116.31181528036583, w0=0.018561391520985925, w1=0.18400260523715\n",
      "Gradient Descent(8336/9999): loss=116.29201494782409, w0=0.018559966474223036, w1=0.18398382452923562\n",
      "Gradient Descent(8337/9999): loss=116.2722190049774, w0=0.018558541559968725, w1=0.18396504558981203\n",
      "Gradient Descent(8338/9999): loss=116.25242745075941, w0=0.018557116778208774, w1=0.183946268418708\n",
      "Gradient Descent(8339/9999): loss=116.23264028410405, w0=0.018555692128928967, w1=0.18392749301575237\n",
      "Gradient Descent(8340/9999): loss=116.21285750394561, w0=0.018554267612115097, w1=0.1839087193807739\n",
      "Gradient Descent(8341/9999): loss=116.19307910921863, w0=0.01855284322775295, w1=0.18388994751360146\n",
      "Gradient Descent(8342/9999): loss=116.17330509885788, w0=0.018551418975828324, w1=0.1838711774140639\n",
      "Gradient Descent(8343/9999): loss=116.15353547179843, w0=0.01854999485632701, w1=0.18385240908199005\n",
      "Gradient Descent(8344/9999): loss=116.1337702269756, w0=0.0185485708692348, w1=0.18383364251720882\n",
      "Gradient Descent(8345/9999): loss=116.11400936332507, w0=0.018547147014537496, w1=0.1838148777195491\n",
      "Gradient Descent(8346/9999): loss=116.09425287978267, w0=0.018545723292220897, w1=0.1837961146888398\n",
      "Gradient Descent(8347/9999): loss=116.0745007752846, w0=0.0185442997022708, w1=0.18377735342490983\n",
      "Gradient Descent(8348/9999): loss=116.05475304876725, w0=0.018542876244673014, w1=0.18375859392758817\n",
      "Gradient Descent(8349/9999): loss=116.03500969916743, w0=0.018541452919413337, w1=0.18373983619670375\n",
      "Gradient Descent(8350/9999): loss=116.01527072542203, w0=0.01854002972647758, w1=0.18372108023208558\n",
      "Gradient Descent(8351/9999): loss=115.99553612646834, w0=0.01853860666585155, w1=0.18370232603356262\n",
      "Gradient Descent(8352/9999): loss=115.97580590124387, w0=0.018537183737521057, w1=0.1836835736009639\n",
      "Gradient Descent(8353/9999): loss=115.9560800486865, w0=0.01853576094147191, w1=0.18366482293411845\n",
      "Gradient Descent(8354/9999): loss=115.9363585677342, w0=0.018534338277689928, w1=0.18364607403285532\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(8355/9999): loss=115.91664145732544, w0=0.018532915746160922, w1=0.18362732689700356\n",
      "Gradient Descent(8356/9999): loss=115.8969287163988, w0=0.01853149334687071, w1=0.18360858152639226\n",
      "Gradient Descent(8357/9999): loss=115.87722034389311, w0=0.018530071079805106, w1=0.18358983792085048\n",
      "Gradient Descent(8358/9999): loss=115.85751633874766, w0=0.018528648944949933, w1=0.18357109608020736\n",
      "Gradient Descent(8359/9999): loss=115.8378166999018, w0=0.018527226942291017, w1=0.183552356004292\n",
      "Gradient Descent(8360/9999): loss=115.8181214262953, w0=0.01852580507181418, w1=0.18353361769293358\n",
      "Gradient Descent(8361/9999): loss=115.79843051686814, w0=0.018524383333505246, w1=0.1835148811459612\n",
      "Gradient Descent(8362/9999): loss=115.77874397056061, w0=0.018522961727350046, w1=0.18349614636320408\n",
      "Gradient Descent(8363/9999): loss=115.75906178631317, w0=0.0185215402533344, w1=0.18347741334449139\n",
      "Gradient Descent(8364/9999): loss=115.7393839630667, w0=0.018520118911444148, w1=0.18345868208965235\n",
      "Gradient Descent(8365/9999): loss=115.71971049976229, w0=0.01851869770166512, w1=0.1834399525985162\n",
      "Gradient Descent(8366/9999): loss=115.70004139534124, w0=0.01851727662398315, w1=0.18342122487091214\n",
      "Gradient Descent(8367/9999): loss=115.68037664874518, w0=0.018515855678384072, w1=0.18340249890666946\n",
      "Gradient Descent(8368/9999): loss=115.66071625891603, w0=0.01851443486485373, w1=0.1833837747056174\n",
      "Gradient Descent(8369/9999): loss=115.64106022479595, w0=0.01851301418337796, w1=0.18336505226758526\n",
      "Gradient Descent(8370/9999): loss=115.62140854532747, w0=0.018511593633942603, w1=0.18334633159240238\n",
      "Gradient Descent(8371/9999): loss=115.60176121945312, w0=0.018510173216533504, w1=0.18332761267989803\n",
      "Gradient Descent(8372/9999): loss=115.58211824611605, w0=0.018508752931136505, w1=0.18330889552990157\n",
      "Gradient Descent(8373/9999): loss=115.56247962425948, w0=0.018507332777737452, w1=0.18329018014224235\n",
      "Gradient Descent(8374/9999): loss=115.54284535282687, w0=0.0185059127563222, w1=0.18327146651674978\n",
      "Gradient Descent(8375/9999): loss=115.52321543076214, w0=0.018504492866876592, w1=0.1832527546532532\n",
      "Gradient Descent(8376/9999): loss=115.50358985700925, w0=0.018503073109386484, w1=0.18323404455158201\n",
      "Gradient Descent(8377/9999): loss=115.48396863051262, w0=0.01850165348383773, w1=0.18321533621156566\n",
      "Gradient Descent(8378/9999): loss=115.46435175021685, w0=0.018500233990216185, w1=0.18319662963303357\n",
      "Gradient Descent(8379/9999): loss=115.4447392150668, w0=0.018498814628507703, w1=0.1831779248158152\n",
      "Gradient Descent(8380/9999): loss=115.4251310240077, w0=0.018497395398698147, w1=0.18315922175974003\n",
      "Gradient Descent(8381/9999): loss=115.40552717598493, w0=0.018495976300773376, w1=0.18314052046463752\n",
      "Gradient Descent(8382/9999): loss=115.38592766994415, w0=0.018494557334719254, w1=0.1831218209303372\n",
      "Gradient Descent(8383/9999): loss=115.36633250483149, w0=0.018493138500521644, w1=0.18310312315666855\n",
      "Gradient Descent(8384/9999): loss=115.346741679593, w0=0.01849171979816641, w1=0.18308442714346113\n",
      "Gradient Descent(8385/9999): loss=115.32715519317536, w0=0.018490301227639425, w1=0.18306573289054448\n",
      "Gradient Descent(8386/9999): loss=115.30757304452521, w0=0.018488882788926558, w1=0.18304704039774816\n",
      "Gradient Descent(8387/9999): loss=115.28799523258974, w0=0.018487464482013674, w1=0.18302834966490175\n",
      "Gradient Descent(8388/9999): loss=115.26842175631622, w0=0.01848604630688665, w1=0.18300966069183489\n",
      "Gradient Descent(8389/9999): loss=115.24885261465225, w0=0.018484628263531363, w1=0.18299097347837714\n",
      "Gradient Descent(8390/9999): loss=115.22928780654571, w0=0.018483210351933687, w1=0.18297228802435817\n",
      "Gradient Descent(8391/9999): loss=115.20972733094474, w0=0.0184817925720795, w1=0.1829536043296076\n",
      "Gradient Descent(8392/9999): loss=115.19017118679773, w0=0.01848037492395468, w1=0.18293492239395515\n",
      "Gradient Descent(8393/9999): loss=115.17061937305338, w0=0.018478957407545116, w1=0.18291624221723043\n",
      "Gradient Descent(8394/9999): loss=115.1510718886607, w0=0.018477540022836688, w1=0.1828975637992632\n",
      "Gradient Descent(8395/9999): loss=115.13152873256882, w0=0.01847612276981528, w1=0.18287888713988312\n",
      "Gradient Descent(8396/9999): loss=115.11198990372729, w0=0.018474705648466774, w1=0.18286021223891996\n",
      "Gradient Descent(8397/9999): loss=115.09245540108583, w0=0.018473288658777065, w1=0.18284153909620343\n",
      "Gradient Descent(8398/9999): loss=115.07292522359451, w0=0.018471871800732044, w1=0.18282286771156334\n",
      "Gradient Descent(8399/9999): loss=115.05339937020364, w0=0.018470455074317604, w1=0.18280419808482942\n",
      "Gradient Descent(8400/9999): loss=115.03387783986382, w0=0.018469038479519636, w1=0.1827855302158315\n",
      "Gradient Descent(8401/9999): loss=115.01436063152582, w0=0.018467622016324035, w1=0.18276686410439935\n",
      "Gradient Descent(8402/9999): loss=114.99484774414078, w0=0.018466205684716703, w1=0.18274819975036286\n",
      "Gradient Descent(8403/9999): loss=114.97533917666009, w0=0.018464789484683535, w1=0.18272953715355184\n",
      "Gradient Descent(8404/9999): loss=114.95583492803539, w0=0.018463373416210436, w1=0.18271087631379612\n",
      "Gradient Descent(8405/9999): loss=114.93633499721864, w0=0.018461957479283305, w1=0.18269221723092563\n",
      "Gradient Descent(8406/9999): loss=114.91683938316196, w0=0.018460541673888047, w1=0.18267355990477024\n",
      "Gradient Descent(8407/9999): loss=114.89734808481792, w0=0.01845912600001057, w1=0.18265490433515985\n",
      "Gradient Descent(8408/9999): loss=114.8778611011392, w0=0.018457710457636785, w1=0.1826362505219244\n",
      "Gradient Descent(8409/9999): loss=114.8583784310787, w0=0.018456295046752596, w1=0.1826175984648938\n",
      "Gradient Descent(8410/9999): loss=114.83890007358985, w0=0.018454879767343915, w1=0.18259894816389807\n",
      "Gradient Descent(8411/9999): loss=114.81942602762614, w0=0.018453464619396658, w1=0.18258029961876715\n",
      "Gradient Descent(8412/9999): loss=114.79995629214133, w0=0.01845204960289674, w1=0.182561652829331\n",
      "Gradient Descent(8413/9999): loss=114.78049086608952, w0=0.018450634717830077, w1=0.18254300779541968\n",
      "Gradient Descent(8414/9999): loss=114.76102974842506, w0=0.018449219964182587, w1=0.18252436451686319\n",
      "Gradient Descent(8415/9999): loss=114.74157293810258, w0=0.018447805341940193, w1=0.18250572299349155\n",
      "Gradient Descent(8416/9999): loss=114.72212043407693, w0=0.01844639085108881, w1=0.18248708322513485\n",
      "Gradient Descent(8417/9999): loss=114.70267223530331, w0=0.01844497649161437, w1=0.18246844521162314\n",
      "Gradient Descent(8418/9999): loss=114.6832283407371, w0=0.018443562263502793, w1=0.18244980895278654\n",
      "Gradient Descent(8419/9999): loss=114.66378874933402, w0=0.01844214816674001, w1=0.1824311744484551\n",
      "Gradient Descent(8420/9999): loss=114.64435346004996, w0=0.018440734201311947, w1=0.182412541698459\n",
      "Gradient Descent(8421/9999): loss=114.62492247184127, w0=0.018439320367204536, w1=0.18239391070262834\n",
      "Gradient Descent(8422/9999): loss=114.6054957836643, w0=0.01843790666440371, w1=0.18237528146079326\n",
      "Gradient Descent(8423/9999): loss=114.58607339447593, w0=0.0184364930928954, w1=0.18235665397278397\n",
      "Gradient Descent(8424/9999): loss=114.56665530323312, w0=0.018435079652665547, w1=0.18233802823843065\n",
      "Gradient Descent(8425/9999): loss=114.54724150889321, w0=0.018433666343700087, w1=0.1823194042575635\n",
      "Gradient Descent(8426/9999): loss=114.52783201041372, w0=0.018432253165984958, w1=0.1823007820300127\n",
      "Gradient Descent(8427/9999): loss=114.50842680675258, w0=0.018430840119506103, w1=0.18228216155560853\n",
      "Gradient Descent(8428/9999): loss=114.48902589686779, w0=0.018429427204249463, w1=0.18226354283418122\n",
      "Gradient Descent(8429/9999): loss=114.46962927971776, w0=0.018428014420200985, w1=0.18224492586556104\n",
      "Gradient Descent(8430/9999): loss=114.45023695426113, w0=0.01842660176734661, w1=0.18222631064957828\n",
      "Gradient Descent(8431/9999): loss=114.4308489194568, w0=0.018425189245672293, w1=0.18220769718606325\n",
      "Gradient Descent(8432/9999): loss=114.411465174264, w0=0.01842377685516398, w1=0.18218908547484625\n",
      "Gradient Descent(8433/9999): loss=114.39208571764208, w0=0.018422364595807626, w1=0.1821704755157576\n",
      "Gradient Descent(8434/9999): loss=114.37271054855083, w0=0.01842095246758918, w1=0.18215186730862767\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(8435/9999): loss=114.35333966595014, w0=0.0184195404704946, w1=0.18213326085328682\n",
      "Gradient Descent(8436/9999): loss=114.33397306880029, w0=0.01841812860450984, w1=0.18211465614956543\n",
      "Gradient Descent(8437/9999): loss=114.31461075606188, w0=0.01841671686962086, w1=0.1820960531972939\n",
      "Gradient Descent(8438/9999): loss=114.29525272669565, w0=0.018415305265813624, w1=0.18207745199630265\n",
      "Gradient Descent(8439/9999): loss=114.2758989796625, w0=0.018413893793074093, w1=0.18205885254642212\n",
      "Gradient Descent(8440/9999): loss=114.25654951392391, w0=0.018412482451388226, w1=0.18204025484748274\n",
      "Gradient Descent(8441/9999): loss=114.2372043284414, w0=0.018411071240741993, w1=0.18202165889931496\n",
      "Gradient Descent(8442/9999): loss=114.21786342217688, w0=0.01840966016112136, w1=0.18200306470174926\n",
      "Gradient Descent(8443/9999): loss=114.19852679409229, w0=0.018408249212512297, w1=0.18198447225461617\n",
      "Gradient Descent(8444/9999): loss=114.17919444315024, w0=0.018406838394900774, w1=0.18196588155774618\n",
      "Gradient Descent(8445/9999): loss=114.15986636831323, w0=0.018405427708272762, w1=0.1819472926109698\n",
      "Gradient Descent(8446/9999): loss=114.14054256854416, w0=0.018404017152614235, w1=0.1819287054141176\n",
      "Gradient Descent(8447/9999): loss=114.12122304280634, w0=0.018402606727911174, w1=0.18191011996702014\n",
      "Gradient Descent(8448/9999): loss=114.10190779006307, w0=0.01840119643414955, w1=0.18189153626950796\n",
      "Gradient Descent(8449/9999): loss=114.08259680927816, w0=0.018399786271315346, w1=0.1818729543214117\n",
      "Gradient Descent(8450/9999): loss=114.06329009941558, w0=0.018398376239394547, w1=0.18185437412256195\n",
      "Gradient Descent(8451/9999): loss=114.0439876594395, w0=0.01839696633837313, w1=0.18183579567278932\n",
      "Gradient Descent(8452/9999): loss=114.02468948831456, w0=0.01839555656823708, w1=0.1818172189719245\n",
      "Gradient Descent(8453/9999): loss=114.00539558500542, w0=0.018394146928972385, w1=0.1817986440197981\n",
      "Gradient Descent(8454/9999): loss=113.98610594847715, w0=0.018392737420565033, w1=0.18178007081624079\n",
      "Gradient Descent(8455/9999): loss=113.96682057769506, w0=0.018391328043001018, w1=0.1817614993610833\n",
      "Gradient Descent(8456/9999): loss=113.94753947162485, w0=0.018389918796266325, w1=0.1817429296541563\n",
      "Gradient Descent(8457/9999): loss=113.9282626292322, w0=0.01838850968034695, w1=0.18172436169529055\n",
      "Gradient Descent(8458/9999): loss=113.90899004948321, w0=0.01838710069522889, w1=0.18170579548431676\n",
      "Gradient Descent(8459/9999): loss=113.88972173134441, w0=0.018385691840898135, w1=0.18168723102106568\n",
      "Gradient Descent(8460/9999): loss=113.8704576737823, w0=0.018384283117340694, w1=0.1816686683053681\n",
      "Gradient Descent(8461/9999): loss=113.8511978757639, w0=0.018382874524542563, w1=0.18165010733705478\n",
      "Gradient Descent(8462/9999): loss=113.83194233625625, w0=0.018381466062489738, w1=0.18163154811595655\n",
      "Gradient Descent(8463/9999): loss=113.81269105422689, w0=0.018380057731168232, w1=0.18161299064190423\n",
      "Gradient Descent(8464/9999): loss=113.79344402864344, w0=0.018378649530564044, w1=0.18159443491472865\n",
      "Gradient Descent(8465/9999): loss=113.77420125847394, w0=0.018377241460663184, w1=0.18157588093426066\n",
      "Gradient Descent(8466/9999): loss=113.75496274268659, w0=0.01837583352145166, w1=0.18155732870033114\n",
      "Gradient Descent(8467/9999): loss=113.73572848024979, w0=0.018374425712915487, w1=0.18153877821277098\n",
      "Gradient Descent(8468/9999): loss=113.7164984701325, w0=0.01837301803504067, w1=0.18152022947141105\n",
      "Gradient Descent(8469/9999): loss=113.69727271130365, w0=0.018371610487813227, w1=0.1815016824760823\n",
      "Gradient Descent(8470/9999): loss=113.67805120273252, w0=0.018370203071219175, w1=0.18148313722661566\n",
      "Gradient Descent(8471/9999): loss=113.65883394338861, w0=0.01836879578524453, w1=0.18146459372284204\n",
      "Gradient Descent(8472/9999): loss=113.63962093224185, w0=0.01836738862987531, w1=0.18144605196459246\n",
      "Gradient Descent(8473/9999): loss=113.62041216826232, w0=0.01836598160509754, w1=0.18142751195169787\n",
      "Gradient Descent(8474/9999): loss=113.6012076504203, w0=0.018364574710897242, w1=0.18140897368398928\n",
      "Gradient Descent(8475/9999): loss=113.58200737768645, w0=0.018363167947260436, w1=0.18139043716129769\n",
      "Gradient Descent(8476/9999): loss=113.56281134903159, w0=0.018361761314173154, w1=0.18137190238345413\n",
      "Gradient Descent(8477/9999): loss=113.5436195634269, w0=0.01836035481162142, w1=0.18135336935028967\n",
      "Gradient Descent(8478/9999): loss=113.52443201984381, w0=0.01835894843959127, w1=0.18133483806163536\n",
      "Gradient Descent(8479/9999): loss=113.50524871725403, w0=0.01835754219806873, w1=0.1813163085173223\n",
      "Gradient Descent(8480/9999): loss=113.4860696546294, w0=0.018356136087039832, w1=0.18129778071718158\n",
      "Gradient Descent(8481/9999): loss=113.46689483094221, w0=0.018354730106490615, w1=0.1812792546610443\n",
      "Gradient Descent(8482/9999): loss=113.44772424516484, w0=0.01835332425640711, w1=0.18126073034874157\n",
      "Gradient Descent(8483/9999): loss=113.42855789627005, w0=0.018351918536775362, w1=0.18124220778010455\n",
      "Gradient Descent(8484/9999): loss=113.4093957832309, w0=0.018350512947581405, w1=0.1812236869549644\n",
      "Gradient Descent(8485/9999): loss=113.3902379050205, w0=0.018349107488811287, w1=0.18120516787315233\n",
      "Gradient Descent(8486/9999): loss=113.37108426061255, w0=0.018347702160451045, w1=0.18118665053449948\n",
      "Gradient Descent(8487/9999): loss=113.3519348489807, w0=0.01834629696248673, w1=0.1811681349388371\n",
      "Gradient Descent(8488/9999): loss=113.33278966909906, w0=0.01834489189490439, w1=0.1811496210859964\n",
      "Gradient Descent(8489/9999): loss=113.31364871994192, w0=0.018343486957690065, w1=0.1811311089758086\n",
      "Gradient Descent(8490/9999): loss=113.29451200048389, w0=0.018342082150829813, w1=0.181112598608105\n",
      "Gradient Descent(8491/9999): loss=113.27537950969968, w0=0.018340677474309685, w1=0.18109408998271684\n",
      "Gradient Descent(8492/9999): loss=113.25625124656453, w0=0.018339272928115734, w1=0.18107558309947544\n",
      "Gradient Descent(8493/9999): loss=113.23712721005386, w0=0.018337868512234016, w1=0.18105707795821208\n",
      "Gradient Descent(8494/9999): loss=113.21800739914312, w0=0.018336464226650587, w1=0.1810385745587581\n",
      "Gradient Descent(8495/9999): loss=113.19889181280828, w0=0.01833506007135151, w1=0.18102007290094482\n",
      "Gradient Descent(8496/9999): loss=113.17978045002549, w0=0.01833365604632284, w1=0.18100157298460362\n",
      "Gradient Descent(8497/9999): loss=113.16067330977125, w0=0.018332252151550647, w1=0.18098307480956585\n",
      "Gradient Descent(8498/9999): loss=113.1415703910221, w0=0.018330848387020992, w1=0.1809645783756629\n",
      "Gradient Descent(8499/9999): loss=113.12247169275503, w0=0.01832944475271994, w1=0.1809460836827262\n",
      "Gradient Descent(8500/9999): loss=113.1033772139473, w0=0.018328041248633558, w1=0.18092759073058712\n",
      "Gradient Descent(8501/9999): loss=113.08428695357634, w0=0.018326637874747916, w1=0.18090909951907713\n",
      "Gradient Descent(8502/9999): loss=113.06520091061988, w0=0.018325234631049084, w1=0.1808906100480277\n",
      "Gradient Descent(8503/9999): loss=113.04611908405592, w0=0.01832383151752314, w1=0.18087212231727026\n",
      "Gradient Descent(8504/9999): loss=113.02704147286269, w0=0.018322428534156155, w1=0.1808536363266363\n",
      "Gradient Descent(8505/9999): loss=113.00796807601874, w0=0.018321025680934206, w1=0.18083515207595738\n",
      "Gradient Descent(8506/9999): loss=112.98889889250286, w0=0.01831962295784337, w1=0.18081666956506492\n",
      "Gradient Descent(8507/9999): loss=112.96983392129411, w0=0.018318220364869728, w1=0.18079818879379053\n",
      "Gradient Descent(8508/9999): loss=112.95077316137166, w0=0.01831681790199936, w1=0.18077970976196572\n",
      "Gradient Descent(8509/9999): loss=112.93171661171525, w0=0.01831541556921835, w1=0.18076123246942208\n",
      "Gradient Descent(8510/9999): loss=112.91266427130465, w0=0.018314013366512786, w1=0.18074275691599118\n",
      "Gradient Descent(8511/9999): loss=112.89361613911989, w0=0.01831261129386875, w1=0.1807242831015046\n",
      "Gradient Descent(8512/9999): loss=112.8745722141414, w0=0.018311209351272336, w1=0.18070581102579397\n",
      "Gradient Descent(8513/9999): loss=112.85553249534979, w0=0.01830980753870963, w1=0.1806873406886909\n",
      "Gradient Descent(8514/9999): loss=112.83649698172589, w0=0.018308405856166724, w1=0.18066887209002708\n",
      "Gradient Descent(8515/9999): loss=112.81746567225089, w0=0.018307004303629713, w1=0.18065040522963413\n",
      "Gradient Descent(8516/9999): loss=112.79843856590617, w0=0.01830560288108469, w1=0.18063194010734376\n",
      "Gradient Descent(8517/9999): loss=112.77941566167334, w0=0.018304201588517754, w1=0.18061347672298766\n",
      "Gradient Descent(8518/9999): loss=112.76039695853443, w0=0.018302800425915006, w1=0.18059501507639752\n",
      "Gradient Descent(8519/9999): loss=112.74138245547154, w0=0.01830139939326254, w1=0.1805765551674051\n",
      "Gradient Descent(8520/9999): loss=112.72237215146711, w0=0.018299998490546462, w1=0.1805580969958421\n",
      "Gradient Descent(8521/9999): loss=112.70336604550393, w0=0.018298597717752878, w1=0.18053964056154032\n",
      "Gradient Descent(8522/9999): loss=112.68436413656492, w0=0.01829719707486789, w1=0.18052118586433152\n",
      "Gradient Descent(8523/9999): loss=112.66536642363333, w0=0.01829579656187761, w1=0.18050273290404747\n",
      "Gradient Descent(8524/9999): loss=112.64637290569256, w0=0.018294396178768144, w1=0.18048428168052003\n",
      "Gradient Descent(8525/9999): loss=112.62738358172649, w0=0.0182929959255256, w1=0.18046583219358098\n",
      "Gradient Descent(8526/9999): loss=112.60839845071906, w0=0.01829159580213609, w1=0.18044738444306216\n",
      "Gradient Descent(8527/9999): loss=112.58941751165456, w0=0.018290195808585735, w1=0.18042893842879545\n",
      "Gradient Descent(8528/9999): loss=112.57044076351754, w0=0.018288795944860648, w1=0.18041049415061272\n",
      "Gradient Descent(8529/9999): loss=112.55146820529276, w0=0.018287396210946945, w1=0.18039205160834584\n",
      "Gradient Descent(8530/9999): loss=112.53249983596533, w0=0.018285996606830748, w1=0.18037361080182673\n",
      "Gradient Descent(8531/9999): loss=112.51353565452048, w0=0.01828459713249818, w1=0.1803551717308873\n",
      "Gradient Descent(8532/9999): loss=112.49457565994389, w0=0.018283197787935357, w1=0.1803367343953595\n",
      "Gradient Descent(8533/9999): loss=112.47561985122134, w0=0.018281798573128404, w1=0.18031829879507527\n",
      "Gradient Descent(8534/9999): loss=112.45666822733894, w0=0.018280399488063453, w1=0.1802998649298666\n",
      "Gradient Descent(8535/9999): loss=112.43772078728301, w0=0.01827900053272663, w1=0.18028143279956546\n",
      "Gradient Descent(8536/9999): loss=112.41877753004027, w0=0.018277601707104064, w1=0.18026300240400384\n",
      "Gradient Descent(8537/9999): loss=112.39983845459744, w0=0.018276203011181887, w1=0.18024457374301378\n",
      "Gradient Descent(8538/9999): loss=112.38090355994186, w0=0.018274804444946228, w1=0.18022614681642732\n",
      "Gradient Descent(8539/9999): loss=112.36197284506078, w0=0.018273406008383227, w1=0.18020772162407647\n",
      "Gradient Descent(8540/9999): loss=112.34304630894187, w0=0.018272007701479017, w1=0.18018929816579332\n",
      "Gradient Descent(8541/9999): loss=112.32412395057308, w0=0.018270609524219737, w1=0.18017087644140997\n",
      "Gradient Descent(8542/9999): loss=112.30520576894263, w0=0.01826921147659153, w1=0.1801524564507585\n",
      "Gradient Descent(8543/9999): loss=112.28629176303889, w0=0.018267813558580537, w1=0.18013403819367102\n",
      "Gradient Descent(8544/9999): loss=112.26738193185062, w0=0.0182664157701729, w1=0.1801156216699797\n",
      "Gradient Descent(8545/9999): loss=112.24847627436668, w0=0.018265018111354763, w1=0.1800972068795166\n",
      "Gradient Descent(8546/9999): loss=112.22957478957638, w0=0.018263620582112273, w1=0.18007879382211398\n",
      "Gradient Descent(8547/9999): loss=112.21067747646917, w0=0.01826222318243158, w1=0.18006038249760398\n",
      "Gradient Descent(8548/9999): loss=112.1917843340348, w0=0.018260825912298835, w1=0.18004197290581878\n",
      "Gradient Descent(8549/9999): loss=112.17289536126322, w0=0.018259428771700188, w1=0.18002356504659062\n",
      "Gradient Descent(8550/9999): loss=112.1540105571447, w0=0.018258031760621792, w1=0.1800051589197517\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(8551/9999): loss=112.13512992066978, w0=0.018256634879049806, w1=0.17998675452513427\n",
      "Gradient Descent(8552/9999): loss=112.11625345082926, w0=0.018255238126970386, w1=0.1799683518625706\n",
      "Gradient Descent(8553/9999): loss=112.09738114661413, w0=0.018253841504369685, w1=0.17994995093189295\n",
      "Gradient Descent(8554/9999): loss=112.07851300701564, w0=0.01825244501123387, w1=0.17993155173293363\n",
      "Gradient Descent(8555/9999): loss=112.05964903102536, w0=0.018251048647549103, w1=0.17991315426552495\n",
      "Gradient Descent(8556/9999): loss=112.04078921763521, w0=0.018249652413301547, w1=0.17989475852949924\n",
      "Gradient Descent(8557/9999): loss=112.0219335658371, w0=0.018248256308477363, w1=0.1798763645246888\n",
      "Gradient Descent(8558/9999): loss=112.00308207462345, w0=0.018246860333062723, w1=0.17985797225092603\n",
      "Gradient Descent(8559/9999): loss=111.98423474298683, w0=0.018245464487043797, w1=0.1798395817080433\n",
      "Gradient Descent(8560/9999): loss=111.96539156992007, w0=0.018244068770406754, w1=0.17982119289587295\n",
      "Gradient Descent(8561/9999): loss=111.9465525544162, w0=0.01824267318313777, w1=0.17980280581424743\n",
      "Gradient Descent(8562/9999): loss=111.92771769546876, w0=0.018241277725223014, w1=0.17978442046299917\n",
      "Gradient Descent(8563/9999): loss=111.90888699207123, w0=0.018239882396648664, w1=0.17976603684196057\n",
      "Gradient Descent(8564/9999): loss=111.8900604432175, w0=0.018238487197400895, w1=0.17974765495096412\n",
      "Gradient Descent(8565/9999): loss=111.87123804790178, w0=0.01823709212746589, w1=0.17972927478984227\n",
      "Gradient Descent(8566/9999): loss=111.85241980511836, w0=0.018235697186829828, w1=0.1797108963584275\n",
      "Gradient Descent(8567/9999): loss=111.8336057138619, w0=0.018234302375478892, w1=0.1796925196565523\n",
      "Gradient Descent(8568/9999): loss=111.81479577312744, w0=0.018232907693399268, w1=0.17967414468404924\n",
      "Gradient Descent(8569/9999): loss=111.79598998191004, w0=0.01823151314057714, w1=0.17965577144075082\n",
      "Gradient Descent(8570/9999): loss=111.7771883392051, w0=0.0182301187169987, w1=0.17963739992648955\n",
      "Gradient Descent(8571/9999): loss=111.7583908440084, w0=0.018228724422650132, w1=0.17961903014109806\n",
      "Gradient Descent(8572/9999): loss=111.73959749531576, w0=0.018227330257517632, w1=0.17960066208440892\n",
      "Gradient Descent(8573/9999): loss=111.72080829212346, w0=0.01822593622158739, w1=0.1795822957562547\n",
      "Gradient Descent(8574/9999): loss=111.702023233428, w0=0.0182245423148456, w1=0.17956393115646802\n",
      "Gradient Descent(8575/9999): loss=111.68324231822598, w0=0.018223148537278463, w1=0.17954556828488152\n",
      "Gradient Descent(8576/9999): loss=111.66446554551442, w0=0.018221754888872174, w1=0.17952720714132786\n",
      "Gradient Descent(8577/9999): loss=111.64569291429059, w0=0.01822036136961293, w1=0.1795088477256397\n",
      "Gradient Descent(8578/9999): loss=111.62692442355194, w0=0.01821896797948694, w1=0.17949049003764966\n",
      "Gradient Descent(8579/9999): loss=111.60816007229622, w0=0.018217574718480397, w1=0.1794721340771905\n",
      "Gradient Descent(8580/9999): loss=111.58939985952135, w0=0.018216181586579516, w1=0.17945377984409494\n",
      "Gradient Descent(8581/9999): loss=111.57064378422571, w0=0.018214788583770496, w1=0.17943542733819567\n",
      "Gradient Descent(8582/9999): loss=111.55189184540778, w0=0.018213395710039552, w1=0.17941707655932543\n",
      "Gradient Descent(8583/9999): loss=111.53314404206633, w0=0.01821200296537289, w1=0.179398727507317\n",
      "Gradient Descent(8584/9999): loss=111.51440037320029, w0=0.018210610349756722, w1=0.17938038018200314\n",
      "Gradient Descent(8585/9999): loss=111.49566083780907, w0=0.018209217863177262, w1=0.17936203458321665\n",
      "Gradient Descent(8586/9999): loss=111.47692543489217, w0=0.018207825505620723, w1=0.17934369071079034\n",
      "Gradient Descent(8587/9999): loss=111.45819416344943, w0=0.018206433277073324, w1=0.17932534856455704\n",
      "Gradient Descent(8588/9999): loss=111.4394670224808, w0=0.018205041177521284, w1=0.17930700814434955\n",
      "Gradient Descent(8589/9999): loss=111.42074401098662, w0=0.018203649206950823, w1=0.17928866945000077\n",
      "Gradient Descent(8590/9999): loss=111.40202512796755, w0=0.018202257365348164, w1=0.17927033248134355\n",
      "Gradient Descent(8591/9999): loss=111.3833103724244, w0=0.018200865652699526, w1=0.17925199723821078\n",
      "Gradient Descent(8592/9999): loss=111.36459974335813, w0=0.01819947406899114, w1=0.17923366372043537\n",
      "Gradient Descent(8593/9999): loss=111.34589323977016, w0=0.01819808261420923, w1=0.17921533192785025\n",
      "Gradient Descent(8594/9999): loss=111.32719086066209, w0=0.018196691288340026, w1=0.17919700186028834\n",
      "Gradient Descent(8595/9999): loss=111.30849260503581, w0=0.01819530009136976, w1=0.17917867351758257\n",
      "Gradient Descent(8596/9999): loss=111.2897984718933, w0=0.01819390902328466, w1=0.17916034689956595\n",
      "Gradient Descent(8597/9999): loss=111.271108460237, w0=0.018192518084070963, w1=0.17914202200607143\n",
      "Gradient Descent(8598/9999): loss=111.25242256906952, w0=0.018191127273714903, w1=0.17912369883693205\n",
      "Gradient Descent(8599/9999): loss=111.23374079739372, w0=0.018189736592202715, w1=0.1791053773919808\n",
      "Gradient Descent(8600/9999): loss=111.21506314421278, w0=0.018188346039520645, w1=0.1790870576710507\n",
      "Gradient Descent(8601/9999): loss=111.19638960853003, w0=0.018186955615654928, w1=0.17906873967397482\n",
      "Gradient Descent(8602/9999): loss=111.17772018934913, w0=0.018185565320591807, w1=0.17905042340058622\n",
      "Gradient Descent(8603/9999): loss=111.15905488567395, w0=0.018184175154317528, w1=0.17903210885071796\n",
      "Gradient Descent(8604/9999): loss=111.1403936965087, w0=0.018182785116818336, w1=0.17901379602420317\n",
      "Gradient Descent(8605/9999): loss=111.12173662085772, w0=0.01818139520808048, w1=0.17899548492087494\n",
      "Gradient Descent(8606/9999): loss=111.10308365772568, w0=0.0181800054280902, w1=0.1789771755405664\n",
      "Gradient Descent(8607/9999): loss=111.08443480611756, w0=0.018178615776833763, w1=0.17895886788311072\n",
      "Gradient Descent(8608/9999): loss=111.06579006503848, w0=0.018177226254297407, w1=0.17894056194834104\n",
      "Gradient Descent(8609/9999): loss=111.0471494334939, w0=0.018175836860467396, w1=0.17892225773609055\n",
      "Gradient Descent(8610/9999): loss=111.02851291048947, w0=0.01817444759532998, w1=0.17890395524619243\n",
      "Gradient Descent(8611/9999): loss=111.00988049503111, w0=0.01817305845887142, w1=0.17888565447847987\n",
      "Gradient Descent(8612/9999): loss=110.99125218612505, w0=0.01817166945107797, w1=0.17886735543278612\n",
      "Gradient Descent(8613/9999): loss=110.97262798277778, w0=0.018170280571935902, w1=0.17884905810894441\n",
      "Gradient Descent(8614/9999): loss=110.95400788399591, w0=0.018168891821431467, w1=0.178830762506788\n",
      "Gradient Descent(8615/9999): loss=110.93539188878644, w0=0.018167503199550936, w1=0.1788124686261502\n",
      "Gradient Descent(8616/9999): loss=110.9167799961566, w0=0.018166114706280573, w1=0.17879417646686424\n",
      "Gradient Descent(8617/9999): loss=110.89817220511385, w0=0.01816472634160665, w1=0.17877588602876346\n",
      "Gradient Descent(8618/9999): loss=110.87956851466589, w0=0.018163338105515427, w1=0.17875759731168117\n",
      "Gradient Descent(8619/9999): loss=110.86096892382072, w0=0.018161949997993185, w1=0.1787393103154507\n",
      "Gradient Descent(8620/9999): loss=110.84237343158652, w0=0.018160562019026192, w1=0.17872102503990542\n",
      "Gradient Descent(8621/9999): loss=110.82378203697183, w0=0.018159174168600724, w1=0.1787027414848787\n",
      "Gradient Descent(8622/9999): loss=110.80519473898539, w0=0.01815778644670306, w1=0.1786844596502039\n",
      "Gradient Descent(8623/9999): loss=110.78661153663616, w0=0.01815639885331947, w1=0.17866617953571445\n",
      "Gradient Descent(8624/9999): loss=110.76803242893341, w0=0.018155011388436244, w1=0.17864790114124376\n",
      "Gradient Descent(8625/9999): loss=110.74945741488663, w0=0.018153624052039654, w1=0.17862962446662525\n",
      "Gradient Descent(8626/9999): loss=110.7308864935056, w0=0.018152236844115988, w1=0.17861134951169239\n",
      "Gradient Descent(8627/9999): loss=110.71231966380026, w0=0.01815084976465153, w1=0.17859307627627863\n",
      "Gradient Descent(8628/9999): loss=110.69375692478096, w0=0.018149462813632566, w1=0.17857480476021745\n",
      "Gradient Descent(8629/9999): loss=110.67519827545814, w0=0.018148075991045388, w1=0.17855653496334234\n",
      "Gradient Descent(8630/9999): loss=110.65664371484263, w0=0.01814668929687628, w1=0.17853826688548682\n",
      "Gradient Descent(8631/9999): loss=110.63809324194543, w0=0.018145302731111538, w1=0.17852000052648445\n",
      "Gradient Descent(8632/9999): loss=110.61954685577781, w0=0.01814391629373745, w1=0.17850173588616874\n",
      "Gradient Descent(8633/9999): loss=110.60100455535127, w0=0.018142529984740315, w1=0.17848347296437328\n",
      "Gradient Descent(8634/9999): loss=110.58246633967767, w0=0.01814114380410643, w1=0.1784652117609316\n",
      "Gradient Descent(8635/9999): loss=110.56393220776908, w0=0.01813975775182209, w1=0.17844695227567736\n",
      "Gradient Descent(8636/9999): loss=110.54540215863766, w0=0.0181383718278736, w1=0.17842869450844412\n",
      "Gradient Descent(8637/9999): loss=110.52687619129601, w0=0.018136986032247258, w1=0.1784104384590655\n",
      "Gradient Descent(8638/9999): loss=110.50835430475695, w0=0.018135600364929366, w1=0.1783921841273752\n",
      "Gradient Descent(8639/9999): loss=110.48983649803354, w0=0.018134214825906235, w1=0.17837393151320682\n",
      "Gradient Descent(8640/9999): loss=110.47132277013903, w0=0.018132829415164167, w1=0.17835568061639406\n",
      "Gradient Descent(8641/9999): loss=110.45281312008701, w0=0.01813144413268947, w1=0.1783374314367706\n",
      "Gradient Descent(8642/9999): loss=110.43430754689132, w0=0.01813005897846846, w1=0.17831918397417015\n",
      "Gradient Descent(8643/9999): loss=110.41580604956597, w0=0.018128673952487444, w1=0.17830093822842644\n",
      "Gradient Descent(8644/9999): loss=110.39730862712533, w0=0.018127289054732737, w1=0.1782826941993732\n",
      "Gradient Descent(8645/9999): loss=110.37881527858389, w0=0.018125904285190654, w1=0.17826445188684417\n",
      "Gradient Descent(8646/9999): loss=110.36032600295655, w0=0.01812451964384751, w1=0.17824621129067314\n",
      "Gradient Descent(8647/9999): loss=110.34184079925834, w0=0.018123135130689624, w1=0.17822797241069388\n",
      "Gradient Descent(8648/9999): loss=110.32335966650457, w0=0.018121750745703318, w1=0.17820973524674022\n",
      "Gradient Descent(8649/9999): loss=110.3048826037109, w0=0.01812036648887491, w1=0.17819149979864596\n",
      "Gradient Descent(8650/9999): loss=110.28640960989306, w0=0.01811898236019073, w1=0.17817326606624492\n",
      "Gradient Descent(8651/9999): loss=110.26794068406723, w0=0.0181175983596371, w1=0.17815503404937097\n",
      "Gradient Descent(8652/9999): loss=110.24947582524965, w0=0.018116214487200348, w1=0.17813680374785795\n",
      "Gradient Descent(8653/9999): loss=110.23101503245697, w0=0.0181148307428668, w1=0.17811857516153978\n",
      "Gradient Descent(8654/9999): loss=110.21255830470601, w0=0.01811344712662279, w1=0.17810034829025034\n",
      "Gradient Descent(8655/9999): loss=110.1941056410138, w0=0.01811206363845465, w1=0.17808212313382354\n",
      "Gradient Descent(8656/9999): loss=110.17565704039785, w0=0.018110680278348713, w1=0.17806389969209332\n",
      "Gradient Descent(8657/9999): loss=110.15721250187559, w0=0.018109297046291314, w1=0.17804567796489362\n",
      "Gradient Descent(8658/9999): loss=110.13877202446496, w0=0.01810791394226879, w1=0.17802745795205838\n",
      "Gradient Descent(8659/9999): loss=110.120335607184, w0=0.018106530966267482, w1=0.1780092396534216\n",
      "Gradient Descent(8660/9999): loss=110.1019032490511, w0=0.018105148118273726, w1=0.17799102306881728\n",
      "Gradient Descent(8661/9999): loss=110.08347494908487, w0=0.018103765398273866, w1=0.17797280819807942\n",
      "Gradient Descent(8662/9999): loss=110.06505070630415, w0=0.018102382806254248, w1=0.17795459504104205\n",
      "Gradient Descent(8663/9999): loss=110.04663051972804, w0=0.018101000342201214, w1=0.17793638359753922\n",
      "Gradient Descent(8664/9999): loss=110.02821438837586, w0=0.018099618006101117, w1=0.17791817386740497\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(8665/9999): loss=110.00980231126726, w0=0.0180982357979403, w1=0.17789996585047335\n",
      "Gradient Descent(8666/9999): loss=109.99139428742212, w0=0.01809685371770512, w1=0.1778817595465785\n",
      "Gradient Descent(8667/9999): loss=109.97299031586054, w0=0.018095471765381926, w1=0.1778635549555545\n",
      "Gradient Descent(8668/9999): loss=109.95459039560285, w0=0.01809408994095707, w1=0.17784535207723548\n",
      "Gradient Descent(8669/9999): loss=109.93619452566966, w0=0.01809270824441691, w1=0.17782715091145557\n",
      "Gradient Descent(8670/9999): loss=109.91780270508188, w0=0.018091326675747803, w1=0.17780895145804892\n",
      "Gradient Descent(8671/9999): loss=109.89941493286058, w0=0.01808994523493611, w1=0.17779075371684974\n",
      "Gradient Descent(8672/9999): loss=109.88103120802718, w0=0.018088563921968184, w1=0.17777255768769218\n",
      "Gradient Descent(8673/9999): loss=109.86265152960323, w0=0.018087182736830396, w1=0.17775436337041042\n",
      "Gradient Descent(8674/9999): loss=109.84427589661061, w0=0.018085801679509104, w1=0.17773617076483872\n",
      "Gradient Descent(8675/9999): loss=109.8259043080715, w0=0.018084420749990678, w1=0.17771797987081128\n",
      "Gradient Descent(8676/9999): loss=109.80753676300823, w0=0.018083039948261485, w1=0.17769979068816238\n",
      "Gradient Descent(8677/9999): loss=109.78917326044338, w0=0.018081659274307892, w1=0.17768160321672627\n",
      "Gradient Descent(8678/9999): loss=109.77081379939986, w0=0.018080278728116274, w1=0.17766341745633724\n",
      "Gradient Descent(8679/9999): loss=109.75245837890078, w0=0.018078898309673, w1=0.17764523340682958\n",
      "Gradient Descent(8680/9999): loss=109.73410699796959, w0=0.018077518018964442, w1=0.1776270510680376\n",
      "Gradient Descent(8681/9999): loss=109.71575965562978, w0=0.018076137855976983, w1=0.17760887043979565\n",
      "Gradient Descent(8682/9999): loss=109.69741635090527, w0=0.018074757820696993, w1=0.17759069152193804\n",
      "Gradient Descent(8683/9999): loss=109.6790770828202, w0=0.018073377913110856, w1=0.17757251431429916\n",
      "Gradient Descent(8684/9999): loss=109.66074185039888, w0=0.018071998133204953, w1=0.17755433881671337\n",
      "Gradient Descent(8685/9999): loss=109.64241065266603, w0=0.018070618480965663, w1=0.17753616502901506\n",
      "Gradient Descent(8686/9999): loss=109.62408348864648, w0=0.018069238956379376, w1=0.17751799295103865\n",
      "Gradient Descent(8687/9999): loss=109.6057603573653, w0=0.01806785955943247, w1=0.17749982258261857\n",
      "Gradient Descent(8688/9999): loss=109.58744125784791, w0=0.01806648029011134, w1=0.17748165392358925\n",
      "Gradient Descent(8689/9999): loss=109.56912618911993, w0=0.01806510114840237, w1=0.17746348697378517\n",
      "Gradient Descent(8690/9999): loss=109.55081515020724, w0=0.018063722134291953, w1=0.17744532173304076\n",
      "Gradient Descent(8691/9999): loss=109.53250814013593, w0=0.018062343247766486, w1=0.17742715820119054\n",
      "Gradient Descent(8692/9999): loss=109.51420515793237, w0=0.018060964488812357, w1=0.177408996378069\n",
      "Gradient Descent(8693/9999): loss=109.4959062026232, w0=0.018059585857415968, w1=0.17739083626351068\n",
      "Gradient Descent(8694/9999): loss=109.47761127323527, w0=0.018058207353563713, w1=0.17737267785735011\n",
      "Gradient Descent(8695/9999): loss=109.45932036879576, w0=0.01805682897724199, w1=0.17735452115942185\n",
      "Gradient Descent(8696/9999): loss=109.44103348833188, w0=0.0180554507284372, w1=0.17733636616956044\n",
      "Gradient Descent(8697/9999): loss=109.42275063087145, w0=0.01805407260713575, w1=0.1773182128876005\n",
      "Gradient Descent(8698/9999): loss=109.40447179544215, w0=0.01805269461332404, w1=0.17730006131337658\n",
      "Gradient Descent(8699/9999): loss=109.3861969810722, w0=0.018051316746988476, w1=0.17728191144672334\n",
      "Gradient Descent(8700/9999): loss=109.36792618678994, w0=0.01804993900811547, w1=0.1772637632874754\n",
      "Gradient Descent(8701/9999): loss=109.34965941162396, w0=0.01804856139669143, w1=0.1772456168354674\n",
      "Gradient Descent(8702/9999): loss=109.33139665460317, w0=0.018047183912702765, w1=0.17722747209053405\n",
      "Gradient Descent(8703/9999): loss=109.31313791475664, w0=0.018045806556135892, w1=0.17720932905250997\n",
      "Gradient Descent(8704/9999): loss=109.29488319111375, w0=0.01804442932697722, w1=0.17719118772122988\n",
      "Gradient Descent(8705/9999): loss=109.2766324827041, w0=0.018043052225213168, w1=0.1771730480965285\n",
      "Gradient Descent(8706/9999): loss=109.25838578855755, w0=0.018041675250830155, w1=0.17715491017824056\n",
      "Gradient Descent(8707/9999): loss=109.24014310770417, w0=0.0180402984038146, w1=0.17713677396620078\n",
      "Gradient Descent(8708/9999): loss=109.2219044391743, w0=0.018038921684152918, w1=0.17711863946024395\n",
      "Gradient Descent(8709/9999): loss=109.20366978199864, w0=0.018037545091831537, w1=0.17710050666020483\n",
      "Gradient Descent(8710/9999): loss=109.18543913520794, w0=0.018036168626836883, w1=0.1770823755659182\n",
      "Gradient Descent(8711/9999): loss=109.16721249783338, w0=0.01803479228915538, w1=0.17706424617721886\n",
      "Gradient Descent(8712/9999): loss=109.14898986890624, w0=0.01803341607877346, w1=0.17704611849394167\n",
      "Gradient Descent(8713/9999): loss=109.13077124745811, w0=0.018032039995677547, w1=0.17702799251592144\n",
      "Gradient Descent(8714/9999): loss=109.11255663252089, w0=0.018030664039854073, w1=0.17700986824299306\n",
      "Gradient Descent(8715/9999): loss=109.0943460231266, w0=0.018029288211289473, w1=0.17699174567499137\n",
      "Gradient Descent(8716/9999): loss=109.07613941830768, w0=0.01802791250997018, w1=0.17697362481175127\n",
      "Gradient Descent(8717/9999): loss=109.05793681709658, w0=0.01802653693588263, w1=0.17695550565310766\n",
      "Gradient Descent(8718/9999): loss=109.03973821852622, w0=0.018025161489013265, w1=0.17693738819889546\n",
      "Gradient Descent(8719/9999): loss=109.02154362162972, w0=0.01802378616934852, w1=0.1769192724489496\n",
      "Gradient Descent(8720/9999): loss=109.00335302544025, w0=0.018022410976874837, w1=0.17690115840310502\n",
      "Gradient Descent(8721/9999): loss=108.98516642899158, w0=0.01802103591157866, w1=0.1768830460611967\n",
      "Gradient Descent(8722/9999): loss=108.9669838313174, w0=0.018019660973446434, w1=0.17686493542305964\n",
      "Gradient Descent(8723/9999): loss=108.94880523145183, w0=0.018018286162464606, w1=0.1768468264885288\n",
      "Gradient Descent(8724/9999): loss=108.93063062842917, w0=0.01801691147861962, w1=0.17682871925743926\n",
      "Gradient Descent(8725/9999): loss=108.91246002128402, w0=0.018015536921897928, w1=0.17681061372962598\n",
      "Gradient Descent(8726/9999): loss=108.89429340905119, w0=0.018014162492285983, w1=0.17679250990492404\n",
      "Gradient Descent(8727/9999): loss=108.87613079076573, w0=0.018012788189770236, w1=0.1767744077831685\n",
      "Gradient Descent(8728/9999): loss=108.8579721654629, w0=0.01801141401433714, w1=0.17675630736419443\n",
      "Gradient Descent(8729/9999): loss=108.83981753217832, w0=0.01801003996597315, w1=0.17673820864783693\n",
      "Gradient Descent(8730/9999): loss=108.82166688994775, w0=0.01800866604466473, w1=0.17672011163393112\n",
      "Gradient Descent(8731/9999): loss=108.80352023780728, w0=0.018007292250398335, w1=0.17670201632231208\n",
      "Gradient Descent(8732/9999): loss=108.78537757479316, w0=0.01800591858316043, w1=0.176683922712815\n",
      "Gradient Descent(8733/9999): loss=108.76723889994193, w0=0.018004545042937473, w1=0.176665830805275\n",
      "Gradient Descent(8734/9999): loss=108.74910421229048, w0=0.018003171629715934, w1=0.17664774059952731\n",
      "Gradient Descent(8735/9999): loss=108.7309735108757, w0=0.018001798343482277, w1=0.1766296520954071\n",
      "Gradient Descent(8736/9999): loss=108.71284679473494, w0=0.018000425184222967, w1=0.17661156529274954\n",
      "Gradient Descent(8737/9999): loss=108.69472406290579, w0=0.017999052151924478, w1=0.17659348019138987\n",
      "Gradient Descent(8738/9999): loss=108.6766053144259, w0=0.017997679246573278, w1=0.17657539679116335\n",
      "Gradient Descent(8739/9999): loss=108.65849054833336, w0=0.01799630646815584, w1=0.1765573150919052\n",
      "Gradient Descent(8740/9999): loss=108.64037976366646, w0=0.01799493381665864, w1=0.1765392350934507\n",
      "Gradient Descent(8741/9999): loss=108.62227295946367, w0=0.017993561292068156, w1=0.17652115679563515\n",
      "Gradient Descent(8742/9999): loss=108.60417013476379, w0=0.01799218889437086, w1=0.17650308019829383\n",
      "Gradient Descent(8743/9999): loss=108.58607128860581, w0=0.01799081662355324, w1=0.17648500530126207\n",
      "Gradient Descent(8744/9999): loss=108.56797642002898, w0=0.017989444479601772, w1=0.1764669321043752\n",
      "Gradient Descent(8745/9999): loss=108.54988552807276, w0=0.01798807246250294, w1=0.17644886060746856\n",
      "Gradient Descent(8746/9999): loss=108.53179861177695, w0=0.01798670057224323, w1=0.17643079081037752\n",
      "Gradient Descent(8747/9999): loss=108.51371567018154, w0=0.017985328808809126, w1=0.17641272271293745\n",
      "Gradient Descent(8748/9999): loss=108.49563670232673, w0=0.017983957172187117, w1=0.17639465631498377\n",
      "Gradient Descent(8749/9999): loss=108.47756170725306, w0=0.01798258566236369, w1=0.17637659161635189\n",
      "Gradient Descent(8750/9999): loss=108.45949068400115, w0=0.01798121427932534, w1=0.1763585286168772\n",
      "Gradient Descent(8751/9999): loss=108.4414236316121, w0=0.017979843023058557, w1=0.1763404673163952\n",
      "Gradient Descent(8752/9999): loss=108.42336054912704, w0=0.017978471893549838, w1=0.17632240771474128\n",
      "Gradient Descent(8753/9999): loss=108.40530143558746, w0=0.017977100890785677, w1=0.17630434981175097\n",
      "Gradient Descent(8754/9999): loss=108.38724629003508, w0=0.017975730014752576, w1=0.17628629360725975\n",
      "Gradient Descent(8755/9999): loss=108.36919511151184, w0=0.017974359265437032, w1=0.17626823910110312\n",
      "Gradient Descent(8756/9999): loss=108.35114789905995, w0=0.017972988642825545, w1=0.1762501862931166\n",
      "Gradient Descent(8757/9999): loss=108.3331046517219, w0=0.01797161814690462, w1=0.17623213518313574\n",
      "Gradient Descent(8758/9999): loss=108.31506536854026, w0=0.017970247777660757, w1=0.1762140857709961\n",
      "Gradient Descent(8759/9999): loss=108.29703004855807, w0=0.01796887753508047, w1=0.17619603805653322\n",
      "Gradient Descent(8760/9999): loss=108.27899869081851, w0=0.017967507419150264, w1=0.1761779920395827\n",
      "Gradient Descent(8761/9999): loss=108.26097129436495, w0=0.017966137429856647, w1=0.17615994771998017\n",
      "Gradient Descent(8762/9999): loss=108.2429478582411, w0=0.017964767567186127, w1=0.17614190509756122\n",
      "Gradient Descent(8763/9999): loss=108.22492838149083, w0=0.017963397831125223, w1=0.1761238641721615\n",
      "Gradient Descent(8764/9999): loss=108.20691286315837, w0=0.017962028221660445, w1=0.17610582494361665\n",
      "Gradient Descent(8765/9999): loss=108.1889013022881, w0=0.01796065873877831, w1=0.17608778741176234\n",
      "Gradient Descent(8766/9999): loss=108.1708936979246, w0=0.017959289382465337, w1=0.17606975157643423\n",
      "Gradient Descent(8767/9999): loss=108.15289004911287, w0=0.017957920152708046, w1=0.17605171743746806\n",
      "Gradient Descent(8768/9999): loss=108.13489035489798, w0=0.01795655104949296, w1=0.1760336849946995\n",
      "Gradient Descent(8769/9999): loss=108.11689461432535, w0=0.017955182072806598, w1=0.1760156542479643\n",
      "Gradient Descent(8770/9999): loss=108.09890282644052, w0=0.017953813222635485, w1=0.17599762519709822\n",
      "Gradient Descent(8771/9999): loss=108.08091499028951, w0=0.01795244449896615, w1=0.17597959784193698\n",
      "Gradient Descent(8772/9999): loss=108.06293110491832, w0=0.01795107590178512, w1=0.1759615721823164\n",
      "Gradient Descent(8773/9999): loss=108.04495116937336, w0=0.01794970743107892, w1=0.17594354821807223\n",
      "Gradient Descent(8774/9999): loss=108.02697518270121, w0=0.01794833908683409, w1=0.17592552594904032\n",
      "Gradient Descent(8775/9999): loss=108.00900314394875, w0=0.017946970869037154, w1=0.17590750537505648\n",
      "Gradient Descent(8776/9999): loss=107.99103505216304, w0=0.01794560277767465, w1=0.17588948649595654\n",
      "Gradient Descent(8777/9999): loss=107.9730709063914, w0=0.01794423481273311, w1=0.17587146931157635\n",
      "Gradient Descent(8778/9999): loss=107.95511070568148, w0=0.017942866974199077, w1=0.1758534538217518\n",
      "Gradient Descent(8779/9999): loss=107.93715444908098, w0=0.01794149926205909, w1=0.1758354400263188\n",
      "Gradient Descent(8780/9999): loss=107.91920213563813, w0=0.017940131676299688, w1=0.1758174279251132\n",
      "Gradient Descent(8781/9999): loss=107.90125376440112, w0=0.017938764216907415, w1=0.17579941751797096\n",
      "Gradient Descent(8782/9999): loss=107.88330933441851, w0=0.017937396883868816, w1=0.17578140880472798\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(8783/9999): loss=107.86536884473917, w0=0.017936029677170437, w1=0.17576340178522024\n",
      "Gradient Descent(8784/9999): loss=107.84743229441207, w0=0.017934662596798823, w1=0.1757453964592837\n",
      "Gradient Descent(8785/9999): loss=107.82949968248653, w0=0.017933295642740526, w1=0.17572739282675434\n",
      "Gradient Descent(8786/9999): loss=107.81157100801208, w0=0.017931928814982095, w1=0.17570939088746818\n",
      "Gradient Descent(8787/9999): loss=107.79364627003852, w0=0.017930562113510084, w1=0.17569139064126119\n",
      "Gradient Descent(8788/9999): loss=107.77572546761579, w0=0.017929195538311047, w1=0.17567339208796942\n",
      "Gradient Descent(8789/9999): loss=107.75780859979417, w0=0.01792782908937154, w1=0.17565539522742893\n",
      "Gradient Descent(8790/9999): loss=107.7398956656242, w0=0.01792646276667812, w1=0.17563740005947578\n",
      "Gradient Descent(8791/9999): loss=107.72198666415657, w0=0.01792509657021735, w1=0.17561940658394604\n",
      "Gradient Descent(8792/9999): loss=107.7040815944423, w0=0.01792373049997579, w1=0.1756014148006758\n",
      "Gradient Descent(8793/9999): loss=107.68618045553261, w0=0.01792236455594, w1=0.17558342470950117\n",
      "Gradient Descent(8794/9999): loss=107.66828324647902, w0=0.01792099873809654, w1=0.17556543631025828\n",
      "Gradient Descent(8795/9999): loss=107.65038996633318, w0=0.017919633046431984, w1=0.17554744960278326\n",
      "Gradient Descent(8796/9999): loss=107.63250061414708, w0=0.017918267480932896, w1=0.1755294645869123\n",
      "Gradient Descent(8797/9999): loss=107.61461518897285, w0=0.017916902041585844, w1=0.17551148126248153\n",
      "Gradient Descent(8798/9999): loss=107.59673368986306, w0=0.0179155367283774, w1=0.17549349962932717\n",
      "Gradient Descent(8799/9999): loss=107.57885611587028, w0=0.017914171541294138, w1=0.1754755196872854\n",
      "Gradient Descent(8800/9999): loss=107.56098246604749, w0=0.017912806480322632, w1=0.17545754143619244\n",
      "Gradient Descent(8801/9999): loss=107.54311273944784, w0=0.017911441545449457, w1=0.17543956487588455\n",
      "Gradient Descent(8802/9999): loss=107.52524693512477, w0=0.017910076736661187, w1=0.17542159000619797\n",
      "Gradient Descent(8803/9999): loss=107.50738505213191, w0=0.017908712053944408, w1=0.17540361682696898\n",
      "Gradient Descent(8804/9999): loss=107.48952708952315, w0=0.017907347497285696, w1=0.17538564533803383\n",
      "Gradient Descent(8805/9999): loss=107.47167304635268, w0=0.017905983066671634, w1=0.17536767553922886\n",
      "Gradient Descent(8806/9999): loss=107.4538229216748, w0=0.017904618762088806, w1=0.17534970743039036\n",
      "Gradient Descent(8807/9999): loss=107.4359767145442, w0=0.0179032545835238, w1=0.17533174101135465\n",
      "Gradient Descent(8808/9999): loss=107.4181344240157, w0=0.0179018905309632, w1=0.1753137762819581\n",
      "Gradient Descent(8809/9999): loss=107.40029604914443, w0=0.017900526604393598, w1=0.17529581324203705\n",
      "Gradient Descent(8810/9999): loss=107.38246158898573, w0=0.017899162803801584, w1=0.1752778518914279\n",
      "Gradient Descent(8811/9999): loss=107.36463104259516, w0=0.01789779912917375, w1=0.17525989222996702\n",
      "Gradient Descent(8812/9999): loss=107.34680440902864, w0=0.01789643558049669, w1=0.17524193425749085\n",
      "Gradient Descent(8813/9999): loss=107.32898168734216, w0=0.017895072157757, w1=0.1752239779738358\n",
      "Gradient Descent(8814/9999): loss=107.31116287659205, w0=0.017893708860941278, w1=0.1752060233788383\n",
      "Gradient Descent(8815/9999): loss=107.29334797583488, w0=0.01789234569003612, w1=0.17518807047233484\n",
      "Gradient Descent(8816/9999): loss=107.27553698412744, w0=0.017890982645028128, w1=0.17517011925416187\n",
      "Gradient Descent(8817/9999): loss=107.25772990052677, w0=0.017889619725903902, w1=0.17515216972415587\n",
      "Gradient Descent(8818/9999): loss=107.23992672409011, w0=0.01788825693265005, w1=0.17513422188215336\n",
      "Gradient Descent(8819/9999): loss=107.22212745387506, w0=0.017886894265253174, w1=0.17511627572799085\n",
      "Gradient Descent(8820/9999): loss=107.2043320889393, w0=0.017885531723699884, w1=0.1750983312615049\n",
      "Gradient Descent(8821/9999): loss=107.18654062834092, w0=0.01788416930797679, w1=0.17508038848253202\n",
      "Gradient Descent(8822/9999): loss=107.16875307113806, w0=0.017882807018070497, w1=0.17506244739090882\n",
      "Gradient Descent(8823/9999): loss=107.15096941638929, w0=0.01788144485396762, w1=0.17504450798647186\n",
      "Gradient Descent(8824/9999): loss=107.13318966315329, w0=0.01788008281565477, w1=0.17502657026905774\n",
      "Gradient Descent(8825/9999): loss=107.115413810489, w0=0.01787872090311857, w1=0.17500863423850307\n",
      "Gradient Descent(8826/9999): loss=107.09764185745577, w0=0.01787735911634563, w1=0.1749906998946445\n",
      "Gradient Descent(8827/9999): loss=107.07987380311285, w0=0.017875997455322572, w1=0.17497276723731867\n",
      "Gradient Descent(8828/9999): loss=107.06210964652007, w0=0.017874635920036016, w1=0.17495483626636224\n",
      "Gradient Descent(8829/9999): loss=107.04434938673727, w0=0.017873274510472582, w1=0.17493690698161188\n",
      "Gradient Descent(8830/9999): loss=107.02659302282468, w0=0.017871913226618897, w1=0.1749189793829043\n",
      "Gradient Descent(8831/9999): loss=107.00884055384267, w0=0.01787055206846158, w1=0.1749010534700762\n",
      "Gradient Descent(8832/9999): loss=106.99109197885193, w0=0.017869191035987265, w1=0.1748831292429643\n",
      "Gradient Descent(8833/9999): loss=106.9733472969133, w0=0.017867830129182576, w1=0.17486520670140537\n",
      "Gradient Descent(8834/9999): loss=106.95560650708796, w0=0.017866469348034146, w1=0.17484728584523612\n",
      "Gradient Descent(8835/9999): loss=106.93786960843723, w0=0.017865108692528605, w1=0.17482936667429336\n",
      "Gradient Descent(8836/9999): loss=106.92013660002274, w0=0.017863748162652588, w1=0.17481144918841388\n",
      "Gradient Descent(8837/9999): loss=106.90240748090638, w0=0.01786238775839273, w1=0.17479353338743445\n",
      "Gradient Descent(8838/9999): loss=106.88468225015018, w0=0.017861027479735666, w1=0.17477561927119192\n",
      "Gradient Descent(8839/9999): loss=106.86696090681652, w0=0.017859667326668036, w1=0.17475770683952313\n",
      "Gradient Descent(8840/9999): loss=106.84924344996787, w0=0.01785830729917648, w1=0.1747397960922649\n",
      "Gradient Descent(8841/9999): loss=106.83152987866718, w0=0.01785694739724764, w1=0.17472188702925412\n",
      "Gradient Descent(8842/9999): loss=106.81382019197741, w0=0.017855587620868155, w1=0.17470397965032766\n",
      "Gradient Descent(8843/9999): loss=106.79611438896188, w0=0.017854227970024675, w1=0.17468607395532243\n",
      "Gradient Descent(8844/9999): loss=106.7784124686841, w0=0.017852868444703848, w1=0.17466816994407536\n",
      "Gradient Descent(8845/9999): loss=106.76071443020786, w0=0.017851509044892317, w1=0.17465026761642335\n",
      "Gradient Descent(8846/9999): loss=106.74302027259714, w0=0.017850149770576734, w1=0.17463236697220336\n",
      "Gradient Descent(8847/9999): loss=106.72532999491622, w0=0.01784879062174375, w1=0.17461446801125236\n",
      "Gradient Descent(8848/9999): loss=106.70764359622952, w0=0.01784743159838002, w1=0.1745965707334073\n",
      "Gradient Descent(8849/9999): loss=106.68996107560187, w0=0.0178460727004722, w1=0.17457867513850522\n",
      "Gradient Descent(8850/9999): loss=106.67228243209811, w0=0.01784471392800694, w1=0.1745607812263831\n",
      "Gradient Descent(8851/9999): loss=106.65460766478355, w0=0.017843355280970905, w1=0.17454288899687798\n",
      "Gradient Descent(8852/9999): loss=106.6369367727236, w0=0.01784199675935075, w1=0.17452499844982686\n",
      "Gradient Descent(8853/9999): loss=106.61926975498386, w0=0.01784063836313314, w1=0.17450710958506682\n",
      "Gradient Descent(8854/9999): loss=106.60160661063041, w0=0.01783928009230474, w1=0.17448922240243495\n",
      "Gradient Descent(8855/9999): loss=106.58394733872926, w0=0.017837921946852207, w1=0.17447133690176833\n",
      "Gradient Descent(8856/9999): loss=106.56629193834691, w0=0.017836563926762215, w1=0.17445345308290405\n",
      "Gradient Descent(8857/9999): loss=106.54864040854996, w0=0.017835206032021426, w1=0.17443557094567924\n",
      "Gradient Descent(8858/9999): loss=106.53099274840523, w0=0.01783384826261651, w1=0.17441769048993103\n",
      "Gradient Descent(8859/9999): loss=106.51334895697991, w0=0.017832490618534147, w1=0.17439981171549657\n",
      "Gradient Descent(8860/9999): loss=106.49570903334134, w0=0.017831133099761, w1=0.17438193462221305\n",
      "Gradient Descent(8861/9999): loss=106.47807297655707, w0=0.017829775706283746, w1=0.17436405920991763\n",
      "Gradient Descent(8862/9999): loss=106.46044078569497, w0=0.01782841843808906, w1=0.1743461854784475\n",
      "Gradient Descent(8863/9999): loss=106.44281245982305, w0=0.017827061295163624, w1=0.17432831342763988\n",
      "Gradient Descent(8864/9999): loss=106.4251879980097, w0=0.017825704277494116, w1=0.174310443057332\n",
      "Gradient Descent(8865/9999): loss=106.40756739932344, w0=0.017824347385067215, w1=0.17429257436736115\n",
      "Gradient Descent(8866/9999): loss=106.38995066283299, w0=0.017822990617869603, w1=0.17427470735756453\n",
      "Gradient Descent(8867/9999): loss=106.3723377876074, w0=0.017821633975887968, w1=0.17425684202777944\n",
      "Gradient Descent(8868/9999): loss=106.354728772716, w0=0.017820277459108993, w1=0.17423897837784316\n",
      "Gradient Descent(8869/9999): loss=106.33712361722817, w0=0.017818921067519367, w1=0.17422111640759302\n",
      "Gradient Descent(8870/9999): loss=106.31952232021374, w0=0.017817564801105776, w1=0.17420325611686635\n",
      "Gradient Descent(8871/9999): loss=106.30192488074262, w0=0.017816208659854917, w1=0.17418539750550047\n",
      "Gradient Descent(8872/9999): loss=106.28433129788503, w0=0.017814852643753477, w1=0.17416754057333272\n",
      "Gradient Descent(8873/9999): loss=106.26674157071142, w0=0.017813496752788153, w1=0.1741496853202005\n",
      "Gradient Descent(8874/9999): loss=106.24915569829253, w0=0.01781214098694564, w1=0.17413183174594118\n",
      "Gradient Descent(8875/9999): loss=106.23157367969917, w0=0.017810785346212634, w1=0.17411397985039218\n",
      "Gradient Descent(8876/9999): loss=106.21399551400259, w0=0.01780942983057584, w1=0.1740961296333909\n",
      "Gradient Descent(8877/9999): loss=106.19642120027417, w0=0.01780807444002195, w1=0.1740782810947748\n",
      "Gradient Descent(8878/9999): loss=106.1788507375855, w0=0.01780671917453767, w1=0.1740604342343813\n",
      "Gradient Descent(8879/9999): loss=106.16128412500854, w0=0.01780536403410971, w1=0.1740425890520479\n",
      "Gradient Descent(8880/9999): loss=106.14372136161528, w0=0.017804009018724764, w1=0.17402474554761202\n",
      "Gradient Descent(8881/9999): loss=106.12616244647818, w0=0.01780265412836955, w1=0.1740069037209112\n",
      "Gradient Descent(8882/9999): loss=106.10860737866975, w0=0.017801299363030768, w1=0.17398906357178298\n",
      "Gradient Descent(8883/9999): loss=106.09105615726286, w0=0.017799944722695134, w1=0.17397122510006485\n",
      "Gradient Descent(8884/9999): loss=106.0735087813305, w0=0.01779859020734936, w1=0.17395338830559434\n",
      "Gradient Descent(8885/9999): loss=106.05596524994604, w0=0.01779723581698016, w1=0.17393555318820905\n",
      "Gradient Descent(8886/9999): loss=106.03842556218298, w0=0.017795881551574244, w1=0.17391771974774653\n",
      "Gradient Descent(8887/9999): loss=106.0208897171151, w0=0.017794527411118335, w1=0.17389988798404438\n",
      "Gradient Descent(8888/9999): loss=106.00335771381631, w0=0.01779317339559915, w1=0.1738820578969402\n",
      "Gradient Descent(8889/9999): loss=105.98582955136102, w0=0.017791819505003408, w1=0.17386422948627162\n",
      "Gradient Descent(8890/9999): loss=105.96830522882354, w0=0.01779046573931783, w1=0.17384640275187627\n",
      "Gradient Descent(8891/9999): loss=105.95078474527875, w0=0.017789112098529146, w1=0.1738285776935918\n",
      "Gradient Descent(8892/9999): loss=105.93326809980144, w0=0.017787758582624075, w1=0.17381075431125592\n",
      "Gradient Descent(8893/9999): loss=105.91575529146688, w0=0.017786405191589343, w1=0.17379293260470627\n",
      "Gradient Descent(8894/9999): loss=105.89824631935053, w0=0.017785051925411677, w1=0.17377511257378056\n",
      "Gradient Descent(8895/9999): loss=105.88074118252797, w0=0.017783698784077814, w1=0.17375729421831654\n",
      "Gradient Descent(8896/9999): loss=105.86323988007514, w0=0.01778234576757448, w1=0.17373947753815192\n",
      "Gradient Descent(8897/9999): loss=105.84574241106819, w0=0.017780992875888413, w1=0.17372166253312443\n",
      "Gradient Descent(8898/9999): loss=105.82824877458341, w0=0.017779640109006345, w1=0.17370384920307186\n",
      "Gradient Descent(8899/9999): loss=105.8107589696975, w0=0.01777828746691501, w1=0.17368603754783196\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(8900/9999): loss=105.79327299548723, w0=0.017776934949601152, w1=0.17366822756724254\n",
      "Gradient Descent(8901/9999): loss=105.77579085102971, w0=0.017775582557051507, w1=0.17365041926114141\n",
      "Gradient Descent(8902/9999): loss=105.75831253540221, w0=0.017774230289252818, w1=0.1736326126293664\n",
      "Gradient Descent(8903/9999): loss=105.74083804768236, w0=0.017772878146191825, w1=0.17361480767175538\n",
      "Gradient Descent(8904/9999): loss=105.72336738694786, w0=0.017771526127855278, w1=0.17359700438814618\n",
      "Gradient Descent(8905/9999): loss=105.7059005522768, w0=0.017770174234229916, w1=0.17357920277837668\n",
      "Gradient Descent(8906/9999): loss=105.68843754274741, w0=0.017768822465302498, w1=0.17356140284228475\n",
      "Gradient Descent(8907/9999): loss=105.6709783574381, w0=0.017767470821059766, w1=0.1735436045797083\n",
      "Gradient Descent(8908/9999): loss=105.65352299542772, w0=0.01776611930148847, w1=0.17352580799048528\n",
      "Gradient Descent(8909/9999): loss=105.63607145579513, w0=0.017764767906575368, w1=0.1735080130744536\n",
      "Gradient Descent(8910/9999): loss=105.61862373761964, w0=0.01776341663630721, w1=0.17349021983145121\n",
      "Gradient Descent(8911/9999): loss=105.60117983998056, w0=0.01776206549067075, w1=0.17347242826131612\n",
      "Gradient Descent(8912/9999): loss=105.58373976195764, w0=0.017760714469652755, w1=0.17345463836388625\n",
      "Gradient Descent(8913/9999): loss=105.56630350263073, w0=0.017759363573239977, w1=0.17343685013899965\n",
      "Gradient Descent(8914/9999): loss=105.54887106108, w0=0.017758012801419178, w1=0.1734190635864943\n",
      "Gradient Descent(8915/9999): loss=105.53144243638582, w0=0.017756662154177122, w1=0.17340127870620828\n",
      "Gradient Descent(8916/9999): loss=105.5140176276288, w0=0.017755311631500573, w1=0.17338349549797957\n",
      "Gradient Descent(8917/9999): loss=105.49659663388977, w0=0.017753961233376296, w1=0.17336571396164627\n",
      "Gradient Descent(8918/9999): loss=105.47917945424977, w0=0.017752610959791058, w1=0.17334793409704646\n",
      "Gradient Descent(8919/9999): loss=105.46176608779021, w0=0.01775126081073163, w1=0.17333015590401823\n",
      "Gradient Descent(8920/9999): loss=105.44435653359255, w0=0.01774991078618478, w1=0.17331237938239968\n",
      "Gradient Descent(8921/9999): loss=105.42695079073864, w0=0.017748560886137285, w1=0.17329460453202894\n",
      "Gradient Descent(8922/9999): loss=105.40954885831039, w0=0.017747211110575912, w1=0.17327683135274413\n",
      "Gradient Descent(8923/9999): loss=105.39215073539012, w0=0.017745861459487443, w1=0.17325905984438345\n",
      "Gradient Descent(8924/9999): loss=105.37475642106038, w0=0.017744511932858652, w1=0.17324129000678504\n",
      "Gradient Descent(8925/9999): loss=105.35736591440374, w0=0.01774316253067632, w1=0.1732235218397871\n",
      "Gradient Descent(8926/9999): loss=105.33997921450324, w0=0.017741813252927223, w1=0.1732057553432278\n",
      "Gradient Descent(8927/9999): loss=105.32259632044212, w0=0.017740464099598147, w1=0.1731879905169454\n",
      "Gradient Descent(8928/9999): loss=105.30521723130371, w0=0.017739115070675875, w1=0.1731702273607781\n",
      "Gradient Descent(8929/9999): loss=105.2878419461717, w0=0.017737766166147192, w1=0.1731524658745642\n",
      "Gradient Descent(8930/9999): loss=105.27047046412996, w0=0.017736417385998886, w1=0.17313470605814194\n",
      "Gradient Descent(8931/9999): loss=105.25310278426265, w0=0.017735068730217744, w1=0.17311694791134957\n",
      "Gradient Descent(8932/9999): loss=105.23573890565409, w0=0.01773372019879056, w1=0.1730991914340254\n",
      "Gradient Descent(8933/9999): loss=105.21837882738892, w0=0.01773237179170412, w1=0.17308143662600778\n",
      "Gradient Descent(8934/9999): loss=105.20102254855189, w0=0.017731023508945218, w1=0.173063683487135\n",
      "Gradient Descent(8935/9999): loss=105.18367006822814, w0=0.017729675350500653, w1=0.1730459320172454\n",
      "Gradient Descent(8936/9999): loss=105.16632138550291, w0=0.01772832731635722, w1=0.17302818221617738\n",
      "Gradient Descent(8937/9999): loss=105.14897649946178, w0=0.017726979406501718, w1=0.1730104340837693\n",
      "Gradient Descent(8938/9999): loss=105.13163540919044, w0=0.017725631620920945, w1=0.17299268761985953\n",
      "Gradient Descent(8939/9999): loss=105.11429811377494, w0=0.017724283959601704, w1=0.1729749428242865\n",
      "Gradient Descent(8940/9999): loss=105.09696461230149, w0=0.017722936422530797, w1=0.1729571996968886\n",
      "Gradient Descent(8941/9999): loss=105.07963490385656, w0=0.01772158900969503, w1=0.1729394582375043\n",
      "Gradient Descent(8942/9999): loss=105.0623089875268, w0=0.01772024172108121, w1=0.17292171844597204\n",
      "Gradient Descent(8943/9999): loss=105.0449868623992, w0=0.01771889455667614, w1=0.17290398032213028\n",
      "Gradient Descent(8944/9999): loss=105.02766852756088, w0=0.01771754751646664, w1=0.17288624386581752\n",
      "Gradient Descent(8945/9999): loss=105.0103539820992, w0=0.017716200600439512, w1=0.17286850907687226\n",
      "Gradient Descent(8946/9999): loss=104.9930432251019, w0=0.017714853808581572, w1=0.172850775955133\n",
      "Gradient Descent(8947/9999): loss=104.97573625565674, w0=0.017713507140879634, w1=0.1728330445004383\n",
      "Gradient Descent(8948/9999): loss=104.95843307285183, w0=0.017712160597320514, w1=0.17281531471262668\n",
      "Gradient Descent(8949/9999): loss=104.94113367577555, w0=0.01771081417789103, w1=0.1727975865915367\n",
      "Gradient Descent(8950/9999): loss=104.92383806351638, w0=0.017709467882578, w1=0.17277986013700697\n",
      "Gradient Descent(8951/9999): loss=104.90654623516315, w0=0.017708121711368246, w1=0.17276213534887605\n",
      "Gradient Descent(8952/9999): loss=104.88925818980488, w0=0.01770677566424859, w1=0.17274441222698256\n",
      "Gradient Descent(8953/9999): loss=104.87197392653083, w0=0.01770542974120586, w1=0.17272669077116512\n",
      "Gradient Descent(8954/9999): loss=104.85469344443051, w0=0.017704083942226875, w1=0.1727089709812624\n",
      "Gradient Descent(8955/9999): loss=104.83741674259359, w0=0.01770273826729847, w1=0.17269125285711304\n",
      "Gradient Descent(8956/9999): loss=104.82014382011006, w0=0.01770139271640747, w1=0.1726735363985557\n",
      "Gradient Descent(8957/9999): loss=104.80287467607009, w0=0.017700047289540707, w1=0.17265582160542908\n",
      "Gradient Descent(8958/9999): loss=104.78560930956417, w0=0.01769870198668501, w1=0.17263810847757188\n",
      "Gradient Descent(8959/9999): loss=104.76834771968281, w0=0.017697356807827216, w1=0.17262039701482282\n",
      "Gradient Descent(8960/9999): loss=104.751089905517, w0=0.01769601175295416, w1=0.17260268721702063\n",
      "Gradient Descent(8961/9999): loss=104.73383586615783, w0=0.017694666822052677, w1=0.17258497908400405\n",
      "Gradient Descent(8962/9999): loss=104.71658560069665, w0=0.01769332201510961, w1=0.17256727261561186\n",
      "Gradient Descent(8963/9999): loss=104.69933910822506, w0=0.017691977332111794, w1=0.17254956781168285\n",
      "Gradient Descent(8964/9999): loss=104.68209638783478, w0=0.017690632773046076, w1=0.1725318646720558\n",
      "Gradient Descent(8965/9999): loss=104.66485743861797, w0=0.017689288337899296, w1=0.17251416319656954\n",
      "Gradient Descent(8966/9999): loss=104.64762225966682, w0=0.0176879440266583, w1=0.17249646338506286\n",
      "Gradient Descent(8967/9999): loss=104.63039085007388, w0=0.01768659983930994, w1=0.17247876523737465\n",
      "Gradient Descent(8968/9999): loss=104.6131632089319, w0=0.017685255775841055, w1=0.17246106875334374\n",
      "Gradient Descent(8969/9999): loss=104.59593933533384, w0=0.0176839118362385, w1=0.17244337393280904\n",
      "Gradient Descent(8970/9999): loss=104.57871922837285, w0=0.017682568020489123, w1=0.1724256807756094\n",
      "Gradient Descent(8971/9999): loss=104.56150288714244, w0=0.017681224328579784, w1=0.17240798928158374\n",
      "Gradient Descent(8972/9999): loss=104.54429031073624, w0=0.017679880760497334, w1=0.172390299450571\n",
      "Gradient Descent(8973/9999): loss=104.52708149824811, w0=0.01767853731622863, w1=0.17237261128241008\n",
      "Gradient Descent(8974/9999): loss=104.50987644877222, w0=0.01767719399576053, w1=0.17235492477693995\n",
      "Gradient Descent(8975/9999): loss=104.49267516140296, w0=0.017675850799079896, w1=0.1723372399339996\n",
      "Gradient Descent(8976/9999): loss=104.47547763523485, w0=0.017674507726173586, w1=0.172319556753428\n",
      "Gradient Descent(8977/9999): loss=104.45828386936277, w0=0.01767316477702846, w1=0.17230187523506413\n",
      "Gradient Descent(8978/9999): loss=104.4410938628817, w0=0.01767182195163139, w1=0.17228419537874703\n",
      "Gradient Descent(8979/9999): loss=104.42390761488703, w0=0.017670479249969236, w1=0.17226651718431574\n",
      "Gradient Descent(8980/9999): loss=104.40672512447414, w0=0.01766913667202887, w1=0.17224884065160928\n",
      "Gradient Descent(8981/9999): loss=104.38954639073887, w0=0.01766779421779716, w1=0.1722311657804667\n",
      "Gradient Descent(8982/9999): loss=104.37237141277723, w0=0.017666451887260976, w1=0.17221349257072713\n",
      "Gradient Descent(8983/9999): loss=104.35520018968529, w0=0.01766510968040719, w1=0.17219582102222963\n",
      "Gradient Descent(8984/9999): loss=104.3380327205596, w0=0.017663767597222678, w1=0.1721781511348133\n",
      "Gradient Descent(8985/9999): loss=104.32086900449681, w0=0.017662425637694316, w1=0.17216048290831729\n",
      "Gradient Descent(8986/9999): loss=104.30370904059379, w0=0.01766108380180898, w1=0.17214281634258072\n",
      "Gradient Descent(8987/9999): loss=104.2865528279477, w0=0.01765974208955355, w1=0.17212515143744275\n",
      "Gradient Descent(8988/9999): loss=104.26940036565583, w0=0.017658400500914908, w1=0.17210748819274257\n",
      "Gradient Descent(8989/9999): loss=104.25225165281586, w0=0.017657059035879934, w1=0.17208982660831934\n",
      "Gradient Descent(8990/9999): loss=104.23510668852559, w0=0.01765571769443551, w1=0.1720721666840123\n",
      "Gradient Descent(8991/9999): loss=104.21796547188305, w0=0.017654376476568524, w1=0.1720545084196606\n",
      "Gradient Descent(8992/9999): loss=104.2008280019865, w0=0.017653035382265862, w1=0.17203685181510353\n",
      "Gradient Descent(8993/9999): loss=104.18369427793452, w0=0.017651694411514415, w1=0.17201919687018033\n",
      "Gradient Descent(8994/9999): loss=104.1665642988257, w0=0.01765035356430107, w1=0.17200154358473024\n",
      "Gradient Descent(8995/9999): loss=104.1494380637592, w0=0.017649012840612724, w1=0.17198389195859257\n",
      "Gradient Descent(8996/9999): loss=104.13231557183414, w0=0.017647672240436264, w1=0.1719662419916066\n",
      "Gradient Descent(8997/9999): loss=104.11519682214993, w0=0.01764633176375859, w1=0.17194859368361165\n",
      "Gradient Descent(8998/9999): loss=104.09808181380623, w0=0.017644991410566593, w1=0.17193094703444706\n",
      "Gradient Descent(8999/9999): loss=104.08097054590301, w0=0.01764365118084718, w1=0.17191330204395214\n",
      "Gradient Descent(9000/9999): loss=104.06386301754031, w0=0.017642311074587247, w1=0.17189565871196627\n",
      "Gradient Descent(9001/9999): loss=104.04675922781848, w0=0.01764097109177369, w1=0.17187801703832883\n",
      "Gradient Descent(9002/9999): loss=104.02965917583815, w0=0.01763963123239342, w1=0.17186037702287918\n",
      "Gradient Descent(9003/9999): loss=104.01256286070007, w0=0.01763829149643334, w1=0.17184273866545674\n",
      "Gradient Descent(9004/9999): loss=103.99547028150533, w0=0.017636951883880352, w1=0.17182510196590095\n",
      "Gradient Descent(9005/9999): loss=103.9783814373552, w0=0.017635612394721365, w1=0.17180746692405122\n",
      "Gradient Descent(9006/9999): loss=103.96129632735112, w0=0.01763427302894329, w1=0.171789833539747\n",
      "Gradient Descent(9007/9999): loss=103.94421495059484, w0=0.01763293378653304, w1=0.1717722018128278\n",
      "Gradient Descent(9008/9999): loss=103.92713730618834, w0=0.017631594667477525, w1=0.17175457174313305\n",
      "Gradient Descent(9009/9999): loss=103.91006339323381, w0=0.01763025567176366, w1=0.17173694333050227\n",
      "Gradient Descent(9010/9999): loss=103.89299321083364, w0=0.017628916799378364, w1=0.17171931657477496\n",
      "Gradient Descent(9011/9999): loss=103.8759267580905, w0=0.01762757805030855, w1=0.17170169147579067\n",
      "Gradient Descent(9012/9999): loss=103.85886403410719, w0=0.01762623942454114, w1=0.17168406803338895\n",
      "Gradient Descent(9013/9999): loss=103.84180503798689, w0=0.017624900922063053, w1=0.17166644624740934\n",
      "Gradient Descent(9014/9999): loss=103.82474976883287, w0=0.017623562542861213, w1=0.17164882611769142\n",
      "Gradient Descent(9015/9999): loss=103.80769822574878, w0=0.017622224286922544, w1=0.17163120764407477\n",
      "Gradient Descent(9016/9999): loss=103.79065040783833, w0=0.017620886154233968, w1=0.17161359082639904\n",
      "Gradient Descent(9017/9999): loss=103.77360631420555, w0=0.017619548144782417, w1=0.17159597566450382\n",
      "Gradient Descent(9018/9999): loss=103.7565659439547, w0=0.01761821025855482, w1=0.17157836215822875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(9019/9999): loss=103.73952929619021, w0=0.017616872495538102, w1=0.17156075030741347\n",
      "Gradient Descent(9020/9999): loss=103.72249637001686, w0=0.0176155348557192, w1=0.17154314011189767\n",
      "Gradient Descent(9021/9999): loss=103.7054671645395, w0=0.017614197339085043, w1=0.17152553157152103\n",
      "Gradient Descent(9022/9999): loss=103.68844167886336, w0=0.017612859945622568, w1=0.17150792468612325\n",
      "Gradient Descent(9023/9999): loss=103.67141991209378, w0=0.01761152267531871, w1=0.17149031945554405\n",
      "Gradient Descent(9024/9999): loss=103.65440186333639, w0=0.017610185528160415, w1=0.17147271587962315\n",
      "Gradient Descent(9025/9999): loss=103.63738753169702, w0=0.017608848504134616, w1=0.17145511395820032\n",
      "Gradient Descent(9026/9999): loss=103.62037691628173, w0=0.01760751160322826, w1=0.1714375136911153\n",
      "Gradient Descent(9027/9999): loss=103.6033700161969, w0=0.017606174825428283, w1=0.17141991507820786\n",
      "Gradient Descent(9028/9999): loss=103.58636683054897, w0=0.017604838170721632, w1=0.17140231811931783\n",
      "Gradient Descent(9029/9999): loss=103.56936735844471, w0=0.017603501639095256, w1=0.171384722814285\n",
      "Gradient Descent(9030/9999): loss=103.55237159899116, w0=0.017602165230536104, w1=0.17136712916294916\n",
      "Gradient Descent(9031/9999): loss=103.53537955129543, w0=0.01760082894503112, w1=0.1713495371651502\n",
      "Gradient Descent(9032/9999): loss=103.51839121446507, w0=0.017599492782567262, w1=0.17133194682072794\n",
      "Gradient Descent(9033/9999): loss=103.5014065876077, w0=0.017598156743131477, w1=0.17131435812952228\n",
      "Gradient Descent(9034/9999): loss=103.48442566983118, w0=0.017596820826710723, w1=0.1712967710913731\n",
      "Gradient Descent(9035/9999): loss=103.46744846024367, w0=0.017595485033291954, w1=0.1712791857061203\n",
      "Gradient Descent(9036/9999): loss=103.45047495795352, w0=0.017594149362862126, w1=0.1712616019736038\n",
      "Gradient Descent(9037/9999): loss=103.43350516206932, w0=0.0175928138154082, w1=0.1712440198936635\n",
      "Gradient Descent(9038/9999): loss=103.41653907169986, w0=0.01759147839091714, w1=0.1712264394661394\n",
      "Gradient Descent(9039/9999): loss=103.39957668595413, w0=0.0175901430893759, w1=0.17120886069087146\n",
      "Gradient Descent(9040/9999): loss=103.38261800394145, w0=0.017588807910771456, w1=0.1711912835676996\n",
      "Gradient Descent(9041/9999): loss=103.3656630247713, w0=0.017587472855090765, w1=0.1711737080964639\n",
      "Gradient Descent(9042/9999): loss=103.34871174755332, w0=0.017586137922320794, w1=0.1711561342770043\n",
      "Gradient Descent(9043/9999): loss=103.33176417139752, w0=0.017584803112448515, w1=0.17113856210916084\n",
      "Gradient Descent(9044/9999): loss=103.3148202954141, w0=0.0175834684254609, w1=0.1711209915927736\n",
      "Gradient Descent(9045/9999): loss=103.29788011871337, w0=0.017582133861344915, w1=0.1711034227276826\n",
      "Gradient Descent(9046/9999): loss=103.28094364040601, w0=0.01758079942008754, w1=0.17108585551372796\n",
      "Gradient Descent(9047/9999): loss=103.26401085960283, w0=0.017579465101675743, w1=0.1710682899507497\n",
      "Gradient Descent(9048/9999): loss=103.24708177541496, w0=0.017578130906096504, w1=0.17105072603858798\n",
      "Gradient Descent(9049/9999): loss=103.23015638695362, w0=0.0175767968333368, w1=0.17103316377708289\n",
      "Gradient Descent(9050/9999): loss=103.21323469333038, w0=0.017575462883383616, w1=0.17101560316607456\n",
      "Gradient Descent(9051/9999): loss=103.196316693657, w0=0.01757412905622393, w1=0.17099804420540315\n",
      "Gradient Descent(9052/9999): loss=103.17940238704546, w0=0.017572795351844722, w1=0.17098048689490883\n",
      "Gradient Descent(9053/9999): loss=103.16249177260798, w0=0.01757146177023298, w1=0.1709629312344318\n",
      "Gradient Descent(9054/9999): loss=103.14558484945702, w0=0.017570128311375693, w1=0.17094537722381223\n",
      "Gradient Descent(9055/9999): loss=103.12868161670514, w0=0.017568794975259847, w1=0.17092782486289032\n",
      "Gradient Descent(9056/9999): loss=103.1117820734653, w0=0.01756746176187243, w1=0.17091027415150634\n",
      "Gradient Descent(9057/9999): loss=103.09488621885065, w0=0.017566128671200432, w1=0.17089272508950049\n",
      "Gradient Descent(9058/9999): loss=103.07799405197443, w0=0.017564795703230846, w1=0.17087517767671306\n",
      "Gradient Descent(9059/9999): loss=103.0611055719503, w0=0.01756346285795067, w1=0.1708576319129843\n",
      "Gradient Descent(9060/9999): loss=103.044220777892, w0=0.017562130135346896, w1=0.17084008779815454\n",
      "Gradient Descent(9061/9999): loss=103.02733966891356, w0=0.017560797535406524, w1=0.17082254533206406\n",
      "Gradient Descent(9062/9999): loss=103.01046224412922, w0=0.01755946505811655, w1=0.17080500451455316\n",
      "Gradient Descent(9063/9999): loss=102.99358850265344, w0=0.017558132703463975, w1=0.1707874653454622\n",
      "Gradient Descent(9064/9999): loss=102.97671844360094, w0=0.017556800471435804, w1=0.17076992782463155\n",
      "Gradient Descent(9065/9999): loss=102.95985206608663, w0=0.01755546836201904, w1=0.17075239195190153\n",
      "Gradient Descent(9066/9999): loss=102.94298936922564, w0=0.017554136375200688, w1=0.17073485772711255\n",
      "Gradient Descent(9067/9999): loss=102.92613035213336, w0=0.017552804510967756, w1=0.17071732515010501\n",
      "Gradient Descent(9068/9999): loss=102.90927501392542, w0=0.01755147276930725, w1=0.17069979422071932\n",
      "Gradient Descent(9069/9999): loss=102.89242335371758, w0=0.01755014115020618, w1=0.1706822649387959\n",
      "Gradient Descent(9070/9999): loss=102.87557537062592, w0=0.01754880965365156, w1=0.17066473730417522\n",
      "Gradient Descent(9071/9999): loss=102.85873106376668, w0=0.0175474782796304, w1=0.17064721131669772\n",
      "Gradient Descent(9072/9999): loss=102.84189043225645, w0=0.01754614702812972, w1=0.17062968697620387\n",
      "Gradient Descent(9073/9999): loss=102.82505347521186, w0=0.01754481589913653, w1=0.17061216428253417\n",
      "Gradient Descent(9074/9999): loss=102.80822019174995, w0=0.017543484892637858, w1=0.17059464323552914\n",
      "Gradient Descent(9075/9999): loss=102.79139058098781, w0=0.017542154008620713, w1=0.1705771238350293\n",
      "Gradient Descent(9076/9999): loss=102.77456464204286, w0=0.01754082324707212, w1=0.17055960608087514\n",
      "Gradient Descent(9077/9999): loss=102.75774237403279, w0=0.017539492607979105, w1=0.17054208997290726\n",
      "Gradient Descent(9078/9999): loss=102.74092377607539, w0=0.01753816209132869, w1=0.17052457551096623\n",
      "Gradient Descent(9079/9999): loss=102.7241088472887, w0=0.017536831697107902, w1=0.17050706269489263\n",
      "Gradient Descent(9080/9999): loss=102.70729758679111, w0=0.017535501425303766, w1=0.17048955152452705\n",
      "Gradient Descent(9081/9999): loss=102.69048999370108, w0=0.017534171275903315, w1=0.1704720419997101\n",
      "Gradient Descent(9082/9999): loss=102.67368606713742, w0=0.01753284124889358, w1=0.17045453412028244\n",
      "Gradient Descent(9083/9999): loss=102.65688580621901, w0=0.017531511344261588, w1=0.17043702788608467\n",
      "Gradient Descent(9084/9999): loss=102.64008921006517, w0=0.017530181561994377, w1=0.17041952329695748\n",
      "Gradient Descent(9085/9999): loss=102.6232962777952, w0=0.017528851902078982, w1=0.17040202035274155\n",
      "Gradient Descent(9086/9999): loss=102.60650700852885, w0=0.017527522364502442, w1=0.17038451905327753\n",
      "Gradient Descent(9087/9999): loss=102.58972140138594, w0=0.01752619294925179, w1=0.17036701939840618\n",
      "Gradient Descent(9088/9999): loss=102.57293945548658, w0=0.017524863656314074, w1=0.17034952138796822\n",
      "Gradient Descent(9089/9999): loss=102.55616116995108, w0=0.01752353448567633, w1=0.17033202502180436\n",
      "Gradient Descent(9090/9999): loss=102.5393865439, w0=0.017522205437325602, w1=0.17031453029975538\n",
      "Gradient Descent(9091/9999): loss=102.52261557645411, w0=0.017520876511248937, w1=0.17029703722166203\n",
      "Gradient Descent(9092/9999): loss=102.5058482667344, w0=0.017519547707433383, w1=0.1702795457873651\n",
      "Gradient Descent(9093/9999): loss=102.48908461386209, w0=0.017518219025865985, w1=0.17026205599670538\n",
      "Gradient Descent(9094/9999): loss=102.47232461695862, w0=0.017516890466533794, w1=0.17024456784952371\n",
      "Gradient Descent(9095/9999): loss=102.45556827514564, w0=0.01751556202942386, w1=0.17022708134566092\n",
      "Gradient Descent(9096/9999): loss=102.43881558754505, w0=0.01751423371452324, w1=0.17020959648495781\n",
      "Gradient Descent(9097/9999): loss=102.422066553279, w0=0.017512905521818986, w1=0.1701921132672553\n",
      "Gradient Descent(9098/9999): loss=102.40532117146977, w0=0.017511577451298155, w1=0.17017463169239425\n",
      "Gradient Descent(9099/9999): loss=102.38857944123994, w0=0.017510249502947803, w1=0.17015715176021554\n",
      "Gradient Descent(9100/9999): loss=102.37184136171234, w0=0.017508921676754993, w1=0.17013967347056008\n",
      "Gradient Descent(9101/9999): loss=102.35510693200992, w0=0.01750759397270678, w1=0.17012219682326882\n",
      "Gradient Descent(9102/9999): loss=102.33837615125594, w0=0.017506266390790233, w1=0.17010472181818265\n",
      "Gradient Descent(9103/9999): loss=102.32164901857382, w0=0.017504938930992413, w1=0.17008724845514256\n",
      "Gradient Descent(9104/9999): loss=102.30492553308727, w0=0.017503611593300383, w1=0.17006977673398951\n",
      "Gradient Descent(9105/9999): loss=102.28820569392022, w0=0.017502284377701214, w1=0.1700523066545645\n",
      "Gradient Descent(9106/9999): loss=102.27148950019674, w0=0.017500957284181974, w1=0.17003483821670853\n",
      "Gradient Descent(9107/9999): loss=102.2547769510412, w0=0.017499630312729733, w1=0.1700173714202626\n",
      "Gradient Descent(9108/9999): loss=102.23806804557822, w0=0.017498303463331564, w1=0.16999990626506775\n",
      "Gradient Descent(9109/9999): loss=102.22136278293247, w0=0.01749697673597454, w1=0.16998244275096502\n",
      "Gradient Descent(9110/9999): loss=102.2046611622291, w0=0.017495650130645735, w1=0.1699649808777955\n",
      "Gradient Descent(9111/9999): loss=102.18796318259331, w0=0.017494323647332225, w1=0.16994752064540022\n",
      "Gradient Descent(9112/9999): loss=102.17126884315054, w0=0.017492997286021092, w1=0.16993006205362032\n",
      "Gradient Descent(9113/9999): loss=102.15457814302647, w0=0.017491671046699412, w1=0.1699126051022969\n",
      "Gradient Descent(9114/9999): loss=102.13789108134706, w0=0.01749034492935427, w1=0.16989514979127107\n",
      "Gradient Descent(9115/9999): loss=102.12120765723841, w0=0.01748901893397275, w1=0.16987769612038398\n",
      "Gradient Descent(9116/9999): loss=102.10452786982687, w0=0.017487693060541928, w1=0.16986024408947678\n",
      "Gradient Descent(9117/9999): loss=102.08785171823902, w0=0.0174863673090489, w1=0.16984279369839064\n",
      "Gradient Descent(9118/9999): loss=102.07117920160167, w0=0.01748504167948075, w1=0.16982534494696674\n",
      "Gradient Descent(9119/9999): loss=102.05451031904185, w0=0.017483716171824565, w1=0.1698078978350463\n",
      "Gradient Descent(9120/9999): loss=102.0378450696868, w0=0.01748239078606744, w1=0.16979045236247053\n",
      "Gradient Descent(9121/9999): loss=102.02118345266398, w0=0.017481065522196467, w1=0.16977300852908067\n",
      "Gradient Descent(9122/9999): loss=102.00452546710105, w0=0.017479740380198738, w1=0.16975556633471794\n",
      "Gradient Descent(9123/9999): loss=101.98787111212596, w0=0.01747841536006135, w1=0.16973812577922362\n",
      "Gradient Descent(9124/9999): loss=101.97122038686685, w0=0.0174770904617714, w1=0.169720686862439\n",
      "Gradient Descent(9125/9999): loss=101.9545732904521, w0=0.017475765685315985, w1=0.16970324958420535\n",
      "Gradient Descent(9126/9999): loss=101.93792982201023, w0=0.01747444103068221, w1=0.169685813944364\n",
      "Gradient Descent(9127/9999): loss=101.9212899806701, w0=0.017473116497857172, w1=0.16966837994275627\n",
      "Gradient Descent(9128/9999): loss=101.90465376556067, w0=0.01747179208682798, w1=0.1696509475792235\n",
      "Gradient Descent(9129/9999): loss=101.8880211758112, w0=0.017470467797581733, w1=0.16963351685360703\n",
      "Gradient Descent(9130/9999): loss=101.87139221055116, w0=0.01746914363010554, w1=0.16961608776574827\n",
      "Gradient Descent(9131/9999): loss=101.85476686891027, w0=0.01746781958438651, w1=0.16959866031548856\n",
      "Gradient Descent(9132/9999): loss=101.83814515001843, w0=0.017466495660411755, w1=0.16958123450266935\n",
      "Gradient Descent(9133/9999): loss=101.82152705300574, w0=0.01746517185816838, w1=0.169563810327132\n",
      "Gradient Descent(9134/9999): loss=101.8049125770026, w0=0.017463848177643508, w1=0.16954638778871797\n",
      "Gradient Descent(9135/9999): loss=101.78830172113955, w0=0.017462524618824245, w1=0.16952896688726873\n",
      "Gradient Descent(9136/9999): loss=101.7716944845474, w0=0.017461201181697708, w1=0.16951154762262574\n",
      "Gradient Descent(9137/9999): loss=101.75509086635718, w0=0.017459877866251017, w1=0.16949412999463045\n",
      "Gradient Descent(9138/9999): loss=101.73849086570009, w0=0.01745855467247129, w1=0.16947671400312436\n",
      "Gradient Descent(9139/9999): loss=101.72189448170762, w0=0.017457231600345655, w1=0.169459299647949\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(9140/9999): loss=101.70530171351147, w0=0.017455908649861223, w1=0.16944188692894588\n",
      "Gradient Descent(9141/9999): loss=101.68871256024352, w0=0.017454585821005127, w1=0.16942447584595655\n",
      "Gradient Descent(9142/9999): loss=101.67212702103588, w0=0.017453263113764488, w1=0.16940706639882255\n",
      "Gradient Descent(9143/9999): loss=101.65554509502091, w0=0.017451940528126434, w1=0.16938965858738547\n",
      "Gradient Descent(9144/9999): loss=101.63896678133118, w0=0.017450618064078095, w1=0.16937225241148687\n",
      "Gradient Descent(9145/9999): loss=101.62239207909948, w0=0.017449295721606597, w1=0.16935484787096838\n",
      "Gradient Descent(9146/9999): loss=101.60582098745881, w0=0.017447973500699078, w1=0.1693374449656716\n",
      "Gradient Descent(9147/9999): loss=101.5892535055424, w0=0.017446651401342668, w1=0.16932004369543818\n",
      "Gradient Descent(9148/9999): loss=101.57268963248373, w0=0.017445329423524503, w1=0.16930264406010975\n",
      "Gradient Descent(9149/9999): loss=101.5561293674164, w0=0.01744400756723172, w1=0.169285246059528\n",
      "Gradient Descent(9150/9999): loss=101.53957270947437, w0=0.017442685832451458, w1=0.1692678496935346\n",
      "Gradient Descent(9151/9999): loss=101.52301965779176, w0=0.017441364219170852, w1=0.16925045496197122\n",
      "Gradient Descent(9152/9999): loss=101.50647021150289, w0=0.01744004272737705, w1=0.1692330618646796\n",
      "Gradient Descent(9153/9999): loss=101.48992436974221, w0=0.01743872135705719, w1=0.16921567040150143\n",
      "Gradient Descent(9154/9999): loss=101.47338213164463, w0=0.017437400108198418, w1=0.16919828057227848\n",
      "Gradient Descent(9155/9999): loss=101.45684349634509, w0=0.017436078980787882, w1=0.1691808923768525\n",
      "Gradient Descent(9156/9999): loss=101.44030846297878, w0=0.017434757974812726, w1=0.16916350581506523\n",
      "Gradient Descent(9157/9999): loss=101.42377703068117, w0=0.0174334370902601, w1=0.1691461208867585\n",
      "Gradient Descent(9158/9999): loss=101.40724919858795, w0=0.017432116327117157, w1=0.1691287375917741\n",
      "Gradient Descent(9159/9999): loss=101.3907249658349, w0=0.017430795685371047, w1=0.16911135592995385\n",
      "Gradient Descent(9160/9999): loss=101.37420433155822, w0=0.017429475165008926, w1=0.1690939759011396\n",
      "Gradient Descent(9161/9999): loss=101.3576872948941, w0=0.017428154766017948, w1=0.16907659750517312\n",
      "Gradient Descent(9162/9999): loss=101.34117385497922, w0=0.017426834488385272, w1=0.16905922074189636\n",
      "Gradient Descent(9163/9999): loss=101.32466401095022, w0=0.017425514332098054, w1=0.16904184561115115\n",
      "Gradient Descent(9164/9999): loss=101.30815776194414, w0=0.017424194297143456, w1=0.1690244721127794\n",
      "Gradient Descent(9165/9999): loss=101.29165510709814, w0=0.01742287438350864, w1=0.16900710024662305\n",
      "Gradient Descent(9166/9999): loss=101.27515604554965, w0=0.017421554591180766, w1=0.168989730012524\n",
      "Gradient Descent(9167/9999): loss=101.25866057643633, w0=0.017420234920147003, w1=0.16897236141032415\n",
      "Gradient Descent(9168/9999): loss=101.24216869889597, w0=0.017418915370394517, w1=0.1689549944398655\n",
      "Gradient Descent(9169/9999): loss=101.22568041206668, w0=0.017417595941910474, w1=0.16893762910099003\n",
      "Gradient Descent(9170/9999): loss=101.20919571508676, w0=0.017416276634682044, w1=0.1689202653935397\n",
      "Gradient Descent(9171/9999): loss=101.19271460709473, w0=0.0174149574486964, w1=0.1689029033173565\n",
      "Gradient Descent(9172/9999): loss=101.1762370872293, w0=0.017413638383940715, w1=0.16888554287228247\n",
      "Gradient Descent(9173/9999): loss=101.1597631546294, w0=0.017412319440402158, w1=0.16886818405815965\n",
      "Gradient Descent(9174/9999): loss=101.14329280843424, w0=0.017411000618067912, w1=0.16885082687483008\n",
      "Gradient Descent(9175/9999): loss=101.12682604778323, w0=0.01740968191692515, w1=0.1688334713221358\n",
      "Gradient Descent(9176/9999): loss=101.11036287181591, w0=0.01740836333696105, w1=0.16881611739991892\n",
      "Gradient Descent(9177/9999): loss=101.09390327967212, w0=0.017407044878162796, w1=0.16879876510802153\n",
      "Gradient Descent(9178/9999): loss=101.07744727049199, w0=0.017405726540517568, w1=0.16878141444628572\n",
      "Gradient Descent(9179/9999): loss=101.06099484341567, w0=0.01740440832401255, w1=0.16876406541455363\n",
      "Gradient Descent(9180/9999): loss=101.04454599758374, w0=0.017403090228634926, w1=0.16874671801266738\n",
      "Gradient Descent(9181/9999): loss=101.02810073213686, w0=0.017401772254371888, w1=0.16872937224046916\n",
      "Gradient Descent(9182/9999): loss=101.01165904621595, w0=0.01740045440121062, w1=0.16871202809780111\n",
      "Gradient Descent(9183/9999): loss=100.99522093896216, w0=0.017399136669138313, w1=0.16869468558450543\n",
      "Gradient Descent(9184/9999): loss=100.97878640951687, w0=0.01739781905814216, w1=0.1686773447004243\n",
      "Gradient Descent(9185/9999): loss=100.9623554570216, w0=0.01739650156820935, w1=0.16866000544539997\n",
      "Gradient Descent(9186/9999): loss=100.9459280806182, w0=0.017395184199327077, w1=0.16864266781927464\n",
      "Gradient Descent(9187/9999): loss=100.9295042794487, w0=0.017393866951482543, w1=0.1686253318218906\n",
      "Gradient Descent(9188/9999): loss=100.91308405265526, w0=0.017392549824662942, w1=0.16860799745309007\n",
      "Gradient Descent(9189/9999): loss=100.8966673993804, w0=0.017391232818855476, w1=0.16859066471271536\n",
      "Gradient Descent(9190/9999): loss=100.88025431876676, w0=0.017389915934047342, w1=0.16857333360060875\n",
      "Gradient Descent(9191/9999): loss=100.86384480995723, w0=0.017388599170225744, w1=0.16855600411661253\n",
      "Gradient Descent(9192/9999): loss=100.84743887209493, w0=0.01738728252737789, w1=0.16853867626056904\n",
      "Gradient Descent(9193/9999): loss=100.83103650432315, w0=0.01738596600549098, w1=0.1685213500323206\n",
      "Gradient Descent(9194/9999): loss=100.8146377057855, w0=0.017384649604552223, w1=0.16850402543170961\n",
      "Gradient Descent(9195/9999): loss=100.79824247562568, w0=0.01738333332454883, w1=0.1684867024585784\n",
      "Gradient Descent(9196/9999): loss=100.78185081298773, w0=0.017382017165468007, w1=0.16846938111276938\n",
      "Gradient Descent(9197/9999): loss=100.76546271701578, w0=0.01738070112729697, w1=0.1684520613941249\n",
      "Gradient Descent(9198/9999): loss=100.74907818685423, w0=0.01737938521002293, w1=0.16843474330248745\n",
      "Gradient Descent(9199/9999): loss=100.73269722164783, w0=0.017378069413633103, w1=0.16841742683769942\n",
      "Gradient Descent(9200/9999): loss=100.71631982054133, w0=0.017376753738114706, w1=0.16840011199960325\n",
      "Gradient Descent(9201/9999): loss=100.69994598267984, w0=0.017375438183454956, w1=0.1683827987880414\n",
      "Gradient Descent(9202/9999): loss=100.68357570720863, w0=0.017374122749641074, w1=0.16836548720285638\n",
      "Gradient Descent(9203/9999): loss=100.6672089932732, w0=0.017372807436660278, w1=0.16834817724389065\n",
      "Gradient Descent(9204/9999): loss=100.6508458400193, w0=0.017371492244499794, w1=0.16833086891098673\n",
      "Gradient Descent(9205/9999): loss=100.6344862465928, w0=0.017370177173146845, w1=0.16831356220398713\n",
      "Gradient Descent(9206/9999): loss=100.61813021213993, w0=0.017368862222588658, w1=0.1682962571227344\n",
      "Gradient Descent(9207/9999): loss=100.60177773580706, w0=0.017367547392812457, w1=0.1682789536670711\n",
      "Gradient Descent(9208/9999): loss=100.58542881674076, w0=0.017366232683805473, w1=0.1682616518368398\n",
      "Gradient Descent(9209/9999): loss=100.56908345408782, w0=0.01736491809555494, w1=0.16824435163188306\n",
      "Gradient Descent(9210/9999): loss=100.55274164699527, w0=0.017363603628048084, w1=0.1682270530520435\n",
      "Gradient Descent(9211/9999): loss=100.53640339461037, w0=0.017362289281272144, w1=0.16820975609716374\n",
      "Gradient Descent(9212/9999): loss=100.52006869608061, w0=0.01736097505521435, w1=0.16819246076708638\n",
      "Gradient Descent(9213/9999): loss=100.50373755055355, w0=0.017359660949861945, w1=0.16817516706165409\n",
      "Gradient Descent(9214/9999): loss=100.48740995717726, w0=0.01735834696520216, w1=0.16815787498070953\n",
      "Gradient Descent(9215/9999): loss=100.47108591509969, w0=0.017357033101222243, w1=0.16814058452409536\n",
      "Gradient Descent(9216/9999): loss=100.45476542346925, w0=0.01735571935790943, w1=0.1681232956916543\n",
      "Gradient Descent(9217/9999): loss=100.43844848143448, w0=0.017354405735250964, w1=0.16810600848322904\n",
      "Gradient Descent(9218/9999): loss=100.42213508814406, w0=0.017353092233234092, w1=0.1680887228986623\n",
      "Gradient Descent(9219/9999): loss=100.40582524274713, w0=0.017351778851846057, w1=0.16807143893779683\n",
      "Gradient Descent(9220/9999): loss=100.38951894439273, w0=0.017350465591074112, w1=0.16805415660047535\n",
      "Gradient Descent(9221/9999): loss=100.37321619223033, w0=0.017349152450905504, w1=0.16803687588654065\n",
      "Gradient Descent(9222/9999): loss=100.35691698540953, w0=0.01734783943132748, w1=0.1680195967958355\n",
      "Gradient Descent(9223/9999): loss=100.34062132308023, w0=0.017346526532327298, w1=0.16800231932820273\n",
      "Gradient Descent(9224/9999): loss=100.32432920439244, w0=0.017345213753892207, w1=0.1679850434834851\n",
      "Gradient Descent(9225/9999): loss=100.30804062849646, w0=0.017343901096009466, w1=0.1679677692615255\n",
      "Gradient Descent(9226/9999): loss=100.29175559454276, w0=0.01734258855866633, w1=0.16795049666216674\n",
      "Gradient Descent(9227/9999): loss=100.27547410168206, w0=0.01734127614185006, w1=0.16793322568525168\n",
      "Gradient Descent(9228/9999): loss=100.25919614906532, w0=0.017339963845547913, w1=0.1679159563306232\n",
      "Gradient Descent(9229/9999): loss=100.2429217358436, w0=0.01733865166974715, w1=0.1678986885981242\n",
      "Gradient Descent(9230/9999): loss=100.22665086116832, w0=0.017337339614435038, w1=0.16788142248759758\n",
      "Gradient Descent(9231/9999): loss=100.21038352419102, w0=0.01733602767959884, w1=0.16786415799888624\n",
      "Gradient Descent(9232/9999): loss=100.19411972406355, w0=0.017334715865225822, w1=0.16784689513183312\n",
      "Gradient Descent(9233/9999): loss=100.17785945993786, w0=0.017333404171303253, w1=0.1678296338862812\n",
      "Gradient Descent(9234/9999): loss=100.16160273096615, w0=0.0173320925978184, w1=0.16781237426207343\n",
      "Gradient Descent(9235/9999): loss=100.14534953630091, w0=0.01733078114475854, w1=0.1677951162590528\n",
      "Gradient Descent(9236/9999): loss=100.12909987509472, w0=0.017329469812110938, w1=0.16777785987706229\n",
      "Gradient Descent(9237/9999): loss=100.11285374650058, w0=0.01732815859986287, w1=0.1677606051159449\n",
      "Gradient Descent(9238/9999): loss=100.09661114967138, w0=0.017326847508001616, w1=0.1677433519755437\n",
      "Gradient Descent(9239/9999): loss=100.08037208376055, w0=0.017325536536514446, w1=0.1677261004557017\n",
      "Gradient Descent(9240/9999): loss=100.06413654792159, w0=0.017324225685388647, w1=0.16770885055626195\n",
      "Gradient Descent(9241/9999): loss=100.04790454130824, w0=0.017322914954611492, w1=0.16769160227706756\n",
      "Gradient Descent(9242/9999): loss=100.03167606307436, w0=0.01732160434417027, w1=0.1676743556179616\n",
      "Gradient Descent(9243/9999): loss=100.01545111237421, w0=0.01732029385405226, w1=0.16765711057878716\n",
      "Gradient Descent(9244/9999): loss=99.99922968836212, w0=0.017318983484244745, w1=0.1676398671593874\n",
      "Gradient Descent(9245/9999): loss=99.98301179019268, w0=0.017317673234735015, w1=0.1676226253596054\n",
      "Gradient Descent(9246/9999): loss=99.96679741702069, w0=0.01731636310551036, w1=0.16760538517928433\n",
      "Gradient Descent(9247/9999): loss=99.9505865680012, w0=0.017315053096558064, w1=0.16758814661826738\n",
      "Gradient Descent(9248/9999): loss=99.9343792422894, w0=0.017313743207865422, w1=0.1675709096763977\n",
      "Gradient Descent(9249/9999): loss=99.91817543904074, w0=0.01731243343941973, w1=0.1675536743535185\n",
      "Gradient Descent(9250/9999): loss=99.90197515741094, w0=0.017311123791208274, w1=0.16753644064947298\n",
      "Gradient Descent(9251/9999): loss=99.88577839655584, w0=0.017309814263218353, w1=0.16751920856410438\n",
      "Gradient Descent(9252/9999): loss=99.86958515563155, w0=0.017308504855437265, w1=0.16750197809725592\n",
      "Gradient Descent(9253/9999): loss=99.85339543379435, w0=0.01730719556785231, w1=0.16748474924877088\n",
      "Gradient Descent(9254/9999): loss=99.83720923020078, w0=0.017305886400450787, w1=0.16746752201849252\n",
      "Gradient Descent(9255/9999): loss=99.82102654400761, w0=0.017304577353219996, w1=0.16745029640626413\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(9256/9999): loss=99.80484737437172, w0=0.017303268426147246, w1=0.167433072411929\n",
      "Gradient Descent(9257/9999): loss=99.78867172045034, w0=0.01730195961921984, w1=0.16741585003533047\n",
      "Gradient Descent(9258/9999): loss=99.77249958140085, w0=0.017300650932425082, w1=0.16739862927631186\n",
      "Gradient Descent(9259/9999): loss=99.7563309563808, w0=0.017299342365750283, w1=0.1673814101347165\n",
      "Gradient Descent(9260/9999): loss=99.74016584454807, w0=0.01729803391918275, w1=0.16736419261038776\n",
      "Gradient Descent(9261/9999): loss=99.7240042450606, w0=0.0172967255927098, w1=0.16734697670316906\n",
      "Gradient Descent(9262/9999): loss=99.70784615707667, w0=0.01729541738631874, w1=0.16732976241290373\n",
      "Gradient Descent(9263/9999): loss=99.69169157975477, w0=0.01729410929999689, w1=0.16731254973943524\n",
      "Gradient Descent(9264/9999): loss=99.67554051225349, w0=0.01729280133373156, w1=0.16729533868260696\n",
      "Gradient Descent(9265/9999): loss=99.65939295373178, w0=0.01729149348751007, w1=0.16727812924226237\n",
      "Gradient Descent(9266/9999): loss=99.64324890334868, w0=0.017290185761319736, w1=0.16726092141824492\n",
      "Gradient Descent(9267/9999): loss=99.62710836026355, w0=0.01728887815514788, w1=0.16724371521039805\n",
      "Gradient Descent(9268/9999): loss=99.61097132363587, w0=0.01728757066898183, w1=0.16722651061856525\n",
      "Gradient Descent(9269/9999): loss=99.59483779262538, w0=0.017286263302808906, w1=0.16720930764259004\n",
      "Gradient Descent(9270/9999): loss=99.57870776639209, w0=0.01728495605661643, w1=0.16719210628231593\n",
      "Gradient Descent(9271/9999): loss=99.56258124409607, w0=0.01728364893039173, w1=0.16717490653758646\n",
      "Gradient Descent(9272/9999): loss=99.54645822489776, w0=0.01728234192412214, w1=0.16715770840824515\n",
      "Gradient Descent(9273/9999): loss=99.53033870795775, w0=0.017281035037794983, w1=0.1671405118941356\n",
      "Gradient Descent(9274/9999): loss=99.51422269243682, w0=0.017279728271397593, w1=0.16712331699510133\n",
      "Gradient Descent(9275/9999): loss=99.49811017749596, w0=0.017278421624917304, w1=0.16710612371098596\n",
      "Gradient Descent(9276/9999): loss=99.48200116229648, w0=0.01727711509834145, w1=0.1670889320416331\n",
      "Gradient Descent(9277/9999): loss=99.46589564599977, w0=0.017275808691657363, w1=0.1670717419868864\n",
      "Gradient Descent(9278/9999): loss=99.44979362776749, w0=0.017274502404852385, w1=0.16705455354658946\n",
      "Gradient Descent(9279/9999): loss=99.4336951067615, w0=0.017273196237913856, w1=0.16703736672058594\n",
      "Gradient Descent(9280/9999): loss=99.4176000821439, w0=0.017271890190829114, w1=0.1670201815087195\n",
      "Gradient Descent(9281/9999): loss=99.40150855307708, w0=0.017270584263585503, w1=0.16700299791083384\n",
      "Gradient Descent(9282/9999): loss=99.38542051872336, w0=0.017269278456170364, w1=0.16698581592677264\n",
      "Gradient Descent(9283/9999): loss=99.3693359782456, w0=0.017267972768571047, w1=0.1669686355563796\n",
      "Gradient Descent(9284/9999): loss=99.35325493080671, w0=0.017266667200774893, w1=0.16695145679949852\n",
      "Gradient Descent(9285/9999): loss=99.33717737556981, w0=0.017265361752769257, w1=0.16693427965597307\n",
      "Gradient Descent(9286/9999): loss=99.32110331169828, w0=0.01726405642454148, w1=0.16691710412564703\n",
      "Gradient Descent(9287/9999): loss=99.30503273835572, w0=0.017262751216078922, w1=0.1668999302083642\n",
      "Gradient Descent(9288/9999): loss=99.28896565470586, w0=0.01726144612736893, w1=0.1668827579039683\n",
      "Gradient Descent(9289/9999): loss=99.27290205991274, w0=0.017260141158398863, w1=0.1668655872123032\n",
      "Gradient Descent(9290/9999): loss=99.25684195314057, w0=0.017258836309156075, w1=0.16684841813321272\n",
      "Gradient Descent(9291/9999): loss=99.24078533355376, w0=0.017257531579627925, w1=0.16683125066654067\n",
      "Gradient Descent(9292/9999): loss=99.22473220031698, w0=0.01725622696980177, w1=0.1668140848121309\n",
      "Gradient Descent(9293/9999): loss=99.20868255259501, w0=0.017254922479664973, w1=0.16679692056982728\n",
      "Gradient Descent(9294/9999): loss=99.19263638955299, w0=0.017253618109204893, w1=0.1667797579394737\n",
      "Gradient Descent(9295/9999): loss=99.17659371035617, w0=0.017252313858408898, w1=0.16676259692091402\n",
      "Gradient Descent(9296/9999): loss=99.16055451416999, w0=0.01725100972726435, w1=0.1667454375139922\n",
      "Gradient Descent(9297/9999): loss=99.14451880016021, w0=0.017249705715758616, w1=0.16672827971855214\n",
      "Gradient Descent(9298/9999): loss=99.1284865674927, w0=0.017248401823879066, w1=0.1667111235344378\n",
      "Gradient Descent(9299/9999): loss=99.11245781533363, w0=0.01724709805161307, w1=0.1666939689614931\n",
      "Gradient Descent(9300/9999): loss=99.0964325428493, w0=0.017245794398947998, w1=0.16667681599956205\n",
      "Gradient Descent(9301/9999): loss=99.08041074920625, w0=0.017244490865871223, w1=0.1666596646484886\n",
      "Gradient Descent(9302/9999): loss=99.06439243357124, w0=0.01724318745237012, w1=0.1666425149081168\n",
      "Gradient Descent(9303/9999): loss=99.04837759511126, w0=0.01724188415843207, w1=0.16662536677829062\n",
      "Gradient Descent(9304/9999): loss=99.03236623299351, w0=0.01724058098404444, w1=0.1666082202588541\n",
      "Gradient Descent(9305/9999): loss=99.01635834638535, w0=0.01723927792919462, w1=0.1665910753496513\n",
      "Gradient Descent(9306/9999): loss=99.00035393445437, w0=0.017237974993869986, w1=0.1665739320505263\n",
      "Gradient Descent(9307/9999): loss=98.98435299636841, w0=0.01723667217805792, w1=0.16655679036132315\n",
      "Gradient Descent(9308/9999): loss=98.96835553129553, w0=0.017235369481745807, w1=0.16653965028188594\n",
      "Gradient Descent(9309/9999): loss=98.95236153840393, w0=0.01723406690492103, w1=0.1665225118120588\n",
      "Gradient Descent(9310/9999): loss=98.93637101686204, w0=0.017232764447570978, w1=0.16650537495168585\n",
      "Gradient Descent(9311/9999): loss=98.9203839658386, w0=0.017231462109683036, w1=0.16648823970061122\n",
      "Gradient Descent(9312/9999): loss=98.90440038450244, w0=0.017230159891244597, w1=0.16647110605867907\n",
      "Gradient Descent(9313/9999): loss=98.88842027202266, w0=0.017228857792243053, w1=0.16645397402573356\n",
      "Gradient Descent(9314/9999): loss=98.87244362756853, w0=0.017227555812665794, w1=0.16643684360161887\n",
      "Gradient Descent(9315/9999): loss=98.85647045030953, w0=0.01722625395250022, w1=0.16641971478617923\n",
      "Gradient Descent(9316/9999): loss=98.84050073941548, w0=0.017224952211733726, w1=0.16640258757925883\n",
      "Gradient Descent(9317/9999): loss=98.82453449405624, w0=0.017223650590353703, w1=0.1663854619807019\n",
      "Gradient Descent(9318/9999): loss=98.80857171340193, w0=0.017222349088347556, w1=0.16636833799035267\n",
      "Gradient Descent(9319/9999): loss=98.79261239662297, w0=0.017221047705702687, w1=0.16635121560805544\n",
      "Gradient Descent(9320/9999): loss=98.77665654288987, w0=0.01721974644240649, w1=0.16633409483365444\n",
      "Gradient Descent(9321/9999): loss=98.76070415137345, w0=0.017218445298446378, w1=0.166316975666994\n",
      "Gradient Descent(9322/9999): loss=98.74475522124465, w0=0.01721714427380975, w1=0.1662998581079184\n",
      "Gradient Descent(9323/9999): loss=98.72880975167469, w0=0.017215843368484016, w1=0.16628274215627198\n",
      "Gradient Descent(9324/9999): loss=98.71286774183498, w0=0.017214542582456582, w1=0.16626562781189907\n",
      "Gradient Descent(9325/9999): loss=98.69692919089714, w0=0.017213241915714862, w1=0.16624851507464403\n",
      "Gradient Descent(9326/9999): loss=98.68099409803295, w0=0.017211941368246265, w1=0.16623140394435118\n",
      "Gradient Descent(9327/9999): loss=98.66506246241451, w0=0.017210640940038203, w1=0.16621429442086494\n",
      "Gradient Descent(9328/9999): loss=98.64913428321407, w0=0.017209340631078093, w1=0.1661971865040297\n",
      "Gradient Descent(9329/9999): loss=98.63320955960401, w0=0.017208040441353347, w1=0.16618008019368988\n",
      "Gradient Descent(9330/9999): loss=98.61728829075706, w0=0.017206740370851386, w1=0.1661629754896899\n",
      "Gradient Descent(9331/9999): loss=98.60137047584611, w0=0.017205440419559626, w1=0.1661458723918742\n",
      "Gradient Descent(9332/9999): loss=98.5854561140442, w0=0.017204140587465493, w1=0.16612877090008724\n",
      "Gradient Descent(9333/9999): loss=98.5695452045247, w0=0.017202840874556403, w1=0.16611167101417348\n",
      "Gradient Descent(9334/9999): loss=98.55363774646108, w0=0.017201541280819785, w1=0.16609457273397743\n",
      "Gradient Descent(9335/9999): loss=98.53773373902706, w0=0.01720024180624306, w1=0.1660774760593436\n",
      "Gradient Descent(9336/9999): loss=98.52183318139653, w0=0.01719894245081366, w1=0.16606038099011644\n",
      "Gradient Descent(9337/9999): loss=98.50593607274371, w0=0.017197643214519005, w1=0.16604328752614056\n",
      "Gradient Descent(9338/9999): loss=98.49004241224289, w0=0.017196344097346532, w1=0.16602619566726046\n",
      "Gradient Descent(9339/9999): loss=98.47415219906864, w0=0.017195045099283667, w1=0.16600910541332073\n",
      "Gradient Descent(9340/9999): loss=98.45826543239575, w0=0.017193746220317847, w1=0.16599201676416594\n",
      "Gradient Descent(9341/9999): loss=98.44238211139914, w0=0.017192447460436504, w1=0.1659749297196407\n",
      "Gradient Descent(9342/9999): loss=98.42650223525409, w0=0.017191148819627074, w1=0.1659578442795896\n",
      "Gradient Descent(9343/9999): loss=98.41062580313597, w0=0.017189850297876997, w1=0.16594076044385725\n",
      "Gradient Descent(9344/9999): loss=98.39475281422035, w0=0.01718855189517371, w1=0.16592367821228832\n",
      "Gradient Descent(9345/9999): loss=98.37888326768308, w0=0.017187253611504655, w1=0.16590659758472742\n",
      "Gradient Descent(9346/9999): loss=98.36301716270013, w0=0.017185955446857273, w1=0.16588951856101927\n",
      "Gradient Descent(9347/9999): loss=98.34715449844785, w0=0.01718465740121901, w1=0.16587244114100852\n",
      "Gradient Descent(9348/9999): loss=98.33129527410256, w0=0.017183359474577303, w1=0.16585536532453987\n",
      "Gradient Descent(9349/9999): loss=98.31543948884101, w0=0.01718206166691961, w1=0.16583829111145806\n",
      "Gradient Descent(9350/9999): loss=98.29958714184, w0=0.017180763978233368, w1=0.1658212185016078\n",
      "Gradient Descent(9351/9999): loss=98.28373823227662, w0=0.017179466408506032, w1=0.16580414749483385\n",
      "Gradient Descent(9352/9999): loss=98.26789275932818, w0=0.017178168957725056, w1=0.16578707809098095\n",
      "Gradient Descent(9353/9999): loss=98.25205072217214, w0=0.01717687162587789, w1=0.16577001028989388\n",
      "Gradient Descent(9354/9999): loss=98.23621211998623, w0=0.017175574412951988, w1=0.16575294409141744\n",
      "Gradient Descent(9355/9999): loss=98.22037695194832, w0=0.017174277318934804, w1=0.16573587949539642\n",
      "Gradient Descent(9356/9999): loss=98.20454521723656, w0=0.017172980343813798, w1=0.16571881650167566\n",
      "Gradient Descent(9357/9999): loss=98.18871691502927, w0=0.017171683487576427, w1=0.1657017551101\n",
      "Gradient Descent(9358/9999): loss=98.17289204450496, w0=0.017170386750210154, w1=0.16568469532051425\n",
      "Gradient Descent(9359/9999): loss=98.15707060484242, w0=0.017169090131702442, w1=0.1656676371327633\n",
      "Gradient Descent(9360/9999): loss=98.14125259522054, w0=0.01716779363204075, w1=0.16565058054669204\n",
      "Gradient Descent(9361/9999): loss=98.12543801481854, w0=0.017166497251212547, w1=0.16563352556214536\n",
      "Gradient Descent(9362/9999): loss=98.10962686281579, w0=0.017165200989205296, w1=0.16561647217896816\n",
      "Gradient Descent(9363/9999): loss=98.09381913839182, w0=0.017163904846006467, w1=0.16559942039700537\n",
      "Gradient Descent(9364/9999): loss=98.07801484072647, w0=0.017162608821603528, w1=0.16558237021610195\n",
      "Gradient Descent(9365/9999): loss=98.06221396899969, w0=0.01716131291598395, w1=0.16556532163610282\n",
      "Gradient Descent(9366/9999): loss=98.0464165223917, w0=0.01716001712913521, w1=0.16554827465685298\n",
      "Gradient Descent(9367/9999): loss=98.03062250008293, w0=0.017158721461044774, w1=0.1655312292781974\n",
      "Gradient Descent(9368/9999): loss=98.01483190125394, w0=0.017157425911700122, w1=0.16551418549998112\n",
      "Gradient Descent(9369/9999): loss=97.99904472508564, w0=0.017156130481088733, w1=0.16549714332204912\n",
      "Gradient Descent(9370/9999): loss=97.98326097075902, w0=0.017154835169198086, w1=0.16548010274424643\n",
      "Gradient Descent(9371/9999): loss=97.96748063745537, w0=0.017153539976015656, w1=0.16546306376641812\n",
      "Gradient Descent(9372/9999): loss=97.95170372435601, w0=0.01715224490152893, w1=0.16544602638840925\n",
      "Gradient Descent(9373/9999): loss=97.9359302306428, w0=0.017150949945725386, w1=0.16542899061006486\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(9374/9999): loss=97.92016015549746, w0=0.017149655108592513, w1=0.16541195643123008\n",
      "Gradient Descent(9375/9999): loss=97.9043934981021, w0=0.017148360390117797, w1=0.16539492385175\n",
      "Gradient Descent(9376/9999): loss=97.88863025763901, w0=0.017147065790288726, w1=0.16537789287146976\n",
      "Gradient Descent(9377/9999): loss=97.87287043329069, w0=0.017145771309092787, w1=0.1653608634902345\n",
      "Gradient Descent(9378/9999): loss=97.85711402423985, w0=0.017144476946517474, w1=0.16534383570788933\n",
      "Gradient Descent(9379/9999): loss=97.84136102966934, w0=0.017143182702550278, w1=0.16532680952427944\n",
      "Gradient Descent(9380/9999): loss=97.82561144876236, w0=0.01714188857717869, w1=0.16530978493925005\n",
      "Gradient Descent(9381/9999): loss=97.80986528070218, w0=0.01714059457039021, w1=0.16529276195264633\n",
      "Gradient Descent(9382/9999): loss=97.7941225246723, w0=0.017139300682172332, w1=0.1652757405643135\n",
      "Gradient Descent(9383/9999): loss=97.77838317985653, w0=0.017138006912512552, w1=0.16525872077409676\n",
      "Gradient Descent(9384/9999): loss=97.76264724543873, w0=0.017136713261398377, w1=0.16524170258184137\n",
      "Gradient Descent(9385/9999): loss=97.74691472060313, w0=0.017135419728817304, w1=0.1652246859873926\n",
      "Gradient Descent(9386/9999): loss=97.73118560453405, w0=0.017134126314756835, w1=0.1652076709905957\n",
      "Gradient Descent(9387/9999): loss=97.71545989641604, w0=0.01713283301920448, w1=0.16519065759129598\n",
      "Gradient Descent(9388/9999): loss=97.69973759543389, w0=0.017131539842147738, w1=0.1651736457893387\n",
      "Gradient Descent(9389/9999): loss=97.68401870077261, w0=0.01713024678357412, w1=0.16515663558456925\n",
      "Gradient Descent(9390/9999): loss=97.66830321161733, w0=0.017128953843471136, w1=0.16513962697683293\n",
      "Gradient Descent(9391/9999): loss=97.65259112715347, w0=0.017127661021826293, w1=0.16512261996597508\n",
      "Gradient Descent(9392/9999): loss=97.63688244656663, w0=0.017126368318627107, w1=0.16510561455184106\n",
      "Gradient Descent(9393/9999): loss=97.62117716904262, w0=0.01712507573386109, w1=0.16508861073427628\n",
      "Gradient Descent(9394/9999): loss=97.60547529376746, w0=0.01712378326751576, w1=0.1650716085131261\n",
      "Gradient Descent(9395/9999): loss=97.58977681992738, w0=0.01712249091957863, w1=0.16505460788823595\n",
      "Gradient Descent(9396/9999): loss=97.57408174670876, w0=0.01712119869003722, w1=0.16503760885945123\n",
      "Gradient Descent(9397/9999): loss=97.55839007329827, w0=0.01711990657887905, w1=0.16502061142661742\n",
      "Gradient Descent(9398/9999): loss=97.54270179888273, w0=0.017118614586091638, w1=0.16500361558957993\n",
      "Gradient Descent(9399/9999): loss=97.52701692264922, w0=0.01711732271166251, w1=0.16498662134818426\n",
      "Gradient Descent(9400/9999): loss=97.51133544378497, w0=0.017116030955579193, w1=0.16496962870227588\n",
      "Gradient Descent(9401/9999): loss=97.49565736147743, w0=0.017114739317829208, w1=0.1649526376517003\n",
      "Gradient Descent(9402/9999): loss=97.47998267491428, w0=0.017113447798400083, w1=0.164935648196303\n",
      "Gradient Descent(9403/9999): loss=97.4643113832834, w0=0.017112156397279345, w1=0.16491866033592956\n",
      "Gradient Descent(9404/9999): loss=97.44864348577282, w0=0.01711086511445453, w1=0.1649016740704255\n",
      "Gradient Descent(9405/9999): loss=97.4329789815709, w0=0.017109573949913165, w1=0.16488468939963638\n",
      "Gradient Descent(9406/9999): loss=97.41731786986608, w0=0.017108282903642785, w1=0.16486770632340778\n",
      "Gradient Descent(9407/9999): loss=97.40166014984707, w0=0.017106991975630925, w1=0.16485072484158528\n",
      "Gradient Descent(9408/9999): loss=97.38600582070279, w0=0.01710570116586512, w1=0.16483374495401448\n",
      "Gradient Descent(9409/9999): loss=97.37035488162232, w0=0.01710441047433291, w1=0.16481676666054101\n",
      "Gradient Descent(9410/9999): loss=97.354707331795, w0=0.017103119901021833, w1=0.16479978996101052\n",
      "Gradient Descent(9411/9999): loss=97.33906317041033, w0=0.017101829445919432, w1=0.16478281485526863\n",
      "Gradient Descent(9412/9999): loss=97.32342239665803, w0=0.017100539109013248, w1=0.16476584134316102\n",
      "Gradient Descent(9413/9999): loss=97.30778500972805, w0=0.017099248890290825, w1=0.16474886942453337\n",
      "Gradient Descent(9414/9999): loss=97.29215100881052, w0=0.017097958789739708, w1=0.1647318990992314\n",
      "Gradient Descent(9415/9999): loss=97.2765203930958, w0=0.017096668807347445, w1=0.16471493036710078\n",
      "Gradient Descent(9416/9999): loss=97.26089316177439, w0=0.017095378943101584, w1=0.16469796322798724\n",
      "Gradient Descent(9417/9999): loss=97.2452693140371, w0=0.017094089196989674, w1=0.16468099768173655\n",
      "Gradient Descent(9418/9999): loss=97.22964884907488, w0=0.01709279956899927, w1=0.16466403372819444\n",
      "Gradient Descent(9419/9999): loss=97.21403176607883, w0=0.017091510059117923, w1=0.1646470713672067\n",
      "Gradient Descent(9420/9999): loss=97.19841806424044, w0=0.01709022066733319, w1=0.16463011059861907\n",
      "Gradient Descent(9421/9999): loss=97.18280774275117, w0=0.01708893139363262, w1=0.1646131514222774\n",
      "Gradient Descent(9422/9999): loss=97.16720080080286, w0=0.017087642238003777, w1=0.16459619383802748\n",
      "Gradient Descent(9423/9999): loss=97.1515972375875, w0=0.017086353200434216, w1=0.16457923784571515\n",
      "Gradient Descent(9424/9999): loss=97.13599705229728, w0=0.017085064280911503, w1=0.16456228344518628\n",
      "Gradient Descent(9425/9999): loss=97.12040024412454, w0=0.017083775479423195, w1=0.16454533063628668\n",
      "Gradient Descent(9426/9999): loss=97.10480681226194, w0=0.017082486795956857, w1=0.16452837941886225\n",
      "Gradient Descent(9427/9999): loss=97.08921675590229, w0=0.017081198230500055, w1=0.1645114297927589\n",
      "Gradient Descent(9428/9999): loss=97.07363007423857, w0=0.017079909783040356, w1=0.16449448175782252\n",
      "Gradient Descent(9429/9999): loss=97.05804676646397, w0=0.017078621453565326, w1=0.16447753531389903\n",
      "Gradient Descent(9430/9999): loss=97.04246683177196, w0=0.01707733324206254, w1=0.16446059046083436\n",
      "Gradient Descent(9431/9999): loss=97.0268902693562, w0=0.017076045148519563, w1=0.16444364719847446\n",
      "Gradient Descent(9432/9999): loss=97.01131707841044, w0=0.01707475717292397, w1=0.1644267055266653\n",
      "Gradient Descent(9433/9999): loss=96.99574725812874, w0=0.017073469315263337, w1=0.16440976544525288\n",
      "Gradient Descent(9434/9999): loss=96.98018080770534, w0=0.017072181575525237, w1=0.16439282695408317\n",
      "Gradient Descent(9435/9999): loss=96.96461772633475, w0=0.01707089395369725, w1=0.16437589005300218\n",
      "Gradient Descent(9436/9999): loss=96.94905801321153, w0=0.01706960644976695, w1=0.16435895474185594\n",
      "Gradient Descent(9437/9999): loss=96.93350166753054, w0=0.017068319063721922, w1=0.1643420210204905\n",
      "Gradient Descent(9438/9999): loss=96.91794868848687, w0=0.017067031795549747, w1=0.1643250888887519\n",
      "Gradient Descent(9439/9999): loss=96.90239907527577, w0=0.01706574464523801, w1=0.16430815834648624\n",
      "Gradient Descent(9440/9999): loss=96.88685282709271, w0=0.01706445761277429, w1=0.16429122939353957\n",
      "Gradient Descent(9441/9999): loss=96.87130994313331, w0=0.01706317069814618, w1=0.164274302029758\n",
      "Gradient Descent(9442/9999): loss=96.85577042259354, w0=0.017061883901341262, w1=0.16425737625498768\n",
      "Gradient Descent(9443/9999): loss=96.8402342646694, w0=0.017060597222347127, w1=0.1642404520690747\n",
      "Gradient Descent(9444/9999): loss=96.82470146855722, w0=0.017059310661151367, w1=0.16422352947186522\n",
      "Gradient Descent(9445/9999): loss=96.80917203345346, w0=0.017058024217741576, w1=0.16420660846320542\n",
      "Gradient Descent(9446/9999): loss=96.79364595855482, w0=0.017056737892105347, w1=0.16418968904294146\n",
      "Gradient Descent(9447/9999): loss=96.77812324305815, w0=0.017055451684230272, w1=0.16417277121091953\n",
      "Gradient Descent(9448/9999): loss=96.76260388616062, w0=0.01705416559410395, w1=0.16415585496698581\n",
      "Gradient Descent(9449/9999): loss=96.7470878870595, w0=0.01705287962171398, w1=0.16413894031098658\n",
      "Gradient Descent(9450/9999): loss=96.73157524495228, w0=0.01705159376704796, w1=0.16412202724276803\n",
      "Gradient Descent(9451/9999): loss=96.7160659590367, w0=0.017050308030093494, w1=0.16410511576217643\n",
      "Gradient Descent(9452/9999): loss=96.70056002851065, w0=0.017049022410838183, w1=0.16408820586905803\n",
      "Gradient Descent(9453/9999): loss=96.68505745257224, w0=0.017047736909269633, w1=0.16407129756325914\n",
      "Gradient Descent(9454/9999): loss=96.6695582304198, w0=0.01704645152537545, w1=0.16405439084462603\n",
      "Gradient Descent(9455/9999): loss=96.65406236125185, w0=0.01704516625914324, w1=0.16403748571300503\n",
      "Gradient Descent(9456/9999): loss=96.63856984426711, w0=0.017043881110560606, w1=0.16402058216824245\n",
      "Gradient Descent(9457/9999): loss=96.62308067866452, w0=0.01704259607961517, w1=0.16400368021018463\n",
      "Gradient Descent(9458/9999): loss=96.6075948636432, w0=0.017041311166294534, w1=0.16398677983867793\n",
      "Gradient Descent(9459/9999): loss=96.5921123984025, w0=0.01704002637058632, w1=0.16396988105356872\n",
      "Gradient Descent(9460/9999): loss=96.57663328214197, w0=0.017038741692478137, w1=0.1639529838547034\n",
      "Gradient Descent(9461/9999): loss=96.56115751406134, w0=0.017037457131957603, w1=0.16393608824192835\n",
      "Gradient Descent(9462/9999): loss=96.54568509336055, w0=0.01703617268901234, w1=0.16391919421509\n",
      "Gradient Descent(9463/9999): loss=96.53021601923972, w0=0.01703488836362996, w1=0.16390230177403475\n",
      "Gradient Descent(9464/9999): loss=96.51475029089926, w0=0.017033604155798087, w1=0.1638854109186091\n",
      "Gradient Descent(9465/9999): loss=96.49928790753968, w0=0.017032320065504345, w1=0.16386852164865948\n",
      "Gradient Descent(9466/9999): loss=96.48382886836175, w0=0.017031036092736356, w1=0.16385163396403235\n",
      "Gradient Descent(9467/9999): loss=96.46837317256644, w0=0.017029752237481748, w1=0.16383474786457425\n",
      "Gradient Descent(9468/9999): loss=96.4529208193549, w0=0.017028468499728146, w1=0.16381786335013165\n",
      "Gradient Descent(9469/9999): loss=96.43747180792852, w0=0.01702718487946318, w1=0.16380098042055108\n",
      "Gradient Descent(9470/9999): loss=96.42202613748883, w0=0.01702590137667448, w1=0.16378409907567906\n",
      "Gradient Descent(9471/9999): loss=96.40658380723761, w0=0.017024617991349675, w1=0.16376721931536217\n",
      "Gradient Descent(9472/9999): loss=96.39114481637684, w0=0.017023334723476398, w1=0.16375034113944695\n",
      "Gradient Descent(9473/9999): loss=96.37570916410871, w0=0.017022051573042287, w1=0.16373346454777998\n",
      "Gradient Descent(9474/9999): loss=96.36027684963555, w0=0.017020768540034974, w1=0.16371658954020787\n",
      "Gradient Descent(9475/9999): loss=96.34484787216, w0=0.0170194856244421, w1=0.16369971611657724\n",
      "Gradient Descent(9476/9999): loss=96.32942223088482, w0=0.0170182028262513, w1=0.1636828442767347\n",
      "Gradient Descent(9477/9999): loss=96.31399992501295, w0=0.017016920145450216, w1=0.1636659740205269\n",
      "Gradient Descent(9478/9999): loss=96.29858095374766, w0=0.017015637582026493, w1=0.16364910534780047\n",
      "Gradient Descent(9479/9999): loss=96.28316531629223, w0=0.01701435513596777, w1=0.16363223825840212\n",
      "Gradient Descent(9480/9999): loss=96.26775301185036, w0=0.017013072807261696, w1=0.1636153727521785\n",
      "Gradient Descent(9481/9999): loss=96.25234403962578, w0=0.01701179059589591, w1=0.16359850882897634\n",
      "Gradient Descent(9482/9999): loss=96.23693839882252, w0=0.01701050850185807, w1=0.16358164648864232\n",
      "Gradient Descent(9483/9999): loss=96.22153608864475, w0=0.017009226525135823, w1=0.1635647857310232\n",
      "Gradient Descent(9484/9999): loss=96.2061371082969, w0=0.017007944665716816, w1=0.16354792655596573\n",
      "Gradient Descent(9485/9999): loss=96.19074145698352, w0=0.017006662923588704, w1=0.16353106896331665\n",
      "Gradient Descent(9486/9999): loss=96.17534913390946, w0=0.017005381298739138, w1=0.16351421295292273\n",
      "Gradient Descent(9487/9999): loss=96.15996013827969, w0=0.017004099791155775, w1=0.1634973585246308\n",
      "Gradient Descent(9488/9999): loss=96.14457446929943, w0=0.017002818400826274, w1=0.16348050567828762\n",
      "Gradient Descent(9489/9999): loss=96.12919212617412, w0=0.017001537127738293, w1=0.16346365441374003\n",
      "Gradient Descent(9490/9999): loss=96.1138131081093, w0=0.017000255971879492, w1=0.16344680473083487\n",
      "Gradient Descent(9491/9999): loss=96.09843741431084, w0=0.016998974933237532, w1=0.16342995662941898\n",
      "Gradient Descent(9492/9999): loss=96.08306504398469, w0=0.016997694011800075, w1=0.16341311010933923\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(9493/9999): loss=96.06769599633714, w0=0.016996413207554784, w1=0.1633962651704425\n",
      "Gradient Descent(9494/9999): loss=96.05233027057454, w0=0.01699513252048933, w1=0.1633794218125757\n",
      "Gradient Descent(9495/9999): loss=96.03696786590353, w0=0.016993851950591374, w1=0.1633625800355857\n",
      "Gradient Descent(9496/9999): loss=96.02160878153092, w0=0.01699257149784859, w1=0.16334573983931946\n",
      "Gradient Descent(9497/9999): loss=96.00625301666376, w0=0.016991291162248647, w1=0.16332890122362392\n",
      "Gradient Descent(9498/9999): loss=95.99090057050924, w0=0.016990010943779218, w1=0.163312064188346\n",
      "Gradient Descent(9499/9999): loss=95.97555144227478, w0=0.016988730842427975, w1=0.16329522873333271\n",
      "Gradient Descent(9500/9999): loss=95.96020563116801, w0=0.016987450858182592, w1=0.16327839485843101\n",
      "Gradient Descent(9501/9999): loss=95.94486313639673, w0=0.01698617099103075, w1=0.16326156256348792\n",
      "Gradient Descent(9502/9999): loss=95.929523957169, w0=0.01698489124096012, w1=0.16324473184835042\n",
      "Gradient Descent(9503/9999): loss=95.91418809269302, w0=0.016983611607958384, w1=0.16322790271286558\n",
      "Gradient Descent(9504/9999): loss=95.89885554217722, w0=0.016982332092013226, w1=0.1632110751568804\n",
      "Gradient Descent(9505/9999): loss=95.88352630483021, w0=0.016981052693112328, w1=0.163194249180242\n",
      "Gradient Descent(9506/9999): loss=95.86820037986087, w0=0.01697977341124337, w1=0.1631774247827974\n",
      "Gradient Descent(9507/9999): loss=95.85287776647814, w0=0.016978494246394037, w1=0.1631606019643937\n",
      "Gradient Descent(9508/9999): loss=95.83755846389133, w0=0.016977215198552022, w1=0.163143780724878\n",
      "Gradient Descent(9509/9999): loss=95.82224247130982, w0=0.01697593626770501, w1=0.16312696106409746\n",
      "Gradient Descent(9510/9999): loss=95.80692978794323, w0=0.01697465745384069, w1=0.1631101429818992\n",
      "Gradient Descent(9511/9999): loss=95.79162041300145, w0=0.016973378756946756, w1=0.16309332647813032\n",
      "Gradient Descent(9512/9999): loss=95.77631434569447, w0=0.016972100177010896, w1=0.16307651155263803\n",
      "Gradient Descent(9513/9999): loss=95.76101158523251, w0=0.01697082171402081, w1=0.1630596982052695\n",
      "Gradient Descent(9514/9999): loss=95.74571213082596, w0=0.016969543367964187, w1=0.16304288643587192\n",
      "Gradient Descent(9515/9999): loss=95.73041598168555, w0=0.01696826513882873, w1=0.1630260762442925\n",
      "Gradient Descent(9516/9999): loss=95.71512313702208, w0=0.016966987026602137, w1=0.16300926763037846\n",
      "Gradient Descent(9517/9999): loss=95.69983359604649, w0=0.01696570903127211, w1=0.16299246059397704\n",
      "Gradient Descent(9518/9999): loss=95.6845473579701, w0=0.016964431152826344, w1=0.1629756551349355\n",
      "Gradient Descent(9519/9999): loss=95.66926442200433, w0=0.01696315339125255, w1=0.16295885125310108\n",
      "Gradient Descent(9520/9999): loss=95.65398478736078, w0=0.01696187574653843, w1=0.1629420489483211\n",
      "Gradient Descent(9521/9999): loss=95.63870845325131, w0=0.01696059821867169, w1=0.16292524822044283\n",
      "Gradient Descent(9522/9999): loss=95.62343541888795, w0=0.016959320807640038, w1=0.1629084490693136\n",
      "Gradient Descent(9523/9999): loss=95.6081656834829, w0=0.016958043513431182, w1=0.16289165149478074\n",
      "Gradient Descent(9524/9999): loss=95.59289924624863, w0=0.016956766336032835, w1=0.1628748554966916\n",
      "Gradient Descent(9525/9999): loss=95.5776361063977, w0=0.016955489275432705, w1=0.1628580610748935\n",
      "Gradient Descent(9526/9999): loss=95.56237626314307, w0=0.01695421233161851, w1=0.16284126822923387\n",
      "Gradient Descent(9527/9999): loss=95.5471197156976, w0=0.016952935504577965, w1=0.16282447695956004\n",
      "Gradient Descent(9528/9999): loss=95.53186646327462, w0=0.016951658794298788, w1=0.16280768726571945\n",
      "Gradient Descent(9529/9999): loss=95.51661650508754, w0=0.016950382200768693, w1=0.1627908991475595\n",
      "Gradient Descent(9530/9999): loss=95.50136984035, w0=0.016949105723975406, w1=0.16277411260492763\n",
      "Gradient Descent(9531/9999): loss=95.48612646827581, w0=0.01694782936390664, w1=0.16275732763767128\n",
      "Gradient Descent(9532/9999): loss=95.470886388079, w0=0.01694655312055012, w1=0.1627405442456379\n",
      "Gradient Descent(9533/9999): loss=95.45564959897374, w0=0.016945276993893577, w1=0.162723762428675\n",
      "Gradient Descent(9534/9999): loss=95.44041610017455, w0=0.016944000983924727, w1=0.16270698218663007\n",
      "Gradient Descent(9535/9999): loss=95.42518589089599, w0=0.016942725090631303, w1=0.1626902035193506\n",
      "Gradient Descent(9536/9999): loss=95.4099589703529, w0=0.016941449314001034, w1=0.1626734264266841\n",
      "Gradient Descent(9537/9999): loss=95.3947353377603, w0=0.01694017365402165, w1=0.1626566509084781\n",
      "Gradient Descent(9538/9999): loss=95.37951499233341, w0=0.016938898110680875, w1=0.1626398769645802\n",
      "Gradient Descent(9539/9999): loss=95.36429793328765, w0=0.016937622683966452, w1=0.16262310459483795\n",
      "Gradient Descent(9540/9999): loss=95.34908415983863, w0=0.016936347373866113, w1=0.1626063337990989\n",
      "Gradient Descent(9541/9999): loss=95.33387367120217, w0=0.01693507218036759, w1=0.16258956457721066\n",
      "Gradient Descent(9542/9999): loss=95.31866646659428, w0=0.016933797103458628, w1=0.16257279692902085\n",
      "Gradient Descent(9543/9999): loss=95.3034625452312, w0=0.016932522143126957, w1=0.1625560308543771\n",
      "Gradient Descent(9544/9999): loss=95.2882619063293, w0=0.016931247299360325, w1=0.16253926635312702\n",
      "Gradient Descent(9545/9999): loss=95.27306454910524, w0=0.01692997257214647, w1=0.1625225034251183\n",
      "Gradient Descent(9546/9999): loss=95.25787047277578, w0=0.016928697961473137, w1=0.16250574207019858\n",
      "Gradient Descent(9547/9999): loss=95.24267967655794, w0=0.01692742346732807, w1=0.1624889822882156\n",
      "Gradient Descent(9548/9999): loss=95.22749215966901, w0=0.016926149089699014, w1=0.162472224079017\n",
      "Gradient Descent(9549/9999): loss=95.21230792132627, w0=0.016924874828573723, w1=0.16245546744245054\n",
      "Gradient Descent(9550/9999): loss=95.19712696074735, w0=0.01692360068393994, w1=0.1624387123783639\n",
      "Gradient Descent(9551/9999): loss=95.18194927715012, w0=0.01692232665578542, w1=0.16242195888660488\n",
      "Gradient Descent(9552/9999): loss=95.16677486975256, w0=0.016921052744097914, w1=0.1624052069670212\n",
      "Gradient Descent(9553/9999): loss=95.15160373777285, w0=0.016919778948865177, w1=0.16238845661946066\n",
      "Gradient Descent(9554/9999): loss=95.13643588042933, w0=0.016918505270074963, w1=0.16237170784377103\n",
      "Gradient Descent(9555/9999): loss=95.1212712969407, w0=0.01691723170771503, w1=0.16235496063980012\n",
      "Gradient Descent(9556/9999): loss=95.10610998652568, w0=0.016915958261773136, w1=0.16233821500739576\n",
      "Gradient Descent(9557/9999): loss=95.09095194840329, w0=0.01691468493223704, w1=0.16232147094640578\n",
      "Gradient Descent(9558/9999): loss=95.07579718179268, w0=0.016913411719094506, w1=0.16230472845667804\n",
      "Gradient Descent(9559/9999): loss=95.06064568591331, w0=0.016912138622333293, w1=0.16228798753806037\n",
      "Gradient Descent(9560/9999): loss=95.04549745998474, w0=0.016910865641941168, w1=0.16227124819040067\n",
      "Gradient Descent(9561/9999): loss=95.03035250322668, w0=0.016909592777905894, w1=0.16225451041354683\n",
      "Gradient Descent(9562/9999): loss=95.01521081485917, w0=0.01690832003021524, w1=0.16223777420734675\n",
      "Gradient Descent(9563/9999): loss=95.00007239410242, w0=0.016907047398856977, w1=0.16222103957164838\n",
      "Gradient Descent(9564/9999): loss=94.98493724017672, w0=0.01690577488381887, w1=0.16220430650629963\n",
      "Gradient Descent(9565/9999): loss=94.96980535230271, w0=0.016904502485088697, w1=0.16218757501114847\n",
      "Gradient Descent(9566/9999): loss=94.95467672970113, w0=0.016903230202654225, w1=0.16217084508604288\n",
      "Gradient Descent(9567/9999): loss=94.93955137159297, w0=0.016901958036503233, w1=0.1621541167308308\n",
      "Gradient Descent(9568/9999): loss=94.92442927719935, w0=0.016900685986623497, w1=0.16213738994536026\n",
      "Gradient Descent(9569/9999): loss=94.90931044574171, w0=0.01689941405300279, w1=0.16212066472947925\n",
      "Gradient Descent(9570/9999): loss=94.8941948764415, w0=0.0168981422356289, w1=0.16210394108303583\n",
      "Gradient Descent(9571/9999): loss=94.87908256852057, w0=0.0168968705344896, w1=0.16208721900587802\n",
      "Gradient Descent(9572/9999): loss=94.86397352120085, w0=0.016895598949572675, w1=0.16207049849785388\n",
      "Gradient Descent(9573/9999): loss=94.84886773370448, w0=0.016894327480865907, w1=0.16205377955881148\n",
      "Gradient Descent(9574/9999): loss=94.83376520525381, w0=0.016893056128357082, w1=0.16203706218859892\n",
      "Gradient Descent(9575/9999): loss=94.8186659350714, w0=0.01689178489203399, w1=0.1620203463870643\n",
      "Gradient Descent(9576/9999): loss=94.80356992237995, w0=0.01689051377188441, w1=0.16200363215405572\n",
      "Gradient Descent(9577/9999): loss=94.78847716640244, w0=0.01688924276789614, w1=0.16198691948942132\n",
      "Gradient Descent(9578/9999): loss=94.77338766636201, w0=0.01688797188005697, w1=0.16197020839300924\n",
      "Gradient Descent(9579/9999): loss=94.75830142148196, w0=0.01688670110835469, w1=0.16195349886466764\n",
      "Gradient Descent(9580/9999): loss=94.74321843098585, w0=0.016885430452777093, w1=0.1619367909042447\n",
      "Gradient Descent(9581/9999): loss=94.72813869409737, w0=0.016884159913311977, w1=0.16192008451158862\n",
      "Gradient Descent(9582/9999): loss=94.71306221004049, w0=0.016882889489947137, w1=0.1619033796865476\n",
      "Gradient Descent(9583/9999): loss=94.69798897803928, w0=0.016881619182670372, w1=0.16188667642896987\n",
      "Gradient Descent(9584/9999): loss=94.6829189973181, w0=0.016880348991469483, w1=0.16186997473870365\n",
      "Gradient Descent(9585/9999): loss=94.66785226710144, w0=0.01687907891633227, w1=0.1618532746155972\n",
      "Gradient Descent(9586/9999): loss=94.652788786614, w0=0.016877808957246537, w1=0.16183657605949878\n",
      "Gradient Descent(9587/9999): loss=94.63772855508071, w0=0.016876539114200086, w1=0.16181987907025666\n",
      "Gradient Descent(9588/9999): loss=94.62267157172667, w0=0.016875269387180727, w1=0.16180318364771915\n",
      "Gradient Descent(9589/9999): loss=94.60761783577713, w0=0.016873999776176263, w1=0.16178648979173454\n",
      "Gradient Descent(9590/9999): loss=94.59256734645766, w0=0.016872730281174503, w1=0.16176979750215117\n",
      "Gradient Descent(9591/9999): loss=94.57752010299392, w0=0.01687146090216326, w1=0.16175310677881738\n",
      "Gradient Descent(9592/9999): loss=94.56247610461178, w0=0.016870191639130342, w1=0.1617364176215815\n",
      "Gradient Descent(9593/9999): loss=94.54743535053733, w0=0.01686892249206357, w1=0.16171973003029194\n",
      "Gradient Descent(9594/9999): loss=94.53239783999685, w0=0.016867653460950746, w1=0.16170304400479704\n",
      "Gradient Descent(9595/9999): loss=94.5173635722168, w0=0.016866384545779697, w1=0.16168635954494523\n",
      "Gradient Descent(9596/9999): loss=94.50233254642387, w0=0.016865115746538237, w1=0.1616696766505849\n",
      "Gradient Descent(9597/9999): loss=94.48730476184495, w0=0.01686384706321418, w1=0.1616529953215645\n",
      "Gradient Descent(9598/9999): loss=94.47228021770707, w0=0.016862578495795356, w1=0.16163631555773245\n",
      "Gradient Descent(9599/9999): loss=94.45725891323747, w0=0.01686131004426958, w1=0.16161963735893722\n",
      "Gradient Descent(9600/9999): loss=94.44224084766364, w0=0.016860041708624676, w1=0.1616029607250273\n",
      "Gradient Descent(9601/9999): loss=94.42722602021324, w0=0.016858773488848473, w1=0.16158628565585118\n",
      "Gradient Descent(9602/9999): loss=94.41221443011409, w0=0.016857505384928795, w1=0.1615696121512573\n",
      "Gradient Descent(9603/9999): loss=94.39720607659424, w0=0.01685623739685347, w1=0.16155294021109426\n",
      "Gradient Descent(9604/9999): loss=94.3822009588819, w0=0.016854969524610325, w1=0.16153626983521052\n",
      "Gradient Descent(9605/9999): loss=94.36719907620554, w0=0.016853701768187193, w1=0.16151960102345467\n",
      "Gradient Descent(9606/9999): loss=94.35220042779379, w0=0.016852434127571907, w1=0.16150293377567523\n",
      "Gradient Descent(9607/9999): loss=94.33720501287539, w0=0.0168511666027523, w1=0.1614862680917208\n",
      "Gradient Descent(9608/9999): loss=94.32221283067943, w0=0.016849899193716208, w1=0.16146960397144\n",
      "Gradient Descent(9609/9999): loss=94.30722388043515, w0=0.01684863190045147, w1=0.16145294141468142\n",
      "Gradient Descent(9610/9999): loss=94.29223816137194, w0=0.01684736472294592, w1=0.16143628042129365\n",
      "Gradient Descent(9611/9999): loss=94.27725567271935, w0=0.016846097661187397, w1=0.16141962099112536\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(9612/9999): loss=94.26227641370724, w0=0.016844830715163746, w1=0.1614029631240252\n",
      "Gradient Descent(9613/9999): loss=94.24730038356553, w0=0.016843563884862807, w1=0.16138630681984178\n",
      "Gradient Descent(9614/9999): loss=94.23232758152449, w0=0.016842297170272427, w1=0.16136965207842385\n",
      "Gradient Descent(9615/9999): loss=94.2173580068145, w0=0.01684103057138045, w1=0.16135299889962007\n",
      "Gradient Descent(9616/9999): loss=94.20239165866607, w0=0.01683976408817472, w1=0.16133634728327914\n",
      "Gradient Descent(9617/9999): loss=94.18742853631001, w0=0.01683849772064309, w1=0.1613196972292498\n",
      "Gradient Descent(9618/9999): loss=94.17246863897732, w0=0.01683723146877341, w1=0.16130304873738077\n",
      "Gradient Descent(9619/9999): loss=94.15751196589913, w0=0.016835965332553528, w1=0.16128640180752082\n",
      "Gradient Descent(9620/9999): loss=94.14255851630678, w0=0.0168346993119713, w1=0.16126975643951874\n",
      "Gradient Descent(9621/9999): loss=94.12760828943186, w0=0.01683343340701458, w1=0.16125311263322328\n",
      "Gradient Descent(9622/9999): loss=94.1126612845061, w0=0.016832167617671222, w1=0.16123647038848327\n",
      "Gradient Descent(9623/9999): loss=94.09771750076148, w0=0.016830901943929083, w1=0.16121982970514748\n",
      "Gradient Descent(9624/9999): loss=94.08277693743003, w0=0.016829636385776024, w1=0.16120319058306476\n",
      "Gradient Descent(9625/9999): loss=94.06783959374421, w0=0.016828370943199906, w1=0.16118655302208396\n",
      "Gradient Descent(9626/9999): loss=94.05290546893646, w0=0.016827105616188588, w1=0.1611699170220539\n",
      "Gradient Descent(9627/9999): loss=94.03797456223954, w0=0.016825840404729935, w1=0.16115328258282352\n",
      "Gradient Descent(9628/9999): loss=94.02304687288631, w0=0.01682457530881181, w1=0.16113664970424166\n",
      "Gradient Descent(9629/9999): loss=94.00812240010994, w0=0.016823310328422082, w1=0.16112001838615722\n",
      "Gradient Descent(9630/9999): loss=93.99320114314371, w0=0.016822045463548617, w1=0.16110338862841914\n",
      "Gradient Descent(9631/9999): loss=93.9782831012211, w0=0.016820780714179282, w1=0.1610867604308763\n",
      "Gradient Descent(9632/9999): loss=93.96336827357584, w0=0.016819516080301952, w1=0.1610701337933777\n",
      "Gradient Descent(9633/9999): loss=93.94845665944175, w0=0.016818251561904494, w1=0.1610535087157723\n",
      "Gradient Descent(9634/9999): loss=93.93354825805295, w0=0.016816987158974787, w1=0.161036885197909\n",
      "Gradient Descent(9635/9999): loss=93.9186430686437, w0=0.0168157228715007, w1=0.1610202632396369\n",
      "Gradient Descent(9636/9999): loss=93.90374109044849, w0=0.016814458699470115, w1=0.16100364284080493\n",
      "Gradient Descent(9637/9999): loss=93.88884232270195, w0=0.016813194642870906, w1=0.16098702400126214\n",
      "Gradient Descent(9638/9999): loss=93.87394676463896, w0=0.016811930701690955, w1=0.16097040672085755\n",
      "Gradient Descent(9639/9999): loss=93.85905441549458, w0=0.016810666875918142, w1=0.1609537909994402\n",
      "Gradient Descent(9640/9999): loss=93.84416527450395, w0=0.016809403165540346, w1=0.1609371768368592\n",
      "Gradient Descent(9641/9999): loss=93.82927934090259, w0=0.016808139570545458, w1=0.16092056423296358\n",
      "Gradient Descent(9642/9999): loss=93.81439661392615, w0=0.01680687609092136, w1=0.16090395318760245\n",
      "Gradient Descent(9643/9999): loss=93.79951709281042, w0=0.016805612726655934, w1=0.16088734370062494\n",
      "Gradient Descent(9644/9999): loss=93.78464077679136, w0=0.016804349477737075, w1=0.16087073577188013\n",
      "Gradient Descent(9645/9999): loss=93.76976766510528, w0=0.01680308634415267, w1=0.16085412940121718\n",
      "Gradient Descent(9646/9999): loss=93.7548977569885, w0=0.016801823325890607, w1=0.16083752458848524\n",
      "Gradient Descent(9647/9999): loss=93.74003105167765, w0=0.016800560422938784, w1=0.16082092133353348\n",
      "Gradient Descent(9648/9999): loss=93.7251675484095, w0=0.016799297635285093, w1=0.16080431963621108\n",
      "Gradient Descent(9649/9999): loss=93.71030724642107, w0=0.016798034962917426, w1=0.16078771949636725\n",
      "Gradient Descent(9650/9999): loss=93.6954501449495, w0=0.01679677240582368, w1=0.16077112091385118\n",
      "Gradient Descent(9651/9999): loss=93.68059624323217, w0=0.01679550996399176, w1=0.1607545238885121\n",
      "Gradient Descent(9652/9999): loss=93.66574554050663, w0=0.016794247637409564, w1=0.16073792842019927\n",
      "Gradient Descent(9653/9999): loss=93.65089803601063, w0=0.01679298542606499, w1=0.16072133450876191\n",
      "Gradient Descent(9654/9999): loss=93.63605372898215, w0=0.016791723329945944, w1=0.16070474215404934\n",
      "Gradient Descent(9655/9999): loss=93.62121261865929, w0=0.016790461349040327, w1=0.1606881513559108\n",
      "Gradient Descent(9656/9999): loss=93.60637470428041, w0=0.01678919948333605, w1=0.16067156211419562\n",
      "Gradient Descent(9657/9999): loss=93.591539985084, w0=0.01678793773282101, w1=0.1606549744287531\n",
      "Gradient Descent(9658/9999): loss=93.5767084603088, w0=0.01678667609748313, w1=0.16063838829943258\n",
      "Gradient Descent(9659/9999): loss=93.56188012919372, w0=0.01678541457731031, w1=0.1606218037260834\n",
      "Gradient Descent(9660/9999): loss=93.54705499097791, w0=0.016784153172290463, w1=0.1606052207085549\n",
      "Gradient Descent(9661/9999): loss=93.5322330449006, w0=0.016782891882411503, w1=0.16058863924669647\n",
      "Gradient Descent(9662/9999): loss=93.5174142902013, w0=0.016781630707661345, w1=0.1605720593403575\n",
      "Gradient Descent(9663/9999): loss=93.50259872611966, w0=0.016780369648027905, w1=0.1605554809893874\n",
      "Gradient Descent(9664/9999): loss=93.48778635189562, w0=0.0167791087034991, w1=0.1605389041936356\n",
      "Gradient Descent(9665/9999): loss=93.47297716676916, w0=0.016777847874062852, w1=0.16052232895295146\n",
      "Gradient Descent(9666/9999): loss=93.4581711699806, w0=0.016776587159707076, w1=0.1605057552671845\n",
      "Gradient Descent(9667/9999): loss=93.44336836077039, w0=0.016775326560419697, w1=0.16048918313618415\n",
      "Gradient Descent(9668/9999): loss=93.42856873837917, w0=0.01677406607618864, w1=0.16047261255979992\n",
      "Gradient Descent(9669/9999): loss=93.41377230204773, w0=0.016772805707001823, w1=0.16045604353788126\n",
      "Gradient Descent(9670/9999): loss=93.39897905101714, w0=0.01677154545284718, w1=0.1604394760702777\n",
      "Gradient Descent(9671/9999): loss=93.3841889845286, w0=0.01677028531371263, w1=0.16042291015683877\n",
      "Gradient Descent(9672/9999): loss=93.36940210182354, w0=0.016769025289586115, w1=0.16040634579741397\n",
      "Gradient Descent(9673/9999): loss=93.35461840214357, w0=0.016767765380455556, w1=0.16038978299185289\n",
      "Gradient Descent(9674/9999): loss=93.33983788473041, w0=0.016766505586308888, w1=0.16037322174000507\n",
      "Gradient Descent(9675/9999): loss=93.32506054882613, w0=0.016765245907134043, w1=0.16035666204172008\n",
      "Gradient Descent(9676/9999): loss=93.31028639367288, w0=0.01676398634291896, w1=0.16034010389684755\n",
      "Gradient Descent(9677/9999): loss=93.29551541851303, w0=0.016762726893651568, w1=0.16032354730523707\n",
      "Gradient Descent(9678/9999): loss=93.28074762258913, w0=0.01676146755931981, w1=0.16030699226673825\n",
      "Gradient Descent(9679/9999): loss=93.26598300514395, w0=0.016760208339911625, w1=0.16029043878120075\n",
      "Gradient Descent(9680/9999): loss=93.25122156542042, w0=0.016758949235414954, w1=0.1602738868484742\n",
      "Gradient Descent(9681/9999): loss=93.23646330266172, w0=0.016757690245817736, w1=0.1602573364684083\n",
      "Gradient Descent(9682/9999): loss=93.22170821611111, w0=0.016756431371107917, w1=0.16024078764085273\n",
      "Gradient Descent(9683/9999): loss=93.20695630501216, w0=0.016755172611273444, w1=0.16022424036565716\n",
      "Gradient Descent(9684/9999): loss=93.19220756860855, w0=0.016753913966302263, w1=0.16020769464267132\n",
      "Gradient Descent(9685/9999): loss=93.17746200614422, w0=0.01675265543618232, w1=0.16019115047174493\n",
      "Gradient Descent(9686/9999): loss=93.1627196168632, w0=0.01675139702090157, w1=0.16017460785272775\n",
      "Gradient Descent(9687/9999): loss=93.14798040000984, w0=0.016750138720447956, w1=0.1601580667854695\n",
      "Gradient Descent(9688/9999): loss=93.1332443548286, w0=0.016748880534809437, w1=0.16014152726982\n",
      "Gradient Descent(9689/9999): loss=93.11851148056411, w0=0.016747622463973964, w1=0.16012498930562902\n",
      "Gradient Descent(9690/9999): loss=93.10378177646129, w0=0.01674636450792949, w1=0.16010845289274633\n",
      "Gradient Descent(9691/9999): loss=93.08905524176511, w0=0.016745106666663977, w1=0.16009191803102177\n",
      "Gradient Descent(9692/9999): loss=93.07433187572092, w0=0.016743848940165378, w1=0.16007538472030516\n",
      "Gradient Descent(9693/9999): loss=93.05961167757403, w0=0.01674259132842166, w1=0.16005885296044636\n",
      "Gradient Descent(9694/9999): loss=93.04489464657011, w0=0.016741333831420777, w1=0.1600423227512952\n",
      "Gradient Descent(9695/9999): loss=93.03018078195502, w0=0.016740076449150695, w1=0.1600257940927016\n",
      "Gradient Descent(9696/9999): loss=93.01547008297473, w0=0.01673881918159938, w1=0.1600092669845154\n",
      "Gradient Descent(9697/9999): loss=93.0007625488754, w0=0.016737562028754795, w1=0.15999274142658654\n",
      "Gradient Descent(9698/9999): loss=92.98605817890345, w0=0.016736304990604904, w1=0.15997621741876492\n",
      "Gradient Descent(9699/9999): loss=92.97135697230547, w0=0.01673504806713768, w1=0.15995969496090048\n",
      "Gradient Descent(9700/9999): loss=92.95665892832822, w0=0.016733791258341093, w1=0.15994317405284317\n",
      "Gradient Descent(9701/9999): loss=92.94196404621863, w0=0.01673253456420311, w1=0.15992665469444295\n",
      "Gradient Descent(9702/9999): loss=92.92727232522385, w0=0.016731277984711714, w1=0.15991013688554978\n",
      "Gradient Descent(9703/9999): loss=92.91258376459126, w0=0.01673002151985487, w1=0.15989362062601367\n",
      "Gradient Descent(9704/9999): loss=92.89789836356833, w0=0.016728765169620553, w1=0.15987710591568463\n",
      "Gradient Descent(9705/9999): loss=92.88321612140282, w0=0.016727508933996747, w1=0.15986059275441267\n",
      "Gradient Descent(9706/9999): loss=92.86853703734263, w0=0.016726252812971427, w1=0.15984408114204782\n",
      "Gradient Descent(9707/9999): loss=92.85386111063585, w0=0.016724996806532572, w1=0.15982757107844012\n",
      "Gradient Descent(9708/9999): loss=92.83918834053078, w0=0.016723740914668165, w1=0.15981106256343966\n",
      "Gradient Descent(9709/9999): loss=92.82451872627588, w0=0.01672248513736619, w1=0.1597945555968965\n",
      "Gradient Descent(9710/9999): loss=92.80985226711982, w0=0.01672122947461463, w1=0.15977805017866079\n",
      "Gradient Descent(9711/9999): loss=92.7951889623115, w0=0.01671997392640147, w1=0.15976154630858255\n",
      "Gradient Descent(9712/9999): loss=92.78052881109996, w0=0.016718718492714696, w1=0.159745043986512\n",
      "Gradient Descent(9713/9999): loss=92.76587181273439, w0=0.0167174631735423, w1=0.15972854321229918\n",
      "Gradient Descent(9714/9999): loss=92.75121796646428, w0=0.016716207968872274, w1=0.1597120439857943\n",
      "Gradient Descent(9715/9999): loss=92.7365672715392, w0=0.016714952878692606, w1=0.15969554630684754\n",
      "Gradient Descent(9716/9999): loss=92.721919727209, w0=0.016713697902991292, w1=0.15967905017530903\n",
      "Gradient Descent(9717/9999): loss=92.70727533272364, w0=0.016712443041756328, w1=0.15966255559102902\n",
      "Gradient Descent(9718/9999): loss=92.69263408733336, w0=0.016711188294975705, w1=0.1596460625538577\n",
      "Gradient Descent(9719/9999): loss=92.6779959902885, w0=0.016709933662637424, w1=0.15962957106364528\n",
      "Gradient Descent(9720/9999): loss=92.6633610408396, w0=0.016708679144729482, w1=0.15961308112024203\n",
      "Gradient Descent(9721/9999): loss=92.64872923823751, w0=0.01670742474123988, w1=0.1595965927234982\n",
      "Gradient Descent(9722/9999): loss=92.63410058173307, w0=0.016706170452156624, w1=0.15958010587326402\n",
      "Gradient Descent(9723/9999): loss=92.61947507057748, w0=0.016704916277467712, w1=0.15956362056938983\n",
      "Gradient Descent(9724/9999): loss=92.60485270402208, w0=0.01670366221716115, w1=0.15954713681172591\n",
      "Gradient Descent(9725/9999): loss=92.59023348131831, w0=0.016702408271224947, w1=0.15953065460012258\n",
      "Gradient Descent(9726/9999): loss=92.57561740171796, w0=0.016701154439647106, w1=0.15951417393443015\n",
      "Gradient Descent(9727/9999): loss=92.56100446447283, w0=0.01669990072241564, w1=0.15949769481449896\n",
      "Gradient Descent(9728/9999): loss=92.54639466883512, w0=0.01669864711951856, w1=0.15948121724017938\n",
      "Gradient Descent(9729/9999): loss=92.53178801405699, w0=0.016697393630943875, w1=0.15946474121132181\n",
      "Gradient Descent(9730/9999): loss=92.51718449939096, w0=0.0166961402566796, w1=0.1594482667277766\n",
      "Gradient Descent(9731/9999): loss=92.50258412408971, w0=0.01669488699671375, w1=0.15943179378939415\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(9732/9999): loss=92.48798688740601, w0=0.01669363385103434, w1=0.1594153223960249\n",
      "Gradient Descent(9733/9999): loss=92.47339278859292, w0=0.01669238081962939, w1=0.1593988525475193\n",
      "Gradient Descent(9734/9999): loss=92.45880182690362, w0=0.01669112790248692, w1=0.15938238424372775\n",
      "Gradient Descent(9735/9999): loss=92.4442140015916, w0=0.016689875099594947, w1=0.15936591748450074\n",
      "Gradient Descent(9736/9999): loss=92.42962931191039, w0=0.016688622410941496, w1=0.15934945226968872\n",
      "Gradient Descent(9737/9999): loss=92.41504775711377, w0=0.01668736983651459, w1=0.15933298859914222\n",
      "Gradient Descent(9738/9999): loss=92.40046933645574, w0=0.016686117376302254, w1=0.15931652647271172\n",
      "Gradient Descent(9739/9999): loss=92.38589404919048, w0=0.016684865030292515, w1=0.15930006589024773\n",
      "Gradient Descent(9740/9999): loss=92.37132189457226, w0=0.0166836127984734, w1=0.15928360685160078\n",
      "Gradient Descent(9741/9999): loss=92.35675287185568, w0=0.01668236068083294, w1=0.15926714935662145\n",
      "Gradient Descent(9742/9999): loss=92.34218698029548, w0=0.016681108677359162, w1=0.1592506934051603\n",
      "Gradient Descent(9743/9999): loss=92.3276242191465, w0=0.016679856788040102, w1=0.15923423899706787\n",
      "Gradient Descent(9744/9999): loss=92.31306458766389, w0=0.016678605012863792, w1=0.15921778613219478\n",
      "Gradient Descent(9745/9999): loss=92.29850808510297, w0=0.01667735335181827, w1=0.15920133481039164\n",
      "Gradient Descent(9746/9999): loss=92.28395471071916, w0=0.016676101804891572, w1=0.15918488503150907\n",
      "Gradient Descent(9747/9999): loss=92.26940446376818, w0=0.016674850372071733, w1=0.1591684367953977\n",
      "Gradient Descent(9748/9999): loss=92.25485734350585, w0=0.016673599053346793, w1=0.15915199010190817\n",
      "Gradient Descent(9749/9999): loss=92.24031334918824, w0=0.016672347848704798, w1=0.15913554495089116\n",
      "Gradient Descent(9750/9999): loss=92.22577248007156, w0=0.016671096758133785, w1=0.15911910134219737\n",
      "Gradient Descent(9751/9999): loss=92.21123473541222, w0=0.0166698457816218, w1=0.15910265927567746\n",
      "Gradient Descent(9752/9999): loss=92.19670011446686, w0=0.016668594919156888, w1=0.15908621875118215\n",
      "Gradient Descent(9753/9999): loss=92.18216861649228, w0=0.016667344170727096, w1=0.15906977976856218\n",
      "Gradient Descent(9754/9999): loss=92.16764024074544, w0=0.01666609353632047, w1=0.15905334232766827\n",
      "Gradient Descent(9755/9999): loss=92.15311498648347, w0=0.016664843015925065, w1=0.1590369064283512\n",
      "Gradient Descent(9756/9999): loss=92.13859285296387, w0=0.01666359260952893, w1=0.1590204720704617\n",
      "Gradient Descent(9757/9999): loss=92.12407383944402, w0=0.01666234231712011, w1=0.15900403925385057\n",
      "Gradient Descent(9758/9999): loss=92.10955794518176, w0=0.01666109213868667, w1=0.1589876079783686\n",
      "Gradient Descent(9759/9999): loss=92.09504516943495, w0=0.016659842074216658, w1=0.15897117824386664\n",
      "Gradient Descent(9760/9999): loss=92.08053551146178, w0=0.016658592123698135, w1=0.15895475005019546\n",
      "Gradient Descent(9761/9999): loss=92.06602897052046, w0=0.016657342287119157, w1=0.15893832339720596\n",
      "Gradient Descent(9762/9999): loss=92.05152554586957, w0=0.016656092564467782, w1=0.15892189828474895\n",
      "Gradient Descent(9763/9999): loss=92.03702523676768, w0=0.016654842955732076, w1=0.15890547471267533\n",
      "Gradient Descent(9764/9999): loss=92.02252804247372, w0=0.0166535934609001, w1=0.15888905268083595\n",
      "Gradient Descent(9765/9999): loss=92.00803396224671, w0=0.01665234407995992, w1=0.15887263218908174\n",
      "Gradient Descent(9766/9999): loss=91.9935429953459, w0=0.016651094812899597, w1=0.1588562132372636\n",
      "Gradient Descent(9767/9999): loss=91.97905514103067, w0=0.0166498456597072, w1=0.15883979582523244\n",
      "Gradient Descent(9768/9999): loss=91.96457039856068, w0=0.016648596620370795, w1=0.15882337995283924\n",
      "Gradient Descent(9769/9999): loss=91.95008876719572, w0=0.016647347694878453, w1=0.15880696561993496\n",
      "Gradient Descent(9770/9999): loss=91.93561024619574, w0=0.016646098883218247, w1=0.15879055282637056\n",
      "Gradient Descent(9771/9999): loss=91.92113483482095, w0=0.01664485018537825, w1=0.158774141571997\n",
      "Gradient Descent(9772/9999): loss=91.90666253233167, w0=0.016643601601346536, w1=0.15875773185666534\n",
      "Gradient Descent(9773/9999): loss=91.89219333798845, w0=0.01664235313111118, w1=0.15874132368022653\n",
      "Gradient Descent(9774/9999): loss=91.8777272510521, w0=0.016641104774660256, w1=0.15872491704253164\n",
      "Gradient Descent(9775/9999): loss=91.86326427078339, w0=0.01663985653198185, w1=0.15870851194343172\n",
      "Gradient Descent(9776/9999): loss=91.84880439644355, w0=0.016638608403064033, w1=0.1586921083827778\n",
      "Gradient Descent(9777/9999): loss=91.8343476272938, w0=0.01663736038789489, w1=0.158675706360421\n",
      "Gradient Descent(9778/9999): loss=91.81989396259567, w0=0.016636112486462507, w1=0.15865930587621238\n",
      "Gradient Descent(9779/9999): loss=91.80544340161082, w0=0.016634864698754966, w1=0.15864290693000305\n",
      "Gradient Descent(9780/9999): loss=91.79099594360103, w0=0.01663361702476035, w1=0.15862650952164412\n",
      "Gradient Descent(9781/9999): loss=91.77655158782842, w0=0.01663236946446675, w1=0.1586101136509867\n",
      "Gradient Descent(9782/9999): loss=91.76211033355517, w0=0.01663112201786225, w1=0.158593719317882\n",
      "Gradient Descent(9783/9999): loss=91.74767218004372, w0=0.01662987468493495, w1=0.15857732652218112\n",
      "Gradient Descent(9784/9999): loss=91.73323712655667, w0=0.016628627465672927, w1=0.1585609352637353\n",
      "Gradient Descent(9785/9999): loss=91.71880517235677, w0=0.016627380360064285, w1=0.1585445455423957\n",
      "Gradient Descent(9786/9999): loss=91.704376316707, w0=0.016626133368097113, w1=0.1585281573580135\n",
      "Gradient Descent(9787/9999): loss=91.6899505588705, w0=0.01662488648975951, w1=0.15851177071043995\n",
      "Gradient Descent(9788/9999): loss=91.67552789811069, w0=0.01662363972503957, w1=0.15849538559952628\n",
      "Gradient Descent(9789/9999): loss=91.66110833369102, w0=0.016622393073925393, w1=0.15847900202512374\n",
      "Gradient Descent(9790/9999): loss=91.64669186487522, w0=0.01662114653640508, w1=0.1584626199870836\n",
      "Gradient Descent(9791/9999): loss=91.63227849092722, w0=0.01661990011246673, w1=0.15844623948525713\n",
      "Gradient Descent(9792/9999): loss=91.61786821111103, w0=0.016618653802098447, w1=0.15842986051949562\n",
      "Gradient Descent(9793/9999): loss=91.60346102469103, w0=0.016617407605288334, w1=0.1584134830896504\n",
      "Gradient Descent(9794/9999): loss=91.58905693093162, w0=0.0166161615220245, w1=0.15839710719557276\n",
      "Gradient Descent(9795/9999): loss=91.57465592909742, w0=0.01661491555229505, w1=0.15838073283711407\n",
      "Gradient Descent(9796/9999): loss=91.56025801845333, w0=0.016613669696088094, w1=0.15836436001412565\n",
      "Gradient Descent(9797/9999): loss=91.54586319826431, w0=0.016612423953391738, w1=0.1583479887264589\n",
      "Gradient Descent(9798/9999): loss=91.53147146779558, w0=0.0166111783241941, w1=0.15833161897396517\n",
      "Gradient Descent(9799/9999): loss=91.51708282631252, w0=0.016609932808483288, w1=0.15831525075649588\n",
      "Gradient Descent(9800/9999): loss=91.50269727308071, w0=0.016608687406247418, w1=0.15829888407390244\n",
      "Gradient Descent(9801/9999): loss=91.4883148073659, w0=0.01660744211747461, w1=0.15828251892603626\n",
      "Gradient Descent(9802/9999): loss=91.47393542843407, w0=0.016606196942152977, w1=0.1582661553127488\n",
      "Gradient Descent(9803/9999): loss=91.45955913555129, w0=0.016604951880270637, w1=0.15824979323389146\n",
      "Gradient Descent(9804/9999): loss=91.44518592798391, w0=0.016603706931815714, w1=0.15823343268931578\n",
      "Gradient Descent(9805/9999): loss=91.43081580499846, w0=0.016602462096776325, w1=0.1582170736788732\n",
      "Gradient Descent(9806/9999): loss=91.41644876586155, w0=0.016601217375140594, w1=0.15820071620241521\n",
      "Gradient Descent(9807/9999): loss=91.4020848098401, w0=0.016599972766896644, w1=0.15818436025979335\n",
      "Gradient Descent(9808/9999): loss=91.38772393620116, w0=0.016598728272032605, w1=0.15816800585085913\n",
      "Gradient Descent(9809/9999): loss=91.37336614421197, w0=0.016597483890536602, w1=0.1581516529754641\n",
      "Gradient Descent(9810/9999): loss=91.35901143313994, w0=0.016596239622396765, w1=0.15813530163345982\n",
      "Gradient Descent(9811/9999): loss=91.3446598022527, w0=0.016594995467601223, w1=0.15811895182469785\n",
      "Gradient Descent(9812/9999): loss=91.33031125081807, w0=0.016593751426138108, w1=0.15810260354902977\n",
      "Gradient Descent(9813/9999): loss=91.315965778104, w0=0.01659250749799555, w1=0.1580862568063072\n",
      "Gradient Descent(9814/9999): loss=91.30162338337864, w0=0.01659126368316169, w1=0.15806991159638173\n",
      "Gradient Descent(9815/9999): loss=91.28728406591037, w0=0.016590019981624658, w1=0.158053567919105\n",
      "Gradient Descent(9816/9999): loss=91.2729478249677, w0=0.01658877639337259, w1=0.15803722577432863\n",
      "Gradient Descent(9817/9999): loss=91.25861465981939, w0=0.016587532918393634, w1=0.15802088516190432\n",
      "Gradient Descent(9818/9999): loss=91.24428456973432, w0=0.01658628955667592, w1=0.1580045460816837\n",
      "Gradient Descent(9819/9999): loss=91.22995755398158, w0=0.016585046308207597, w1=0.1579882085335185\n",
      "Gradient Descent(9820/9999): loss=91.21563361183044, w0=0.016583803172976804, w1=0.1579718725172604\n",
      "Gradient Descent(9821/9999): loss=91.20131274255036, w0=0.016582560150971686, w1=0.1579555380327611\n",
      "Gradient Descent(9822/9999): loss=91.18699494541099, w0=0.01658131724218039, w1=0.15793920507987236\n",
      "Gradient Descent(9823/9999): loss=91.17268021968216, w0=0.01658007444659106, w1=0.1579228736584459\n",
      "Gradient Descent(9824/9999): loss=91.15836856463389, w0=0.01657883176419185, w1=0.1579065437683335\n",
      "Gradient Descent(9825/9999): loss=91.1440599795364, w0=0.016577589194970908, w1=0.1578902154093869\n",
      "Gradient Descent(9826/9999): loss=91.12975446366, w0=0.016576346738916385, w1=0.15787388858145796\n",
      "Gradient Descent(9827/9999): loss=91.1154520162753, w0=0.01657510439601643, w1=0.1578575632843984\n",
      "Gradient Descent(9828/9999): loss=91.10115263665304, w0=0.01657386216625921, w1=0.1578412395180601\n",
      "Gradient Descent(9829/9999): loss=91.08685632406419, w0=0.016572620049632867, w1=0.15782491728229484\n",
      "Gradient Descent(9830/9999): loss=91.07256307777982, w0=0.016571378046125563, w1=0.15780859657695448\n",
      "Gradient Descent(9831/9999): loss=91.05827289707123, w0=0.01657013615572546, w1=0.1577922774018909\n",
      "Gradient Descent(9832/9999): loss=91.04398578120995, w0=0.016568894378420716, w1=0.15777595975695596\n",
      "Gradient Descent(9833/9999): loss=91.02970172946762, w0=0.016567652714199493, w1=0.15775964364200157\n",
      "Gradient Descent(9834/9999): loss=91.01542074111607, w0=0.016566411163049955, w1=0.15774332905687963\n",
      "Gradient Descent(9835/9999): loss=91.0011428154274, w0=0.016565169724960263, w1=0.15772701600144204\n",
      "Gradient Descent(9836/9999): loss=90.98686795167379, w0=0.016563928399918586, w1=0.15771070447554075\n",
      "Gradient Descent(9837/9999): loss=90.9725961491277, w0=0.01656268718791309, w1=0.1576943944790277\n",
      "Gradient Descent(9838/9999): loss=90.95832740706162, w0=0.016561446088931946, w1=0.15767808601175484\n",
      "Gradient Descent(9839/9999): loss=90.94406172474838, w0=0.01656020510296332, w1=0.15766177907357418\n",
      "Gradient Descent(9840/9999): loss=90.92979910146094, w0=0.016558964229995386, w1=0.15764547366433768\n",
      "Gradient Descent(9841/9999): loss=90.91553953647247, w0=0.016557723470016317, w1=0.15762916978389738\n",
      "Gradient Descent(9842/9999): loss=90.90128302905623, w0=0.016556482823014286, w1=0.15761286743210526\n",
      "Gradient Descent(9843/9999): loss=90.88702957848577, w0=0.01655524228897747, w1=0.15759656660881338\n",
      "Gradient Descent(9844/9999): loss=90.87277918403477, w0=0.016554001867894047, w1=0.15758026731387378\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(9845/9999): loss=90.85853184497708, w0=0.016552761559752195, w1=0.15756396954713855\n",
      "Gradient Descent(9846/9999): loss=90.84428756058679, w0=0.016551521364540096, w1=0.15754767330845973\n",
      "Gradient Descent(9847/9999): loss=90.83004633013816, w0=0.016550281282245928, w1=0.15753137859768945\n",
      "Gradient Descent(9848/9999): loss=90.81580815290553, w0=0.01654904131285788, w1=0.1575150854146798\n",
      "Gradient Descent(9849/9999): loss=90.80157302816365, w0=0.016547801456364128, w1=0.15749879375928288\n",
      "Gradient Descent(9850/9999): loss=90.78734095518716, w0=0.016546561712752865, w1=0.15748250363135086\n",
      "Gradient Descent(9851/9999): loss=90.77311193325113, w0=0.016545322082012273, w1=0.15746621503073588\n",
      "Gradient Descent(9852/9999): loss=90.75888596163064, w0=0.016544082564130546, w1=0.1574499279572901\n",
      "Gradient Descent(9853/9999): loss=90.7446630396011, w0=0.01654284315909587, w1=0.15743364241086572\n",
      "Gradient Descent(9854/9999): loss=90.73044316643801, w0=0.01654160386689644, w1=0.1574173583913149\n",
      "Gradient Descent(9855/9999): loss=90.71622634141707, w0=0.01654036468752044, w1=0.1574010758984899\n",
      "Gradient Descent(9856/9999): loss=90.70201256381415, w0=0.016539125620956076, w1=0.1573847949322429\n",
      "Gradient Descent(9857/9999): loss=90.68780183290535, w0=0.01653788666719154, w1=0.15736851549242614\n",
      "Gradient Descent(9858/9999): loss=90.67359414796695, w0=0.016536647826215025, w1=0.15735223757889188\n",
      "Gradient Descent(9859/9999): loss=90.65938950827532, w0=0.016535409098014736, w1=0.1573359611914924\n",
      "Gradient Descent(9860/9999): loss=90.64518791310714, w0=0.01653417048257887, w1=0.15731968633007998\n",
      "Gradient Descent(9861/9999): loss=90.63098936173917, w0=0.016532931979895627, w1=0.1573034129945069\n",
      "Gradient Descent(9862/9999): loss=90.6167938534484, w0=0.01653169358995321, w1=0.15728714118462547\n",
      "Gradient Descent(9863/9999): loss=90.60260138751201, w0=0.016530455312739827, w1=0.15727087090028805\n",
      "Gradient Descent(9864/9999): loss=90.58841196320738, w0=0.01652921714824368, w1=0.15725460214134696\n",
      "Gradient Descent(9865/9999): loss=90.57422557981198, w0=0.01652797909645298, w1=0.15723833490765454\n",
      "Gradient Descent(9866/9999): loss=90.56004223660352, w0=0.01652674115735593, w1=0.15722206919906317\n",
      "Gradient Descent(9867/9999): loss=90.54586193285998, w0=0.016525503330940747, w1=0.15720580501542522\n",
      "Gradient Descent(9868/9999): loss=90.5316846678594, w0=0.016524265617195638, w1=0.1571895423565931\n",
      "Gradient Descent(9869/9999): loss=90.51751044087999, w0=0.016523028016108814, w1=0.15717328122241922\n",
      "Gradient Descent(9870/9999): loss=90.50333925120026, w0=0.016521790527668494, w1=0.15715702161275602\n",
      "Gradient Descent(9871/9999): loss=90.4891710980988, w0=0.016520553151862894, w1=0.15714076352745593\n",
      "Gradient Descent(9872/9999): loss=90.47500598085443, w0=0.016519315888680226, w1=0.1571245069663714\n",
      "Gradient Descent(9873/9999): loss=90.46084389874616, w0=0.01651807873810871, w1=0.1571082519293549\n",
      "Gradient Descent(9874/9999): loss=90.44668485105313, w0=0.01651684170013657, w1=0.15709199841625893\n",
      "Gradient Descent(9875/9999): loss=90.43252883705472, w0=0.016515604774752026, w1=0.15707574642693597\n",
      "Gradient Descent(9876/9999): loss=90.41837585603042, w0=0.0165143679619433, w1=0.15705949596123853\n",
      "Gradient Descent(9877/9999): loss=90.40422590726, w0=0.016513131261698615, w1=0.15704324701901914\n",
      "Gradient Descent(9878/9999): loss=90.39007899002331, w0=0.016511894674006197, w1=0.15702699960013036\n",
      "Gradient Descent(9879/9999): loss=90.37593510360048, w0=0.016510658198854274, w1=0.15701075370442474\n",
      "Gradient Descent(9880/9999): loss=90.36179424727177, w0=0.016509421836231074, w1=0.15699450933175485\n",
      "Gradient Descent(9881/9999): loss=90.34765642031758, w0=0.016508185586124827, w1=0.15697826648197327\n",
      "Gradient Descent(9882/9999): loss=90.33352162201855, w0=0.016506949448523763, w1=0.1569620251549326\n",
      "Gradient Descent(9883/9999): loss=90.31938985165553, w0=0.016505713423416116, w1=0.15694578535048548\n",
      "Gradient Descent(9884/9999): loss=90.30526110850948, w0=0.016504477510790122, w1=0.1569295470684845\n",
      "Gradient Descent(9885/9999): loss=90.29113539186156, w0=0.016503241710634015, w1=0.15691331030878233\n",
      "Gradient Descent(9886/9999): loss=90.27701270099311, w0=0.016502006022936035, w1=0.15689707507123163\n",
      "Gradient Descent(9887/9999): loss=90.26289303518574, w0=0.016500770447684412, w1=0.15688084135568506\n",
      "Gradient Descent(9888/9999): loss=90.24877639372107, w0=0.016499534984867392, w1=0.1568646091619953\n",
      "Gradient Descent(9889/9999): loss=90.23466277588106, w0=0.016498299634473218, w1=0.15684837849001507\n",
      "Gradient Descent(9890/9999): loss=90.22055218094773, w0=0.016497064396490128, w1=0.15683214933959708\n",
      "Gradient Descent(9891/9999): loss=90.2064446082034, w0=0.016495829270906367, w1=0.15681592171059405\n",
      "Gradient Descent(9892/9999): loss=90.19234005693048, w0=0.01649459425771018, w1=0.15679969560285872\n",
      "Gradient Descent(9893/9999): loss=90.17823852641156, w0=0.01649335935688982, w1=0.15678347101624387\n",
      "Gradient Descent(9894/9999): loss=90.1641400159295, w0=0.01649212456843353, w1=0.15676724795060226\n",
      "Gradient Descent(9895/9999): loss=90.15004452476725, w0=0.01649088989232956, w1=0.1567510264057867\n",
      "Gradient Descent(9896/9999): loss=90.135952052208, w0=0.016489655328566163, w1=0.15673480638164997\n",
      "Gradient Descent(9897/9999): loss=90.12186259753507, w0=0.016488420877131586, w1=0.15671858787804488\n",
      "Gradient Descent(9898/9999): loss=90.107776160032, w0=0.01648718653801409, w1=0.15670237089482428\n",
      "Gradient Descent(9899/9999): loss=90.09369273898245, w0=0.016485952311201927, w1=0.156686155431841\n",
      "Gradient Descent(9900/9999): loss=90.07961233367038, w0=0.016484718196683356, w1=0.15666994148894792\n",
      "Gradient Descent(9901/9999): loss=90.06553494337984, w0=0.016483484194446632, w1=0.1566537290659979\n",
      "Gradient Descent(9902/9999): loss=90.05146056739504, w0=0.016482250304480018, w1=0.15663751816284383\n",
      "Gradient Descent(9903/9999): loss=90.03738920500044, w0=0.016481016526771773, w1=0.1566213087793386\n",
      "Gradient Descent(9904/9999): loss=90.0233208554806, w0=0.01647978286131016, w1=0.15660510091533517\n",
      "Gradient Descent(9905/9999): loss=90.0092555181204, w0=0.016478549308083443, w1=0.15658889457068642\n",
      "Gradient Descent(9906/9999): loss=89.99519319220472, w0=0.01647731586707989, w1=0.15657268974524532\n",
      "Gradient Descent(9907/9999): loss=89.98113387701875, w0=0.016476082538287765, w1=0.15655648643886486\n",
      "Gradient Descent(9908/9999): loss=89.96707757184784, w0=0.016474849321695333, w1=0.156540284651398\n",
      "Gradient Descent(9909/9999): loss=89.95302427597748, w0=0.016473616217290866, w1=0.1565240843826977\n",
      "Gradient Descent(9910/9999): loss=89.93897398869339, w0=0.016472383225062635, w1=0.15650788563261697\n",
      "Gradient Descent(9911/9999): loss=89.92492670928138, w0=0.016471150344998915, w1=0.15649168840100885\n",
      "Gradient Descent(9912/9999): loss=89.91088243702755, w0=0.01646991757708798, w1=0.1564754926877264\n",
      "Gradient Descent(9913/9999): loss=89.89684117121816, w0=0.0164686849213181, w1=0.1564592984926226\n",
      "Gradient Descent(9914/9999): loss=89.88280291113954, w0=0.016467452377677553, w1=0.15644310581555057\n",
      "Gradient Descent(9915/9999): loss=89.86876765607833, w0=0.01646621994615462, w1=0.15642691465636335\n",
      "Gradient Descent(9916/9999): loss=89.8547354053213, w0=0.01646498762673758, w1=0.15641072501491407\n",
      "Gradient Descent(9917/9999): loss=89.84070615815544, w0=0.01646375541941471, w1=0.1563945368910558\n",
      "Gradient Descent(9918/9999): loss=89.82667991386782, w0=0.016462523324174298, w1=0.15637835028464167\n",
      "Gradient Descent(9919/9999): loss=89.81265667174577, w0=0.016461291341004623, w1=0.15636216519552482\n",
      "Gradient Descent(9920/9999): loss=89.79863643107683, w0=0.01646005946989397, w1=0.1563459816235584\n",
      "Gradient Descent(9921/9999): loss=89.78461919114861, w0=0.01645882771083063, w1=0.15632979956859555\n",
      "Gradient Descent(9922/9999): loss=89.77060495124897, w0=0.016457596063802885, w1=0.1563136190304895\n",
      "Gradient Descent(9923/9999): loss=89.75659371066597, w0=0.016456364528799027, w1=0.15629744000909337\n",
      "Gradient Descent(9924/9999): loss=89.74258546868776, w0=0.01645513310580735, w1=0.15628126250426042\n",
      "Gradient Descent(9925/9999): loss=89.72858022460284, w0=0.01645390179481614, w1=0.15626508651584384\n",
      "Gradient Descent(9926/9999): loss=89.71457797769965, w0=0.016452670595813696, w1=0.1562489120436969\n",
      "Gradient Descent(9927/9999): loss=89.70057872726703, w0=0.01645143950878831, w1=0.15623273908767282\n",
      "Gradient Descent(9928/9999): loss=89.68658247259391, w0=0.016450208533728278, w1=0.15621656764762487\n",
      "Gradient Descent(9929/9999): loss=89.67258921296934, w0=0.0164489776706219, w1=0.15620039772340633\n",
      "Gradient Descent(9930/9999): loss=89.65859894768263, w0=0.01644774691945747, w1=0.1561842293148705\n",
      "Gradient Descent(9931/9999): loss=89.64461167602326, w0=0.016446516280223297, w1=0.15616806242187067\n",
      "Gradient Descent(9932/9999): loss=89.63062739728089, w0=0.016445285752907678, w1=0.1561518970442602\n",
      "Gradient Descent(9933/9999): loss=89.6166461107453, w0=0.016444055337498917, w1=0.1561357331818924\n",
      "Gradient Descent(9934/9999): loss=89.60266781570651, w0=0.01644282503398532, w1=0.1561195708346206\n",
      "Gradient Descent(9935/9999): loss=89.58869251145474, w0=0.016441594842355186, w1=0.15610341000229822\n",
      "Gradient Descent(9936/9999): loss=89.57472019728031, w0=0.016440364762596832, w1=0.15608725068477858\n",
      "Gradient Descent(9937/9999): loss=89.56075087247378, w0=0.01643913479469856, w1=0.1560710928819151\n",
      "Gradient Descent(9938/9999): loss=89.54678453632586, w0=0.016437904938648686, w1=0.15605493659356118\n",
      "Gradient Descent(9939/9999): loss=89.53282118812746, w0=0.01643667519443552, w1=0.15603878181957026\n",
      "Gradient Descent(9940/9999): loss=89.51886082716965, w0=0.016435445562047377, w1=0.15602262855979576\n",
      "Gradient Descent(9941/9999): loss=89.50490345274369, w0=0.01643421604147257, w1=0.15600647681409113\n",
      "Gradient Descent(9942/9999): loss=89.49094906414102, w0=0.016432986632699414, w1=0.15599032658230985\n",
      "Gradient Descent(9943/9999): loss=89.47699766065321, w0=0.016431757335716224, w1=0.1559741778643054\n",
      "Gradient Descent(9944/9999): loss=89.46304924157215, w0=0.016430528150511322, w1=0.15595803065993125\n",
      "Gradient Descent(9945/9999): loss=89.44910380618971, w0=0.01642929907707303, w1=0.15594188496904093\n",
      "Gradient Descent(9946/9999): loss=89.4351613537981, w0=0.016428070115389666, w1=0.15592574079148797\n",
      "Gradient Descent(9947/9999): loss=89.42122188368964, w0=0.016426841265449556, w1=0.15590959812712588\n",
      "Gradient Descent(9948/9999): loss=89.40728539515679, w0=0.016425612527241024, w1=0.15589345697580823\n",
      "Gradient Descent(9949/9999): loss=89.3933518874923, w0=0.016424383900752394, w1=0.15587731733738858\n",
      "Gradient Descent(9950/9999): loss=89.37942135998901, w0=0.016423155385971992, w1=0.15586117921172052\n",
      "Gradient Descent(9951/9999): loss=89.36549381193997, w0=0.016421926982888153, w1=0.15584504259865764\n",
      "Gradient Descent(9952/9999): loss=89.35156924263838, w0=0.0164206986914892, w1=0.15582890749805356\n",
      "Gradient Descent(9953/9999): loss=89.33764765137765, w0=0.01641947051176347, w1=0.1558127739097619\n",
      "Gradient Descent(9954/9999): loss=89.32372903745134, w0=0.016418242443699292, w1=0.15579664183363628\n",
      "Gradient Descent(9955/9999): loss=89.30981340015323, w0=0.016417014487285005, w1=0.15578051126953038\n",
      "Gradient Descent(9956/9999): loss=89.29590073877728, w0=0.016415786642508944, w1=0.15576438221729785\n",
      "Gradient Descent(9957/9999): loss=89.28199105261753, w0=0.01641455890935944, w1=0.15574825467679237\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(9958/9999): loss=89.26808434096829, w0=0.016413331287824837, w1=0.15573212864786765\n",
      "Gradient Descent(9959/9999): loss=89.25418060312407, w0=0.016412103777893473, w1=0.15571600413037737\n",
      "Gradient Descent(9960/9999): loss=89.24027983837945, w0=0.01641087637955369, w1=0.15569988112417527\n",
      "Gradient Descent(9961/9999): loss=89.22638204602933, w0=0.016409649092793827, w1=0.1556837596291151\n",
      "Gradient Descent(9962/9999): loss=89.2124872253686, w0=0.016408421917602233, w1=0.1556676396450506\n",
      "Gradient Descent(9963/9999): loss=89.19859537569255, w0=0.016407194853967254, w1=0.15565152117183553\n",
      "Gradient Descent(9964/9999): loss=89.1847064962965, w0=0.016405967901877233, w1=0.1556354042093237\n",
      "Gradient Descent(9965/9999): loss=89.17082058647597, w0=0.01640474106132052, w1=0.15561928875736888\n",
      "Gradient Descent(9966/9999): loss=89.15693764552667, w0=0.016403514332285464, w1=0.1556031748158249\n",
      "Gradient Descent(9967/9999): loss=89.14305767274453, w0=0.01640228771476042, w1=0.15558706238454556\n",
      "Gradient Descent(9968/9999): loss=89.12918066742554, w0=0.016401061208733737, w1=0.1555709514633847\n",
      "Gradient Descent(9969/9999): loss=89.115306628866, w0=0.01639983481419377, w1=0.15555484205219622\n",
      "Gradient Descent(9970/9999): loss=89.1014355563623, w0=0.016398608531128872, w1=0.15553873415083394\n",
      "Gradient Descent(9971/9999): loss=89.08756744921111, w0=0.016397382359527403, w1=0.15552262775915177\n",
      "Gradient Descent(9972/9999): loss=89.0737023067091, w0=0.01639615629937772, w1=0.1555065228770036\n",
      "Gradient Descent(9973/9999): loss=89.05984012815328, w0=0.01639493035066818, w1=0.1554904195042433\n",
      "Gradient Descent(9974/9999): loss=89.04598091284079, w0=0.01639370451338715, w1=0.15547431764072486\n",
      "Gradient Descent(9975/9999): loss=89.03212466006892, w0=0.01639247878752299, w1=0.15545821728630219\n",
      "Gradient Descent(9976/9999): loss=89.01827136913515, w0=0.016391253173064058, w1=0.15544211844082922\n",
      "Gradient Descent(9977/9999): loss=89.00442103933717, w0=0.016390027669998725, w1=0.15542602110415996\n",
      "Gradient Descent(9978/9999): loss=88.9905736699728, w0=0.016388802278315358, w1=0.15540992527614836\n",
      "Gradient Descent(9979/9999): loss=88.97672926034004, w0=0.01638757699800232, w1=0.15539383095664844\n",
      "Gradient Descent(9980/9999): loss=88.9628878097371, w0=0.016386351829047987, w1=0.1553777381455142\n",
      "Gradient Descent(9981/9999): loss=88.94904931746235, w0=0.016385126771440724, w1=0.15536164684259968\n",
      "Gradient Descent(9982/9999): loss=88.93521378281433, w0=0.016383901825168905, w1=0.1553455570477589\n",
      "Gradient Descent(9983/9999): loss=88.92138120509179, w0=0.016382676990220904, w1=0.15532946876084594\n",
      "Gradient Descent(9984/9999): loss=88.9075515835936, w0=0.016381452266585095, w1=0.15531338198171485\n",
      "Gradient Descent(9985/9999): loss=88.89372491761884, w0=0.016380227654249856, w1=0.15529729671021972\n",
      "Gradient Descent(9986/9999): loss=88.87990120646677, w0=0.016379003153203564, w1=0.15528121294621464\n",
      "Gradient Descent(9987/9999): loss=88.86608044943684, w0=0.016377778763434596, w1=0.15526513068955372\n",
      "Gradient Descent(9988/9999): loss=88.85226264582862, w0=0.016376554484931336, w1=0.1552490499400911\n",
      "Gradient Descent(9989/9999): loss=88.83844779494189, w0=0.016375330317682162, w1=0.15523297069768088\n",
      "Gradient Descent(9990/9999): loss=88.82463589607669, w0=0.016374106261675462, w1=0.15521689296217728\n",
      "Gradient Descent(9991/9999): loss=88.81082694853305, w0=0.01637288231689962, w1=0.15520081673343442\n",
      "Gradient Descent(9992/9999): loss=88.79702095161134, w0=0.01637165848334302, w1=0.1551847420113065\n",
      "Gradient Descent(9993/9999): loss=88.78321790461203, w0=0.01637043476099405, w1=0.15516866879564772\n",
      "Gradient Descent(9994/9999): loss=88.76941780683585, w0=0.016369211149841096, w1=0.1551525970863123\n",
      "Gradient Descent(9995/9999): loss=88.75562065758353, w0=0.016367987649872552, w1=0.15513652688315444\n",
      "Gradient Descent(9996/9999): loss=88.74182645615618, w0=0.01636676426107681, w1=0.1551204581860284\n",
      "Gradient Descent(9997/9999): loss=88.72803520185492, w0=0.01636554098344226, w1=0.15510439099478843\n",
      "Gradient Descent(9998/9999): loss=88.7142468939812, w0=0.0163643178169573, w1=0.1550883253092888\n",
      "Gradient Descent(9999/9999): loss=88.7004615318365, w0=0.016363094761610325, w1=0.15507226112938377\n"
     ]
    }
   ],
   "source": [
    "w = np.random.rand(x.shape[1])\n",
    "w, loss = least_squares(y, x, w, max_iters=10000, gamma=0.0000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(1/99): loss=45258.78507090701, w0=0.11293539437844557, w1=0.6455004821126032\n",
      "Gradient Descent(1/99): loss=7190.511424623448, w0=0.11118639937176304, w1=0.6451110397971282\n",
      "Gradient Descent(1/99): loss=3837.0191198747643, w0=0.1094944235868044, w1=0.6449565710772031\n",
      "Gradient Descent(1/99): loss=4293.6945415792725, w0=0.106969596885694, w1=0.6450228795100331\n",
      "Gradient Descent(1/99): loss=7400.96843130032, w0=0.10719027662879528, w1=0.6443888756886011\n",
      "Gradient Descent(1/99): loss=4043.2134814057663, w0=0.10579230988482818, w1=0.6442773453210575\n",
      "Gradient Descent(1/99): loss=10266.902082366654, w0=0.10563103099283752, w1=0.6434370481621378\n",
      "Gradient Descent(1/99): loss=6071.894996608618, w0=0.10341692039120715, w1=0.6433767366971894\n",
      "Gradient Descent(1/99): loss=2852.8892142817126, w0=0.10229211745153118, w1=0.6435456436871679\n",
      "Gradient Descent(1/99): loss=5531.618183653429, w0=0.09651865728760854, w1=0.6439977107493577\n",
      "Gradient Descent(1/99): loss=2035.9673440934062, w0=0.0951295705910206, w1=0.6438822518316559\n",
      "Gradient Descent(1/99): loss=4135.7762654945445, w0=0.09391919906637473, w1=0.6435898818389646\n",
      "Gradient Descent(1/99): loss=3415.4005140461377, w0=0.09318830392034946, w1=0.6432726690778665\n",
      "Gradient Descent(1/99): loss=9769.85465666068, w0=0.0921768091765385, w1=0.6430312187665471\n",
      "Gradient Descent(1/99): loss=9667.130930170973, w0=0.08998371691378311, w1=0.6427377548717389\n",
      "Gradient Descent(1/99): loss=5661.710309030892, w0=0.08791630438464525, w1=0.6426421981352308\n",
      "Gradient Descent(1/99): loss=6869.188653957379, w0=0.08692662593442123, w1=0.642213545545533\n",
      "Gradient Descent(1/99): loss=2442.9090191963996, w0=0.08574309726799209, w1=0.6422005540974719\n",
      "Gradient Descent(1/99): loss=3633.262914265691, w0=0.08412939764581294, w1=0.6420280231454885\n",
      "Gradient Descent(1/99): loss=1369.72490218248, w0=0.0832311216076256, w1=0.6420173149155217\n",
      "Gradient Descent(1/99): loss=5829.5902069875465, w0=0.08144878706390436, w1=0.641980427499378\n",
      "Gradient Descent(1/99): loss=891.6500027540797, w0=0.08137504802800098, w1=0.6419725421632684\n",
      "Gradient Descent(1/99): loss=3445.068582286831, w0=0.08084868254057448, w1=0.6418954485394424\n",
      "Gradient Descent(1/99): loss=10107.636038939407, w0=0.08076290729400579, w1=0.6413038188353405\n",
      "Gradient Descent(1/99): loss=6005.779391773582, w0=0.08383316575154333, w1=0.6411286093652367\n",
      "Gradient Descent(1/99): loss=1498.976530329038, w0=0.08314208811497074, w1=0.6409855032616821\n",
      "Gradient Descent(1/99): loss=7263.2446706593855, w0=0.08249338474503418, w1=0.6405441329605671\n",
      "Gradient Descent(1/99): loss=2427.1680905720764, w0=0.0809481054654439, w1=0.6405740985821108\n",
      "Gradient Descent(1/99): loss=1296.0597762898517, w0=0.07976279055024184, w1=0.6406343472234423\n",
      "Gradient Descent(1/99): loss=10336.262567209425, w0=0.07966977580980153, w1=0.6406600481816396\n",
      "Gradient Descent(1/99): loss=6066.566641419284, w0=0.07900547400215663, w1=0.6405779022599499\n",
      "Gradient Descent(1/99): loss=5509.893613458158, w0=0.0788540489684398, w1=0.6404433439540161\n",
      "Gradient Descent(1/99): loss=3906.446061848769, w0=0.07763149844114667, w1=0.6401416285453264\n",
      "Gradient Descent(1/99): loss=8521.848972366977, w0=0.07668913170627836, w1=0.6402690832151758\n",
      "Gradient Descent(1/99): loss=3439.741526716005, w0=0.07611194628629941, w1=0.6401457878650847\n",
      "Gradient Descent(1/99): loss=2143.3536325806117, w0=0.07440458106719705, w1=0.6402899173282276\n",
      "Gradient Descent(1/99): loss=4416.738572489677, w0=0.07371183691628577, w1=0.6402080557288204\n",
      "Gradient Descent(1/99): loss=5000.082183891484, w0=0.07328279937333854, w1=0.639986582164313\n",
      "Gradient Descent(1/99): loss=2549.9830194515325, w0=0.07342015858569016, w1=0.6398742633296021\n",
      "Gradient Descent(1/99): loss=7440.072326475816, w0=0.07113372113176308, w1=0.6397586400919332\n",
      "Gradient Descent(1/99): loss=2928.416051035427, w0=0.07308696650230623, w1=0.6394171759474967\n",
      "Gradient Descent(1/99): loss=3127.0229122268997, w0=0.0726760311223722, w1=0.6392252218173401\n",
      "Gradient Descent(1/99): loss=1876.8065030123844, w0=0.07233772415102904, w1=0.6391622896937633\n",
      "Gradient Descent(1/99): loss=1829.821364014864, w0=0.07056056436986097, w1=0.6391581941450234\n",
      "Gradient Descent(1/99): loss=16236.361496955207, w0=0.07224661259857756, w1=0.6392272232514356\n",
      "Gradient Descent(1/99): loss=2908.6398333805028, w0=0.07109645748579867, w1=0.6392913702926398\n",
      "Gradient Descent(1/99): loss=1231.6419117691971, w0=0.0698638663969836, w1=0.6392563155184308\n",
      "Gradient Descent(1/99): loss=12952.767828739992, w0=0.07047939720340342, w1=0.6392441797078764\n",
      "Gradient Descent(1/99): loss=2526.907633770801, w0=0.06966902327740179, w1=0.639051463590568\n",
      "Gradient Descent(1/99): loss=4337.621329615512, w0=0.06935933859361144, w1=0.6389210852844658\n",
      "Gradient Descent(1/99): loss=5771.694870279447, w0=0.06880775858385382, w1=0.6385982265925921\n",
      "Gradient Descent(1/99): loss=6876.1377220437635, w0=0.06993551976177116, w1=0.6383910651749332\n",
      "Gradient Descent(1/99): loss=11591.29567794853, w0=0.07031065697417112, w1=0.6383868106171264\n",
      "Gradient Descent(1/99): loss=4307.35675992106, w0=0.06891683360046591, w1=0.63820485808681\n",
      "Gradient Descent(1/99): loss=2017.3927077507856, w0=0.06870432106783561, w1=0.6381261948150896\n",
      "Gradient Descent(1/99): loss=11240.849291335773, w0=0.06841908502954144, w1=0.6376797260667232\n",
      "Gradient Descent(1/99): loss=781.2880295700655, w0=0.06757468952048296, w1=0.6376399570853458\n",
      "Gradient Descent(1/99): loss=1737.533449786498, w0=0.06747846710977667, w1=0.6375598828600945\n",
      "Gradient Descent(1/99): loss=5268.388336504533, w0=0.06702590578766081, w1=0.6372188483208097\n",
      "Gradient Descent(1/99): loss=1017.8877507889098, w0=0.06656187135091332, w1=0.6372275545571887\n",
      "Gradient Descent(1/99): loss=612.9488432860164, w0=0.06669903261124215, w1=0.6371977424146159\n",
      "Gradient Descent(1/99): loss=3563.701049328438, w0=0.0657263876083631, w1=0.6368894477574089\n",
      "Gradient Descent(1/99): loss=3211.7638667792853, w0=0.06710050093872524, w1=0.636541215129852\n",
      "Gradient Descent(1/99): loss=5873.812183313188, w0=0.06828553011987121, w1=0.6362799785622135\n",
      "Gradient Descent(1/99): loss=1860.035480363646, w0=0.06757184370272246, w1=0.6362626820067319\n",
      "Gradient Descent(1/99): loss=7007.640928819123, w0=0.06680171607712782, w1=0.6358948231182998\n",
      "Gradient Descent(1/99): loss=7762.685244887892, w0=0.06485437309756673, w1=0.6353575795912809\n",
      "Gradient Descent(1/99): loss=4701.7821987238485, w0=0.06450845720357092, w1=0.6351694676704484\n",
      "Gradient Descent(1/99): loss=6362.637720106475, w0=0.06364919952628546, w1=0.6354746056691543\n",
      "Gradient Descent(1/99): loss=3921.489372541206, w0=0.06255315602712319, w1=0.635424529963127\n",
      "Gradient Descent(1/99): loss=1837.0171279921624, w0=0.06185006619939926, w1=0.6353746127609717\n",
      "Gradient Descent(1/99): loss=2220.6885197145853, w0=0.06347229700808292, w1=0.6351551655379536\n",
      "Gradient Descent(1/99): loss=1671.203329980326, w0=0.06262955808430896, w1=0.6350934979584769\n",
      "Gradient Descent(1/99): loss=4308.428235533267, w0=0.06182473232729303, w1=0.6347733339147618\n",
      "Gradient Descent(1/99): loss=633.1417118268668, w0=0.06225166055061832, w1=0.6347025450641398\n",
      "Gradient Descent(1/99): loss=4727.8199613388615, w0=0.06122673121464489, w1=0.6344048406801593\n",
      "Gradient Descent(1/99): loss=3305.5692431018115, w0=0.05899527440210973, w1=0.634356171941494\n",
      "Gradient Descent(1/99): loss=2303.092960777203, w0=0.0583276202779315, w1=0.6342060407632806\n",
      "Gradient Descent(1/99): loss=3706.6530522855996, w0=0.05728013095073652, w1=0.6341046029969992\n",
      "Gradient Descent(1/99): loss=8413.707925491586, w0=0.05913968822211861, w1=0.634016433127271\n",
      "Gradient Descent(1/99): loss=535.0450401371594, w0=0.05818812188355412, w1=0.6340104227541327\n",
      "Gradient Descent(1/99): loss=1647.2854265964427, w0=0.05670856173142749, w1=0.633961019755301\n",
      "Gradient Descent(1/99): loss=6038.023997950995, w0=0.05627565869965405, w1=0.6339066937814185\n",
      "Gradient Descent(1/99): loss=3314.3552913088556, w0=0.056061398488804086, w1=0.633767336905889\n",
      "Gradient Descent(1/99): loss=20281.128961385955, w0=0.05598606983152449, w1=0.6337738745578262\n",
      "Gradient Descent(1/99): loss=16034.67322195688, w0=0.05576564704508168, w1=0.6338628135575991\n",
      "Gradient Descent(1/99): loss=1952.4533015072566, w0=0.055036528754394055, w1=0.6336857513835409\n",
      "Gradient Descent(1/99): loss=482.3343279213334, w0=0.05578907726937585, w1=0.6336577610856601\n",
      "Gradient Descent(1/99): loss=8332.394777621243, w0=0.05489446444999607, w1=0.6334197827539637\n",
      "Gradient Descent(1/99): loss=3956.258278738073, w0=0.05420779863121236, w1=0.6332311209206902\n",
      "Gradient Descent(1/99): loss=6722.721048022947, w0=0.052909098726865694, w1=0.632849320056104\n",
      "Gradient Descent(1/99): loss=6160.608552558961, w0=0.05206357186673165, w1=0.6326804429093976\n",
      "Gradient Descent(1/99): loss=1605.5224502498056, w0=0.05095664646938844, w1=0.6325722557902131\n",
      "Gradient Descent(1/99): loss=4053.8321522546694, w0=0.050103911198031266, w1=0.6323277190958135\n",
      "Gradient Descent(1/99): loss=1947.2173044688805, w0=0.05115666604330348, w1=0.6322475555864043\n",
      "Gradient Descent(1/99): loss=15215.229729332495, w0=0.04809738829198891, w1=0.6310250120049418\n",
      "Gradient Descent(1/99): loss=10478.489302905149, w0=0.04709547765442307, w1=0.6308599272478432\n",
      "Gradient Descent(1/99): loss=14924.439140090888, w0=0.04751736013765502, w1=0.6310010585131137\n",
      "Gradient Descent(1/99): loss=580.0450630779039, w0=0.047422866164365515, w1=0.6309666899356543\n",
      "Gradient Descent(1/99): loss=1781.233176727935, w0=0.047204106128672645, w1=0.6307577299442193\n"
     ]
    }
   ],
   "source": [
    "w, loss = least_squares_SGD(y, x, w, 10, max_iters=100, gamma=0.0000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4772"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_accuracy(x, y, w):\n",
    "    pred_y = predict_labels(w, x)\n",
    "    correct_count = 0\n",
    "    for index, yi in enumerate(y):\n",
    "        pred_yi = pred_y[index]\n",
    "        if pred_yi == yi:\n",
    "            correct_count += 1\n",
    "    return correct_count/len(y)\n",
    "        \n",
    "get_accuracy(x, y, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
