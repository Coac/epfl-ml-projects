{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from helpers import *\n",
    "from costs import *\n",
    "from gradient_descent import *\n",
    "from stochastic_gradient_descent import *\n",
    "\n",
    "random.seed(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "y, x, ids = load_csv_data(data_path=\"datas/train.csv\", sub_sample=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set to 0 all column containing -999\n",
    "def remove_NaN(x):\n",
    "    columns_with_NaN = set(\"\")\n",
    "    for row in x:\n",
    "        for i,feature in enumerate(row):\n",
    "            if feature == -999:\n",
    "                columns_with_NaN.add(i)\n",
    "\n",
    "    for col in columns_with_NaN:\n",
    "        x[:, col] = 0\n",
    "        \n",
    "    return x\n",
    "        \n",
    "x = remove_NaN(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.06833197,  0.40768027, ...,  0.        ,\n",
       "         0.        ,  0.4125105 ],\n",
       "       [ 0.        ,  0.55250482,  0.54013641, ...,  0.        ,\n",
       "         0.        , -0.27381996],\n",
       "       [ 0.        ,  3.19515553,  1.09655998, ...,  0.        ,\n",
       "         0.        , -0.29396985],\n",
       "       ..., \n",
       "       [ 0.        ,  0.31931645, -0.13086367, ...,  0.        ,\n",
       "         0.        , -0.31701723],\n",
       "       [ 0.        , -0.84532397, -0.30297338, ...,  0.        ,\n",
       "         0.        , -0.74543941],\n",
       "       [ 0.        ,  0.66533608, -0.25352276, ...,  0.        ,\n",
       "         0.        , -0.74543941]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def normalize(x):\n",
    "    return (x - x.mean(axis=0)) / (x.std(axis=0) + 0.0000000001)\n",
    "x = normalize(x)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/999): loss=7.803076073096768\t\t0.549992\n",
      "Gradient Descent(1/999): loss=6.690601548708\t\t0.545052\n",
      "Gradient Descent(2/999): loss=5.788049060693789\t\t0.540264\n",
      "Gradient Descent(3/999): loss=5.05234115373521\t\t0.535732\n",
      "Gradient Descent(4/999): loss=4.449503404070839\t\t0.531456\n",
      "Gradient Descent(5/999): loss=3.952720587870469\t\t0.527588\n",
      "Gradient Descent(6/999): loss=3.540809526715271\t\t0.523948\n",
      "Gradient Descent(7/999): loss=3.1970191619039596\t\t0.520784\n",
      "Gradient Descent(8/999): loss=2.9080876220051133\t\t0.517764\n",
      "Gradient Descent(9/999): loss=2.6635011331479013\t\t0.514996\n",
      "Gradient Descent(10/999): loss=2.454911465838178\t\t0.512612\n",
      "Gradient Descent(11/999): loss=2.275677911902412\t\t0.510888\n",
      "Gradient Descent(12/999): loss=2.120507087186857\t\t0.509836\n",
      "Gradient Descent(13/999): loss=1.9851695890993455\t\t0.508716\n",
      "Gradient Descent(14/999): loss=1.8662770399773092\t\t0.50792\n",
      "Gradient Descent(15/999): loss=1.761106582191091\t\t0.507508\n",
      "Gradient Descent(16/999): loss=1.667462666589086\t\t0.507144\n",
      "Gradient Descent(17/999): loss=1.583568155468364\t\t0.506936\n",
      "Gradient Descent(18/999): loss=1.507978472774468\t\t0.506684\n",
      "Gradient Descent(19/999): loss=1.4395138782452064\t\t0.506984\n",
      "Gradient Descent(20/999): loss=1.37720599766128\t\t0.507132\n",
      "Gradient Descent(21/999): loss=1.3202555702356482\t\t0.507936\n",
      "Gradient Descent(22/999): loss=1.2679990251311783\t\t0.508776\n",
      "Gradient Descent(23/999): loss=1.2198820103540693\t\t0.509684\n",
      "Gradient Descent(24/999): loss=1.175438398831404\t\t0.51052\n",
      "Gradient Descent(25/999): loss=1.1342736119021644\t\t0.512032\n",
      "Gradient Descent(26/999): loss=1.0960513482287464\t\t0.513452\n",
      "Gradient Descent(27/999): loss=1.0604830007937758\t\t0.5147\n",
      "Gradient Descent(28/999): loss=1.0273191975884517\t\t0.516372\n",
      "Gradient Descent(29/999): loss=0.9963430217780067\t\t0.517904\n",
      "Gradient Descent(30/999): loss=0.9673645615781662\t\t0.519492\n",
      "Gradient Descent(31/999): loss=0.9402165143147497\t\t0.521252\n",
      "Gradient Descent(32/999): loss=0.9147506275021493\t\t0.52308\n",
      "Gradient Descent(33/999): loss=0.8908348056698188\t\t0.52488\n",
      "Gradient Descent(34/999): loss=0.8683507477625739\t\t0.527112\n",
      "Gradient Descent(35/999): loss=0.8471920083400357\t\t0.529344\n",
      "Gradient Descent(36/999): loss=0.827262398151799\t\t0.531216\n",
      "Gradient Descent(37/999): loss=0.8084746572629316\t\t0.533308\n",
      "Gradient Descent(38/999): loss=0.7907493477663139\t\t0.53564\n",
      "Gradient Descent(39/999): loss=0.7740139240431322\t\t0.537836\n",
      "Gradient Descent(40/999): loss=0.7582019471480679\t\t0.540132\n",
      "Gradient Descent(41/999): loss=0.7432524166945043\t\t0.542416\n",
      "Gradient Descent(42/999): loss=0.7291091989847446\t\t0.544792\n",
      "Gradient Descent(43/999): loss=0.7157205343752008\t\t0.547108\n",
      "Gradient Descent(44/999): loss=0.7030386102259981\t\t0.549532\n",
      "Gradient Descent(45/999): loss=0.6910191884464699\t\t0.552072\n",
      "Gradient Descent(46/999): loss=0.6796212787603949\t\t0.554472\n",
      "Gradient Descent(47/999): loss=0.6688068504937867\t\t0.556856\n",
      "Gradient Descent(48/999): loss=0.658540577025012\t\t0.559004\n",
      "Gradient Descent(49/999): loss=0.6487896081038805\t\t0.561476\n",
      "Gradient Descent(50/999): loss=0.639523366099725\t\t0.564148\n",
      "Gradient Descent(51/999): loss=0.6307133629229454\t\t0.566916\n",
      "Gradient Descent(52/999): loss=0.6223330349150897\t\t0.569336\n",
      "Gradient Descent(53/999): loss=0.6143575934469802\t\t0.571892\n",
      "Gradient Descent(54/999): loss=0.6067638893244316\t\t0.574356\n",
      "Gradient Descent(55/999): loss=0.5995302893940034\t\t0.576744\n",
      "Gradient Descent(56/999): loss=0.5926365639805153\t\t0.579168\n",
      "Gradient Descent(57/999): loss=0.5860637839845093\t\t0.581576\n",
      "Gradient Descent(58/999): loss=0.5797942266299609\t\t0.58388\n",
      "Gradient Descent(59/999): loss=0.5738112889870146\t\t0.58632\n",
      "Gradient Descent(60/999): loss=0.568099408506747\t\t0.588744\n",
      "Gradient Descent(61/999): loss=0.5626439898991222\t\t0.590912\n",
      "Gradient Descent(62/999): loss=0.557431337764838\t\t0.593268\n",
      "Gradient Descent(63/999): loss=0.5524485944592858\t\t0.595388\n",
      "Gradient Descent(64/999): loss=0.547683682724605\t\t0.597612\n",
      "Gradient Descent(65/999): loss=0.5431252526754309\t\t0.599604\n",
      "Gradient Descent(66/999): loss=0.5387626327668906\t\t0.6017\n",
      "Gradient Descent(67/999): loss=0.5345857844107328\t\t0.603944\n",
      "Gradient Descent(68/999): loss=0.5305852599381613\t\t0.60604\n",
      "Gradient Descent(69/999): loss=0.5267521636366482\t\t0.60796\n",
      "Gradient Descent(70/999): loss=0.5230781156133798\t\t0.61\n",
      "Gradient Descent(71/999): loss=0.5195552182604973\t\t0.612012\n",
      "Gradient Descent(72/999): loss=0.5161760251173584\t\t0.61382\n",
      "Gradient Descent(73/999): loss=0.5129335119429816\t\t0.615748\n",
      "Gradient Descent(74/999): loss=0.5098210498279497\t\t0.61742\n",
      "Gradient Descent(75/999): loss=0.5068323801895308\t\t0.619448\n",
      "Gradient Descent(76/999): loss=0.5039615915068789\t\t0.621388\n",
      "Gradient Descent(77/999): loss=0.5012030976650119\t\t0.623512\n",
      "Gradient Descent(78/999): loss=0.49855161778701507\t\t0.625252\n",
      "Gradient Descent(79/999): loss=0.4960021574436833\t\t0.62702\n",
      "Gradient Descent(80/999): loss=0.49354999113870757\t\t0.628888\n",
      "Gradient Descent(81/999): loss=0.4911906459756218\t\t0.630484\n",
      "Gradient Descent(82/999): loss=0.4889198864201297\t\t0.632144\n",
      "Gradient Descent(83/999): loss=0.4867337000782066\t\t0.63382\n",
      "Gradient Descent(84/999): loss=0.4846282844165647\t\t0.635112\n",
      "Gradient Descent(85/999): loss=0.48260003435775317\t\t0.6368\n",
      "Gradient Descent(86/999): loss=0.480645530687374\t\t0.638164\n",
      "Gradient Descent(87/999): loss=0.47876152921566945\t\t0.639636\n",
      "Gradient Descent(88/999): loss=0.47694495064013465\t\t0.641148\n",
      "Gradient Descent(89/999): loss=0.47519287105984165\t\t0.6426\n",
      "Gradient Descent(90/999): loss=0.4735025130958706\t\t0.644168\n",
      "Gradient Descent(91/999): loss=0.47187123757566235\t\t0.645628\n",
      "Gradient Descent(92/999): loss=0.47029653574225005\t\t0.647052\n",
      "Gradient Descent(93/999): loss=0.46877602195221924\t\t0.648436\n",
      "Gradient Descent(94/999): loss=0.46730742682891996\t\t0.649572\n",
      "Gradient Descent(95/999): loss=0.46588859083991185\t\t0.6508\n",
      "Gradient Descent(96/999): loss=0.46451745826988844\t\t0.652036\n",
      "Gradient Descent(97/999): loss=0.46319207156242775\t\t0.6533\n",
      "Gradient Descent(98/999): loss=0.46191056600583813\t\t0.6543\n",
      "Gradient Descent(99/999): loss=0.4606711647401603\t\t0.655208\n",
      "Gradient Descent(100/999): loss=0.45947217406402924\t\t0.656216\n",
      "Gradient Descent(101/999): loss=0.45831197902161946\t\t0.657304\n",
      "Gradient Descent(102/999): loss=0.45718903925131216\t\t0.658276\n",
      "Gradient Descent(103/999): loss=0.45610188507900906\t\t0.659192\n",
      "Gradient Descent(104/999): loss=0.45504911384023733\t\t0.660136\n",
      "Gradient Descent(105/999): loss=0.4540293864162855\t\t0.66116\n",
      "Gradient Descent(106/999): loss=0.45304142397065195\t\t0.661956\n",
      "Gradient Descent(107/999): loss=0.452084004873036\t\t0.662884\n",
      "Gradient Descent(108/999): loss=0.451155961798984\t\t0.663732\n",
      "Gradient Descent(109/999): loss=0.45025617899411835\t\t0.664532\n",
      "Gradient Descent(110/999): loss=0.4493835896926414\t\t0.665496\n",
      "Gradient Descent(111/999): loss=0.4485371736805019\t\t0.666328\n",
      "Gradient Descent(112/999): loss=0.447715954994264\t\t0.6671\n",
      "Gradient Descent(113/999): loss=0.4469189997473259\t\t0.667988\n",
      "Gradient Descent(114/999): loss=0.44614541407569114\t\t0.66884\n",
      "Gradient Descent(115/999): loss=0.44539434219601903\t\t0.669636\n",
      "Gradient Descent(116/999): loss=0.44466496456916155\t\t0.670332\n",
      "Gradient Descent(117/999): loss=0.4439564961628424\t\t0.671088\n",
      "Gradient Descent(118/999): loss=0.4432681848075524\t\t0.671736\n",
      "Gradient Descent(119/999): loss=0.44259930964012106\t\t0.672452\n",
      "Gradient Descent(120/999): loss=0.44194917962978153\t\t0.673172\n",
      "Gradient Descent(121/999): loss=0.44131713218188984\t\t0.673848\n",
      "Gradient Descent(122/999): loss=0.440702531814758\t\t0.674504\n",
      "Gradient Descent(123/999): loss=0.44010476890536127\t\t0.675116\n",
      "Gradient Descent(124/999): loss=0.439523258499946\t\t0.675744\n",
      "Gradient Descent(125/999): loss=0.4389574391858099\t\t0.676256\n",
      "Gradient Descent(126/999): loss=0.43840677202077577\t\t0.676916\n",
      "Gradient Descent(127/999): loss=0.4378707395170786\t\t0.677496\n",
      "Gradient Descent(128/999): loss=0.4373488446766047\t\t0.677852\n",
      "Gradient Descent(129/999): loss=0.43684061007460484\t\t0.678372\n",
      "Gradient Descent(130/999): loss=0.4363455769891819\t\t0.678928\n",
      "Gradient Descent(131/999): loss=0.4358633045740146\t\t0.67964\n",
      "Gradient Descent(132/999): loss=0.4353933690719418\t\t0.6801\n",
      "Gradient Descent(133/999): loss=0.4349353630671656\t\t0.680816\n",
      "Gradient Descent(134/999): loss=0.4344888947739773\t\t0.681292\n",
      "Gradient Descent(135/999): loss=0.43405358736002153\t\t0.681724\n",
      "Gradient Descent(136/999): loss=0.43362907830224934\t\t0.682144\n",
      "Gradient Descent(137/999): loss=0.43321501877380264\t\t0.6825\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(138/999): loss=0.43281107306019145\t\t0.68302\n",
      "Gradient Descent(139/999): loss=0.43241691800320947\t\t0.683432\n",
      "Gradient Descent(140/999): loss=0.43203224247113187\t\t0.683888\n",
      "Gradient Descent(141/999): loss=0.43165674685381655\t\t0.684224\n",
      "Gradient Descent(142/999): loss=0.4312901425814184\t\t0.684604\n",
      "Gradient Descent(143/999): loss=0.4309321516654896\t\t0.684952\n",
      "Gradient Descent(144/999): loss=0.43058250626131805\t\t0.685364\n",
      "Gradient Descent(145/999): loss=0.43024094825041287\t\t0.68586\n",
      "Gradient Descent(146/999): loss=0.42990722884211696\t\t0.6862\n",
      "Gradient Descent(147/999): loss=0.4295811081933713\t\t0.686496\n",
      "Gradient Descent(148/999): loss=0.4292623550457229\t\t0.686884\n",
      "Gradient Descent(149/999): loss=0.4289507463787094\t\t0.687328\n",
      "Gradient Descent(150/999): loss=0.4286460670788036\t\t0.687604\n",
      "Gradient Descent(151/999): loss=0.4283481096231492\t\t0.687808\n",
      "Gradient Descent(152/999): loss=0.42805667377735585\t\t0.688144\n",
      "Gradient Descent(153/999): loss=0.4277715663066649\t\t0.688548\n",
      "Gradient Descent(154/999): loss=0.4274926006998326\t\t0.68882\n",
      "Gradient Descent(155/999): loss=0.4272195969051131\t\t0.689184\n",
      "Gradient Descent(156/999): loss=0.42695238107775746\t\t0.689532\n",
      "Gradient Descent(157/999): loss=0.4266907853384723\t\t0.689768\n",
      "Gradient Descent(158/999): loss=0.4264346475423184\t\t0.68988\n",
      "Gradient Descent(159/999): loss=0.4261838110575455\t\t0.690028\n",
      "Gradient Descent(160/999): loss=0.4259381245538999\t\t0.690296\n",
      "Gradient Descent(161/999): loss=0.42569744179995295\t\t0.690432\n",
      "Gradient Descent(162/999): loss=0.42546162146902705\t\t0.690724\n",
      "Gradient Descent(163/999): loss=0.4252305269533234\t\t0.690992\n",
      "Gradient Descent(164/999): loss=0.4250040261858614\t\t0.691148\n",
      "Gradient Descent(165/999): loss=0.42478199146987716\t\t0.691348\n",
      "Gradient Descent(166/999): loss=0.42456429931533096\t\t0.691684\n",
      "Gradient Descent(167/999): loss=0.4243508302822008\t\t0.691996\n",
      "Gradient Descent(168/999): loss=0.4241414688302521\t\t0.692108\n",
      "Gradient Descent(169/999): loss=0.42393610317499003\t\t0.6923\n",
      "Gradient Descent(170/999): loss=0.4237346251495127\t\t0.692468\n",
      "Gradient Descent(171/999): loss=0.42353693007200127\t\t0.692672\n",
      "Gradient Descent(172/999): loss=0.4233429166185946\t\t0.692852\n",
      "Gradient Descent(173/999): loss=0.42315248670140704\t\t0.693032\n",
      "Gradient Descent(174/999): loss=0.42296554535146164\t\t0.69332\n",
      "Gradient Descent(175/999): loss=0.4227820006063207\t\t0.693528\n",
      "Gradient Descent(176/999): loss=0.4226017634022085\t\t0.693796\n",
      "Gradient Descent(177/999): loss=0.4224247474704255\t\t0.693984\n",
      "Gradient Descent(178/999): loss=0.4222508692378715\t\t0.694148\n",
      "Gradient Descent(179/999): loss=0.42208004773149466\t\t0.694296\n",
      "Gradient Descent(180/999): loss=0.42191220448649996\t\t0.694452\n",
      "Gradient Descent(181/999): loss=0.42174726345815067\t\t0.69468\n",
      "Gradient Descent(182/999): loss=0.4215851509370146\t\t0.694824\n",
      "Gradient Descent(183/999): loss=0.42142579546750014\t\t0.694996\n",
      "Gradient Descent(184/999): loss=0.42126912776955044\t\t0.69524\n",
      "Gradient Descent(185/999): loss=0.42111508066335324\t\t0.695376\n",
      "Gradient Descent(186/999): loss=0.42096358899694536\t\t0.695596\n",
      "Gradient Descent(187/999): loss=0.42081458957658685\t\t0.695732\n",
      "Gradient Descent(188/999): loss=0.4206680210997905\t\t0.69588\n",
      "Gradient Descent(189/999): loss=0.4205238240908917\t\t0.69612\n",
      "Gradient Descent(190/999): loss=0.4203819408390597\t\t0.6963\n",
      "Gradient Descent(191/999): loss=0.4202423153386431\t\t0.696508\n",
      "Gradient Descent(192/999): loss=0.4201048932317554\t\t0.696608\n",
      "Gradient Descent(193/999): loss=0.4199696217530092\t\t0.696828\n",
      "Gradient Descent(194/999): loss=0.4198364496763104\t\t0.69702\n",
      "Gradient Descent(195/999): loss=0.41970532726362725\t\t0.69706\n",
      "Gradient Descent(196/999): loss=0.4195762062156558\t\t0.697176\n",
      "Gradient Descent(197/999): loss=0.41944903962430347\t\t0.697384\n",
      "Gradient Descent(198/999): loss=0.419323781926918\t\t0.697568\n",
      "Gradient Descent(199/999): loss=0.4192003888621912\t\t0.697644\n",
      "Gradient Descent(200/999): loss=0.4190788174276709\t\t0.697728\n",
      "Gradient Descent(201/999): loss=0.4189590258388164\t\t0.697892\n",
      "Gradient Descent(202/999): loss=0.41884097348953564\t\t0.69798\n",
      "Gradient Descent(203/999): loss=0.41872462091414686\t\t0.698108\n",
      "Gradient Descent(204/999): loss=0.4186099297507069\t\t0.69826\n",
      "Gradient Descent(205/999): loss=0.4184968627056521\t\t0.698412\n",
      "Gradient Descent(206/999): loss=0.4183853835197034\t\t0.698476\n",
      "Gradient Descent(207/999): loss=0.4182754569349803\t\t0.698536\n",
      "Gradient Descent(208/999): loss=0.4181670486632842\t\t0.698636\n",
      "Gradient Descent(209/999): loss=0.4180601253554984\t\t0.698712\n",
      "Gradient Descent(210/999): loss=0.41795465457206765\t\t0.698784\n",
      "Gradient Descent(211/999): loss=0.41785060475450975\t\t0.698984\n",
      "Gradient Descent(212/999): loss=0.4177479451979252\t\t0.699064\n",
      "Gradient Descent(213/999): loss=0.41764664602446344\t\t0.6991\n",
      "Gradient Descent(214/999): loss=0.4175466781577088\t\t0.699148\n",
      "Gradient Descent(215/999): loss=0.41744801329795317\t\t0.699248\n",
      "Gradient Descent(216/999): loss=0.41735062389832084\t\t0.699372\n",
      "Gradient Descent(217/999): loss=0.41725448314171276\t\t0.699448\n",
      "Gradient Descent(218/999): loss=0.4171595649185402\t\t0.699496\n",
      "Gradient Descent(219/999): loss=0.4170658438052177\t\t0.6995\n",
      "Gradient Descent(220/999): loss=0.4169732950433865\t\t0.699604\n",
      "Gradient Descent(221/999): loss=0.4168818945198429\t\t0.699696\n",
      "Gradient Descent(222/999): loss=0.41679161874714404\t\t0.699824\n",
      "Gradient Descent(223/999): loss=0.4167024448448641\t\t0.699928\n",
      "Gradient Descent(224/999): loss=0.41661435052148194\t\t0.7\n",
      "Gradient Descent(225/999): loss=0.41652731405687216\t\t0.700132\n",
      "Gradient Descent(226/999): loss=0.41644131428537906\t\t0.700232\n",
      "Gradient Descent(227/999): loss=0.4163563305794536\t\t0.700376\n",
      "Gradient Descent(228/999): loss=0.41627234283383013\t\t0.700448\n",
      "Gradient Descent(229/999): loss=0.41618933145022613\t\t0.700496\n",
      "Gradient Descent(230/999): loss=0.41610727732254305\t\t0.700524\n",
      "Gradient Descent(231/999): loss=0.41602616182255353\t\t0.70058\n",
      "Gradient Descent(232/999): loss=0.41594596678605356\t\t0.700612\n",
      "Gradient Descent(233/999): loss=0.41586667449946674\t\t0.700684\n",
      "Gradient Descent(234/999): loss=0.4157882676868818\t\t0.7008\n",
      "Gradient Descent(235/999): loss=0.4157107294975097\t\t0.700876\n",
      "Gradient Descent(236/999): loss=0.41563404349354305\t\t0.700912\n",
      "Gradient Descent(237/999): loss=0.41555819363840635\t\t0.700952\n",
      "Gradient Descent(238/999): loss=0.415483164285382\t\t0.700996\n",
      "Gradient Descent(239/999): loss=0.41540894016659774\t\t0.701096\n",
      "Gradient Descent(240/999): loss=0.41533550638236527\t\t0.701136\n",
      "Gradient Descent(241/999): loss=0.41526284839085525\t\t0.701176\n",
      "Gradient Descent(242/999): loss=0.41519095199809886\t\t0.701256\n",
      "Gradient Descent(243/999): loss=0.41511980334830284\t\t0.701352\n",
      "Gradient Descent(244/999): loss=0.4150493889144701\t\t0.701408\n",
      "Gradient Descent(245/999): loss=0.4149796954893106\t\t0.7015\n",
      "Gradient Descent(246/999): loss=0.4149107101764375\t\t0.701544\n",
      "Gradient Descent(247/999): loss=0.41484242038183483\t\t0.701636\n",
      "Gradient Descent(248/999): loss=0.4147748138055905\t\t0.701708\n",
      "Gradient Descent(249/999): loss=0.41470787843388335\t\t0.701816\n",
      "Gradient Descent(250/999): loss=0.41464160253121574\t\t0.701824\n",
      "Gradient Descent(251/999): loss=0.41457597463288554\t\t0.701896\n",
      "Gradient Descent(252/999): loss=0.414510983537687\t\t0.701884\n",
      "Gradient Descent(253/999): loss=0.41444661830083285\t\t0.70192\n",
      "Gradient Descent(254/999): loss=0.41438286822709275\t\t0.70198\n",
      "Gradient Descent(255/999): loss=0.41431972286413704\t\t0.702064\n",
      "Gradient Descent(256/999): loss=0.41425717199608353\t\t0.702144\n",
      "Gradient Descent(257/999): loss=0.4141952056372348\t\t0.702196\n",
      "Gradient Descent(258/999): loss=0.414133814026005\t\t0.70226\n",
      "Gradient Descent(259/999): loss=0.4140729876190286\t\t0.702304\n",
      "Gradient Descent(260/999): loss=0.4140127170854406\t\t0.702376\n",
      "Gradient Descent(261/999): loss=0.41395299330132984\t\t0.702436\n",
      "Gradient Descent(262/999): loss=0.4138938073443543\t\t0.702412\n",
      "Gradient Descent(263/999): loss=0.4138351504885152\t\t0.702424\n",
      "Gradient Descent(264/999): loss=0.41377701419908414\t\t0.702476\n",
      "Gradient Descent(265/999): loss=0.4137193901276786\t\t0.702584\n",
      "Gradient Descent(266/999): loss=0.4136622701074819\t\t0.702604\n",
      "Gradient Descent(267/999): loss=0.41360564614859935\t\t0.702636\n",
      "Gradient Descent(268/999): loss=0.41354951043355226\t\t0.702716\n",
      "Gradient Descent(269/999): loss=0.41349385531289906\t\t0.702772\n",
      "Gradient Descent(270/999): loss=0.4134386733009838\t\t0.702884\n",
      "Gradient Descent(271/999): loss=0.4133839570718064\t\t0.702884\n",
      "Gradient Descent(272/999): loss=0.4133296994550099\t\t0.702916\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(273/999): loss=0.413275893431983\t\t0.7029\n",
      "Gradient Descent(274/999): loss=0.41322253213207305\t\t0.702936\n",
      "Gradient Descent(275/999): loss=0.41316960882890513\t\t0.702984\n",
      "Gradient Descent(276/999): loss=0.4131171169368069\t\t0.703048\n",
      "Gradient Descent(277/999): loss=0.41306505000733096\t\t0.703108\n",
      "Gradient Descent(278/999): loss=0.41301340172587814\t\t0.703128\n",
      "Gradient Descent(279/999): loss=0.4129621659084115\t\t0.703116\n",
      "Gradient Descent(280/999): loss=0.4129113364982647\t\t0.70314\n",
      "Gradient Descent(281/999): loss=0.41286090756303695\t\t0.703196\n",
      "Gradient Descent(282/999): loss=0.4128108732915746\t\t0.703256\n",
      "Gradient Descent(283/999): loss=0.41276122799103626\t\t0.70324\n",
      "Gradient Descent(284/999): loss=0.4127119660840367\t\t0.70326\n",
      "Gradient Descent(285/999): loss=0.412663082105872\t\t0.703296\n",
      "Gradient Descent(286/999): loss=0.4126145707018169\t\t0.703344\n",
      "Gradient Descent(287/999): loss=0.4125664266244984\t\t0.703396\n",
      "Gradient Descent(288/999): loss=0.4125186447313395\t\t0.703496\n",
      "Gradient Descent(289/999): loss=0.4124712199820717\t\t0.7035\n",
      "Gradient Descent(290/999): loss=0.4124241474363148\t\t0.703528\n",
      "Gradient Descent(291/999): loss=0.4123774222512222\t\t0.703572\n",
      "Gradient Descent(292/999): loss=0.4123310396791879\t\t0.7036\n",
      "Gradient Descent(293/999): loss=0.41228499506561606\t\t0.703596\n",
      "Gradient Descent(294/999): loss=0.41223928384674885\t\t0.703592\n",
      "Gradient Descent(295/999): loss=0.41219390154755314\t\t0.703676\n",
      "Gradient Descent(296/999): loss=0.4121488437796609\t\t0.70372\n",
      "Gradient Descent(297/999): loss=0.41210410623936566\t\t0.703756\n",
      "Gradient Descent(298/999): loss=0.41205968470567134\t\t0.703816\n",
      "Gradient Descent(299/999): loss=0.4120155750383914\t\t0.703832\n",
      "Gradient Descent(300/999): loss=0.4119717731762974\t\t0.703824\n",
      "Gradient Descent(301/999): loss=0.4119282751353175\t\t0.70384\n",
      "Gradient Descent(302/999): loss=0.4118850770067791\t\t0.703896\n",
      "Gradient Descent(303/999): loss=0.4118421749556992\t\t0.703896\n",
      "Gradient Descent(304/999): loss=0.41179956521911665\t\t0.703944\n",
      "Gradient Descent(305/999): loss=0.4117572441044692\t\t0.704\n",
      "Gradient Descent(306/999): loss=0.4117152079880112\t\t0.704012\n",
      "Gradient Descent(307/999): loss=0.41167345331327104\t\t0.704052\n",
      "Gradient Descent(308/999): loss=0.4116319765895486\t\t0.704076\n",
      "Gradient Descent(309/999): loss=0.41159077439045133\t\t0.7041\n",
      "Gradient Descent(310/999): loss=0.4115498433524653\t\t0.704192\n",
      "Gradient Descent(311/999): loss=0.41150918017356564\t\t0.704196\n",
      "Gradient Descent(312/999): loss=0.4114687816118572\t\t0.70426\n",
      "Gradient Descent(313/999): loss=0.41142864448425415\t\t0.704304\n",
      "Gradient Descent(314/999): loss=0.4113887656651891\t\t0.704344\n",
      "Gradient Descent(315/999): loss=0.41134914208535617\t\t0.70442\n",
      "Gradient Descent(316/999): loss=0.4113097707304834\t\t0.704504\n",
      "Gradient Descent(317/999): loss=0.41127064864013846\t\t0.704544\n",
      "Gradient Descent(318/999): loss=0.41123177290656077\t\t0.704568\n",
      "Gradient Descent(319/999): loss=0.4111931406735241\t\t0.704568\n",
      "Gradient Descent(320/999): loss=0.41115474913522626\t\t0.704616\n",
      "Gradient Descent(321/999): loss=0.41111659553520724\t\t0.70462\n",
      "Gradient Descent(322/999): loss=0.4110786771652912\t\t0.704664\n",
      "Gradient Descent(323/999): loss=0.41104099136455696\t\t0.704676\n",
      "Gradient Descent(324/999): loss=0.4110035355183316\t\t0.704672\n",
      "Gradient Descent(325/999): loss=0.4109663070572086\t\t0.704696\n",
      "Gradient Descent(326/999): loss=0.410929303456091\t\t0.70472\n",
      "Gradient Descent(327/999): loss=0.41089252223325545\t\t0.704784\n",
      "Gradient Descent(328/999): loss=0.4108559609494397\t\t0.704776\n",
      "Gradient Descent(329/999): loss=0.41081961720695265\t\t0.704784\n",
      "Gradient Descent(330/999): loss=0.41078348864880365\t\t0.704836\n",
      "Gradient Descent(331/999): loss=0.41074757295785475\t\t0.704832\n",
      "Gradient Descent(332/999): loss=0.4107118678559908\t\t0.704868\n",
      "Gradient Descent(333/999): loss=0.41067637110331096\t\t0.704912\n",
      "Gradient Descent(334/999): loss=0.4106410804973376\t\t0.704936\n",
      "Gradient Descent(335/999): loss=0.41060599387224594\t\t0.705008\n",
      "Gradient Descent(336/999): loss=0.41057110909810796\t\t0.705036\n",
      "Gradient Descent(337/999): loss=0.4105364240801596\t\t0.70506\n",
      "Gradient Descent(338/999): loss=0.4105019367580782\t\t0.705044\n",
      "Gradient Descent(339/999): loss=0.41046764510528344\t\t0.705124\n",
      "Gradient Descent(340/999): loss=0.4104335471282488\t\t0.70516\n",
      "Gradient Descent(341/999): loss=0.4103996408658328\t\t0.705196\n",
      "Gradient Descent(342/999): loss=0.4103659243886235\t\t0.70526\n",
      "Gradient Descent(343/999): loss=0.41033239579829867\t\t0.705276\n",
      "Gradient Descent(344/999): loss=0.41029905322700105\t\t0.705328\n",
      "Gradient Descent(345/999): loss=0.4102658948367267\t\t0.705344\n",
      "Gradient Descent(346/999): loss=0.410232918818729\t\t0.705368\n",
      "Gradient Descent(347/999): loss=0.4102001233929342\t\t0.705376\n",
      "Gradient Descent(348/999): loss=0.4101675068073713\t\t0.7054\n",
      "Gradient Descent(349/999): loss=0.41013506733761534\t\t0.705432\n",
      "Gradient Descent(350/999): loss=0.4101028032862415\t\t0.705464\n",
      "Gradient Descent(351/999): loss=0.4100707129822935\t\t0.705476\n",
      "Gradient Descent(352/999): loss=0.4100387947807616\t\t0.705524\n",
      "Gradient Descent(353/999): loss=0.41000704706207475\t\t0.70556\n",
      "Gradient Descent(354/999): loss=0.4099754682316022\t\t0.705584\n",
      "Gradient Descent(355/999): loss=0.4099440567191667\t\t0.70556\n",
      "Gradient Descent(356/999): loss=0.4099128109785689\t\t0.705592\n",
      "Gradient Descent(357/999): loss=0.4098817294871214\t\t0.70564\n",
      "Gradient Descent(358/999): loss=0.4098508107451945\t\t0.70562\n",
      "Gradient Descent(359/999): loss=0.40982005327576976\t\t0.705616\n",
      "Gradient Descent(360/999): loss=0.40978945562400604\t\t0.705628\n",
      "Gradient Descent(361/999): loss=0.40975901635681233\t\t0.70566\n",
      "Gradient Descent(362/999): loss=0.40972873406243177\t\t0.705656\n",
      "Gradient Descent(363/999): loss=0.4096986073500344\t\t0.705656\n",
      "Gradient Descent(364/999): loss=0.4096686348493177\t\t0.705648\n",
      "Gradient Descent(365/999): loss=0.40963881521011675\t\t0.70566\n",
      "Gradient Descent(366/999): loss=0.40960914710202384\t\t0.705684\n",
      "Gradient Descent(367/999): loss=0.40957962921401253\t\t0.705716\n",
      "Gradient Descent(368/999): loss=0.4095502602540749\t\t0.705736\n",
      "Gradient Descent(369/999): loss=0.4095210389488623\t\t0.705736\n",
      "Gradient Descent(370/999): loss=0.4094919640433361\t\t0.70574\n",
      "Gradient Descent(371/999): loss=0.409463034300425\t\t0.705732\n",
      "Gradient Descent(372/999): loss=0.4094342485006901\t\t0.70576\n",
      "Gradient Descent(373/999): loss=0.4094056054419965\t\t0.705756\n",
      "Gradient Descent(374/999): loss=0.40937710393919285\t\t0.705756\n",
      "Gradient Descent(375/999): loss=0.4093487428237964\t\t0.705784\n",
      "Gradient Descent(376/999): loss=0.40932052094368526\t\t0.705784\n",
      "Gradient Descent(377/999): loss=0.409292437162798\t\t0.705828\n",
      "Gradient Descent(378/999): loss=0.409264490360838\t\t0.705832\n",
      "Gradient Descent(379/999): loss=0.4092366794329848\t\t0.705844\n",
      "Gradient Descent(380/999): loss=0.40920900328961207\t\t0.705912\n",
      "Gradient Descent(381/999): loss=0.4091814608560095\t\t0.70592\n",
      "Gradient Descent(382/999): loss=0.4091540510721134\t\t0.705916\n",
      "Gradient Descent(383/999): loss=0.40912677289224025\t\t0.705984\n",
      "Gradient Descent(384/999): loss=0.4090996252848266\t\t0.70602\n",
      "Gradient Descent(385/999): loss=0.4090726072321753\t\t0.706044\n",
      "Gradient Descent(386/999): loss=0.40904571773020576\t\t0.706092\n",
      "Gradient Descent(387/999): loss=0.4090189557882095\t\t0.706088\n",
      "Gradient Descent(388/999): loss=0.4089923204286119\t\t0.7061\n",
      "Gradient Descent(389/999): loss=0.4089658106867367\t\t0.706104\n",
      "Gradient Descent(390/999): loss=0.40893942561057733\t\t0.706108\n",
      "Gradient Descent(391/999): loss=0.4089131642605717\t\t0.706096\n",
      "Gradient Descent(392/999): loss=0.4088870257093825\t\t0.706104\n",
      "Gradient Descent(393/999): loss=0.40886100904168077\t\t0.706076\n",
      "Gradient Descent(394/999): loss=0.4088351133539349\t\t0.706116\n",
      "Gradient Descent(395/999): loss=0.40880933775420325\t\t0.706104\n",
      "Gradient Descent(396/999): loss=0.4087836813619313\t\t0.706116\n",
      "Gradient Descent(397/999): loss=0.40875814330775295\t\t0.70612\n",
      "Gradient Descent(398/999): loss=0.4087327227332958\t\t0.706132\n",
      "Gradient Descent(399/999): loss=0.40870741879098904\t\t0.706144\n",
      "Gradient Descent(400/999): loss=0.40868223064387793\t\t0.70616\n",
      "Gradient Descent(401/999): loss=0.408657157465439\t\t0.70616\n",
      "Gradient Descent(402/999): loss=0.40863219843940135\t\t0.706152\n",
      "Gradient Descent(403/999): loss=0.40860735275956983\t\t0.706192\n",
      "Gradient Descent(404/999): loss=0.4085826196296521\t\t0.70618\n",
      "Gradient Descent(405/999): loss=0.4085579982630902\t\t0.706204\n",
      "Gradient Descent(406/999): loss=0.40853348788289434\t\t0.706224\n",
      "Gradient Descent(407/999): loss=0.4085090877214793\t\t0.70622\n",
      "Gradient Descent(408/999): loss=0.4084847970205062\t\t0.706244\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(409/999): loss=0.408460615030726\t\t0.7062\n",
      "Gradient Descent(410/999): loss=0.40843654101182514\t\t0.706224\n",
      "Gradient Descent(411/999): loss=0.40841257423227656\t\t0.706256\n",
      "Gradient Descent(412/999): loss=0.40838871396919185\t\t0.706236\n",
      "Gradient Descent(413/999): loss=0.4083649595081766\t\t0.706252\n",
      "Gradient Descent(414/999): loss=0.408341310143189\t\t0.706236\n",
      "Gradient Descent(415/999): loss=0.4083177651764011\t\t0.706252\n",
      "Gradient Descent(416/999): loss=0.4082943239180618\t\t0.706272\n",
      "Gradient Descent(417/999): loss=0.4082709856863642\t\t0.706252\n",
      "Gradient Descent(418/999): loss=0.4082477498073145\t\t0.706244\n",
      "Gradient Descent(419/999): loss=0.40822461561460255\t\t0.706224\n",
      "Gradient Descent(420/999): loss=0.408201582449477\t\t0.706268\n",
      "Gradient Descent(421/999): loss=0.408178649660621\t\t0.706248\n",
      "Gradient Descent(422/999): loss=0.4081558166040311\t\t0.706236\n",
      "Gradient Descent(423/999): loss=0.40813308264289894\t\t0.706256\n",
      "Gradient Descent(424/999): loss=0.4081104471474935\t\t0.706276\n",
      "Gradient Descent(425/999): loss=0.4080879094950476\t\t0.706308\n",
      "Gradient Descent(426/999): loss=0.40806546906964514\t\t0.70632\n",
      "Gradient Descent(427/999): loss=0.40804312526211123\t\t0.706344\n",
      "Gradient Descent(428/999): loss=0.40802087746990423\t\t0.706332\n",
      "Gradient Descent(429/999): loss=0.4079987250970096\t\t0.706316\n",
      "Gradient Descent(430/999): loss=0.4079766675538354\t\t0.706332\n",
      "Gradient Descent(431/999): loss=0.40795470425711167\t\t0.706356\n",
      "Gradient Descent(432/999): loss=0.4079328346297885\t\t0.706416\n",
      "Gradient Descent(433/999): loss=0.4079110581009397\t\t0.70642\n",
      "Gradient Descent(434/999): loss=0.4078893741056644\t\t0.70642\n",
      "Gradient Descent(435/999): loss=0.40786778208499447\t\t0.7064\n",
      "Gradient Descent(436/999): loss=0.4078462814857997\t\t0.706404\n",
      "Gradient Descent(437/999): loss=0.40782487176069854\t\t0.706388\n",
      "Gradient Descent(438/999): loss=0.40780355236796734\t\t0.706428\n",
      "Gradient Descent(439/999): loss=0.4077823227714534\t\t0.706436\n",
      "Gradient Descent(440/999): loss=0.40776118244048815\t\t0.706464\n",
      "Gradient Descent(441/999): loss=0.40774013084980343\t\t0.706464\n",
      "Gradient Descent(442/999): loss=0.4077191674794481\t\t0.706456\n",
      "Gradient Descent(443/999): loss=0.40769829181470646\t\t0.706444\n",
      "Gradient Descent(444/999): loss=0.4076775033460181\t\t0.706472\n",
      "Gradient Descent(445/999): loss=0.40765680156890055\t\t0.706508\n",
      "Gradient Descent(446/999): loss=0.40763618598387075\t\t0.706512\n",
      "Gradient Descent(447/999): loss=0.407615656096371\t\t0.706556\n",
      "Gradient Descent(448/999): loss=0.40759521141669275\t\t0.706588\n",
      "Gradient Descent(449/999): loss=0.4075748514599058\t\t0.706624\n",
      "Gradient Descent(450/999): loss=0.4075545757457852\t\t0.706624\n",
      "Gradient Descent(451/999): loss=0.4075343837987417\t\t0.706608\n",
      "Gradient Descent(452/999): loss=0.4075142751477525\t\t0.706568\n",
      "Gradient Descent(453/999): loss=0.4074942493262937\t\t0.706564\n",
      "Gradient Descent(454/999): loss=0.4074743058722736\t\t0.706616\n",
      "Gradient Descent(455/999): loss=0.40745444432796746\t\t0.706604\n",
      "Gradient Descent(456/999): loss=0.4074346642399535\t\t0.706584\n",
      "Gradient Descent(457/999): loss=0.4074149651590491\t\t0.706572\n",
      "Gradient Descent(458/999): loss=0.4073953466402508\t\t0.706588\n",
      "Gradient Descent(459/999): loss=0.407375808242671\t\t0.70666\n",
      "Gradient Descent(460/999): loss=0.4073563495294807\t\t0.706668\n",
      "Gradient Descent(461/999): loss=0.40733697006784897\t\t0.706672\n",
      "Gradient Descent(462/999): loss=0.40731766942888675\t\t0.706676\n",
      "Gradient Descent(463/999): loss=0.40729844718758984\t\t0.706708\n",
      "Gradient Descent(464/999): loss=0.4072793029227829\t\t0.706716\n",
      "Gradient Descent(465/999): loss=0.4072602362170655\t\t0.706736\n",
      "Gradient Descent(466/999): loss=0.4072412466567587\t\t0.70672\n",
      "Gradient Descent(467/999): loss=0.40722233383185175\t\t0.706712\n",
      "Gradient Descent(468/999): loss=0.40720349733595057\t\t0.706728\n",
      "Gradient Descent(469/999): loss=0.4071847367662273\t\t0.706736\n",
      "Gradient Descent(470/999): loss=0.40716605172337\t\t0.706768\n",
      "Gradient Descent(471/999): loss=0.40714744181153406\t\t0.70674\n",
      "Gradient Descent(472/999): loss=0.4071289066382934\t\t0.706772\n",
      "Gradient Descent(473/999): loss=0.4071104458145934\t\t0.706796\n",
      "Gradient Descent(474/999): loss=0.407092058954705\t\t0.7068\n",
      "Gradient Descent(475/999): loss=0.40707374567617793\t\t0.706776\n",
      "Gradient Descent(476/999): loss=0.4070555055997964\t\t0.706756\n",
      "Gradient Descent(477/999): loss=0.40703733834953504\t\t0.706736\n",
      "Gradient Descent(478/999): loss=0.40701924355251523\t\t0.706728\n",
      "Gradient Descent(479/999): loss=0.4070012208389627\t\t0.706752\n",
      "Gradient Descent(480/999): loss=0.4069832698421655\t\t0.706764\n",
      "Gradient Descent(481/999): loss=0.40696539019843303\t\t0.706812\n",
      "Gradient Descent(482/999): loss=0.4069475815470552\t\t0.7068\n",
      "Gradient Descent(483/999): loss=0.4069298435302632\t\t0.706844\n",
      "Gradient Descent(484/999): loss=0.4069121757931902\t\t0.706844\n",
      "Gradient Descent(485/999): loss=0.4068945779838324\t\t0.706864\n",
      "Gradient Descent(486/999): loss=0.40687704975301214\t\t0.706868\n",
      "Gradient Descent(487/999): loss=0.40685959075434036\t\t0.706876\n",
      "Gradient Descent(488/999): loss=0.40684220064417986\t\t0.7069\n",
      "Gradient Descent(489/999): loss=0.40682487908161025\t\t0.706908\n",
      "Gradient Descent(490/999): loss=0.4068076257283913\t\t0.706916\n",
      "Gradient Descent(491/999): loss=0.40679044024892974\t\t0.706912\n",
      "Gradient Descent(492/999): loss=0.40677332231024443\t\t0.706892\n",
      "Gradient Descent(493/999): loss=0.40675627158193267\t\t0.706924\n",
      "Gradient Descent(494/999): loss=0.406739287736138\t\t0.706956\n",
      "Gradient Descent(495/999): loss=0.4067223704475168\t\t0.706964\n",
      "Gradient Descent(496/999): loss=0.4067055193932075\t\t0.706988\n",
      "Gradient Descent(497/999): loss=0.40668873425279833\t\t0.706988\n",
      "Gradient Descent(498/999): loss=0.40667201470829717\t\t0.707008\n",
      "Gradient Descent(499/999): loss=0.40665536044410056\t\t0.707004\n",
      "Gradient Descent(500/999): loss=0.4066387711469651\t\t0.706988\n",
      "Gradient Descent(501/999): loss=0.4066222465059768\t\t0.707012\n",
      "Gradient Descent(502/999): loss=0.4066057862125229\t\t0.706964\n",
      "Gradient Descent(503/999): loss=0.4065893899602636\t\t0.707004\n",
      "Gradient Descent(504/999): loss=0.4065730574451046\t\t0.707016\n",
      "Gradient Descent(505/999): loss=0.40655678836516834\t\t0.707036\n",
      "Gradient Descent(506/999): loss=0.4065405824207681\t\t0.707012\n",
      "Gradient Descent(507/999): loss=0.40652443931438165\t\t0.707028\n",
      "Gradient Descent(508/999): loss=0.4065083587506246\t\t0.707016\n",
      "Gradient Descent(509/999): loss=0.4064923404362244\t\t0.707056\n",
      "Gradient Descent(510/999): loss=0.4064763840799963\t\t0.707064\n",
      "Gradient Descent(511/999): loss=0.40646048939281737\t\t0.707072\n",
      "Gradient Descent(512/999): loss=0.40644465608760244\t\t0.70708\n",
      "Gradient Descent(513/999): loss=0.40642888387928033\t\t0.707112\n",
      "Gradient Descent(514/999): loss=0.4064131724847698\t\t0.707136\n",
      "Gradient Descent(515/999): loss=0.4063975216229567\t\t0.707172\n",
      "Gradient Descent(516/999): loss=0.40638193101467024\t\t0.707172\n",
      "Gradient Descent(517/999): loss=0.4063664003826614\t\t0.707176\n",
      "Gradient Descent(518/999): loss=0.40635092945158047\t\t0.707204\n",
      "Gradient Descent(519/999): loss=0.4063355179479548\t\t0.707196\n",
      "Gradient Descent(520/999): loss=0.4063201656001678\t\t0.707208\n",
      "Gradient Descent(521/999): loss=0.40630487213843747\t\t0.707212\n",
      "Gradient Descent(522/999): loss=0.40628963729479634\t\t0.707228\n",
      "Gradient Descent(523/999): loss=0.40627446080306984\t\t0.707224\n",
      "Gradient Descent(524/999): loss=0.4062593423988566\t\t0.70724\n",
      "Gradient Descent(525/999): loss=0.4062442818195094\t\t0.707248\n",
      "Gradient Descent(526/999): loss=0.4062292788041144\t\t0.707244\n",
      "Gradient Descent(527/999): loss=0.406214333093473\t\t0.707256\n",
      "Gradient Descent(528/999): loss=0.4061994444300819\t\t0.707268\n",
      "Gradient Descent(529/999): loss=0.4061846125581155\t\t0.707256\n",
      "Gradient Descent(530/999): loss=0.4061698372234069\t\t0.707252\n",
      "Gradient Descent(531/999): loss=0.40615511817342986\t\t0.707228\n",
      "Gradient Descent(532/999): loss=0.4061404551572812\t\t0.707236\n",
      "Gradient Descent(533/999): loss=0.40612584792566386\t\t0.707228\n",
      "Gradient Descent(534/999): loss=0.40611129623086795\t\t0.70724\n",
      "Gradient Descent(535/999): loss=0.40609679982675634\t\t0.707244\n",
      "Gradient Descent(536/999): loss=0.40608235846874546\t\t0.707256\n",
      "Gradient Descent(537/999): loss=0.40606797191379046\t\t0.707256\n",
      "Gradient Descent(538/999): loss=0.40605363992036814\t\t0.707272\n",
      "Gradient Descent(539/999): loss=0.4060393622484617\t\t0.707272\n",
      "Gradient Descent(540/999): loss=0.40602513865954465\t\t0.707268\n",
      "Gradient Descent(541/999): loss=0.4060109689165652\t\t0.707272\n",
      "Gradient Descent(542/999): loss=0.40599685278393094\t\t0.707256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(543/999): loss=0.40598279002749454\t\t0.70728\n",
      "Gradient Descent(544/999): loss=0.4059687804145382\t\t0.707284\n",
      "Gradient Descent(545/999): loss=0.40595482371375946\t\t0.707288\n",
      "Gradient Descent(546/999): loss=0.4059409196952569\t\t0.707308\n",
      "Gradient Descent(547/999): loss=0.4059270681305153\t\t0.707304\n",
      "Gradient Descent(548/999): loss=0.4059132687923931\t\t0.707284\n",
      "Gradient Descent(549/999): loss=0.40589952145510716\t\t0.707284\n",
      "Gradient Descent(550/999): loss=0.40588582589422\t\t0.707292\n",
      "Gradient Descent(551/999): loss=0.40587218188662716\t\t0.707296\n",
      "Gradient Descent(552/999): loss=0.4058585892105425\t\t0.70734\n",
      "Gradient Descent(553/999): loss=0.4058450476454862\t\t0.707328\n",
      "Gradient Descent(554/999): loss=0.40583155697227224\t\t0.707324\n",
      "Gradient Descent(555/999): loss=0.40581811697299536\t\t0.707336\n",
      "Gradient Descent(556/999): loss=0.40580472743101886\t\t0.707344\n",
      "Gradient Descent(557/999): loss=0.40579138813096255\t\t0.707328\n",
      "Gradient Descent(558/999): loss=0.40577809885869026\t\t0.707312\n",
      "Gradient Descent(559/999): loss=0.40576485940129847\t\t0.70732\n",
      "Gradient Descent(560/999): loss=0.4057516695471044\t\t0.70734\n",
      "Gradient Descent(561/999): loss=0.40573852908563457\t\t0.707348\n",
      "Gradient Descent(562/999): loss=0.40572543780761355\t\t0.707384\n",
      "Gradient Descent(563/999): loss=0.40571239550495264\t\t0.707372\n",
      "Gradient Descent(564/999): loss=0.4056994019707385\t\t0.707384\n",
      "Gradient Descent(565/999): loss=0.40568645699922323\t\t0.707372\n",
      "Gradient Descent(566/999): loss=0.40567356038581276\t\t0.707408\n",
      "Gradient Descent(567/999): loss=0.4056607119270565\t\t0.707404\n",
      "Gradient Descent(568/999): loss=0.40564791142063744\t\t0.707416\n",
      "Gradient Descent(569/999): loss=0.40563515866536104\t\t0.707392\n",
      "Gradient Descent(570/999): loss=0.40562245346114534\t\t0.707388\n",
      "Gradient Descent(571/999): loss=0.4056097956090119\t\t0.707424\n",
      "Gradient Descent(572/999): loss=0.4055971849110742\t\t0.707388\n",
      "Gradient Descent(573/999): loss=0.40558462117052996\t\t0.707428\n",
      "Gradient Descent(574/999): loss=0.4055721041916494\t\t0.707428\n",
      "Gradient Descent(575/999): loss=0.40555963377976795\t\t0.707404\n",
      "Gradient Descent(576/999): loss=0.4055472097412749\t\t0.707436\n",
      "Gradient Descent(577/999): loss=0.4055348318836062\t\t0.7074\n",
      "Gradient Descent(578/999): loss=0.40552250001523316\t\t0.70742\n",
      "Gradient Descent(579/999): loss=0.40551021394565584\t\t0.707436\n",
      "Gradient Descent(580/999): loss=0.40549797348539246\t\t0.70744\n",
      "Gradient Descent(581/999): loss=0.40548577844597156\t\t0.70746\n",
      "Gradient Descent(582/999): loss=0.4054736286399228\t\t0.707468\n",
      "Gradient Descent(583/999): loss=0.4054615238807693\t\t0.707464\n",
      "Gradient Descent(584/999): loss=0.4054494639830183\t\t0.707472\n",
      "Gradient Descent(585/999): loss=0.4054374487621542\t\t0.707464\n",
      "Gradient Descent(586/999): loss=0.40542547803462875\t\t0.707456\n",
      "Gradient Descent(587/999): loss=0.4054135516178548\t\t0.70744\n",
      "Gradient Descent(588/999): loss=0.4054016693301965\t\t0.707448\n",
      "Gradient Descent(589/999): loss=0.4053898309909629\t\t0.707448\n",
      "Gradient Descent(590/999): loss=0.40537803642040027\t\t0.707444\n",
      "Gradient Descent(591/999): loss=0.4053662854396831\t\t0.70742\n",
      "Gradient Descent(592/999): loss=0.40535457787090784\t\t0.707392\n",
      "Gradient Descent(593/999): loss=0.40534291353708507\t\t0.707404\n",
      "Gradient Descent(594/999): loss=0.40533129226213205\t\t0.707416\n",
      "Gradient Descent(595/999): loss=0.4053197138708655\t\t0.707428\n",
      "Gradient Descent(596/999): loss=0.405308178188995\t\t0.707444\n",
      "Gradient Descent(597/999): loss=0.4052966850431148\t\t0.707452\n",
      "Gradient Descent(598/999): loss=0.4052852342606978\t\t0.707484\n",
      "Gradient Descent(599/999): loss=0.4052738256700885\t\t0.7075\n",
      "Gradient Descent(600/999): loss=0.40526245910049596\t\t0.707516\n",
      "Gradient Descent(601/999): loss=0.4052511343819872\t\t0.707516\n",
      "Gradient Descent(602/999): loss=0.4052398513454805\t\t0.707532\n",
      "Gradient Descent(603/999): loss=0.4052286098227393\t\t0.707528\n",
      "Gradient Descent(604/999): loss=0.4052174096463649\t\t0.707556\n",
      "Gradient Descent(605/999): loss=0.40520625064979066\t\t0.70756\n",
      "Gradient Descent(606/999): loss=0.40519513266727597\t\t0.707584\n",
      "Gradient Descent(607/999): loss=0.4051840555338989\t\t0.707588\n",
      "Gradient Descent(608/999): loss=0.40517301908555164\t\t0.707608\n",
      "Gradient Descent(609/999): loss=0.405162023158933\t\t0.7076\n",
      "Gradient Descent(610/999): loss=0.4051510675915429\t\t0.707632\n",
      "Gradient Descent(611/999): loss=0.40514015222167676\t\t0.707652\n",
      "Gradient Descent(612/999): loss=0.40512927688841954\t\t0.707668\n",
      "Gradient Descent(613/999): loss=0.4051184414316394\t\t0.70762\n",
      "Gradient Descent(614/999): loss=0.4051076456919825\t\t0.707656\n",
      "Gradient Descent(615/999): loss=0.40509688951086714\t\t0.707676\n",
      "Gradient Descent(616/999): loss=0.4050861727304786\t\t0.707668\n",
      "Gradient Descent(617/999): loss=0.40507549519376274\t\t0.707688\n",
      "Gradient Descent(618/999): loss=0.4050648567444215\t\t0.707708\n",
      "Gradient Descent(619/999): loss=0.4050542572269068\t\t0.707684\n",
      "Gradient Descent(620/999): loss=0.4050436964864155\t\t0.707728\n",
      "Gradient Descent(621/999): loss=0.40503317436888453\t\t0.707724\n",
      "Gradient Descent(622/999): loss=0.4050226907209844\t\t0.707728\n",
      "Gradient Descent(623/999): loss=0.4050122453901157\t\t0.707724\n",
      "Gradient Descent(624/999): loss=0.4050018382244029\t\t0.7077\n",
      "Gradient Descent(625/999): loss=0.40499146907268957\t\t0.707704\n",
      "Gradient Descent(626/999): loss=0.4049811377845337\t\t0.707696\n",
      "Gradient Descent(627/999): loss=0.4049708442102022\t\t0.707692\n",
      "Gradient Descent(628/999): loss=0.40496058820066655\t\t0.707672\n",
      "Gradient Descent(629/999): loss=0.4049503696075982\t\t0.707676\n",
      "Gradient Descent(630/999): loss=0.4049401882833626\t\t0.707636\n",
      "Gradient Descent(631/999): loss=0.4049300440810165\t\t0.707632\n",
      "Gradient Descent(632/999): loss=0.4049199368543008\t\t0.70762\n",
      "Gradient Descent(633/999): loss=0.4049098664576384\t\t0.707632\n",
      "Gradient Descent(634/999): loss=0.40489983274612784\t\t0.707664\n",
      "Gradient Descent(635/999): loss=0.4048898355755397\t\t0.707652\n",
      "Gradient Descent(636/999): loss=0.40487987480231225\t\t0.70766\n",
      "Gradient Descent(637/999): loss=0.4048699502835462\t\t0.70766\n",
      "Gradient Descent(638/999): loss=0.40486006187700124\t\t0.707628\n",
      "Gradient Descent(639/999): loss=0.40485020944109124\t\t0.707636\n",
      "Gradient Descent(640/999): loss=0.4048403928348801\t\t0.707616\n",
      "Gradient Descent(641/999): loss=0.40483061191807757\t\t0.70764\n",
      "Gradient Descent(642/999): loss=0.40482086655103533\t\t0.707668\n",
      "Gradient Descent(643/999): loss=0.40481115659474204\t\t0.707688\n",
      "Gradient Descent(644/999): loss=0.40480148191082005\t\t0.707712\n",
      "Gradient Descent(645/999): loss=0.40479184236152155\t\t0.707704\n",
      "Gradient Descent(646/999): loss=0.4047822378097233\t\t0.70772\n",
      "Gradient Descent(647/999): loss=0.40477266811892404\t\t0.707704\n",
      "Gradient Descent(648/999): loss=0.40476313315324003\t\t0.707708\n",
      "Gradient Descent(649/999): loss=0.40475363277740084\t\t0.707724\n",
      "Gradient Descent(650/999): loss=0.40474416685674613\t\t0.707728\n",
      "Gradient Descent(651/999): loss=0.4047347352572214\t\t0.707788\n",
      "Gradient Descent(652/999): loss=0.40472533784537457\t\t0.707796\n",
      "Gradient Descent(653/999): loss=0.40471597448835156\t\t0.707816\n",
      "Gradient Descent(654/999): loss=0.4047066450538937\t\t0.707788\n",
      "Gradient Descent(655/999): loss=0.40469734941033336\t\t0.7078\n",
      "Gradient Descent(656/999): loss=0.40468808742659\t\t0.707776\n",
      "Gradient Descent(657/999): loss=0.4046788589721677\t\t0.707772\n",
      "Gradient Descent(658/999): loss=0.40466966391715087\t\t0.707776\n",
      "Gradient Descent(659/999): loss=0.40466050213220034\t\t0.7078\n",
      "Gradient Descent(660/999): loss=0.4046513734885508\t\t0.7078\n",
      "Gradient Descent(661/999): loss=0.40464227785800666\t\t0.707788\n",
      "Gradient Descent(662/999): loss=0.4046332151129396\t\t0.707824\n",
      "Gradient Descent(663/999): loss=0.4046241851262836\t\t0.707828\n",
      "Gradient Descent(664/999): loss=0.40461518777153327\t\t0.707824\n",
      "Gradient Descent(665/999): loss=0.4046062229227391\t\t0.707844\n",
      "Gradient Descent(666/999): loss=0.40459729045450565\t\t0.707848\n",
      "Gradient Descent(667/999): loss=0.40458839024198723\t\t0.707828\n",
      "Gradient Descent(668/999): loss=0.40457952216088483\t\t0.707836\n",
      "Gradient Descent(669/999): loss=0.4045706860874427\t\t0.707836\n",
      "Gradient Descent(670/999): loss=0.4045618818984466\t\t0.707832\n",
      "Gradient Descent(671/999): loss=0.4045531094712186\t\t0.707836\n",
      "Gradient Descent(672/999): loss=0.4045443686836161\t\t0.707816\n",
      "Gradient Descent(673/999): loss=0.40453565941402647\t\t0.707824\n",
      "Gradient Descent(674/999): loss=0.404526981541366\t\t0.707848\n",
      "Gradient Descent(675/999): loss=0.4045183349450762\t\t0.70784\n",
      "Gradient Descent(676/999): loss=0.4045097195051204\t\t0.70786\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(677/999): loss=0.40450113510198077\t\t0.70784\n",
      "Gradient Descent(678/999): loss=0.40449258161665663\t\t0.707828\n",
      "Gradient Descent(679/999): loss=0.4044840589306599\t\t0.70784\n",
      "Gradient Descent(680/999): loss=0.40447556692601294\t\t0.707836\n",
      "Gradient Descent(681/999): loss=0.4044671054852464\t\t0.707868\n",
      "Gradient Descent(682/999): loss=0.40445867449139467\t\t0.707852\n",
      "Gradient Descent(683/999): loss=0.4044502738279949\t\t0.707852\n",
      "Gradient Descent(684/999): loss=0.4044419033790836\t\t0.707848\n",
      "Gradient Descent(685/999): loss=0.40443356302919276\t\t0.707872\n",
      "Gradient Descent(686/999): loss=0.4044252526633486\t\t0.707884\n",
      "Gradient Descent(687/999): loss=0.4044169721670684\t\t0.707888\n",
      "Gradient Descent(688/999): loss=0.40440872142635736\t\t0.70792\n",
      "Gradient Descent(689/999): loss=0.4044005003277067\t\t0.707936\n",
      "Gradient Descent(690/999): loss=0.40439230875809046\t\t0.707944\n",
      "Gradient Descent(691/999): loss=0.4043841466049631\t\t0.707932\n",
      "Gradient Descent(692/999): loss=0.4043760137562563\t\t0.707944\n",
      "Gradient Descent(693/999): loss=0.4043679101003778\t\t0.707964\n",
      "Gradient Descent(694/999): loss=0.40435983552620725\t\t0.707968\n",
      "Gradient Descent(695/999): loss=0.4043517899230948\t\t0.707964\n",
      "Gradient Descent(696/999): loss=0.4043437731808581\t\t0.70796\n",
      "Gradient Descent(697/999): loss=0.40433578518977975\t\t0.707964\n",
      "Gradient Descent(698/999): loss=0.40432782584060506\t\t0.707948\n",
      "Gradient Descent(699/999): loss=0.4043198950245396\t\t0.707928\n",
      "Gradient Descent(700/999): loss=0.4043119926332463\t\t0.707908\n",
      "Gradient Descent(701/999): loss=0.404304118558844\t\t0.707924\n",
      "Gradient Descent(702/999): loss=0.404296272693904\t\t0.70792\n",
      "Gradient Descent(703/999): loss=0.4042884549314484\t\t0.707904\n",
      "Gradient Descent(704/999): loss=0.4042806651649473\t\t0.7079\n",
      "Gradient Descent(705/999): loss=0.4042729032883168\t\t0.7079\n",
      "Gradient Descent(706/999): loss=0.40426516919591665\t\t0.707908\n",
      "Gradient Descent(707/999): loss=0.40425746278254726\t\t0.707908\n",
      "Gradient Descent(708/999): loss=0.40424978394344946\t\t0.707888\n",
      "Gradient Descent(709/999): loss=0.40424213257429864\t\t0.707876\n",
      "Gradient Descent(710/999): loss=0.4042345085712065\t\t0.707888\n",
      "Gradient Descent(711/999): loss=0.4042269118307162\t\t0.707896\n",
      "Gradient Descent(712/999): loss=0.4042193422498008\t\t0.707908\n",
      "Gradient Descent(713/999): loss=0.4042117997258616\t\t0.70788\n",
      "Gradient Descent(714/999): loss=0.40420428415672516\t\t0.707896\n",
      "Gradient Descent(715/999): loss=0.40419679544064163\t\t0.707888\n",
      "Gradient Descent(716/999): loss=0.40418933347628266\t\t0.70788\n",
      "Gradient Descent(717/999): loss=0.40418189816273925\t\t0.707888\n",
      "Gradient Descent(718/999): loss=0.40417448939951883\t\t0.707916\n",
      "Gradient Descent(719/999): loss=0.4041671070865443\t\t0.707924\n",
      "Gradient Descent(720/999): loss=0.4041597511241516\t\t0.707908\n",
      "Gradient Descent(721/999): loss=0.40415242141308766\t\t0.707912\n",
      "Gradient Descent(722/999): loss=0.40414511785450724\t\t0.707908\n",
      "Gradient Descent(723/999): loss=0.4041378403499729\t\t0.707932\n",
      "Gradient Descent(724/999): loss=0.40413058880145164\t\t0.707932\n",
      "Gradient Descent(725/999): loss=0.4041233631113132\t\t0.70792\n",
      "Gradient Descent(726/999): loss=0.4041161631823274\t\t0.707932\n",
      "Gradient Descent(727/999): loss=0.40410898891766417\t\t0.70792\n",
      "Gradient Descent(728/999): loss=0.4041018402208891\t\t0.707924\n",
      "Gradient Descent(729/999): loss=0.40409471699596294\t\t0.70792\n",
      "Gradient Descent(730/999): loss=0.4040876191472398\t\t0.707912\n",
      "Gradient Descent(731/999): loss=0.40408054657946424\t\t0.70792\n",
      "Gradient Descent(732/999): loss=0.40407349919776997\t\t0.707932\n",
      "Gradient Descent(733/999): loss=0.4040664769076782\t\t0.707956\n",
      "Gradient Descent(734/999): loss=0.40405947961509525\t\t0.707976\n",
      "Gradient Descent(735/999): loss=0.40405250722631075\t\t0.707976\n",
      "Gradient Descent(736/999): loss=0.40404555964799627\t\t0.707968\n",
      "Gradient Descent(737/999): loss=0.40403863678720287\t\t0.707956\n",
      "Gradient Descent(738/999): loss=0.40403173855135915\t\t0.707964\n",
      "Gradient Descent(739/999): loss=0.4040248648482707\t\t0.70796\n",
      "Gradient Descent(740/999): loss=0.4040180155861169\t\t0.707948\n",
      "Gradient Descent(741/999): loss=0.40401119067344926\t\t0.707924\n",
      "Gradient Descent(742/999): loss=0.4040043900191908\t\t0.70792\n",
      "Gradient Descent(743/999): loss=0.40399761353263286\t\t0.707924\n",
      "Gradient Descent(744/999): loss=0.4039908611234341\t\t0.707912\n",
      "Gradient Descent(745/999): loss=0.40398413270161865\t\t0.707896\n",
      "Gradient Descent(746/999): loss=0.4039774281775745\t\t0.707904\n",
      "Gradient Descent(747/999): loss=0.40397074746205147\t\t0.707896\n",
      "Gradient Descent(748/999): loss=0.40396409046615966\t\t0.707916\n",
      "Gradient Descent(749/999): loss=0.4039574571013677\t\t0.707912\n",
      "Gradient Descent(750/999): loss=0.40395084727950137\t\t0.707908\n",
      "Gradient Descent(751/999): loss=0.40394426091274127\t\t0.707916\n",
      "Gradient Descent(752/999): loss=0.40393769791362155\t\t0.7079\n",
      "Gradient Descent(753/999): loss=0.4039311581950287\t\t0.707888\n",
      "Gradient Descent(754/999): loss=0.40392464167019904\t\t0.707872\n",
      "Gradient Descent(755/999): loss=0.4039181482527175\t\t0.707864\n",
      "Gradient Descent(756/999): loss=0.40391167785651627\t\t0.707868\n",
      "Gradient Descent(757/999): loss=0.4039052303958726\t\t0.707844\n",
      "Gradient Descent(758/999): loss=0.4038988057854076\t\t0.70786\n",
      "Gradient Descent(759/999): loss=0.4038924039400846\t\t0.707844\n",
      "Gradient Descent(760/999): loss=0.4038860247752075\t\t0.707844\n",
      "Gradient Descent(761/999): loss=0.4038796682064193\t\t0.707852\n",
      "Gradient Descent(762/999): loss=0.4038733341496999\t\t0.70786\n",
      "Gradient Descent(763/999): loss=0.40386702252136597\t\t0.707868\n",
      "Gradient Descent(764/999): loss=0.40386073323806776\t\t0.707872\n",
      "Gradient Descent(765/999): loss=0.4038544662167887\t\t0.707864\n",
      "Gradient Descent(766/999): loss=0.4038482213748436\t\t0.707864\n",
      "Gradient Descent(767/999): loss=0.4038419986298767\t\t0.707864\n",
      "Gradient Descent(768/999): loss=0.4038357978998609\t\t0.707876\n",
      "Gradient Descent(769/999): loss=0.40382961910309534\t\t0.707896\n",
      "Gradient Descent(770/999): loss=0.4038234621582048\t\t0.707868\n",
      "Gradient Descent(771/999): loss=0.4038173269841381\t\t0.707868\n",
      "Gradient Descent(772/999): loss=0.4038112135001656\t\t0.707884\n",
      "Gradient Descent(773/999): loss=0.4038051216258791\t\t0.707876\n",
      "Gradient Descent(774/999): loss=0.4037990512811902\t\t0.707868\n",
      "Gradient Descent(775/999): loss=0.4037930023863274\t\t0.70786\n",
      "Gradient Descent(776/999): loss=0.40378697486183646\t\t0.707864\n",
      "Gradient Descent(777/999): loss=0.4037809686285784\t\t0.707876\n",
      "Gradient Descent(778/999): loss=0.4037749836077272\t\t0.707872\n",
      "Gradient Descent(779/999): loss=0.40376901972076956\t\t0.707876\n",
      "Gradient Descent(780/999): loss=0.40376307688950336\t\t0.707888\n",
      "Gradient Descent(781/999): loss=0.40375715503603515\t\t0.707892\n",
      "Gradient Descent(782/999): loss=0.4037512540827805\t\t0.707876\n",
      "Gradient Descent(783/999): loss=0.403745373952461\t\t0.707868\n",
      "Gradient Descent(784/999): loss=0.4037395145681038\t\t0.70784\n",
      "Gradient Descent(785/999): loss=0.40373367585304015\t\t0.70784\n",
      "Gradient Descent(786/999): loss=0.40372785773090386\t\t0.707872\n",
      "Gradient Descent(787/999): loss=0.4037220601256297\t\t0.707884\n",
      "Gradient Descent(788/999): loss=0.40371628296145295\t\t0.70788\n",
      "Gradient Descent(789/999): loss=0.4037105261629074\t\t0.70788\n",
      "Gradient Descent(790/999): loss=0.40370478965482365\t\t0.707864\n",
      "Gradient Descent(791/999): loss=0.40369907336232863\t\t0.707884\n",
      "Gradient Descent(792/999): loss=0.40369337721084414\t\t0.707892\n",
      "Gradient Descent(793/999): loss=0.4036877011260847\t\t0.707888\n",
      "Gradient Descent(794/999): loss=0.40368204503405763\t\t0.707904\n",
      "Gradient Descent(795/999): loss=0.403676408861061\t\t0.707916\n",
      "Gradient Descent(796/999): loss=0.4036707925336815\t\t0.707948\n",
      "Gradient Descent(797/999): loss=0.403665195978795\t\t0.707924\n",
      "Gradient Descent(798/999): loss=0.40365961912356385\t\t0.707916\n",
      "Gradient Descent(799/999): loss=0.40365406189543657\t\t0.707932\n",
      "Gradient Descent(800/999): loss=0.4036485242221456\t\t0.707928\n",
      "Gradient Descent(801/999): loss=0.403643006031707\t\t0.707932\n",
      "Gradient Descent(802/999): loss=0.4036375072524185\t\t0.707924\n",
      "Gradient Descent(803/999): loss=0.403632027812859\t\t0.707948\n",
      "Gradient Descent(804/999): loss=0.40362656764188626\t\t0.70792\n",
      "Gradient Descent(805/999): loss=0.40362112666863725\t\t0.707944\n",
      "Gradient Descent(806/999): loss=0.4036157048225251\t\t0.70796\n",
      "Gradient Descent(807/999): loss=0.4036103020332396\t\t0.70796\n",
      "Gradient Descent(808/999): loss=0.4036049182307449\t\t0.707956\n",
      "Gradient Descent(809/999): loss=0.4035995533452786\t\t0.707988\n",
      "Gradient Descent(810/999): loss=0.40359420730735057\t\t0.70796\n",
      "Gradient Descent(811/999): loss=0.4035888800477425\t\t0.707936\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(812/999): loss=0.4035835714975051\t\t0.707948\n",
      "Gradient Descent(813/999): loss=0.4035782815879584\t\t0.707976\n",
      "Gradient Descent(814/999): loss=0.40357301025069\t\t0.707972\n",
      "Gradient Descent(815/999): loss=0.4035677574175542\t\t0.707988\n",
      "Gradient Descent(816/999): loss=0.4035625230206701\t\t0.70798\n",
      "Gradient Descent(817/999): loss=0.4035573069924219\t\t0.707996\n",
      "Gradient Descent(818/999): loss=0.403552109265456\t\t0.708004\n",
      "Gradient Descent(819/999): loss=0.40354692977268086\t\t0.708\n",
      "Gradient Descent(820/999): loss=0.4035417684472663\t\t0.708012\n",
      "Gradient Descent(821/999): loss=0.4035366252226417\t\t0.708012\n",
      "Gradient Descent(822/999): loss=0.40353150003249466\t\t0.70802\n",
      "Gradient Descent(823/999): loss=0.4035263928107704\t\t0.708008\n",
      "Gradient Descent(824/999): loss=0.4035213034916709\t\t0.707992\n",
      "Gradient Descent(825/999): loss=0.4035162320096528\t\t0.708008\n",
      "Gradient Descent(826/999): loss=0.40351117829942773\t\t0.708004\n",
      "Gradient Descent(827/999): loss=0.40350614229595955\t\t0.708\n",
      "Gradient Descent(828/999): loss=0.40350112393446513\t\t0.707996\n",
      "Gradient Descent(829/999): loss=0.4034961231504114\t\t0.707976\n",
      "Gradient Descent(830/999): loss=0.40349113987951585\t\t0.707944\n",
      "Gradient Descent(831/999): loss=0.40348617405774473\t\t0.707952\n",
      "Gradient Descent(832/999): loss=0.40348122562131183\t\t0.707956\n",
      "Gradient Descent(833/999): loss=0.4034762945066775\t\t0.70794\n",
      "Gradient Descent(834/999): loss=0.40347138065054844\t\t0.707936\n",
      "Gradient Descent(835/999): loss=0.4034664839898756\t\t0.707932\n",
      "Gradient Descent(836/999): loss=0.4034616044618536\t\t0.707932\n",
      "Gradient Descent(837/999): loss=0.40345674200391973\t\t0.707924\n",
      "Gradient Descent(838/999): loss=0.4034518965537529\t\t0.707916\n",
      "Gradient Descent(839/999): loss=0.40344706804927216\t\t0.707912\n",
      "Gradient Descent(840/999): loss=0.4034422564286368\t\t0.707908\n",
      "Gradient Descent(841/999): loss=0.40343746163024397\t\t0.707912\n",
      "Gradient Descent(842/999): loss=0.403432683592729\t\t0.707916\n",
      "Gradient Descent(843/999): loss=0.40342792225496305\t\t0.707904\n",
      "Gradient Descent(844/999): loss=0.4034231775560533\t\t0.707916\n",
      "Gradient Descent(845/999): loss=0.40341844943534133\t\t0.707924\n",
      "Gradient Descent(846/999): loss=0.4034137378324024\t\t0.707924\n",
      "Gradient Descent(847/999): loss=0.4034090426870445\t\t0.707908\n",
      "Gradient Descent(848/999): loss=0.4034043639393064\t\t0.707912\n",
      "Gradient Descent(849/999): loss=0.4033997015294587\t\t0.707916\n",
      "Gradient Descent(850/999): loss=0.40339505539800086\t\t0.707924\n",
      "Gradient Descent(851/999): loss=0.4033904254856614\t\t0.707916\n",
      "Gradient Descent(852/999): loss=0.4033858117333965\t\t0.707924\n",
      "Gradient Descent(853/999): loss=0.403381214082389\t\t0.70794\n",
      "Gradient Descent(854/999): loss=0.40337663247404815\t\t0.707948\n",
      "Gradient Descent(855/999): loss=0.4033720668500072\t\t0.707968\n",
      "Gradient Descent(856/999): loss=0.4033675171521247\t\t0.707984\n",
      "Gradient Descent(857/999): loss=0.40336298332248105\t\t0.70798\n",
      "Gradient Descent(858/999): loss=0.4033584653033795\t\t0.707984\n",
      "Gradient Descent(859/999): loss=0.40335396303734417\t\t0.707996\n",
      "Gradient Descent(860/999): loss=0.40334947646711955\t\t0.707976\n",
      "Gradient Descent(861/999): loss=0.40334500553566976\t\t0.70798\n",
      "Gradient Descent(862/999): loss=0.403340550186177\t\t0.707996\n",
      "Gradient Descent(863/999): loss=0.40333611036204114\t\t0.708\n",
      "Gradient Descent(864/999): loss=0.4033316860068792\t\t0.70798\n",
      "Gradient Descent(865/999): loss=0.40332727706452304\t\t0.707992\n",
      "Gradient Descent(866/999): loss=0.4033228834790203\t\t0.707988\n",
      "Gradient Descent(867/999): loss=0.40331850519463214\t\t0.707976\n",
      "Gradient Descent(868/999): loss=0.403314142155833\t\t0.70798\n",
      "Gradient Descent(869/999): loss=0.4033097943073093\t\t0.707976\n",
      "Gradient Descent(870/999): loss=0.40330546159395925\t\t0.707988\n",
      "Gradient Descent(871/999): loss=0.4033011439608911\t\t0.707996\n",
      "Gradient Descent(872/999): loss=0.403296841353423\t\t0.707992\n",
      "Gradient Descent(873/999): loss=0.40329255371708145\t\t0.707968\n",
      "Gradient Descent(874/999): loss=0.40328828099760144\t\t0.707976\n",
      "Gradient Descent(875/999): loss=0.40328402314092476\t\t0.707992\n",
      "Gradient Descent(876/999): loss=0.40327978009319904\t\t0.70798\n",
      "Gradient Descent(877/999): loss=0.40327555180077773\t\t0.707964\n",
      "Gradient Descent(878/999): loss=0.4032713382102186\t\t0.707956\n",
      "Gradient Descent(879/999): loss=0.4032671392682828\t\t0.707956\n",
      "Gradient Descent(880/999): loss=0.40326295492193515\t\t0.707956\n",
      "Gradient Descent(881/999): loss=0.4032587851183412\t\t0.707952\n",
      "Gradient Descent(882/999): loss=0.4032546298048687\t\t0.707932\n",
      "Gradient Descent(883/999): loss=0.40325048892908527\t\t0.70794\n",
      "Gradient Descent(884/999): loss=0.4032463624387583\t\t0.707928\n",
      "Gradient Descent(885/999): loss=0.40324225028185373\t\t0.707928\n",
      "Gradient Descent(886/999): loss=0.40323815240653516\t\t0.707952\n",
      "Gradient Descent(887/999): loss=0.4032340687611639\t\t0.707956\n",
      "Gradient Descent(888/999): loss=0.4032299992942971\t\t0.707976\n",
      "Gradient Descent(889/999): loss=0.4032259439546876\t\t0.707952\n",
      "Gradient Descent(890/999): loss=0.4032219026912822\t\t0.70798\n",
      "Gradient Descent(891/999): loss=0.4032178754532231\t\t0.707988\n",
      "Gradient Descent(892/999): loss=0.4032138621898439\t\t0.707992\n",
      "Gradient Descent(893/999): loss=0.40320986285067184\t\t0.707992\n",
      "Gradient Descent(894/999): loss=0.40320587738542496\t\t0.70796\n",
      "Gradient Descent(895/999): loss=0.4032019057440124\t\t0.707988\n",
      "Gradient Descent(896/999): loss=0.40319794787653307\t\t0.707984\n",
      "Gradient Descent(897/999): loss=0.40319400373327513\t\t0.708\n",
      "Gradient Descent(898/999): loss=0.4031900732647151\t\t0.708\n",
      "Gradient Descent(899/999): loss=0.4031861564215178\t\t0.707988\n",
      "Gradient Descent(900/999): loss=0.4031822531545339\t\t0.707996\n",
      "Gradient Descent(901/999): loss=0.40317836341480107\t\t0.708028\n",
      "Gradient Descent(902/999): loss=0.40317448715354204\t\t0.708024\n",
      "Gradient Descent(903/999): loss=0.40317062432216433\t\t0.708016\n",
      "Gradient Descent(904/999): loss=0.4031667748722593\t\t0.708024\n",
      "Gradient Descent(905/999): loss=0.4031629387556016\t\t0.708\n",
      "Gradient Descent(906/999): loss=0.40315911592414766\t\t0.708\n",
      "Gradient Descent(907/999): loss=0.4031553063300367\t\t0.707984\n",
      "Gradient Descent(908/999): loss=0.403151509925588\t\t0.707976\n",
      "Gradient Descent(909/999): loss=0.40314772666330123\t\t0.707964\n",
      "Gradient Descent(910/999): loss=0.4031439564958562\t\t0.707964\n",
      "Gradient Descent(911/999): loss=0.4031401993761103\t\t0.707972\n",
      "Gradient Descent(912/999): loss=0.4031364552571001\t\t0.707976\n",
      "Gradient Descent(913/999): loss=0.40313272409203893\t\t0.707988\n",
      "Gradient Descent(914/999): loss=0.40312900583431654\t\t0.707968\n",
      "Gradient Descent(915/999): loss=0.4031253004374995\t\t0.707968\n",
      "Gradient Descent(916/999): loss=0.4031216078553283\t\t0.70798\n",
      "Gradient Descent(917/999): loss=0.40311792804171903\t\t0.707988\n",
      "Gradient Descent(918/999): loss=0.4031142609507608\t\t0.708008\n",
      "Gradient Descent(919/999): loss=0.4031106065367165\t\t0.70802\n",
      "Gradient Descent(920/999): loss=0.40310696475402064\t\t0.707996\n",
      "Gradient Descent(921/999): loss=0.40310333555728006\t\t0.707992\n",
      "Gradient Descent(922/999): loss=0.40309971890127244\t\t0.707996\n",
      "Gradient Descent(923/999): loss=0.40309611474094564\t\t0.707996\n",
      "Gradient Descent(924/999): loss=0.4030925230314172\t\t0.708004\n",
      "Gradient Descent(925/999): loss=0.4030889437279739\t\t0.708004\n",
      "Gradient Descent(926/999): loss=0.40308537678607037\t\t0.70798\n",
      "Gradient Descent(927/999): loss=0.40308182216132915\t\t0.707972\n",
      "Gradient Descent(928/999): loss=0.40307827980953986\t\t0.707996\n",
      "Gradient Descent(929/999): loss=0.40307474968665774\t\t0.707964\n",
      "Gradient Descent(930/999): loss=0.4030712317488045\t\t0.707952\n",
      "Gradient Descent(931/999): loss=0.40306772595226614\t\t0.707952\n",
      "Gradient Descent(932/999): loss=0.40306423225349347\t\t0.707936\n",
      "Gradient Descent(933/999): loss=0.40306075060910046\t\t0.707912\n",
      "Gradient Descent(934/999): loss=0.40305728097586413\t\t0.707948\n",
      "Gradient Descent(935/999): loss=0.4030538233107239\t\t0.707932\n",
      "Gradient Descent(936/999): loss=0.40305037757078144\t\t0.707944\n",
      "Gradient Descent(937/999): loss=0.4030469437132986\t\t0.707936\n",
      "Gradient Descent(938/999): loss=0.4030435216956977\t\t0.707932\n",
      "Gradient Descent(939/999): loss=0.40304011147556135\t\t0.707932\n",
      "Gradient Descent(940/999): loss=0.403036713010631\t\t0.707956\n",
      "Gradient Descent(941/999): loss=0.40303332625880633\t\t0.70796\n",
      "Gradient Descent(942/999): loss=0.40302995117814555\t\t0.707944\n",
      "Gradient Descent(943/999): loss=0.4030265877268632\t\t0.707936\n",
      "Gradient Descent(944/999): loss=0.40302323586333116\t\t0.707948\n",
      "Gradient Descent(945/999): loss=0.40301989554607676\t\t0.707952\n",
      "Gradient Descent(946/999): loss=0.4030165667337831\t\t0.707948\n",
      "Gradient Descent(947/999): loss=0.4030132493852879\t\t0.707944\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(948/999): loss=0.40300994345958263\t\t0.70796\n",
      "Gradient Descent(949/999): loss=0.403006648915813\t\t0.707928\n",
      "Gradient Descent(950/999): loss=0.40300336571327694\t\t0.707928\n",
      "Gradient Descent(951/999): loss=0.40300009381142504\t\t0.707928\n",
      "Gradient Descent(952/999): loss=0.4029968331698593\t\t0.707928\n",
      "Gradient Descent(953/999): loss=0.4029935837483332\t\t0.707916\n",
      "Gradient Descent(954/999): loss=0.40299034550675034\t\t0.707912\n",
      "Gradient Descent(955/999): loss=0.4029871184051645\t\t0.7079\n",
      "Gradient Descent(956/999): loss=0.4029839024037786\t\t0.707912\n",
      "Gradient Descent(957/999): loss=0.40298069746294424\t\t0.707876\n",
      "Gradient Descent(958/999): loss=0.4029775035431613\t\t0.707876\n",
      "Gradient Descent(959/999): loss=0.40297432060507704\t\t0.707872\n",
      "Gradient Descent(960/999): loss=0.4029711486094857\t\t0.707868\n",
      "Gradient Descent(961/999): loss=0.4029679875173279\t\t0.707864\n",
      "Gradient Descent(962/999): loss=0.40296483728969\t\t0.707884\n",
      "Gradient Descent(963/999): loss=0.40296169788780395\t\t0.707892\n",
      "Gradient Descent(964/999): loss=0.4029585692730459\t\t0.707884\n",
      "Gradient Descent(965/999): loss=0.4029554514069359\t\t0.707904\n",
      "Gradient Descent(966/999): loss=0.402952344251138\t\t0.707924\n",
      "Gradient Descent(967/999): loss=0.4029492477674593\t\t0.707932\n",
      "Gradient Descent(968/999): loss=0.4029461619178485\t\t0.70794\n",
      "Gradient Descent(969/999): loss=0.40294308666439704\t\t0.707952\n",
      "Gradient Descent(970/999): loss=0.40294002196933654\t\t0.707968\n",
      "Gradient Descent(971/999): loss=0.4029369677950404\t\t0.707992\n",
      "Gradient Descent(972/999): loss=0.40293392410402157\t\t0.707988\n",
      "Gradient Descent(973/999): loss=0.4029308908589328\t\t0.707984\n",
      "Gradient Descent(974/999): loss=0.4029278680225654\t\t0.70798\n",
      "Gradient Descent(975/999): loss=0.40292485555784996\t\t0.707972\n",
      "Gradient Descent(976/999): loss=0.40292185342785447\t\t0.707972\n",
      "Gradient Descent(977/999): loss=0.40291886159578444\t\t0.707956\n",
      "Gradient Descent(978/999): loss=0.4029158800249823\t\t0.707944\n",
      "Gradient Descent(979/999): loss=0.4029129086789268\t\t0.707936\n",
      "Gradient Descent(980/999): loss=0.4029099475212323\t\t0.707936\n",
      "Gradient Descent(981/999): loss=0.4029069965156487\t\t0.707932\n",
      "Gradient Descent(982/999): loss=0.4029040556260607\t\t0.70792\n",
      "Gradient Descent(983/999): loss=0.40290112481648677\t\t0.70792\n",
      "Gradient Descent(984/999): loss=0.4028982040510793\t\t0.707916\n",
      "Gradient Descent(985/999): loss=0.4028952932941239\t\t0.707924\n",
      "Gradient Descent(986/999): loss=0.40289239251003856\t\t0.707932\n",
      "Gradient Descent(987/999): loss=0.4028895016633738\t\t0.707936\n",
      "Gradient Descent(988/999): loss=0.4028866207188111\t\t0.707964\n",
      "Gradient Descent(989/999): loss=0.4028837496411635\t\t0.707956\n",
      "Gradient Descent(990/999): loss=0.4028808883953749\t\t0.707944\n",
      "Gradient Descent(991/999): loss=0.40287803694651825\t\t0.707948\n",
      "Gradient Descent(992/999): loss=0.40287519525979687\t\t0.70794\n",
      "Gradient Descent(993/999): loss=0.4028723633005427\t\t0.70796\n",
      "Gradient Descent(994/999): loss=0.4028695410342165\t\t0.707964\n",
      "Gradient Descent(995/999): loss=0.40286672842640686\t\t0.707952\n",
      "Gradient Descent(996/999): loss=0.40286392544283006\t\t0.707944\n",
      "Gradient Descent(997/999): loss=0.4028611320493291\t\t0.707928\n",
      "Gradient Descent(998/999): loss=0.40285834821187383\t\t0.707928\n",
      "Gradient Descent(999/999): loss=0.40285557389655974\t\t0.707916\n"
     ]
    }
   ],
   "source": [
    "w_init = np.random.rand(x.shape[1])\n",
    "w, loss = least_squares_GD(y, x, w_init, max_iters=1000, gamma=0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w, loss = least_squares_SGD(y, x, w_init, 100, gamma=0.03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_accuracy(x, y, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test, x_test, ids_test = load_csv_data(data_path=\"datas/test.csv\", sub_sample=False)\n",
    "pred_y = predict_labels(w, x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done !\n"
     ]
    }
   ],
   "source": [
    "create_csv_submission(ids_test, pred_y, \"datas/submission.csv\")\n",
    "print('Done !')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# https://stackoverflow.com/a/7941594/4810319\n",
    "def main():\n",
    "    np.random.seed(1977)\n",
    "    numvars, numdata = 5, 100\n",
    "    data = 10 * np.random.random((numvars, numdata))\n",
    "    data = x[0:300, 0:7].T\n",
    "    print(x[0:200, 7])\n",
    "    fig = scatterplot_matrix(data, ['DER_mass_MMC', 'DER_mass_transverse_met_lep', 'DER_mass_vis', 'DER_pt_h', 'DER_deltaeta_jet_jet', 'DER_mass_jet_jet', 'DER_prodeta_jet_jet'],\n",
    "            linestyle='none', marker='o', color='black', mfc='none')\n",
    "    fig.suptitle('Simple Scatterplot Matrix')\n",
    "    plt.show()\n",
    "\n",
    "def scatterplot_matrix(data, names, **kwargs):\n",
    "    \"\"\"Plots a scatterplot matrix of subplots.  Each row of \"data\" is plotted\n",
    "    against other rows, resulting in a nrows by nrows grid of subplots with the\n",
    "    diagonal subplots labeled with \"names\".  Additional keyword arguments are\n",
    "    passed on to matplotlib's \"plot\" command. Returns the matplotlib figure\n",
    "    object containg the subplot grid.\"\"\"\n",
    "    numvars, numdata = data.shape\n",
    "    fig, axes = plt.subplots(nrows=numvars, ncols=numvars, figsize=(8,8))\n",
    "    fig.subplots_adjust(hspace=0.05, wspace=0.05)\n",
    "\n",
    "    for ax in axes.flat:\n",
    "        # Hide all ticks and labels\n",
    "        ax.xaxis.set_visible(False)\n",
    "        ax.yaxis.set_visible(False)\n",
    "\n",
    "        # Set up ticks only on one side for the \"edge\" subplots...\n",
    "        if ax.is_first_col():\n",
    "            ax.yaxis.set_ticks_position('left')\n",
    "        if ax.is_last_col():\n",
    "            ax.yaxis.set_ticks_position('right')\n",
    "        if ax.is_first_row():\n",
    "            ax.xaxis.set_ticks_position('top')\n",
    "        if ax.is_last_row():\n",
    "            ax.xaxis.set_ticks_position('bottom')\n",
    "\n",
    "    # Plot the data.\n",
    "    for i, j in zip(*np.triu_indices_from(axes, k=1)):\n",
    "        for x, y in [(i,j), (j,i)]:\n",
    "            axes[x,y].plot(data[x], data[y], **kwargs)\n",
    "\n",
    "    # Label the diagonal subplots...\n",
    "    for i, label in enumerate(names):\n",
    "        axes[i,i].annotate(label, (0.5, 0.5), xycoords='axes fraction',\n",
    "                ha='center', va='center')\n",
    "\n",
    "    # Turn on the proper x or y axes ticks.\n",
    "    for i, j in zip(range(numvars), itertools.cycle((-1, 0))):\n",
    "        axes[j,i].xaxis.set_visible(True)\n",
    "        axes[i,j].yaxis.set_visible(True)\n",
    "\n",
    "    return fig\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
